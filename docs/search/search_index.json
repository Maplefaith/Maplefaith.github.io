{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Keep in low-key wind","text":"<p>Here you can catch a glimpse of notes of courses at ZJU from a student majoring in CSE.</p> <p>Also, I share some of my experience in Blog.</p> <p>Wish you a good reading time.</p> <ul> <li> <p> \u88c5\u4fee\u65f6\u6bb5</p> <ul> <li> <p>Basic Topology</p> </li> <li> <p>Differential Geometry</p> </li> <li> <p>Functional Analysis</p> </li> </ul> </li> <li> <p> \u5fc3\u8a00\u5fc3\u8bed</p> <ul> <li> <p>May Day Holiday</p> </li> <li> <p>Mount Lu</p> </li> <li> <p>Xi'an</p> </li> </ul> </li> <li> <p> \u7a7a\u8bf4\u65e0\u51ed</p> <ul> <li> <p>Complex Analysis</p> </li> <li> <p>Numerical Analysis</p> </li> <li> <p>Real Analysis</p> </li> </ul> </li> <li> <p> \u5c0f\u9a6c\u91cc\u5965</p> <ul> <li> <p>Manipulators Modelling &amp; Control</p> </li> <li> <p>Conan Visualization</p> </li> <li> <p>Process Control</p> </li> </ul> </li> <li> <p> \u5149\u8f89\u5c81\u6708</p> <ul> <li>Linear Algebra </li> <li> <p>Ordinary Differential Equation</p> </li> <li> <p>Stochastic Processes</p> </li> </ul> </li> </ul>"},{"location":"friends/","title":"Welcome to Join my Friend Zone!","text":"Maythics Little Mouse SyncrnzdClk Medium Cat Little_Whale Great Whale LastingWind Giant in Stu'Union"},{"location":"Ctrl/","title":"Control","text":"<p>This is a part for control theory &amp; technology, made up of courses set up in College of Control Science and Engineering, Zhejiang University.</p>"},{"location":"Ctrl/#manipulator-modelling-control","title":"Manipulator Modelling &amp; Control","text":"<p>to be continued...</p>"},{"location":"Ctrl/#modern-control-theory","title":"Modern Control Theory","text":"<p>completed.</p>"},{"location":"Ctrl/#process-control","title":"Process Control","text":"<p>completed.</p>"},{"location":"Ctrl/#sensing-detection","title":"Sensing &amp; Detection","text":"<p>Maybe will only present the midterm exam.</p>"},{"location":"Ctrl/Comp_Ctrl_Sys/","title":"Computer control system","text":""},{"location":"Ctrl/Comp_Ctrl_Sys/#_1","title":"\u8ba1\u7b97\u673a\u63a7\u5236\u7cfb\u7edf\u7684\u7ec4\u6210\u8ba4\u77e5\u8ba8\u8bba","text":"<p>\u90e8\u4ef6/\u8bbe\u5907\uff1a</p> <p>\u8ba1\u7b97\u673a\u63a7\u5236\u7cfb\u7edf\u3001\u6d4b\u91cf\u53d8\u9001\u88c5\u7f6e\u3001\u6267\u884c\u5668\u3001\u88ab\u63a7\u5bf9\u8c61\u3002</p> <p>\u53ef\u4ee5\u5177\u4f53\u5206\u4e3a\u8f6f\u786c\u4ef6\u4e24\u90e8\u5206\u3002</p>"},{"location":"Ctrl/Comp_Ctrl_Sys/#_2","title":"\u8ba1\u7b97\u673a\u63a7\u5236\u7cfb\u7edf\u7684\u7ec4\u7f51\u8ba4\u77e5\u8ba8\u8bba","text":"<p>\u90e8\u4ef6\u5982\u4f55\u76f8\u8fde\uff1f</p>"},{"location":"Ctrl/Comp_Ctrl_Sys/#_3","title":"\u68c0\u6d4b\u4eea\u8868\u7684\u9009\u578b\u5173\u6ce8\u8981\u70b9","text":"<ul> <li>\u53c2\u6570\uff1a</li> </ul>"},{"location":"Ctrl/Manipulators_Modelling/","title":"Manipulators Modelling &amp; Control","text":""},{"location":"Ctrl/Manipulators_Modelling/#spatial-description-transformation","title":"Spatial Description &amp; Transformation","text":""},{"location":"Ctrl/Manipulators_Modelling/#manipulator-kinematics","title":"Manipulator Kinematics","text":""},{"location":"Ctrl/Manipulators_Modelling/#jacobians-velocities-static-forces","title":"Jacobians: Velocities &amp; Static Forces","text":""},{"location":"Ctrl/Manipulators_Modelling/#dynamics","title":"Dynamics","text":""},{"location":"Ctrl/Manipulators_Modelling/#motion-control","title":"Motion Control","text":""},{"location":"Ctrl/Manipulators_Modelling/#force-control","title":"Force Control","text":""},{"location":"Ctrl/Manipulators_Modelling/Dynamics/","title":"Dynamics","text":""},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#acceleration-of-a-rigid-body","title":"Acceleration of a rigid body","text":"<p>Linear Velocity</p> <p>(i) Since from equation in general case</p> \\[ \\begin{align} \\frac{d}{dt}({^A_BR}{^BQ})={^AV_Q}={^A_BR}{^BV_Q}+{^A\\Omega_B}\\times {^A_BR}{^BQ}+{^AV_B}.\\label{linear-vel} \\end{align} \\] <p>take derivative on both sides of the second \"\\(=\\)\" we have</p> \\[ ^A\\dot{V}_Q=\\frac{d}{dt}({^A_BR}{^BV_Q})+{^A\\dot{\\Omega}_B}\\times ({^A_BR {^BQ}})+{^A\\Omega_B}\\times \\frac{d}{dt}({^A_BR}{^BQ})+{^A\\dot{V}_B}, \\] <p>apply first \"\\(=\\)\" of equation \\(\\ref{linear-vel}\\) to the brackets, we have</p> \\[ \\begin{align} \\nonumber{^A\\dot{V}_Q}=&amp;{^A_BR}{^B\\dot{V}_Q}+{^A\\Omega_B}\\times{^A_BR}{^BV_Q}+{^A\\dot{\\Omega}_B}\\times {^A_BR {^BQ}}\\\\ \\nonumber&amp;+{^A\\Omega_B}\\times({^A_BR}{^BV_Q}+{^A\\Omega_B}\\times {^A_BR}{^BQ})+{^A\\dot{V}_B}\\\\ \\nonumber=&amp;{^A_BR}{^B\\dot{V}_Q}+2{^A\\Omega_B}\\times{^A_BR}{^BV_Q}+{^A\\dot{\\Omega}_B}\\times {^A_BR {^BQ}}\\\\ &amp;+{^A\\Omega_B}\\times{^A\\Omega_B}\\times {^A_BR}{^BQ}+{^A\\dot{V}_B}.\\label{linear-vel-general} \\end{align} \\] <p>For revolute joint, we have \\(^BV_Q={^B\\dot{V}_Q}=0\\), so the above equation could be simplified as</p> \\[ \\begin{align} {^A\\dot{V}_Q}={^A\\dot{\\Omega}_B}\\times {^A_BR {^BQ}}+{^A\\Omega_B}\\times{^A\\Omega_B}\\times {^A_BR}{^BQ}+{^A\\dot{V}_B}.\\label{linear-vel-revolute} \\end{align} \\] <p>Angular Velocity</p> <p>Consider the case in which \\(\\{B\\}\\) is rotating relative to \\(\\{A\\}\\) with \\(^AQ_B\\) and \\(\\{C\\}\\) is rotating relative to \\(\\{B\\}\\) with \\(^B\\Omega_C\\). Then the angular velocity of \\(\\{C\\}\\) in frame \\(\\{A\\}\\) is </p> \\[ ^A\\Omega_C={^A\\Omega_B}+{^AR_B}{^B\\Omega_C}, \\] <p>take derivative on both sides we have</p> \\[ \\begin{align} \\nonumber {^A\\dot{\\Omega}_C}=&amp;{^A\\dot{\\Omega}_B}+\\frac{d}{dt}({^A_BR}{^B\\Omega_C})\\\\ =&amp;{^A\\dot{\\Omega}_B}+{^A\\Omega_B}\\times{^A_BR}{^B\\Omega_C}+{^A_BR}{^B\\dot{\\Omega}_C}.\\label{angular-vel} \\end{align} \\]"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#supplementary-info-fixed-point-motion-of-a-rigid-body","title":"supplementary info: Fixed point motion of a rigid body","text":"<p>Reference</p> <ul> <li> <p>\u7406\u8bba\u529b\u5b66, \u9a6c\u5c14\u5951\u592b</p> </li> <li> <p>\u7406\u8bba\u7269\u7406\u5b66\u6559\u7a0b, \u6717\u9053</p> </li> </ul> <p>Here we give a deduction of motion of a rigid body in terms of physics.</p> <p>To easily get the result, we view the rigid body as a set of discrete points. As for a continuous body, we change the mass \\(m_i\\) of a point \\(i\\) into \\(dV\\) with unit mass \\(\\rho dV\\) and then integrate over it.</p> <p>We have two basic frames, the fixed one and the moving one. The former one fr1: \\(\\{X,Y,Z\\}\\) is viewed as a global frame in our discussion and the latter one fr2: \\(\\{x_1,x_2,x_3\\}\\) is moving with the rigid, whose origin is usually set in the center of mass. The origin vector of fr2 is expressed in fr1 as \\(\\pmb{R}\\) and the orientation of fr2 expressed in fr1 is expressed as a rotation matrix, which we would later discuss.</p> <p> </p> <p>Then the motion can be decomposed into two parts, the first is the translation of center of mass, and the second is the rotation along the center of mass.</p> <p>Assume any point \\(P\\) on the rigid body, we have its position \\(\\pmb{r}\\) expressed in fr2 and \\(\\pmb{\\tau}\\) expressed in fr1. Assume is small time, it has a rotation \\(d\\pmb{\\phi}\\times \\pmb{r}\\), then </p> \\[ d\\pmb{\\tau} = d\\pmb{R} + d\\pmb{\\phi}\\times \\pmb{r} \\] <p>divide both sides with \\(dt\\) we have</p> \\[ \\frac{d\\pmb{\\tau}}{dt}=\\frac{d\\pmb{R}}{dt}+ \\frac{d\\pmb{\\phi}}{dt}\\times \\pmb{r} \\] <p>introduce velocity </p> \\[ \\pmb{v}=\\frac{d\\pmb{\\tau}}{dt}, \\quad \\pmb{V}=\\frac{d\\pmb{R}}{dt},\\quad \\pmb{\\Omega}=\\frac{d\\pmb{\\phi}}{dt} \\] <p>and we have</p> \\[ \\begin{equation} \\pmb{v}=\\pmb{V}+\\pmb{\\Omega}\\times \\pmb{r}.\\label{vel} \\end{equation} \\] <p>interpretation 1</p> <p>Here \\(\\pmb{V}\\) is the velocity of the origin of center of mass, \\(\\pmb{\\Omega}\\) is the rotation velocity of the rigid body.</p> <p>fr2 do not coincide with center of mass</p> <p>Assume center of mass if \\(O\\) and the origin of fr2 is \\(O'\\), with \\(O'O=\\pmb{a}\\). Assume we have velocity of \\(O'\\) is \\(\\pmb{V}'\\), its rotation velocity is \\(\\pmb{\\Omega}'\\). </p> <p>Then the same point \\(P\\) has its position expressed in fr2 is \\(\\pmb{r}'\\). So we have \\(\\pmb{r}=\\pmb{r}'+\\pmb{a}\\). From equation \\(\\ref{vel}\\) we have</p> \\[ \\pmb{v}=\\pmb{V}+\\pmb{\\Omega}\\times \\pmb{r}'+\\pmb{\\Omega}\\times \\pmb{a}. \\] <p>However we the same definition we still should have</p> \\[ \\pmb{v}=\\pmb{V}'+\\pmb{\\Omega}'\\times \\pmb{r}' \\] <p>combining the above two we have</p> \\[ \\begin{align} \\pmb{V}'&amp;=\\pmb{V}+\\pmb{\\Omega}\\times \\pmb{a}\\label{vel-translate-relation}\\\\  \\pmb{\\Omega}'&amp;=\\pmb{\\Omega}.\\label{vel-angular-relation} \\end{align} \\] <p>The above deduction is important, since equation \\(\\ref{vel-angular-relation}\\) tells us the rotation velocity is independent of the frame 2. For all the frames, the rigid nody rotates around the axes which are parallel. Equation \\(\\ref{vel-translate-relation}\\) tells us the following property.</p> <p>Relationship of \\(\\pmb{V}\\) and \\(\\pmb{\\Omega}\\)</p> <p>If origin \\(O\\) is chosen, \\(\\pmb{V}\\) and \\(\\pmb{\\Omega}\\) are perpendicular, then for all \\(O'\\), its \\(\\pmb{V}'\\) and \\(\\pmb{\\Omega}'\\) are also perpendicular, since</p> \\[ \\pmb{V}'\\cdot\\pmb{\\Omega}'=(\\pmb{V}+\\pmb{\\Omega}\\times \\pmb{a})\\cdot \\pmb{\\Omega}=0. \\] <p>Easy to check that, in this situation, we the velocity of all points are located on the same plane. Then by equation \\(\\ref{vel-translate-relation}\\), we could always find another origin \\(O'\\) such that \\(\\pmb{V}'=0\\), which means the rigid body is only making rotation motion.</p>"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#mass-distribution","title":"Mass Distribution","text":"<p>We introduce inertia tensor, which can be thought of as a generalization of the scalar moment of inertia of an object.</p> Introduce with kineticsIntroduce with definition <p>The kinetics of the rigid body expressed in discrete points, is</p> \\[ T = \\sum_{i}\\frac{m_i \\pmb{v}_i^2}{2}=\\sum \\frac{m\\pmb{v}^2}{2} \\] <p>to simplify we assume the summation is over \\(i\\).</p> <p>Substitute equation \\(\\ref{vel}\\) in and we have</p> \\[ \\begin{align*} T &amp;= \\sum\\frac{m}{2}(\\pmb{V}+\\pmb{\\Omega}\\times \\pmb{r})^2\\\\ &amp;=\\sum \\frac{m}{2}\\pmb{V}^2+\\sum m\\pmb{V}\\cdot (\\pmb{\\Omega}\\times \\pmb{r})+\\sum\\frac{m}{2}(\\pmb{\\Omega}\\times \\pmb{r})^2 \\end{align*} \\] <p>the first is easy to be written as \\(\\frac{\\mu\\pmb{V}^2}{2}\\), the translation of rigid boty. The second item could be derived by mixed product \\(a\\cdot(b\\times c)=c\\cdot(a\\times b)\\)</p> \\[ \\sum m\\pmb{V}\\cdot (\\pmb{\\Omega}\\times \\pmb{r})=\\sum m\\pmb{r}\\cdot (\\pmb{V}\\times\\pmb{\\Omega}) \\] <p>for all the point in the rigid body, \\(\\pmb{V}\\) and \\(\\pmb{\\Omega}\\) are of the same, then by definition of center of mass</p> \\[ \\sum m\\pmb{V}\\cdot (\\pmb{\\Omega}\\times \\pmb{r})=(\\pmb{V}\\times\\pmb{\\Omega})\\cdot (\\sum m\\pmb{r})=0. \\] <p>As for the third part, we have</p> \\[ \\sum\\frac{m}{2}(\\pmb{\\Omega}\\times \\pmb{r})^2=\\sum\\frac{m}{2} (\\pmb{\\Omega}^2\\pmb{r}^2-(\\pmb{\\Omega}\\cdot\\pmb{r})^2), \\] <p>which is called the rotational part of kinetics. To simplify the notation, we use \\(x_i\\), \\(\\Omega_k\\) to be the coordinate of \\(\\pmb{r}\\) and \\(\\pmb{\\Omega}\\), respectively. So </p> \\[ \\begin{align*} T_{bp}&amp;=\\frac{1}{2}\\sum m [\\Omega_i^2x_l^2-\\Omega_ix_i\\Omega_kx_k]\\\\ &amp;=\\frac{1}{2}\\sum m [\\Omega_i\\Omega_k\\delta_{ik}x_l^2-\\Omega_ix_i\\Omega_kx_k]\\\\ &amp;=\\frac{1}{2}\\Omega_i\\Omega_k\\sum m (x_l^2\\delta_{ik}-x_ix_k)\\\\ \\end{align*} \\] <p>If we introduce tensor</p> \\[ I_{ik}=\\sum m (x_l^2\\delta_{ik}-x_ix_k) \\] <p>then</p> \\[ T = \\frac{\\mu \\pmb{V}^2}{2} + \\frac{1}{2}I_{ik}\\Omega_i\\Omega_k \\] <p>Define a function called Lagrange function by subtracting \\(T\\) with potential energy \\(U\\)</p> \\[ \\begin{align} L=T-U=\\frac{\\mu \\pmb{V}^2}{2} + \\frac{1}{2}I_{ik}\\Omega_i\\Omega_k-U.\\label{Lagrange-func} \\end{align} \\] <p>We define the inertia od moment of the system with respect to an axis \\(u\\), </p> \\[ J_u=\\sum_{v=1}^N m_v\\rho_v^2, \\] <p>where \\(\\rho_v\\) equals the distance of point \\(P_v\\) of the system. Then we focus on passing on difference axes which pass the same point.</p> <p><p> </p> </p> <p>Assume \\(u\\) pass the origin of the coordinate \\(Oxyz\\), with its cosine along \\(Ox\\), \\(Oy\\), \\(Oz\\) is denoted by \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), respectively. Then denote \\(e_u=(\\alpha, \\beta, \\gamma)\\), we have</p> \\[ \\begin{align} \\nonumber J_u&amp;=\\sum_{v=1}^N m_v \\rho_v^2=\\sum_{v=1}^N m_v [r_v^2-(r_v\\cdot e_u)^2]\\\\ \\nonumber &amp;=\\sum_{v=1}^N m_v[x_v^2+y_v^2+z_v^2- (x_v\\alpha+y_v\\beta+z_v\\gamma)^2]\\\\ \\nonumber &amp;=\\sum_{v=1}^N m_v [(1-\\alpha^2)x_v^2+(1-\\beta^2)y_v^2+(1-\\gamma^2)z_v^2-2\\alpha \\beta x_vy_v -2\\alpha \\gamma x_vz_v-2\\gamma \\beta z_vy_v]\\\\ \\nonumber &amp;=\\sum_{v=1}^N m_v[(\\beta^2+\\gamma^2)x_v^2+(\\alpha^2+\\gamma^2)y_v^2+(\\alpha^2+\\beta^2)z_v^2-2\\alpha \\beta x_vy_v -2\\alpha \\gamma x_vz_v-2\\gamma \\beta z_vy_v]\\\\ \\nonumber &amp;=\\sum_{v=1}^N m_v [(y_v^2+z_v^2)\\alpha^2+(x_v^2+z_v^2)\\beta^2+(z_v^2+y_v^2)\\gamma^2 -2\\alpha \\beta x_vy_v -2\\alpha \\gamma x_vz_v-2\\gamma \\beta z_vy_v]\\\\ &amp;=J_x\\alpha^2+J_y\\beta^2+J_z\\gamma^2-2J_{xy}\\alpha\\beta-2J_{xz}\\alpha\\gamma-2J_{yz}\\beta\\gamma\\label{expand-expresstion} \\end{align} \\] <p>where \\(J_x, J_y, J_z\\) are the same definition as the following definition.</p> <p>Properties of Inertia Tensor</p> <p>(i) \\(I_{ik}=I_{ki}\\).</p> <p>(ii) we have the following tensor matrix</p> \\[ \\pmb{J}=I_{ik}=\\left[\\begin{array}{ccc} \\sum m (y^2+z^2) &amp; -\\sum m xy &amp; -\\sum m xz\\\\ -\\sum m yx &amp; \\sum m (x^2+z^2) &amp; -\\sum m yz\\\\ -\\sum m zx &amp; -\\sum m zy &amp; \\sum m (x^2+y^2) \\end{array}\\right]=\\left[\\begin{array}{ccc} I_{xx} &amp; -I_{xy} &amp; -I_{xz}\\\\ -I_{yx} &amp; I_{yy} &amp; -I_{yz}\\\\ -I_{zz} &amp; -I_{zy} &amp; I_{zz} \\end{array}\\right]. \\] <p>where \\(I_{xx}\\), \\(I_{yy}\\) and \\(I_{zz}\\) are called the mass moments of inertia, or axis moment of inertia. And the other elements, whose integrals over mixed indices are called the mass products of inertia, or centrifugal moment of inertia.</p> <p>(iii) for each element, we could integrate</p> \\[ I_{ik}=\\int \\rho (x_l^2\\delta_{ik}-x_ix_k)dV \\] <p>(iv) Equation \\(\\ref{expand-expresstion}\\) has a geometric meaning. That is, if we cut down a segment on axis \\(u\\), denoted by \\(ON\\), satisfies \\(|ON|=1/\\sqrt{J_u}\\), then \\(N(x,y,z)\\) satisfies</p> \\[ \\alpha=\\sqrt{J_u}x,\\quad \\beta=\\sqrt{J_u}y,\\quad \\gamma=\\sqrt{J_u}z. \\] <p>substitute in the equation \\(\\ref{expand-expresstion}\\), we have</p> \\[ J_x x^2+J_y y^2+J_z z^2-2J_{xy}xy-2J_{xz} xz-2J_{yz} yz=1. \\] <p>which is an ellipsoid. If we choose an appropriate coordinate, we have its expression simplified as</p> \\[ Ax_*^2+By_*^2+Cz_*^2=1. \\] <p>that is, the mass product of moments equal zero. In this case, we call \\(x_*\\), \\(y_*\\), \\(z_*\\) the prinpiple axes, the corresponding coefficients \\(A\\), \\(B\\), \\(C\\) are called principle moments of inertia. </p> <p>Actually, \\(A,B,C\\) are the eigenvalue of the matrix \\(\\pmb{J}\\). If they are distinct, then the principle axis is uniquely determined. </p> <p>Example.  Some typical inertia tensor.</p> <p>(i) For planar system, we have \\(x_1,x_2\\) in the plane, so \\(x_3=0\\) for all particles,</p> \\[ I_1=\\sum m x_2^2,\\quad I_2=\\sum m x_1^2,\\quad I_3=I_1+I_2. \\] <p>(ii) For linear system, we have \\(x_3\\) in the line, so \\(x_1=x_2=0\\), so</p> \\[ I_1=I_2=\\sum m x_3^2, \\quad I_3=0. \\]"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#moment-of-momentum","title":"Moment of Momentum","text":"<p>Multiple cross product</p> <p>Show that </p> \\[ \\pmb{a}\\times (\\pmb{b}\\times \\pmb{c})=(\\pmb{a}\\cdot \\pmb{c})\\pmb{b}-(\\pmb{a}\\cdot\\pmb{b})\\pmb{c}. \\] Proof <p>Notice that \\(e_i\\times (e_j\\times e_i)=e_j\\), so</p> \\[ \\begin{align*} \\pmb{a}\\times (\\pmb{b}\\times \\pmb{c})&amp;=\\sum a_i b_j c_k(e_i\\times(e_j\\times e_k))\\\\ &amp;=\\sum a_i b_j c_i e_j - \\sum a_i b_i c_j e_j\\\\ &amp;=(\\sum a_i c_i)\\sum b_je_j-(\\sum a_i b_i)\\sum c_je_j\\\\ &amp;=(\\pmb{a}\\cdot \\pmb{c})\\pmb{b}-(\\pmb{a}\\cdot\\pmb{b})\\pmb{c} \\end{align*} \\] <p>Still, we have the origin of moving frame coincides with the center of mass. Then by definition of moment of momentum</p> \\[ \\begin{align*} \\pmb{M}&amp;=\\sum m \\pmb{r}\\times \\pmb{v}\\\\ &amp;=\\sum m \\pmb{r}\\times (\\pmb{\\Omega}\\times \\pmb{r})\\\\ &amp;=\\sum m (\\pmb{r}\\cdot\\pmb{r})\\pmb{\\Omega}-(\\pmb{r}\\cdot\\pmb{\\Omega})\\pmb{r}. \\end{align*} \\] Using description of tensorUsing description of expression \\[ \\begin{align} \\nonumber M_i&amp;=\\sum m (x_l^2\\Omega_i-x_ix_k\\Omega_k)\\\\ \\nonumber&amp;=\\Omega_k \\sum m (x_l^2\\delta_{ik}-x_ix_k)\\\\ &amp;=\\Omega_kI_{ik}.\\label{moment-tensor} \\end{align} \\] <p>here we have to sum over \\(k\\).</p> <p>If \\(\\pmb{r}=(x_1,x_2,x_3)^T\\), \\(\\pmb{\\Omega}=(\\Omega_1, \\Omega_2, \\Omega_3)^T\\), then </p> \\[ \\begin{align*} M_i&amp;=\\sum_v m_v (\\sum_j x_j^2)\\Omega_i - \\sum_v m_v (\\sum_j x_j\\Omega_j)x_i\\\\ &amp;=\\sum_v m_v (\\sum_{j\\neq i} x_j^2)\\Omega_i- \\sum m_v [\\sum_{j\\neq i} x_ix_j \\Omega_j]\\\\ &amp;=J_i \\Omega_i - \\sum_{j\\neq i}J_{ij}\\Omega_j. \\end{align*} \\] <p>or in language of matrix</p> \\[ \\pmb{M}=\\pmb{J}\\pmb{\\Omega} \\] <p>If \\(Ox\\), \\(Oy\\), \\(Oz\\) are the principle axes, then \\(\\pmb{J}\\) is diagonal, so \\(M_1=J_x\\Omega_1\\), \\(M_2=J_y\\Omega_2\\), \\(M_3=J_z\\Omega_3\\). Generally speaking, the moment of momentum \\(\\pmb{M}\\) is not parallel to angular velocity \\(\\pmb{\\Omega}\\), unless \\(\\pmb{J}\\) is disgomal, or the coordinate is the principle axes, meaning the three directions are decoupled.</p>"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#dynamic-equations","title":"Dynamic Equations","text":"<p>If we know the center of mass and the inertia of the link, then its mass distribution is completely characterized. To accelerate or decelerate it, we have to apply forces and torque. </p> <p>To get the equations, we take derivative of \\(\\pmb{p}\\) and \\(\\pmb{M}\\) with respect to \\(t\\).</p>"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#translation-ode","title":"Translation ODE","text":"<p>Translation ODE</p> <p>(i) For each particle, we have \\(\\dot{\\pmb{p}}_i=\\pmb{f}_i\\), then sum over \\(i\\), and we have</p> \\[ \\begin{align} \\pmb{F}=\\mu \\dot{V}.\\label{eq1} \\end{align} \\] <p>the force is from the outside of system. </p> <p>(ii) And the force \\(\\pmb{F}\\) could be expressed by the potential energy of the rigid body in the outer field</p> \\[ \\pmb{F}=-\\frac{\\partial U}{\\partial \\pmb{R}}. \\] <p>(iii) so we have equation \\(\\ref{eq1}\\) using Lagrange function</p> \\[ \\frac{d}{dt}\\frac{\\partial L}{\\partial \\pmb{V}}=\\frac{\\partial L}{\\partial \\pmb{R}}. \\] Proof for (i)Proof for (ii)Proof for (iii) <p>Since \\(\\pmb{P}=\\sum m\\pmb{p}=\\mu \\pmb{V}\\), we have</p> \\[ \\begin{align*} \\pmb{F}&amp;=\\sum\\pmb{f}=\\sum \\frac{\\pmb{p}}{dt}=\\mu \\dot{\\pmb{V}}. \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>When the rigid body translates \\(\\delta\\pmb{R}\\), then the position vector \\(\\tau\\) of each particle changes, the change of the corresponding potential</p> \\[ \\begin{align*} \\delta U&amp;=\\sum \\frac{\\partial U}{\\partial \\pmb{\\tau}}\\cdot \\pmb{\\tau}\\\\ &amp;=\\delta \\pmb{R}\\cdot \\sum \\frac{\\partial U}{\\partial \\pmb{\\tau}}\\quad (\\delta\\pmb{R}=d\\pmb{\\tau})\\\\ &amp;=-\\delta \\pmb{R}\\cdot\\sum \\pmb{f}\\\\ &amp;=-\\pmb{F}\\cdot \\delta \\pmb{R}. \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>By equation \\(\\ref{Lagrange-func}\\), we have</p> \\[ \\frac{\\partial L}{\\partial \\pmb{V}}=\\mu \\pmb{V}. \\] <p>and using (ii) we have the result.</p>"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#rotation-ode","title":"Rotation ODE","text":"<p>Rotation ODE</p> <p>Assume the center of mass does not move. Then </p> <p>(i)</p> \\[ \\frac{d\\pmb{M}}{dt}=\\pmb{K}=\\sum \\pmb{r}\\times \\pmb{f}. \\] <p>where \\(\\pmb{K}\\) is called the torque. </p> <p>(ii) Using Lagrange function</p> \\[ \\frac{d}{dt}\\frac{\\partial L}{\\partial \\pmb{\\Omega}}=\\frac{\\partial L}{\\partial \\pmb{\\varphi}}. \\] <p>(iii) when the origin of the frame translates, we have \\(\\pmb{r}=\\pmb{r}'+\\pmb{a}\\), so </p> \\[ \\pmb{K}=\\sum \\pmb{r}\\times \\pmb{f}=\\sum \\pmb{r}'\\times \\pmb{f}+\\sum \\pmb{a}\\times \\pmb{f}=\\pmb{K}'+\\pmb{a}\\times \\pmb{F}. \\] Proof for (i)Proof for (ii) <p>Taking derivative of moment of momentum \\(\\pmb{M}\\)</p> \\[ \\dot{\\pmb{M}}=\\frac{d}{dt} \\sum \\pmb{r}\\times \\pmb{p}=\\sum \\dot{\\pmb{r}}\\times \\pmb{p} + \\sum \\pmb{r}\\times \\dot{\\pmb{p}}. \\] <p>Because \\(\\pmb{V}=0\\), we have \\(\\dot{\\pmb{r}}=\\pmb{v}=\\dot{\\pmb{\\tau}}\\). Since \\(\\pmb{v}\\) and \\(\\pmb{p}\\) share the same direction, we have \\(\\dot{\\pmb{r}}\\times \\pmb{p}=0.\\) Replacing \\(\\dot{\\pmb{p}}\\) by \\(\\pmb{f}\\), we have</p> \\[ \\frac{d\\pmb{M}}{dt}=\\pmb{K}=\\sum \\pmb{r}\\times \\pmb{f}. \\] <p><p>\\(\\square\\)</p></p> <p>Take derivative of Lagrange function, or with respect to the coordinate of \\(\\pmb{\\Omega}\\)</p> \\[ \\frac{\\partial L}{\\partial \\Omega_i}=I_{ik}\\Omega_k=M_i. \\] <p>the same as equation \\(\\ref{moment-tensor}\\).</p> <p>Then when the rigid body rotates about a small angle \\(\\delta \\varphi\\), the change of potential </p> \\[ \\begin{align*} \\delta U&amp;=-\\sum \\pmb{f}\\cdot \\delta\\pmb{\\tau}\\\\ &amp;=-\\sum \\pmb{f}\\cdot (\\delta\\pmb{\\varphi}\\times\\pmb{r})\\\\ &amp;=-\\delta\\pmb{\\varphi}\\sum \\pmb{r}\\times \\pmb{f}\\quad \\delta\\pmb{\\varphi}\\text{ are the same}\\\\ &amp;=-\\pmb{K}\\cdot \\delta\\pmb{\\varphi}. \\end{align*} \\] <p>and we are done.</p> <p><p>\\(\\square\\)</p></p> <p>When \\(\\pmb{F}\\) and \\(\\pmb{K}\\) are perpendicular, then we could find a vector \\(\\pmb{a}\\) such that</p> \\[ \\pmb{K}=\\pmb{a}\\times \\pmb{F}, \\quad \\pmb{K}'=0. \\] <p>The \\(\\pmb{a}+\\pmb{F}\\) is also an acceptable solution. </p> <p>Example. Uniform force field. The force acting on particle is \\(\\pmb{f}=e\\pmb{E}\\), then </p> \\[ \\pmb{F}=\\sum \\pmb{f}=\\pmb{E}\\sum e,\\quad \\pmb{K}=\\sum e\\pmb{r}\\times \\pmb{E}. \\] <p>assume \\(\\sum e\\neq 0\\), then let the center of 'mass' to be</p> \\[ \\pmb{r}_0=\\frac{\\sum e\\pmb{r}}{\\sum e} \\] <p>so </p> \\[ \\pmb{K}=\\pmb{r}_0\\times \\pmb{F}. \\] <p>The above deduction is based on a fixed frame (either not translation or rotation). </p>"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#newton-eulers-equation","title":"Newton-Euler's Equation","text":"<p>Newton-Euler's Equation</p> <p>For a fixed point motion of a rigid body, we have translation part ODE and rotation part ODE</p> \\[ \\mu\\left[\\frac{d\\pmb{V}}{dt}+\\pmb{\\Omega}\\times \\pmb{V}\\right]=\\pmb{F}, \\quad \\pmb{J}\\frac{d\\pmb{\\Omega}}{dt}+\\pmb{\\Omega}\\times \\pmb{J}\\pmb{\\Omega}=\\pmb{K}. \\] Proof <p>Any vector \\(\\pmb{A}\\) has a changing velocity with respect to a frame which is moving. </p> <p>Assume \\(d\\pmb{A}/dt\\) is the changing velocity of \\(\\pmb{A}\\) with respect to a fixed frame. If \\(\\pmb{A}\\) do not change in a rotating frame, then </p> \\[ \\frac{d\\pmb{A}}{dt}=\\pmb{\\Omega}\\times \\pmb{A}. \\] <p>If \\(\\pmb{A}\\) moves in the rotating frame, then we add \\(d'\\pmb{A}/{dt}\\) (we just use the results from fixed frame here)</p> \\[ \\frac{d\\pmb{A}}{dt}=\\frac{d'\\pmb{A}}{dt}+\\pmb{\\Omega}\\times \\pmb{A}. \\] <p>So apply this regulation to \\(\\pmb{P}\\) and \\(\\pmb{M}\\), we have</p> \\[ \\frac{d'\\pmb{P}}{dt}+\\pmb{\\Omega}\\times \\pmb{P}=\\pmb{F}, \\quad \\frac{d'\\pmb{M}}{dt}+\\pmb{\\Omega}\\times \\pmb{M}=\\pmb{K}. \\] <p>Project to the coordinate of the moving frame, we have \\(\\left(\\frac{d'\\pmb{P}}{dt}\\right)=\\frac{dP_1}{dt},\\cdots, \\left(\\frac{d'\\pmb{M}}{dt}\\right)=(\\pmb{J}\\frac{d\\pmb{\\Omega}}{dt})_1\\), so the translation part ODE is</p> \\[ \\begin{align*} \\mu\\left(\\frac{dV_1}{dt}+\\Omega_2V_3-\\Omega_3V_2\\right)&amp;=F_1,\\\\ \\mu\\left(\\frac{dV_2}{dt}+\\Omega_3V_1-\\Omega_1V_3\\right)&amp;=F_2,\\\\ \\mu\\left(\\frac{dV_3}{dt}+\\Omega_1V_2-\\Omega_2V_1\\right)&amp;=F_3.\\\\ \\end{align*} \\] <p>and the rotation part ODE is (a little complicated, only present the first line)</p> \\[ I_x \\frac{d\\Omega_1}{dt}-I_{xy}\\frac{d\\Omega_2}{dt}-I_{xz}\\frac{d\\Omega_3}{dt}+ (I_z-I_y)\\Omega_2\\Omega_3+ I_{yz}(\\Omega_3^2-\\Omega_2^2)+\\Omega_1(I_{xy}\\Omega_3-I_{xz}\\Omega_2)=K_1, \\] <p>if the coordinate \\(Ox,Oy,Oz\\) of moving system are principle axes, then \\(M_i=I_i\\Omega_i (i=1,2,3)\\), and \\(I_{ik}=0 (i\\neq k)\\), so</p> \\[ \\begin{align*} I_1\\frac{d\\Omega_1}{dt}+(I_3-I_2)\\Omega_2\\Omega_3=K_1,\\\\ I_2\\frac{d\\Omega_2}{dt}+(I_1-I_3)\\Omega_3\\Omega_1=K_2,\\\\ I_3\\frac{d\\Omega_3}{dt}+(I_2-I_1)\\Omega_1\\Omega_2=K_3.\\\\ \\end{align*} \\]"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#iterative-dynamic-formulation","title":"Iterative Dynamic Formulation","text":"<p>Starting with link \\(1\\) and moving successively, link by link, outward to link \\(n\\).</p> <p>Deduction: Outward</p> <p>(i) Rotational velocity.</p> \\[ {^{i+1}\\omega_{i+1}}={^{i+1}_i R}{^i\\omega_i}+\\dot{\\theta}_{i+1} {^{i+1}\\hat{Z}_{i+1}}. \\] <p>from equation \\(\\ref{angular-vel}\\), we have</p> \\[ \\begin{align*} \\dot{\\omega}_{i+1}&amp;=\\dot{\\omega}_{i}+{^0_iR} {^i \\dot{\\Omega}_{i+1}}+\\omega_i\\times {^0_iR}{^i\\Omega_{i+1}}\\\\ &amp;=\\dot{\\omega}_{i}+{^0_iR}(\\ddot{\\theta}_{i+1} {^i_{i+1}R}{^{i+1}\\hat{Z}}_{i+1})+\\omega_i\\times {^0_iR}(\\dot{\\theta}_{i+1} {^i_{i+1}R}{^{i+1}\\hat{Z}}_{i+1})\\\\ &amp;=\\dot{\\omega}_{i}+\\ddot{\\theta}_{i+1}{^0_{i+1}R}{^{i+1}\\hat{Z}_{i+1}}+\\omega_i\\times (\\dot{\\theta}_{i+1} {^0_{i+1}R}{^{i+1}\\hat{Z}_{i+1}})\\\\ \\Rightarrow \\quad {^{i+1}\\dot{\\omega}_{i+1}}&amp;={^{i+1}_i R}{^i\\dot{\\omega}_{i}}+\\ddot{\\theta}_{i+1}{^{i+1}\\hat{Z}_{i+1}}+{^{i+1}_i R}{^i\\omega_i}\\times \\dot{\\theta}_{i+1}{^{i+1}\\hat{Z}_{i+1}}. \\end{align*} \\] <p>For prismatic, we have \\(\\dot{\\theta}_{i+1}=\\ddot{\\theta}_{i+1}=0\\), so we only have the first item.</p> <p>(ii) Linear velocity.</p> <p>from equation \\(\\ref{linear-vel-general}\\), we have</p> \\[ \\begin{align*} \\dot{v}_{i+1}&amp;=\\dot{v}_i + {^0_iR} {^i\\dot{V}_{i+1}}+2\\omega_i\\times {^0_iR}{^iV_{i+1}} + \\dot{\\omega}_i\\times {^0_iR}  {^iO_{i+1}}+\\omega_i\\times (\\omega_i \\times {^0_iR}{^iO_{i+1}})\\\\ &amp;={^{i+1}_i R}[{^i\\dot{v}_i}+{^i\\dot{\\omega}_i}\\times {^iO_{i+1}}+{^i\\omega_i}\\times ({^i\\omega_i} \\times {^iO_{i+1}})]+{^{i+1}_i R}[{^i\\dot{V}_{i+1}}+2{^i\\omega_i}\\times {^iV_{i+1}}]. \\end{align*} \\] <p>For prismatic, we have the second item with its corresonding change</p> \\[ \\begin{align*} {^{i+1}_i R}[{^i\\dot{V}_{i+1}}+2{^i\\omega_i}\\times {^iV_{i+1}}]&amp;= \\ddot{d}_{i+1}{^{i+1}\\hat{Z}_{i+1}}+{^{i+1}_i R}{^i\\omega_i}\\times \\dot{d}_{i+1}{^{i+1}\\hat{Z}_{i+1}}\\\\ &amp;=\\ddot{d}_{i+1}{^{i+1}\\hat{Z}_{i+1}}+ {^{i+1}\\omega_{i+1}}\\times \\dot{d}_{i+1}{^{i+1}\\hat{Z}_{i+1}}. \\end{align*} \\] <p>For revolute, \\(^iV_{i+1}={^i\\dot{V}_{i+1}}=0\\), so we only have the first item.</p> <p>(iii) Acceleration of center of mass of link</p> <p>Still from equation \\(\\ref{linear-vel-general}\\), we have the second and third item equal zero</p> \\[ \\begin{align*} \\dot{v}_{C_i}&amp;=\\dot{v}_i + {^0_iR} {^i\\dot{V}_{C_i}}+2\\omega_i\\times {^0_iR}{^iV_{C_i}} + \\dot{\\omega}_i\\times {^0_iR}  {^iO_{C_i}}+\\omega_i\\times (\\omega_i \\times {^0_iR}{^iO_{C_i}})\\\\ &amp;=\\dot{v}_i +\\dot{\\omega}_i\\times {^0_iR}  {^iO_{C_i}}+\\omega_i\\times (\\omega_i \\times {^0_iR}{^iO_{C_i}})\\\\ \\Rightarrow \\quad {^i\\dot{v}_{C_i}}&amp;={^i\\dot{v}_i} +{^i\\dot{\\omega}_i}\\times {^iO_{C_i}}+{^i\\omega_i}\\times ({^i\\omega_i} \\times {^iO_{C_i}}). \\end{align*} \\] <p>(iv) force and torque</p> <p>From Newton-Euler's Equation, we have</p> \\[ \\begin{align*} F_i&amp;=m\\dot{v}_{C_i},\\\\  {^{C_i}N_i}&amp;={^{C_i}I}{^{C_i}\\dot{\\omega}_i}+ {^{C_i}{\\omega}_i}\\times {^{C_i}I}{^{C_i}{\\omega}_i}\\\\ &amp;={^{C_i}I}{^{i}\\dot{\\omega}_i}+ {^{i}{\\omega}_i}\\times {^{C_i}I}{^{i}{\\omega}_i}. \\end{align*} \\] <p>Deduction: Inward</p> <p>With forces and torques acting on links, we have to calculate joint torques that would result in these net forces and torques on the links. Here we denote \\(f_i\\) as force exerted on link \\(i\\) by link \\(i-1\\), \\(n_i\\) as torque exerted on link \\(i\\) by link \\(i-1\\).</p> <p><p> </p></p> <p>Apparently net force on link \\(i\\) is</p> \\[ \\begin{align*} {^iF_i}&amp;={^if_i}-{^i_{i+1}R}{^{i+1}f_{i+1}}.\\\\ \\Rightarrow \\quad {^if_i}&amp;={^iF_i}+{^i_{i+1}R}{^{i+1}f_{i+1}}. \\end{align*} \\] <p>and for torque we substitute the above force relationship in</p> \\[ \\begin{align*} {^iN_i}&amp;={^in_i}-{^in_{i+1}} +(-{^iO_{C_i}})\\times {^if_i}+({^iO_{i+1}}-{^iO_{C_i}})\\times (-{^if_{i+1}})\\\\ \\Rightarrow {^in_i} &amp;={^iN_i}+{^i_{i+1}R}{^{i+1}n_{i+1}}+{^iO_{C_i}}\\times {^if_i}+({^iO_{i+1}}-{^iO_{C_i}})\\times {^if_{i+1}}\\\\ &amp;={^iN_i}+{^i_{i+1}R}{^{i+1}n_{i+1}}+{^iO_{C_i}}\\times {^iF_i}+{^iO_{i+1}}\\times {^i_{i+1}R}{^{i+1}f_{i+1}}. \\end{align*} \\] <p>For revolute, \\(\\tau_i={^in_i^T}{^i\\hat{Z}_i}\\), and for prismatic, \\(\\tau_i={^if_i^T}{^i\\hat{Z}_i}\\).</p>"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#structure-of-a-manipulators-dynamic-equations","title":"Structure of a manipulator's dynamic equations","text":"<p>State-space equation</p> <p>The above Newton-Euler equations apply for any manipulators, and this yields a dynamic equation which could be written in the following form</p> \\[ \\begin{align} \\tau=M(\\Theta)\\ddot{\\Theta}+V(\\Theta,\\dot{\\Theta})+G(\\Theta),\\label{state-space} \\end{align} \\] <p>or for every row with \\(\\Theta=(\\theta_1,\\cdots,\\theta_n)^T\\),</p> \\[ \\tau_i=\\sum_{j=1}^N m_{ij} \\ddot{\\theta_j}+\\sum_{j,k=1}^Nc_{kji}\\dot{\\theta}_{k}\\dot{\\theta}_{j}-\\sum_{j=1}^N m_j {^0\\pmb{g}^T}\\frac{\\partial O_{C_j}}{\\partial \\theta_i} \\] <p>where </p> \\[ c_{kji}=\\frac{1}{2}\\left(\\frac{\\partial m_{ij}}{\\partial \\theta_k} + \\frac{\\partial m_{ik}}{\\partial \\theta_j}-\\frac{\\partial m_{kj}}{\\partial \\theta_i} \\right)=c_{jki} \\] <p>are called the first Christoffel sign, \\(j\\) and \\(k\\) are of equal state. </p> <p>equation \\(\\ref{state-space}\\) are called state-space equation, \\(M(\\Theta)_{N\\times N}\\) is called mass matrix of the manipulator, \\(V(\\Theta,\\dot{\\Theta})_{N\\times 1}\\) is called vector of centrifugal and Coriolis terms, which is where the main idea of the formula come from.</p> <p>Configuration-space equation</p> <p>Rewrite the velocity-dependent term \\(V(\\Theta, \\dot{\\Theta})\\), we have</p> \\[ \\begin{align} \\tau=M(\\Theta)\\ddot{\\Theta}+B(\\Theta)[\\dot{\\Theta}\\dot{\\Theta}]+C(\\Theta)\\dot{\\Theta}^2+G(\\Theta),\\label{configuration-space} \\end{align} \\] <p>where \\(B(\\Theta)_{n\\times n(n-1)/2}\\) is a matrix of Coriolis coefficients, \\([\\dot{\\Theta}\\dot{\\Theta}]_{n(n-1)/2\\times 1}\\) is a vector of joint velocity products given by</p> \\[ [\\dot{\\Theta}\\dot{\\Theta}]=[\\dot{\\theta}_1\\dot{\\theta}_2, \\dot{\\theta}_1\\dot{\\theta}_3,\\cdots,\\dot{\\theta}_{n-1}\\dot{\\theta}_n]^T. \\] <p>\\(C(\\Theta)_{n\\times n}\\) is an matrix of centrifugal coefficients, and \\(\\dot{\\Theta}^2_{n\\times 1}\\) is a vector given by</p> \\[ \\dot{\\Theta}^2=[\\dot{\\theta}_1^2, \\dot{\\theta}_2^2,\\cdots, \\dot{\\theta}_n^2]. \\] <p>Equation \\(\\ref{configuration-space}\\) is called configuration-space equation because of \\(B\\) and \\(C\\).</p>"},{"location":"Ctrl/Manipulators_Modelling/Dynamics/#dynamics-in-cartesian-space","title":"Dynamics in Cartesian Space","text":"<p>From equation \\(\\ref{state-space}\\), which is in joint space, we could get the corresponding result in terms of the acceleration of end-effector expressed in Cartesian space.</p> <p>Dynamics</p> <p>From Force-torque relationship, we know</p> \\[ \\tau=J(\\Theta)^T \\mathcal{F}. \\] <p>and </p> \\[ \\begin{align*} \\dot{\\mathcal{X}}&amp;=J\\dot{\\Theta}.\\\\ \\Rightarrow \\quad \\ddot{\\mathcal{X}}&amp;=\\dot{J}\\dot{\\Theta}+J\\ddot{\\Theta}\\\\ \\Rightarrow \\quad \\ddot{\\Theta}&amp;=J^{-1}\\ddot{\\mathcal{X}}-J^{-1}\\dot{J}\\dot{\\Theta} \\end{align*} \\] <p>So equation \\(\\ref{state-space}\\) becomes</p> \\[ \\begin{align*} J^{-T}\\tau&amp;=J^{-T}[M(\\Theta)\\ddot{\\Theta}+V(\\Theta,\\dot{\\Theta})+G(\\Theta)]\\\\ \\mathcal{F}&amp;=J^{-T}M(\\Theta)[J^{-1}\\ddot{\\mathcal{X}}-J^{-1}\\dot{J}\\dot{\\Theta}]+J^{-T}V(\\Theta,\\dot{\\Theta})+J^{-T}G(\\Theta)\\\\ \\mathcal{F}&amp;=J^{-T}M(\\Theta)J^{-1}\\ddot{\\mathcal{X}}-J^{-T}J^{-1}\\dot{J}\\dot{\\Theta}+J^{-T}V(\\Theta,\\dot{\\Theta})+J^{-T}G(\\Theta)\\\\ \\mathcal{F}&amp;=M_{\\mathcal{X}}(\\Theta)\\ddot{\\mathcal{X}}+V_{\\mathcal{X}}(\\Theta)+G_{\\mathcal{X}}(\\Theta) \\end{align*} \\] <p>where </p> \\[ \\begin{cases} M_{\\mathcal{X}}(\\Theta)=J^{-T}M(\\Theta)J^{-1}\\\\ V_{\\mathcal{X}}(\\Theta)=-J^{-T}J^{-1}\\dot{J}\\dot{\\Theta}+J^{-T}V(\\Theta,\\dot{\\Theta})\\\\ G_{\\mathcal{X}}(\\Theta)=J^{-T}G(\\Theta). \\end{cases} \\]"},{"location":"Ctrl/Manipulators_Modelling/Force_Control/","title":"Force Control","text":""},{"location":"Ctrl/Manipulators_Modelling/Force_Control/#natural-artificial-constraints","title":"Natural &amp; Artificial Constraints","text":"<p>We use \\(\\xi=(v^T, \\omega^T)^T\\) to represent the instantaneous linear and angular velocity of end-effector, \\(F=(f^T, n^T)^T\\) to represent the instantaneous force and torque acting on the end-effector. Easy to show that \\(\\xi\\) and \\(F\\) belongs to linear space which is a subspace of \\(\\mathbb{R^6}\\), and denoted by \\(\\mathcal{M}\\) and \\(\\mathcal{F}\\).</p> <p>We form a compliance frame \\(o_cx_cy_cz_c\\) at the end-effector.</p> <p>Relationship of Two constraints</p> <p>Define Admissible Motion Space \\(V_a\\subset \\mathbb{R}^6\\), to be all the possible motion which satisfies the geometric constraints. Define \\(V_c=V_a^{\\perp}\\).</p> <p>Assume \\(F\\in \\mathcal{F}\\), \\(\\xi\\in \\mathcal{M}\\), then the work done by end-effector is</p> \\[ W=F^T \\xi. \\] <p>Partition \\(F\\) and \\(\\xi\\) into two subspace \\(V_a\\) and \\(V_c\\)</p> \\[ F=F_a+F_c,\\quad \\xi=\\xi_a+\\xi_c, \\] <p>where \\(F_a,\\xi_a\\in V_a\\) and \\(F_c,\\xi_c\\in V_c\\). Substitute in and </p> \\[ W=F_a^T\\xi_a +F_c^T \\xi_c. \\] <p>In \\(V_c\\), we must have \\(\\xi_c=0\\) then the motion is admissible. Then the above equation is reduced to virtual work. However, since the system is kept in static equivalence, we must have the virtual work to be zero for all virtual motion \\(\\xi_a\\). So \\(F_a=0\\).</p> <p>Actually, we have \\(\\mathcal{M}=V_a\\) and \\(\\mathcal{F}=V_c\\).</p> <p>Example. Write down the natural and artificial constraints in the end-effector of the following manipulator.</p> <p> </p> Answer <p>For \\(\\mathcal{M}\\), we have</p> \\[ v_x=0,v_z=0,\\omega_x=0,\\omega_y=0.\\quad v_y=\\alpha_1, \\omega_z=\\alpha_2. \\] <p>For \\(\\mathcal{F}\\), we have</p> \\[ f_y=0, n_z=0.\\quad f_x=f_z=n_x=n_y=\\alpha_3=0. \\]"},{"location":"Ctrl/Manipulators_Modelling/Force_Control/#hybrid-positionforce-control","title":"Hybrid Position/Force Control","text":""},{"location":"Ctrl/Manipulators_Modelling/Force_Control/#contact-with-environment","title":"Contact with environment","text":"<p>In considering forces of contact, we assume our system is rigid and the environment has some stiffness \\(k_e\\) (\u52b2\u5ea6).</p> <p>Consider the control of a mass attached to a spring.</p> <p> </p> <p>The variable we wish to control is the force acting on the environment \\(f_e\\), which is also the force acting in the spring</p> \\[ f_e=k_e x. \\] <p>We also include other forces, like \\(f_{dist}\\) from unknown friction or cogging in the manipulator's gearing. </p> \\[ \\begin{align*} f&amp;=m\\ddot{x}+k_ex+f_{dist}\\\\ &amp;=mk_e^{-1}\\ddot{f_e}+f_e+f_{dist}. \\end{align*} \\] <p>Choose \\(\\alpha=mk_e^{-1}\\) and \\(\\beta=f_e\\), so the control law is</p> \\[ f=\\alpha f+\\beta.  \\] <p>with closed loop system </p> \\[ \\ddot{e}_f+k_{vf}\\dot{e}_f+k_{pf}e_f=0, \\] <p>where \\(e_f=f_d-f_e\\). Notice the above control law do not have knowledge of \\(f_{dist}\\), so we have a steady error by setting all derivatives to zero</p> \\[ e_f=\\frac{f_{dist}}{\\alpha}=\\frac{f_{dist}}{mk_e^{-1}k_{pf}} \\] <p>if we use \\(\\beta=f_D\\) in the control law, we have</p> \\[ e_f=\\frac{f_{dist}}{1+\\alpha} \\] <p>when the environment is stiff, \\(\\alpha\\) could be small, so the above control law effectively reduce the noise.</p> <p>In practice, force trajectory is usually constant, so \\(\\ddot{f}_d=\\dot{f}_d=0\\). Another reality is that sensed force are quite \"noisy\", and differentiation of \\(\\dot{f}_e\\) is ill-advised. We could get \\(\\dot{f}_e=k_e \\dot{x}\\), if we could obtain good measures of velocity.</p>"},{"location":"Ctrl/Manipulators_Modelling/Kinematics/","title":"Manipulator Kinematics","text":"<p>Joints form a connection between a neighboring pair of links.</p> <p>Most manipulators have Lower pairs, with joints exhibiting one degree of freedom. Specifically, we have revolute joints and sliding joints called prismatic joints.</p>"},{"location":"Ctrl/Manipulators_Modelling/Kinematics/#link-parameters","title":"Link parameters","text":"<p>Any manipulator could be described kinematically by giving the value of 4 quantities for each link. Two describe the link itself, while the other two describe the link's connection to a neighboring link. The definition os mechanisms by means of these 4 quantities is a convention usually called the Denavit-Hartenberg notation.</p> <p>Link between joints</p> <p>Here, a link is considered only as a rigid body defining the relationship between two neighboring joint axes ( axis \\(i-1\\) and \\(i\\), namely) of a manipulator. Joint axes are defined by lines in space.</p> <p>We could model kinematics like in the following steps, as a well-known basis of 3-D geometry. That is, there exists a perpandicular vector between any two axes that does not coincide (construct the vector by cross-product). This vector is unique unless both axes are parallel. we could use this vector to measure the link distance between the two axes, denoted by \\(a_{i-1}\\).</p> <p>The second parameter to be considered is called link twist, denoted by \\(\\alpha_{i-1}\\). That is, using the plane whose norm vector is the perpandicular vector, we could project neighboring axes onto it and measure the angle between them.</p> <p>Neighboring links: link-Connection</p> <p>Neighboring links have a common joint axis, which is usually perpendicular to both of them (links). There are also two parameters to consider bwtween them. </p> <p>The first is the distance bwtween two neighboring links, which is called link offset, denoted by \\(d_i\\) (according to axis \\(i\\)). The second is the angle bwtween them, which is called link angle, denoted by \\(\\theta_i\\).</p> <p>For revolute joint, \\(\\theta_i\\) is called joint variable while the other 3 are fixed link variables. But for prismatic joints, \\(d_i\\) is a joint variable while the other 3 are fixed link variables. </p>"},{"location":"Ctrl/Manipulators_Modelling/Kinematics/#frames","title":"Frames","text":"<p>In order to describe the location of each link relative to its neighbors, we have to define a frame attached to each link. Assume frame \\(\\{i\\}\\) is attached regidly to link \\(i\\).</p> <p>Construction of frames</p> <p>The \\(\\hat{Z}_i\\) of frame \\(\\{i\\}\\), is coincident with the joint axis \\(i\\). The origin of frame \\(\\{i\\}\\) is located where link \\(i\\) intersects the joint axis \\(i\\). Let \\(\\hat{X}_i\\) points along link \\(i\\) in a direction from joint axis \\(i\\) to the next joint axis \\(i+1\\). Thus \\(\\hat{Y}_i\\) is determined by right-hand principle. Check the following image.</p> <p><p> </p></p> <p>Four parameters measured in frames</p> <p>Within frames \\(\\{i-1\\}\\) and \\(\\{i\\}\\), we could interpret the above 4 parameters as follows.</p> <p>(i) \\(a_{i-1}\\) equals the distance between \\(\\hat{Z}_{i-1}\\) and \\(\\hat{Z}_{i}\\) measured along \\(\\hat{X}_{i-1}\\).</p> <p>(ii) \\(\\alpha_i\\) equals the angle from \\(\\hat{Z}_{i-1}\\) to \\(\\hat{Z}_i\\) measured about \\(\\hat{X}_{i-1}\\).</p> <p>(iii) \\(d_i\\) equals the distnce between \\(\\hat{X}_{i}\\) and \\(\\hat{X}_{i-1}\\) measured along \\(\\hat{Z}_{i}\\).</p> <p>(iv) \\(\\theta\\) equals the angle from \\(\\hat{X}_{i-1}\\) to \\(\\hat{X}_i\\) measured about \\(\\hat{Z}_{i}\\).</p>"},{"location":"Ctrl/Manipulators_Modelling/Kinematics/#calculations-using-homogeneous-transform-matrix","title":"Calculations using Homogeneous Transform Matrix","text":"<p>Equiped with the above knowledge between two frames, we could use homogeneous transform matrix to describe the relationship.</p> <p>Calculations</p> <p>We could conceive the relationship between frame \\(\\{i-1\\}\\) and \\(\\{i\\}\\) as the following transformation. Here we use Euler angles.</p> <p>(i) move frame \\(\\{i-1\\}\\) along \\(\\hat{X}_{i-1}\\) with a distance of \\(a_{i-1}\\).</p> <p>(ii) rotate frame \\(\\{i-1\\}\\) about \\(\\hat{X}_{i-1}\\) with an angle \\(\\alpha_{i-1}\\).</p> <p>(iii) move frame \\(\\{i-1\\}\\) along \\(\\hat{Z}_{i-1}\\) with a distance of \\(d_{i}\\).</p> <p>(iv) rotate frame \\(\\{i-1\\}\\) about \\(\\hat{Z}_{i-1}\\) with an angle \\(\\theta_{i}\\).</p> <p>After the above four steps, frame \\(\\{i-1\\}\\) becomes \\(\\{i\\}\\).</p> <p>Using postmultiplication, we have</p> \\[ ^{i-1}_iT=\\left[\\begin{array}{ccc:c} 1 &amp; 0&amp; 0 &amp; a_{i-1}\\\\ 0&amp; \\cos \\alpha_{i-1} &amp; -\\sin \\alpha_{i-1} &amp; 0\\\\ 0&amp; \\sin \\alpha_{i-1} &amp; \\cos\\alpha_{i-1} &amp; 0\\\\ \\hdashline 0&amp;0&amp;0&amp;1 \\end{array}\\right]\\cdot \\left[\\begin{array}{ccc:c} \\cos \\theta_i &amp; -\\sin \\theta_i &amp; 0&amp; 0\\\\ \\sin \\theta_i &amp; \\cos \\theta_i &amp; 0 &amp; 0\\\\ 0 &amp; 0&amp; 1 &amp; d_{i}\\\\ \\hdashline 0&amp;0&amp;0&amp;1 \\end{array}\\right], \\] <p>where the first matrix denote the movement (i) (ii) and the second matrix denote the movement (iii) (iv).</p> <p>We define \\(\\{B\\}\\) as basix frame, which is located at the base of manipulator, and \\(\\{S\\}\\) as sttion frame, which is located in a task-relevant location, and \\(\\{W\\}\\) as wrist frame, which is affixed to the last link of the manipulator, and \\(\\{T\\}\\) as tool frame, and \\(\\{G\\}\\) as goal frame. To find tool from station frame, by using chain rule, we have</p> \\[ ^S_TT={^S_BT}\\cdot{^B_WT}\\cdot {^W_TT}={^B_ST}^{-1}\\cdot{^B_WT}\\cdot {^W_TT} \\]"},{"location":"Ctrl/Manipulators_Modelling/Kinematics/#inverse-kinematics","title":"Inverse Kinematics","text":"<p>Given the result matrix \\(^0_NT\\), how to calculate the corresponding submatrices \\({^0_1T}\\), \\({^1_2T}\\), \\(\\cdots\\), \\({^{N-1}_NT}\\)? That is, we are in a logic direction from Cartesian coordinate to joint coordinate.</p> <p>Existence of solutions</p> <p>(i) Workspace.</p> <p>(ii) Dextrous workspace.</p> <p>(iii) Reachable workspace.</p> <p>The solutions could be partitioned in to two broad classes: closed-form solution and numerical solutions.</p>"},{"location":"Ctrl/Manipulators_Modelling/Kinematics/#algebraic-solution","title":"Algebraic solution","text":""},{"location":"Ctrl/Manipulators_Modelling/Motion_Control/","title":"Motion Control","text":"<p>We would partition the controller into two parts, model-based portion and servo portion.</p> <p>The model-based portion makes use of feedback to reduce the system such that it appears like a unit mass.</p> <p>Control-law decomposition</p> <p>Given model </p> \\[ m\\ddot{x}+b\\dot{x}+kx=f, \\] <p>choose \\(f=\\alpha f'+\\beta\\) such that</p> \\[ \\begin{align} \\ddot{x}=f'\\quad\\text{open-loop}\\label{servo-portion} \\end{align} \\] <p>so </p> \\[ \\alpha=m, \\quad \\beta=b\\dot{x}+kx. \\] <p><p> </p></p> <p>we design a control law to compute \\(f'\\), so </p> \\[ f'=-k_v \\dot{x}-k_px \\] <p>the system yields</p> \\[ \\ddot{x}+k_v\\dot{x}+k_px=0. \\]"},{"location":"Ctrl/Manipulators_Modelling/Motion_Control/#trajectory-following-control","title":"Trajectory-following control","text":"<p>Given a planned trajectory \\(x_d(t)\\) is smooth, which means our trajectory generator could give access to \\(x_d,\\dot{x}_d,\\ddot{x}_d\\) all the time.</p> <p>Trajectory following</p> <p>Define error \\(e=x_d-x\\), then a servo-control law that cause trajectory following</p> \\[ f'=\\ddot{x}_d+k_v\\dot{e}+k_pe. \\] <p>combined with equation \\(\\ref{servo-portion}\\), we have</p> \\[ \\begin{align} \\nonumber \\ddot{x}_d+k_v\\dot{e}+k_pe&amp;=\\ddot{x}\\\\ \\Rightarrow \\quad \\ddot{e}+k_v\\dot{e}+k_pe&amp;=0.\\label{error-space} \\end{align} \\] <p>which is also called error space.</p>"},{"location":"Ctrl/Manipulators_Modelling/Motion_Control/#disturbance-rejection","title":"Disturbance Rejection","text":"<p>Consider a noise \\(f_{dist}\\), then equation \\(\\ref{error-space}\\) becomes</p> \\[ \\ddot{e}+k_v\\dot{e}+k_p e=f_{dist}. \\] <p>If \\(\\max_t f_{dist}(t)&lt;a\\), then the consequent error \\(e\\) is a also bounded.</p>"},{"location":"Ctrl/Manipulators_Modelling/Motion_Control/#modelling-control-of-a-single-joint","title":"Modelling &amp; Control of a single joint","text":"<p>We first develop a simplified model of a single rotary joint.</p> <p>Modelling</p> <p>For direct current (DC) motor, we have</p> \\[ \\tau_m=k_mi_a,\\quad V=k_e \\dot{\\theta}_m. \\] <p>where the first one is a driven torque of the motor, and the second one is the generated voltage of rotation.</p> <p><p> </p></p> <p>In the circuit of the armature, we have</p> \\[ l_a \\dot{i}_a+r_ai_a=V_a-V=V_a-k_e\\dot{\\theta}_m. \\] <p>The gear ratio \\(\\eta\\) causes an increase in the torque seen at the load and a reduction in the speed of the load, given by</p> \\[ \\begin{align} \\tau=\\eta\\tau_m,\\quad \\dot{\\theta}=\\frac{\\dot{\\theta}}{\\eta}.\\label{relation-joint-rotor} \\end{align} \\] <ul> <li>Write a dynamic equaiton of the rotor</li> </ul> \\[ \\tau_m= I_m \\ddot{\\theta}_m+b_m \\dot{\\theta}_m+(1/\\eta)(I\\ddot{\\theta}+b\\dot{\\theta}). \\] <p>where \\(I_m\\), \\(I\\) are the inertias of the motor rotor and of the load, respectively, and \\(b_m\\), \\(b\\) are viscous friction coefficients for the rotor and load bearings, respectively.</p> <p>Using equation \\(\\ref{relation-joint-rotor}\\), we write the above equation in terms of motor variables \\(\\theta_m\\) or joint/load variables \\(\\theta\\)</p> \\[ \\begin{align*} \\tau_m&amp;=\\left(I_m+\\frac{I}{\\eta^2}\\right)\\ddot{\\theta}_m+\\left(b_m+\\frac{b}{\\eta^2}\\right)\\dot{\\theta}_m\\\\ \\tau&amp;=\\left(I+I_m \\eta^2\\right)\\ddot{\\theta}+\\left(b+b_m\\eta^2\\right)\\dot{\\theta}. \\end{align*} \\] <p>where \\(I+I_m \\eta^2\\) is called the effective inertia seen at the output of the gearing, \\(b+b_m\\eta^2\\) is called the effective damping.</p>"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/","title":"Spatial Description &amp; Transformation","text":""},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#descriptions","title":"Descriptions","text":"<p>Note that we use capital form (upper-case) \\(X\\) or bold form of lower case \\(\\pmb{x}\\) to denote vectors. </p> <p>Descriptions of a position, an orientation</p> <p>In a coordinate system \\(\\{A\\}\\) (three orthogonal unit vectors), we use \\(\\hat{X}\\) to denote the norm of vector \\(X\\) equals \\(1\\), and use subscript \\(\\hat{X}_A\\) to denote \\(X\\) is the unit vector of frame \\(\\{A\\}\\) along \\(x\\)-axis.</p> <p>(i) A point \\(^AP\\) (superscript on the left means reference frame is \\(\\{A\\}\\)) is expressed by a vector</p> \\[ ^AP=\\left[\\begin{array}{c} p_x\\\\p_y\\\\p_z \\end{array}\\right] \\] <p>or a vector expressed by unit vectors</p> \\[ \\pmb{r}_{OP}=\\left[\\begin{array}{ccc}\\hat{X}_A&amp;\\hat{X}_A&amp;\\hat{X}_A\\end{array}\\right]^AP. \\] <p>(ii) Attach another coordinate system \\(\\{B\\}\\), with its unit vectors \\(\\hat{X}_B\\), \\(\\hat{Y}_B\\) and \\(\\hat{Z}_B\\). If they are represented by system \\(\\{A\\}\\), as \\(^A\\hat{X}_B\\), \\(^A\\hat{Y}_B\\), and \\(^A\\hat{Z}_B\\). Stack these three vectors together, and we have a Rotation Matrix from \\(A\\) to \\(B\\)</p> \\[ ^A_BR=\\left[\\begin{array}{ccc}^A\\hat{X}_B&amp; ^A\\hat{Y}_B&amp; ^A\\hat{Z}_B\\end{array}\\right]. \\] <p>Actually, it represents the poze (orientation) transfer from \\(\\{A\\}\\) to \\(\\{B\\}\\). It is also a method to get a rotation matrix, using expression of \\(\\{B\\}\\) basis expressed in \\(\\{A\\}\\).</p> <p></p> <p>Properties of Rotation Matrix</p> <p>(i) Calculate elements of rotation matrix. </p> \\[ ^A_BR=\\left[\\begin{array}{ccc} \\langle\\hat{X}_B, \\hat{X}_A\\rangle&amp;\\langle\\hat{Y}_B, \\hat{X}_A\\rangle&amp;\\langle\\hat{Z}_B, \\hat{X}_A\\rangle\\\\ \\langle\\hat{X}_B, \\hat{Y}_A\\rangle&amp;\\langle\\hat{Y}_B, \\hat{Y}_A\\rangle&amp;\\langle\\hat{Z}_B, \\hat{Y}_A\\rangle\\\\ \\langle\\hat{X}_B, \\hat{Z}_A\\rangle&amp;\\langle\\hat{Y}_B, \\hat{Z}_A\\rangle&amp;\\langle\\hat{Z}_B, \\hat{Z}_A\\rangle \\end{array}\\right] \\] <p>Note that inner product is irrelevant with the choice of frame.</p> <p>(ii) Show that</p> \\[ ^A_BR=\\left[\\begin{array}{c} ^B\\hat{X}_A^T\\\\^B\\hat{Y}_A^T\\\\^B\\hat{Z}_A^T \\end{array}\\right], \\] <p>and then </p> \\[ ^B_AR= {^A_B}R^T. \\] <p>(iii) \\(^A_BR\\) is reversible, and </p> \\[ (^A_BR)^{-1}={^B_A}R^T. \\] Proof for (i)Proof for (ii)Proof for (iii) <p>If we consider from linear algebra, denoting \\(\\hat{\\pmb{e}}_1,\\hat{\\pmb{e}}_2,\\hat{\\pmb{e}}_3\\) as basis of \\(\\{B\\}\\), \\(\\pmb{e}_1,\\pmb{e}_2,\\pmb{e}_3\\) as basis of \\(\\{A\\}\\), so </p> \\[ \\hat{\\pmb{e}}_{j}=\\left[\\begin{array}{ccc}\\pmb{e}_1&amp;\\pmb{e}_2&amp;\\pmb{e}_3\\end{array}\\right]\\left[\\begin{array}{c} r_{1j}\\\\r_{2j}\\\\r_{3j} \\end{array}\\right]=\\sum_{i=1}^3 r_{ij} \\pmb{e}_i,\\quad j=1,2,3. \\] <p>From inner product, we have</p> \\[ \\langle\\pmb{e}_i, \\hat{\\pmb{e}}_j\\rangle=r_{ij}. \\] <p>So a rotation matrix from \\(\\{A\\}\\) to \\(\\{B\\}\\), we have</p> \\[ ^A_BR=\\left[\\begin{array}{ccc} r_{11}&amp;r_{12}&amp;r_{13}\\\\ r_{21}&amp;r_{22}&amp;r_{23}\\\\ r_{31}&amp;r_{32}&amp;r_{33} \\end{array}\\right] \\] <p><p>\\(\\square\\)</p></p> <p>from (i) we could know that</p> \\[ ^A\\hat{X}_B=\\left[\\begin{array}{c} \\langle\\hat{X}_B, \\hat{X}_A\\rangle\\\\ \\langle\\hat{X}_B, \\hat{Y}_A\\rangle\\\\ \\langle\\hat{X}_B, \\hat{Z}_A\\rangle \\end{array}\\right] \\] <p>Similarly we have</p> \\[ ^B\\hat{X}_A=\\left[\\begin{array}{c} \\langle\\hat{X}_B, \\hat{X}_A\\rangle\\\\ \\langle\\hat{Y}_B, \\hat{X}_A\\rangle\\\\ \\langle\\hat{Z}_B, \\hat{X}_A\\rangle \\end{array}\\right] \\] <p>which is just the transpose of the first line of \\(^A_BR\\).</p> <p><p>\\(\\square\\)</p></p> <p>Since </p> \\[ \\begin{align*} ^B_AR\\cdot {^A_B}R={^A_B}R^T \\cdot{^A_B}R&amp;=\\left[\\begin{array}{ccc}^A\\hat{X}^T_B&amp; ^A\\hat{Y}^T_B&amp; ^A\\hat{Z}^T_B\\end{array}\\right]^T\\left[\\begin{array}{ccc}^A\\hat{X}_B&amp; ^A\\hat{Y}_B&amp; ^A\\hat{Z}_B\\end{array}\\right]\\\\ &amp;=\\left[\\begin{array}{c}^A\\hat{X}^T_B\\\\ ^A\\hat{Y}^T_B\\\\ ^A\\hat{Z}^T_B\\end{array}\\right]\\left[\\begin{array}{ccc}^A\\hat{X}_B&amp; ^A\\hat{Y}_B&amp; ^A\\hat{Z}_B\\end{array}\\right]\\\\ &amp;=I_{3\\times 3}. \\end{align*} \\] <p>So \\((^A_BR)^{-1}={^B_A}R^T\\).</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#same-points-in-different-frames","title":"Same Points in different frames","text":"<p>Now we could discuss the description of points within different frames.</p> <p>Same points within different frames in simple cases</p> <p>The following process is called analyze points from moving frame into fixed frame.</p> <p>(i) \\(\\{B\\}\\) is a translation of \\(\\{A\\}\\). Then </p> \\[ ^AP={^B}P+{^A}O_B. \\] <p>(ii) \\(\\{B\\}\\) is a rotation of \\(\\{A\\}\\). Then </p> \\[ ^AP={^A_B}R {^B}P. \\] <p>Note the we premultiply rotation matrix to \\(^BP\\), while we postmultiply rotation matrix to basis vectors in \\(\\{B\\}\\) to get basis vectors in \\(\\{A\\}\\).</p> Proof <p>(i) Easy to see.</p> <p>(ii) Using relationship between basis vectors</p> \\[ \\left[\\begin{array}{ccc}\\hat{X}_B&amp;\\hat{X}_B&amp;\\hat{X}_B\\end{array}\\right]=\\left[\\begin{array}{ccc}\\hat{X}_A&amp;\\hat{X}_A&amp;\\hat{X}_A\\end{array}\\right]{^A_B}R, \\] <p>we have</p> \\[ \\begin{align*} \\pmb{r}_{OP}&amp;=\\left[\\begin{array}{ccc}\\hat{X}_B&amp;\\hat{X}_B&amp;\\hat{X}_B\\end{array}\\right]{^B}P\\\\ &amp;=\\left[\\begin{array}{ccc}\\hat{X}_A&amp;\\hat{X}_A&amp;\\hat{X}_A\\end{array}\\right]{^A_BR} \\cdot{^BP}\\\\ &amp;:=\\left[\\begin{array}{ccc}\\hat{X}_A&amp;\\hat{X}_A&amp;\\hat{X}_A\\end{array}\\right]{^A}P. \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>Combined from the above two simple cases, we have the following general cases. </p> <p>Same points within different frames in general cases</p> <p>Assume \\(\\{B\\}\\) is translation and rotation of \\(\\{A\\}\\), then </p> \\[ ^AP= {^A_BR} {^BP}+ {^AO_B} \\]"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#homogeneous-transform-matrix","title":"Homogeneous Transform Matrix","text":"<p>It is usually good to think of the above formula as a mapping from \\(^BP\\) to \\(^AP\\), but if we write</p> \\[ \\begin{align} ^AP=\\phi({^BP})=T \\cdot{^BP},\\label{3times3} \\end{align} \\] <p>where \\(T_{3\\times 3}\\) is not just the rotation matrix or the translation, which is left out. We are adept to making use of \\(^A_BR\\) and \\(^AO_B\\) if we expend one dimension, as the following definition shows.</p> <p></p> <p>Homogeneous Transform</p> <p>Define Homogeneous Transform to be </p> \\[ ^A_BT_{4\\times 4}:=\\left[\\begin{array}{ccc:c} &amp;^A_BR_{3\\times 3}&amp; &amp;^AO_B\\\\ \\hdashline 0&amp;0&amp;0&amp; 1 \\end{array}\\right] \\] <p>such that</p> \\[ \\begin{align} \\left[\\begin{array}{c} ^AP\\\\1 \\end{array}\\right]={^A_BT}\\left[\\begin{array}{c} ^BP\\\\1 \\end{array}\\right] \\label{4times4} \\end{align} \\] <p>Actually it is equivalent to equation \\(\\ref{3times3}\\), because equation \\(\\ref{4times4}\\) demonstrates</p> \\[ \\begin{cases} ^AP&amp;= {^A_BR} {^BP}+ {^AO_B}\\\\ 1&amp;=1. \\end{cases} \\] <p>Properties of Homogeneous Transform</p> <p>Inverse of Homogeneous Transform Matrix.</p> <p>Assume homogeneous transform matrix \\(^A_BT\\) from \\(\\{A\\}\\) to \\(\\{B\\}\\), then it is reversible and </p> \\[ {^B_AT}={^A_BT^{-1}}. \\] Proof <p>We make use of structure inherent of \\(^A_BT\\). </p> <ul> <li>For rotation matrix \\(^A_BR\\), by Properties of Rotation Matrix we have</li> </ul> \\[ \\begin{align} ^B_AR={^A_BR^T}.\\label{Rotation} \\end{align} \\] <ul> <li>For translation vector \\(^AO_B\\), we want to know \\(^BO_A\\). This is a little tricky. We apply Same points within different frames in general cases to \\(O_B\\), in terms of \\(\\{B\\}\\). That is, we have the \\(O_B\\) in frame \\(\\{B\\}\\) expressed by frame \\(\\{A\\}\\)</li> </ul> \\[ \\pmb{0}={^BO_B}={^B_AR}{^AO_B}+ {^BO_A} \\] <p>which gives </p> \\[ \\begin{align} {^BO_A}=-{^B_AR}{^AO_B}=-{^A_BR^T}{^AO_B}.\\label{Translation} \\end{align} \\] <p>Conbined with equation \\(\\ref{Rotation}\\) and \\(\\ref{Translation}\\), we have</p> \\[ ^B_AT=\\left[\\begin{array}{ccc:c} &amp;^A_BR^T&amp; &amp;-{^A_BR^T}{^AO_B}\\\\ \\hdashline 0&amp;0&amp;0&amp; 1 \\end{array}\\right] \\] <p>So \\({^A_BT}\\cdot {^B_AT}=I_{4\\times 4}\\), which is reversible.</p> <p><p>\\(\\square\\)</p></p> <p>The above proof also gives the method to quickly solve the inverse of \\(^A_BT\\).</p>"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#chain-rule","title":"Chain Rule","text":"<p>Chain Rule: Same points with multiple different frames</p> <p>Assume frame \\(\\{C\\}\\) is known relative to frame \\(\\{B\\}\\), frame \\(\\{B\\}\\) is known relative to frame \\(\\{A\\}\\). Their relationship is expressed by \\(^B_CT\\) and \\(^A_BT\\), respectively. Then expression of point \\(P\\) in frame \\(A\\) could be expressed by transform of its expression in frame \\(C\\) as</p> \\[ ^AP={^A_BT}{^B_CT}{^CP}. \\] <p>That is, the relationship from \\(\\{A\\}\\) to \\(\\{C\\}\\) is </p> \\[ {^A_CT}={^A_BT}{^B_CT}. \\] Proof <p>Notice that by Homogeneous Transform</p> \\[ \\begin{align*} \\left[\\begin{array}{c} ^AP\\\\1 \\end{array}\\right]&amp;={^A_BT}\\left[\\begin{array}{c} ^BP\\\\1 \\end{array}\\right]\\\\ &amp;={^A_BT}{^B_CT}\\left[\\begin{array}{c} ^CP\\\\1 \\end{array}\\right]\\\\ &amp;:={^A_CT}\\left[\\begin{array}{c} ^CP\\\\1 \\end{array}\\right]. \\end{align*} \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#operators","title":"Operators","text":"<p>Operators like translation, rotation could ba accomplished by the same mathematics as mapping the point to a second frame.</p> <p>Translation Operator</p> <p>In frame \\(\\{A\\}\\), vector \\(^AP_1\\) is translated by a vector \\(^AQ\\), the result denoted by </p> \\[ ^AP_2={^AP_1}+{^AQ}. \\] <p>We could use another perspective, i.e. construct a imaginary frame \\(\\{B\\}\\) with same orientation of \\(\\{A\\}\\) but origins at point \\(Q\\). The homogeneous transform matrix is</p> \\[ ^A_BT=\\left[\\begin{array}{ccc:c} &amp;I_{3\\times 3}&amp;&amp; {^AO_B}\\\\ \\hdashline 0&amp;0&amp;0&amp;1 \\end{array}\\right]=\\left[\\begin{array}{ccc:c} &amp;I_{3\\times 3}&amp;&amp; {^AQ}\\\\ \\hdashline 0&amp;0&amp;0&amp;1 \\end{array}\\right]. \\] <p>so the same point \\(P\\) (after translation) in frame \\(\\{A\\}\\) could be expressed in terms of that in frame \\(\\{B\\}\\)</p> \\[ ^AP_2={^A_BT}{^BP_2}={^A_BT}{^AP_1}. \\] <p>Rotation Operator</p> <p>Similar as we have in translation operator, we shall interpret a rotation to be a transform between two frames, i.e. </p> \\[ ^AP_2={^A_BT}{^AP_1}. \\] <p>where </p> \\[ ^A_BT=\\left[\\begin{array}{ccc:c} &amp;R_K(\\theta)&amp;&amp; \\pmb{0}\\\\ \\hdashline 0&amp;0&amp;0&amp;1 \\end{array}\\right]. \\] <p>Here we discuss a little more about \\(R_K(\\theta)\\) using different perspectives.</p>"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#fixed-angles","title":"Fixed angles","text":"<p>X-Y-Z Fixed angles</p> <p>Assume frame \\(\\{B\\}\\) coincides with a known reference frame \\(\\{A\\}\\), then rotate \\(\\{B\\}\\) about \\(\\hat{X}_A\\) by an angle \\(\\gamma\\), then rotate about \\(\\hat{Y}_A\\) by an angle \\(\\beta\\), and finally about \\(\\hat{Z}_A\\) by an angle \\(\\alpha\\).</p> <p>We can interpret these three steps as moving the same vector \\(P\\) in three steps. Assume initially its expression in frame \\(\\{A\\}\\) is \\(^AP\\), the final point \\(P'\\) could be expressed by</p> \\[ ^AP'=R_Z(\\alpha)\\cdot R_Y(\\beta)\\cdot R_X(\\gamma)\\cdot{^AP}. \\] <p>On the other hand, expression of \\(P'\\) in frame \\(\\{B\\}\\) is the same as in frame \\(\\{A\\}\\), i.e. \\({^BP'}={^AP}\\). So </p> \\[ ^AP'=R_Z(\\alpha)\\cdot R_Y(\\beta)\\cdot R_X(\\gamma)\\cdot{^BP'}. \\] <p>meaning \\(^A_BR=R_Z(\\alpha)\\cdot R_Y(\\beta)\\cdot R_X(\\gamma)\\) by same point in different frames, which is also called X-Y-Z fixed angle, achieved by premultiplying rotation matrices. We define X-Y-Z fixed angle as</p> \\[ ^A_BR_{XYZ}(\\gamma, \\beta,\\alpha)={R_{xyz}(\\gamma,\\beta, \\alpha)}. \\]"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#euler-angles","title":"Euler angles","text":"<p>Z-Y-X Euler angles</p> <p>Assume frame \\(\\{B\\}\\) coincides with a known reference frame \\(\\{A\\}\\), then rotate \\(\\{B\\}\\) about \\(\\hat{Z}_B\\) by an angle \\(\\alpha\\), then rotate about \\(\\hat{Y}_B\\) by an angle \\(\\beta\\), and finally about \\(\\hat{X}_B\\) by an angle \\(\\gamma\\).</p> <p>For these three steps, we could construct three frames \\(\\{C\\}\\), \\(\\{D\\}\\) and \\(\\{E\\}\\). Easy to see that</p> \\[ ^A_CR=R_Z(\\alpha)=\\left[\\begin{array}{ccc} 1&amp;0&amp;0\\\\ 0&amp;\\cos \\alpha&amp;-\\sin \\alpha\\\\ 0 &amp; \\sin \\alpha&amp; \\cos \\alpha \\end{array}\\right]. \\] <p>and </p> \\[ ^C_DR=R_Y(\\beta)=\\left[\\begin{array}{ccc} \\cos \\beta&amp;0&amp;\\sin \\beta\\\\ 0&amp;1&amp;0\\\\ -\\sin \\beta&amp;0&amp; \\cos \\beta \\end{array}\\right],\\quad ^D_ER=R_X(\\gamma)=\\left[\\begin{array}{ccc} \\cos \\gamma&amp;-\\sin \\gamma&amp;0\\\\ \\sin \\gamma&amp; \\cos \\gamma&amp;0\\\\ 0&amp;0&amp;1 \\end{array}\\right]. \\] <p>By chain rule we have</p> \\[ ^A_BR={^A_ER}={^A_CR}\\cdot{^C_DR}\\cdot{^D_ER}=R_Z(\\alpha)\\cdot R_Y(\\beta)\\cdot R_X(\\gamma). \\] <p>meaning the whole process could be achieved by postmultiplying rotation matrices, which is also called Z-Y-X fixed angles. We define Z-Y-X Euler angle as</p> \\[ ^A_BR_{Z'Y'X'}(\\beta,\\alpha,\\gamma)={R_{z'y'x'}(\\beta, \\alpha,\\gamma)}. \\] <p>Notice the above process is the same as we have in fixed angles.</p> <p>We have 6 symmetric and 6 non-symmetric forms of fixed angles or Euler angles.</p>"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#equivalent-angle-axis","title":"Equivalent angle-axis","text":"<p>Definition of Equivalent angle-axis representations</p> <p>Given a random axis \\(^AK={^A\\pmb{r}_{OK}}=[\\begin{array}{ccc}k_x &amp;k_y&amp;k_z\\end{array}]^T\\) expressed in frame \\(\\{A\\}\\), rotate the original frame \\(\\{A\\}\\) about axis \\(\\pmb{r}_{OK}\\) and get another frame \\(\\{B\\}\\). How to get the corresponding rotation matrix?</p> <p>Assume \\(\\pmb{r}_{OK}\\) is a unit vector. We want to find the expression of base vector \\(\\hat{X}_A\\) after it is rotated.</p>"},{"location":"Ctrl/Manipulators_Modelling/Spatial_Desc/#gimbal-lock","title":"Gimbal Lock","text":"<p>Does every matrix could be expressed by \\(Z-Y-X\\) angle?</p> <p>When \\(\\beta=\\frac{\\pi}{2}\\) in Euler's angle, then we lose a degree of freedom, solution is not unique.</p>"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/","title":"Jacobians: Velocities &amp; Static Forces","text":""},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#linear-velocity-vector","title":"Linear Velocity Vector","text":"<p>Differentiation of Position Vector</p> <p>We calculate the velocity of a point in frame \\(\\{B\\}\\), by derivative of its position</p> \\[ ^B V_Q = \\frac{d}{dt} {^BQ}=\\lim_{\\Delta t \\rightarrow 0}\\frac{{^B}Q(t+\\Delta t)-{^B}Q(t)}{\\Delta t}. \\] <p>which is also a vector, so could be expressed in another frame \\(\\{A\\}\\), denoted as</p> \\[ ^A(^BV_Q)=\\frac{^A d}{dt}{^BQ}. \\] <p>Here we have two notations. Note that we express a velocity in two frames, or the velocity is dependent on two frames.The first one with respect to which is the differentiation is done, and one in which the resulting velocity is done.</p> <p>Linear velocity expressed in different frames</p> <p>Actually, we calculate differential in the frame of \\(\\{B\\}\\), we mean that the two frames are both \\(\\{B\\}\\), i.e.</p> \\[ ^B({^BV_Q})={^B V_Q}. \\] <p>For simplicity, we write the latter form usually.</p> <p>Since the resulting velocities are expressed usually in the same frame which it gets differential, we want to get rid of it by rotation matrix</p> \\[ ^A({^BV_Q})={^A_BR}{^BV_Q}. \\] <p>We usually very often consider the velocity of the origin of a frame raletive to an understood universe reference frame, like Solid frame. And we denote this velocity of an origin as </p> \\[ v_C={^UV_{CORG}}. \\] <p>where the reference frame is \\(\\{U\\}\\) and the frame whose origin measured is \\(\\{C\\}\\).</p>"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#angular-velocity-vector","title":"Angular Velocity Vector","text":"<p>Angular Velocity Vector</p> <p>Define \\(^A\\Omega_B\\) to be the instantaneous axis of the rotation of frame \\(\\{B\\}\\) relative to \\(\\{A\\}\\). In part Supplementary info, we will discuss more about why we use these vector to represent angular velocity.</p> <p>Due to a vector could be determined on if its direction and magnitude are both determined, we let its magnitude to express the speed of rotation.</p> <p>Similar to Linear velocity vector, we can express angular velocity vector in different frames. We introduce</p> \\[ \\omega_C={^U\\Omega_C} \\] <p>to show the angular velocity of frame \\(\\{C\\}\\) relative to some understood reference frame \\(\\{U\\}\\). Thus \\(^A\\omega_C\\) means angular velocity of frame \\(\\{C\\}\\) expressed in frame \\(\\{A\\}\\) but default relative to frame \\(\\{U\\}\\).</p>"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#linear-motions-in-different-frames","title":"Linear Motions in different frames","text":"<p>Linear motion in frame with Linear velocity: Prismatic</p> <p>Assume frame \\(\\{B\\}\\) is located relative to \\(\\{A\\}\\), with a position vector \\(^AO_B\\), and rotation matrix \\(^A_BR\\). Here we assume \\(^A_BR\\) does not change with time. A point \\(Q\\) expressed in frame \\(B\\) is \\(^BQ\\), and has its linear velocity \\(^BV_Q\\). Then </p> \\[ ^AV_Q={^AV_{O_B}}+{^A_BR}{^BV_Q}, \\] <p>where we call \\(^AV_{O_B}=^AV_B\\) are the linear velocity of frame \\(\\{B\\}\\) relative to frame \\(\\{A\\}\\).</p> Proof <p>recall same point in different frames, we have \\(Q\\) express in frame \\(\\{A\\}\\)</p> \\[ ^AQ={^AO_B}+{^A_BR}{^BQ}. \\] <p>If we have velocity of \\(Q\\) in frame \\(\\{B\\}\\)</p> \\[ ^B V_Q =\\lim_{\\Delta t \\rightarrow 0}\\frac{{^B}Q(t+\\Delta t)-{^B}Q(t)}{\\Delta t}, \\] <p>then its expression in frame \\(\\{A\\}\\) is</p> \\[ \\begin{align*} ^AV_Q&amp;= \\lim_{\\Delta t \\rightarrow 0}\\frac{{^A}Q(t+\\Delta t)-{^A}Q(t)}{\\Delta t}\\\\ &amp;=\\lim_{\\Delta t \\rightarrow 0}\\frac{[{^AO_B}(t+\\Delta t)-{^AO_B}(t)]+[{^AR_B}{^BQ}(t+\\Delta t)-{^AR_B}{^BQ}(t)]}{\\Delta t}\\\\ &amp;={^AV_{O_B}}+\\lim_{\\Delta t \\rightarrow 0}\\frac{{^AR_B}[{^BQ}(t+\\Delta t)-{^BQ}(t)]}{\\Delta t}\\\\ &amp;={^AV_{O_B}}+{^A_BR}{^BV_Q}. \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p></p> <p>Linear motion in frame with Rotational velocity: Revolute</p> <p>Assume frame \\(\\{B\\}\\) coincide with \\(\\{A\\}\\) with zero linear velocity, i.e. their origins \\(O_A\\) and \\(O_B\\) remain coincident all the time. Let the orientation of \\(\\{B\\}\\) changes with time, expressed by \\(^A\\Omega_B\\). </p> <p>(i) A point \\(Q\\) fixed in frame \\(\\{B\\}\\) (i.e. \\(^BV_Q=0\\)), would apparently move because of the rotation of frame \\(\\{B\\}\\). Then</p> \\[ ^AV_Q={^A\\Omega_B}\\times {^AQ}. \\] <p>(ii) The point \\(Q\\) also moves in frame \\(B\\), i.e. \\(^BQ\\) (or \\(^AQ\\)) would change and \\(^BV_Q\\neq 0\\). Then</p> \\[ ^AV_Q={^A_BR}{^BV_Q}+{^A\\Omega_B}\\times ({^A_BR}{^BQ}). \\] Proof <p>(i) Here we have to find out the movement of \\(Q\\) viewed in frame \\(\\{A\\}\\). See the following picture in details.</p> <p><p> </p></p> <p>Check that the magnitude of position change in small time \\(\\Delta t\\) is </p> \\[ |\\Delta Q|=|{^AQ}|\\sin \\theta{|^A\\Omega_B}|\\Delta t, \\] <p>and the direction is perpendicular to both vector \\(^A\\Omega_B\\) and \\(^AQ\\). Actually, we can easily get the following relation if readers have learned general physics</p> \\[ ^AV_Q={^A\\Omega_B}\\times {^AQ}. \\] <p>(ii) By superposition of velocities and former equation of linear velocity relationship</p> \\[ \\begin{align*} ^AV_Q&amp;={^A({^BV_Q})}+{^A\\Omega_B}\\times {^AQ}\\\\ &amp;={^A_BR}{^BV_Q}+{^A\\Omega_B}\\times ({^A_BR}{^BQ}). \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p></p> <p>General case</p> <p>Adding the above two cases up, we have a formula for linear velocity of a point \\(Q\\) in frame \\(\\{B\\}\\) expressed in another frame \\(\\{A\\}\\)</p> \\[ ^AV_Q={^AV_B}+{^A\\Omega_B}\\times {{^A_BR}{^BQ}} +{^A_BR}{^BV_Q} \\] <p>where the first item is the translation of \\(\\{B\\}\\) relative to \\(\\{A\\}\\), the second item is the rotation of \\(\\{B\\}\\) relative to \\(\\{A\\}\\) and the last item is the translation movement of \\(Q\\) itself expressed in \\(\\{A\\}\\), respectively.</p>"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#supplementary-info","title":"supplementary info","text":"<p>Here we talk a little more with rotation item \\({^A\\Omega_B}\\times {^AQ}\\).</p> <p></p> <p>Lemma 1: properties of the derivative of an orthonormal matrix</p> <p>For orthonormal matrix \\(R\\in \\mathbb{R}^n\\), or rotation matrix (\\(n=3\\)) in this course, define \\(S=\\dot{R} R^T\\), then it satisfies</p> \\[ S+S^T=\\pmb{0} \\] <p>which is a skew-symmetric matrix.</p> Proof <p>By properties of orthonormal matrix, we have</p> \\[ RR^T=I_{n\\times n}. \\] <p>then we take a derivative on both sides</p> \\[ \\dot{R} R^T+ R \\dot{R}^T=\\pmb{0}. \\] <p>by propeties of transpose, we have</p> \\[ \\dot{R} R^T+ (\\dot{R} R^T)^T=\\pmb{0}. \\] <p>which just means the result.</p> <p><p>\\(\\square\\)</p></p> <p>Still the case (ii) in Linear motion in frame with Rotational velocity: Revolute, now we deduce the velocity relationship in another perspective.</p> <p>Lemma 2: rotational description</p> <p>If we define \\(^A_BS={^A_B\\dot{R}} {^A_BR}^{T}\\), then </p> \\[ ^AV_Q={^A_BS}{^AQ}+{^A_BR}{^BV_Q}. \\] <p>Easy to see that \\(^A_BS\\) satisfies the property in Lemma 1.</p> Proof <p>We start from </p> \\[ ^AQ={^A_BR}{^BQ}. \\] <p>Take a derivative on both sides, we have</p> \\[ \\begin{align*} ^AV_Q&amp;={^A_B\\dot{R}}{^BQ}+{^A_BR}{^BV_Q}\\\\ &amp;={^A_B\\dot{R}} {^A_BR}^{T}{^AQ}+ {^A_BR}{^BV_Q}\\\\ \\end{align*} \\] <p>which gives the result if we substitute \\(^A_BS\\) in.</p> <p><p>\\(\\square\\)</p></p> <p>Compared to the result we get in Linear motion in frame with Rotational velocity: Revolute, the following theorem comes out naturally.</p> <p></p> <p>Theorem for skew-sysmmetric matrix and vector cross-product</p> <p>Define a vector \\(\\Omega=[\\begin{array}{ccc}\\Omega_x&amp; \\Omega_y&amp; \\Omega_z\\end{array}]^T\\) and a point \\(Q\\), then</p> \\[ \\Omega\\times Q=S\\cdot Q \\] <p>where \\(S\\) is a skew-symmetric matrix with an expression</p> \\[ S=\\left[\\begin{array}{ccc} 0 &amp; -\\Omega_z &amp; \\Omega_y\\\\ \\Omega_z &amp; 0 &amp; -\\Omega_x\\\\ -\\Omega_y &amp; \\Omega_x &amp; 0\\\\ \\end{array}\\right]. \\] <p>We call \\(\\Omega\\) angular velocity vector, the same meaning as we defined in Angular Velocity Vector.</p> Proof <p>We calculate the cross product \\(\\Omega\\times Q\\) gives</p> \\[ \\Omega\\times Q=\\left|\\begin{array}{ccc} i&amp;j&amp;k\\\\ \\Omega_x &amp;\\Omega_y&amp;\\Omega_z\\\\ Q_x &amp;Q_y &amp;Q_z  \\end{array}\\right| \\] <p>rewrite the above calculation in form of a linear transformation, or matrix premultiplication \\(S\\cdot Q\\) as</p> \\[ \\Omega\\times Q = \\left[\\begin{array}{ccc} 0 &amp; -\\Omega_z &amp; \\Omega_y\\\\ \\Omega_z &amp; 0 &amp; -\\Omega_x\\\\ -\\Omega_y &amp; \\Omega_x &amp; 0\\\\ \\end{array}\\right] \\cdot \\left[\\begin{array}{c} Q_x\\\\Q_y\\\\Q_z \\end{array}\\right] :=SQ. \\] <p><p>\\(\\square\\)</p></p> <p>We dig into the representation of angular velocity vector \\(\\Omega\\).</p> <p>Theorem for physical insight concerning angular velocity vector</p> <p>Angular velocity vector could be expressed as</p> \\[ \\Omega=\\dot{\\theta}\\hat{K}, \\] <p>where \\(\\hat{K}\\) is the rotation axis and \\(\\dot{\\theta}\\) is the speed of rotation about the axis.</p> Proof <p>Consider the differentiation of a rotation matrix \\(R\\)</p> \\[ \\begin{equation} \\dot{R}=\\frac{dR}{dt}=\\lim_{\\Delta\\rightarrow \\infty}\\frac{R(t+\\Delta t)-R(t)}{\\Delta t}.\\label{R-differential} \\end{equation} \\] <p>Here we rewrite \\(R(t+\\Delta t)\\) in a sense of rotation. That is, in time period \\(\\Delta t\\), frame \\(\\{B\\}\\) rotates about some axis \\(\\hat{K}\\) with a little angle \\(\\Delta \\theta\\) (using expression of equivalent axis), and then we get the final rotation matrix</p> \\[ R(t+\\Delta t)=R_{K}(\\Delta\\theta)R(t). \\] <p>equation \\(\\ref{R-differential}\\) becomes</p> \\[ \\begin{align} \\nonumber\\dot{R}&amp;=\\lim_{\\Delta\\rightarrow \\infty}\\frac{R_{K}(\\Delta\\theta)R(t)-R(t)}{\\Delta t}\\\\ &amp;=\\lim_{\\Delta\\rightarrow \\infty}\\left[\\frac{R_{K}(\\Delta\\theta)-I_{3}}{\\Delta t}\\right]R(t)\\label{R-differential2} \\end{align} \\] <p>recall that we have a linear approximation of \\(R_K(\\Delta\\theta)\\) when \\(\\Delta\\theta\\) is quite small, i.e.</p> \\[ R_K(\\Delta\\theta)=\\left[\\begin{array}{ccc} 1 &amp; -k_z \\Delta\\theta &amp; k_y \\Delta\\theta\\\\ k_z\\Delta\\theta &amp; 1 &amp; -k_x\\Delta\\theta\\\\ -k_y\\Delta\\theta &amp; k_x\\Delta\\theta &amp;1 \\end{array}\\right] \\] <p>so equation \\(\\ref{R-differential2}\\) becomes</p> \\[ \\begin{align} \\dot{R}=\\left[\\begin{array}{ccc} 0 &amp; -k_z \\dot{\\theta} &amp; k_y \\dot{\\theta}\\\\ k_z\\dot{\\theta} &amp; 0 &amp; -k_x\\dot{\\theta}\\\\ -k_y\\dot{\\theta} &amp; k_x\\dot{\\theta} &amp;0 \\end{array}\\right] R(t).\\label{physical-rotation} \\end{align} \\] <p>Postmultiply \\(R^{-1}\\) on both sides we have</p> \\[ \\dot{R}R^{T}=\\left[\\begin{array}{ccc} 0 &amp; -k_z \\dot{\\theta} &amp; k_y \\dot{\\theta}\\\\ k_z\\dot{\\theta} &amp; 0 &amp; -k_x\\dot{\\theta}\\\\ -k_y\\dot{\\theta} &amp; k_x\\dot{\\theta} &amp;0 \\end{array}\\right]. \\] <p>Comparing the result in Theorem for skew-sysmmetric matrix and vector cross-product, we have</p> \\[ \\Omega = \\left[\\begin{array}{ccc} \\Omega_x&amp; \\Omega_y&amp;\\Omega_z \\end{array}\\right]^T=\\left[\\begin{array}{ccc} k_x\\dot{\\theta}&amp; k_y\\dot{\\theta}&amp;k_z\\dot{\\theta} \\end{array}\\right]^T=\\dot{\\theta}\\hat{K}. \\] <p><p>\\(\\square\\)</p></p> <p>Actually, equation \\(\\ref{physical-rotation}\\) deduced by its physical meaning, is equivalent to what we have deduced in Lemma 1: properties of the derivative of an orthonormal matrix.</p>"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#velocity-propagation","title":"Velocity Propagation","text":"<p>Now we consider velocity propagations. Here we use \\(\\omega_i\\) and \\(v_i\\) to be the angular and linear velocity of the center of frame \\(i\\) (derivative calculated in frame \\(0\\)), respectively. And their expression in frame \\(\\{i-1\\}\\) is \\(^{i-1}\\omega_i\\), \\(^{i-1}v_i\\), respectively.</p> <p>Linear Velocity</p> <p>Assume we have frame \\(\\{i\\}\\) and \\(\\{i+1\\}\\).</p> <p>(i) Next joint is Revolute.</p> <p>In frame \\(i\\), we have \\(^{i}v_{i+1}\\) expressed by </p> \\[ ^iv_{i+1}=^iv_{i}+{^i\\omega_{i+1}}\\times {^iO_{i+1}}. \\] <p>Premultiply both sides \\(^{i+1}_i R\\), we have </p> \\[ ^{i+1}v_{i+1}={^{i+1}_i R}[^iv_{i}+{^i\\omega_{i+1}}\\times {^iO_{i+1}}]. \\] <p>(ii) Next joint is Prismatic.</p> <p>In frame \\(i\\), we have \\(^{i}v_{i+1}\\) expressed by </p> \\[ ^{i+1}v_{i+1}={^{i+1}_i R}[^iv_{i}+{^i\\omega_{i+1}}\\times {^iO_{i+1}}] + \\dot{d}_{i+1}\\hat{Z}_{i+1}. \\] <p>Angular Velocity</p> <p>Assume we have frame \\(\\{i\\}\\) and \\(\\{i+1\\}\\).</p> <p>(i) Next joint is revolute.</p> <p>In frame \\(i\\), we have \\(^{i}\\omega_{i+1}\\) expressed by </p> \\[ ^i\\omega_{i+1}=^i\\omega_i+{^i_{i+1}R} \\dot{\\theta}_{i+1} {^{i+1}\\hat{Z}_{i+1}}  \\] <p>where \\(\\dot{\\theta}_{i+1}\\) means the magnitude of rotation speed, \\({^{i+1}\\hat{Z}_{i+1}}\\) means the rotation aixs expressed in frame \\(i+1\\), i.e. \\({^{i+1}\\hat{Z}_{i+1}}=\\left[\\begin{array}{ccc} 0&amp;0&amp;1\\end{array}\\right]^T\\). Note that rotation matrix \\(^i_{i+1}R\\) rotates the axis of rotation of joint \\(i+1\\) into its description in frame \\(i\\).</p> <p>Premultiply both sides \\(^{i+1}_i R\\), we have </p> \\[ \\begin{align} ^{i+1}\\omega_{i+1}={^{i+1}_i R}^i\\omega_i+ \\dot{\\theta}_{i+1} {^{i+1}\\hat{Z}_{i+1}}.\\label{revolute-angular} \\end{align} \\] <p>(ii) Next joint is Prismatic.</p> <p>Remove item for axis rotation from equation \\(\\ref{revolute-angular}\\), we have a much simpler one for prismatic.</p> \\[ \\begin{align} ^{i+1}\\omega_{i+1}={^{i+1}_i R}^i\\omega_i.\\label{prismatic-revolute} \\end{align} \\]"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#jacobians","title":"Jacobians","text":"<p>In the field of robotics, we generally use Jacobians that relate joint velocities to Cartesian velocities of the tip of the arm. In 3D circumstances, we have the small change of linear and angular displacement of the end effector</p> \\[ ^0\\Delta r=\\left[\\begin{array}{c} ^0\\Delta R\\\\ ^0\\Delta \\alpha \\end{array}\\right]={^0J(\\Theta)}\\Delta {\\Theta}, \\] <p>by dividing both sides \\(\\Delta t\\) and let \\(\\Delta t\\rightarrow \\infty\\), we have linear and angular velocity of the end effector</p> \\[ ^0\\nu=\\left[\\begin{array}{c} ^0v\\\\ ^0\\omega \\end{array}\\right]={^0J(\\Theta)}\\dot{\\Theta}, \\] <p>where \\(\\dot{\\Theta}=\\left[\\begin{array}{cccccc} \\dot{\\theta_1}&amp; \\dot{\\theta_2}&amp; \\dot{\\theta_3}&amp; \\dot{\\theta_4}&amp; \\dot{\\theta_5 }&amp; \\dot{\\theta_6} \\end{array}\\right]^T\\) denotes six (generally) rotation axis of the manipulator.</p> <p>Changing a Jacobian's frame of reference</p> <p>Given a Jacobian matrix in frame \\(\\{B\\}\\), i.e.</p> \\[ \\left[\\begin{array}{c} ^Bv\\\\ ^B\\omega \\end{array}\\right]={^B\\nu}={^BJ(\\Theta)}\\dot{\\Theta}, \\] <p>then </p> \\[ \\left[\\begin{array}{c} ^Av\\\\ ^A\\omega \\end{array}\\right] = \\left[\\begin{array}{c:c} {^A_BR}&amp; 0\\\\  \\hdashline 0&amp; {^A_BR}\\\\ \\end{array}\\right]\\left[\\begin{array}{c} ^Bv\\\\ ^B\\omega \\end{array}\\right]=\\left[\\begin{array}{c:c} {^A_BR}&amp; 0\\\\  \\hdashline 0&amp; {^A_BR}\\\\ \\end{array}\\right]{^BJ(\\Theta)}\\dot{\\Theta} \\] <p>which gives</p> \\[ {^AJ(\\Theta)}=\\left[\\begin{array}{c:c} {^A_BR}&amp; 0\\\\  \\hdashline 0&amp; {^A_BR}\\\\ \\end{array}\\right]{^BJ(\\Theta)}. \\]"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#static-forces","title":"Static Forces","text":"<p>We also have to consider how forces and moments \"propagate\" from one link to the next. We wish to solve for the joint torques that must be acting to keep the system in static equilibrium. Here we do not consider the gravity of links.</p> <p>Symbols for the force and torque</p> <p>Denote \\(f_i\\) to be the force exerted on link \\(i\\) by link \\(i-1\\), \\(n_i\\) to be the torque exerted on link \\(i\\) by link \\(i-1\\). Check the following image for details.</p> <p><p> </p></p> <p>so for link \\(i\\), it follows force and torque equilibrium</p> \\[ \\begin{cases} ^if_i-{^if_{i+1}}=0,\\\\ {^in_i}-{^in_{i+1}}-{^iP_{i+1}}\\times {^if_{i+1}}=0.    \\end{cases} \\] <p>thus we have the propagation formula</p> \\[ \\begin{cases} ^if_i={^if_{i+1}},\\\\ {^in_i}={^in_{i+1}}+{^iP_{i+1}}\\times {^if_{i+1}}.    \\end{cases} \\] <p>which transfer the force from the end effector to the base \\(0\\).</p> <p>To write these equations in terms of only forces and moments defined within their own link frames, we transform with the rotation matrix</p> \\[ \\begin{cases} ^if_i={^i_{i+1}R}\\cdot {^{i+1}f_{i+1}},\\\\ {^in_i}={^i_{i+1}R}\\cdot {^{i+1}n_{i+1}}+{^iP_{i+1}}\\times {^if_{i}}.     \\end{cases} \\] <p>After computing the forces and torques exerted on each link, now we have to consider, in each joint, how much force or torque should we provide so as to keep the system in equilibrium?</p> <p>Forces or Torques provided by joints</p> <p>(i) For revolute joint \\(i\\), \\(^if_i\\) prevents it from translation, and \\(^in_i\\) prevents it from rotation in other directions. So if the joint want to rotate, we project \\(^in_i\\) into the rotation axis, i.e. \\(\\hat{Z}_i\\), and </p> \\[ \\tau_i={^in_i}\\cdot {^i\\hat{Z}_i}={^in_i^T} {^i\\hat{Z}_i}. \\] <p>(ii) For Prismatic joint \\(i\\), \\(^in_i\\) pervents it from rotation, and \\(^if_i\\) prevents it from translation in other directions. So if the joint want to translate, we project \\(^if_i\\) into the translation axis, i.e. \\(\\hat{Z}_i\\), and </p> \\[ \\tau_i={^if_i}\\cdot {^i\\hat{Z}_i}={^if_i^T} {^i\\hat{Z}_i}. \\] <p>where we still use \\(\\tau_i\\) to represent the linear force in (ii).</p>"},{"location":"Ctrl/Manipulators_Modelling/Velocities_Forces/#jacobians-in-the-force-domain","title":"Jacobians in the force domain","text":"<p>Here we use the principle of virtual work to demonstrate the relationship in force.</p> <p>From a general frame, we have</p> \\[ \\mathcal{F}^T \\Delta r=\\tau^T\\Delta \\theta \\] <p>where \\(\\mathcal{F}_{6\\times 1}=[\\begin{array}{cc} F&amp; N\\end{array}]^T\\), \\(F\\) is a force vector, and \\(N\\) is a moment vector.</p> <p>by definition of Jacobian, we have</p> \\[ \\mathcal{F}^T J(\\Theta)\\Theta=\\tau^T\\Delta \\theta \\] <p>So</p> \\[ \\begin{align} \\mathcal{F}^T J(\\Theta)=\\tau^T, \\text{ or }\\tau=J(\\Theta)^T\\mathcal{F}.\\label{F-tau-relation} \\end{align} \\]"},{"location":"Ctrl/Modern_Control_Theory/MCT/","title":"Modern Control Theory","text":"<p>I show the outline of the course for the coming exam.</p> {\"url\": \"../MCT_review.pdf\"}"},{"location":"Ctrl/ProcessControl/","title":"Process Control","text":""},{"location":"Ctrl/ProcessControl/#introduction","title":"Introduction","text":""},{"location":"Ctrl/ProcessControl/#pid-controller","title":"PID Controller","text":""},{"location":"Ctrl/ProcessControl/#control-strategy","title":"Control Strategy","text":""},{"location":"Ctrl/ProcessControl/#nonlinear-conpensation","title":"Nonlinear Conpensation","text":""},{"location":"Ctrl/ProcessControl/Intro/","title":"Introduction","text":""},{"location":"Ctrl/ProcessControl/Intro/#basic-ideas","title":"Basic Ideas","text":"<p>Besic Concepts</p> <ul> <li> <p>\u88ab\u63a7\u53d8\u91cf(Controlled Variable, CV)</p> </li> <li> <p>\u8bbe\u5b9a\u503c(Setpoint, SP)</p> </li> <li> <p>\u64cd\u7eb5\u53d8\u91cf(Manipulated Variable, MV). Directed controlled by actuator and influence CV.</p> </li> <li> <p>\u6270\u52a8\u53d8\u91cf(Disturbance Variables, DV). Variables that also influence CV and is not influenced by MV.</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Intro/#dynamic-characteristics-of-process","title":"Dynamic Characteristics of Process","text":""},{"location":"Ctrl/ProcessControl/Intro/#typical-process","title":"Typical Process","text":"<ul> <li>\u81ea\u8861\u8fc7\u7a0b(self-balanced process)</li> </ul> <p>When input changes, process can converge to another equilibrium. This process includes \u7eaf\u6ede\u540e\u8fc7\u7a0b, \u5355\u5bb9\u8fc7\u7a0b and \u591a\u5bb9\u8fc7\u7a0b.</p> <p>\u7eaf\u6ede\u540e\u8fc7\u7a0b\u3001\u5355\u5bb9\u8fc7\u7a0b and \u591a\u5bb9\u8fc7\u7a0b</p> <ul> <li>\u7eaf\u6ede\u540e\u8fc7\u7a0b</li> </ul> \\[ G(s)=e^{-\\tau s} \\] <ul> <li>\u5355\u5bb9\u8fc7\u7a0b</li> </ul> \\[ G(s)=\\frac{K}{1+T s}e^{-\\tau s} \\] <ul> <li>\u591a\u5bb9\u8fc7\u7a0b</li> </ul> <p>we use one order or two order model to approximate high order object</p> \\[ G(s)=\\frac{K}{(1+T_1 s)(1+T_2 s)}e^{-\\tau s} \\] <ul> <li>\u975e\u81ea\u8861\u8fc7\u7a0b(Non-self-balanced process)</li> </ul> <p>\u79ef\u5206\u8fc7\u7a0b, \u5f00\u73af\u4e0d\u7a33\u5b9a</p> <ul> <li>\u79ef\u5206\u8fc7\u7a0b</li> </ul> <p>we change valve into \u8ba1\u91cf\u6cf5, then </p> \\[ A\\frac{dH}{dt}= q_i-q_o \\] <p>making Laplace transformation and we get</p> \\[ \\frac{H}{Q_i}=\\frac{1}{As} \\] <ul> <li>\u5f00\u73af\u4e0d\u7a33\u5b9a</li> </ul> \\[ G(s)=\\frac{|K|}{|T|s-1} \\]"},{"location":"Ctrl/ProcessControl/Intro/#mechanisim-modelling","title":"Mechanisim Modelling","text":"<p>There are typical steps.</p> <ul> <li> <p>List Differential equations by physical mechanisim.</p> </li> <li> <p>find its static point.</p> </li> <li> <p>find dynamic relationship at this static point. Linearize the model if necessary.</p> </li> </ul> <p>Q1. Model dynamic process of liquid tank.</p> Answer <p>we know that </p> \\[ A\\frac{dh}{dt}=Q_i-Q_o \\] <p>with \\(Q_O=k\\sqrt{h}\\), where \\(A\\) is the cross area of the tank, \\(k\\) is the coefficient of the pipe. So </p> \\[ A\\frac{dh}{dt}=Q_i-k\\sqrt{h} \\] <p>Now we have to linearize it.</p> <p>at static point \\(t=0\\), we have \\(dh/dt=0\\), so \\(Q_{i0}=Q_{o0}\\). Denote</p> \\[ \\begin{align*} \\Delta h&amp;=h-h_0 \\\\ \\Delta Q_i&amp;=Q_i-Q_{i0}\\\\ \\Delta Q_o&amp;=Q_o-Q_{o0} \\end{align*} \\] <p>So</p> \\[ \\begin{align} A\\frac{d\\Delta h}{dt}=\\Delta Q_i-\\Delta Q_o \\label{eqh} \\end{align} \\] <ul> <li>Use Taylor extansion to get the linear part.</li> </ul> \\[ \\begin{align*} \\Delta Q_o&amp;=k\\sqrt{h} - Q_{o0}\\\\ &amp;\\approx \\frac{d Q_o}{dt}\\Bigg|_{h=h_0} \\Delta h \\\\ &amp;= \\frac{k}{2\\sqrt{h_0}}\\Delta h \\end{align*} \\] <p>So back in euqation \\(\\ref{eqh}\\), we have</p> \\[ A\\frac{d\\Delta h}{dt}=\\Delta Q_i- \\frac{k}{2\\sqrt{h_0}}\\Delta h \\] <p>Make Laplace Transiformation and we get</p> \\[ AsH(s)=Q_i(s)-\\frac{k}{2\\sqrt{h_0}}H(s) \\] <p>If we denote \\(R=\\frac{2\\sqrt{h_0}}{k}\\), we get</p> \\[ G(s)=\\frac{H(s)}{Q_i(s)}=\\frac{R}{RAs+1} \\]"},{"location":"Ctrl/ProcessControl/Intro/#detection-part","title":"Detection Part","text":"<p>This Detection part can usually be described as an one order model</p> \\[ G_m(s) = \\frac{K_m}{T_m s+1}e^{-\\tau_m s} \\]"},{"location":"Ctrl/ProcessControl/Intro/#valve-part","title":"Valve Part","text":"<ul> <li>\u6c14\u52a8\u9600\u7684\u6c14\u5f00\u3001\u6c14\u5173\u9009\u62e9</li> </ul> <p>\u6c14\u5f00\u3001\u6c14\u5173\u9600</p> <ul> <li> <p>\u6c14\u5f00\u9600: \u6709\u6c14\u5219\u5f00\uff0c\u65e0\u6c14\u5219\u5173</p> </li> <li> <p>\u6c14\u5173\u9600: \u65e0\u6c14\u5219\u5f00\uff0c\u6709\u6c14\u5219\u5173</p> </li> </ul> <p>Criteria: Without signal, should the valve be closed or open.</p> <ul> <li>Characteristics</li> </ul> <p>A valve is composed by two parts in series.</p> \\[ G(s)=G_{v1}(s)\\times G_{v2}(s) \\] <p>where \\(G_{v1}(s)\\) denotes the relationship between the input signal and the displacement of valve core, which is usually one order model and \\(G_{v2}(s)\\) denotes the relationship between the displacement and the output flow, which is usually non-linear and can be used to compensate some non-linear part in the control system.</p> <p>Here we talk about some details about the latter relationship. Firstly, we denote the flow characteristic \\(f\\) to be </p> \\[ \\frac{Q}{Q_{max}}=f\\left(\\frac{l}{L}\\right) \\] <p>where \\(\\frac{Q}{Q_{max}}\\) denotes the relative flow and \\(\\frac{l}{L}\\) denotes the relative displacement of the valve core. And we define \\(R=\\frac{Q_{max}}{Q_{min}}=30\\). The following graph is acquired when the pressure difference between the front and the back of the vavle stays the same.</p> <p>Typical Valves</p> <ul> <li>(i) Linear valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=K\\frac{l}{L}+C \\] <p>with \\(K=\\frac{R-1}{R}\\) and \\(C=\\frac{1}{R}\\).</p> <ul> <li>(ii) Equipercent valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=R^{\\left( \\frac{l}{L}-1 \\right)} \\] <p>which is deduced by ODE</p> \\[ \\frac{d(Q/Q_{max})}{d(l/L)}=K\\frac{Q}{Q_{max}} \\] <ul> <li>(iii) Quick-Opening valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=\\frac{1}{R}\\left[1+(R^2-1)\\frac{l}{L}\\right]^{1/2} \\] <p>which is deduced by </p> \\[ \\frac{d(Q/Q_{max})}{d(l/L)}=K\\left(\\frac{Q}{Q_{max}}\\right)^{-1} \\] <ul> <li>(iv) ParaBola valve</li> </ul> \\[ \\frac{Q}{Q_{max}}=\\frac{1}{R}\\left[1+(\\sqrt{R}-1)\\frac{l}{L}\\right]^{2} \\] <p>which is deduced by </p> \\[ \\frac{d(Q/Q_{max})}{d(l/L)}=K\\left(\\frac{Q}{Q_{max}}\\right)^{1/2} \\] <p><p> </p></p> <p>When we install a valve, the pressure would not be constant, cause the resistence of the pipe. Usually the curve would bend left up, which is determined by Pressure-drop ratio </p> \\[ s=\\frac{\\Delta P|_{l=L}}{P} \\] <p>if \\(s=1\\), then the curve does not change.</p> <p>Define the gain of valve to be</p> \\[ K_v=\\frac{\\Delta Q/Q_{max}}{\\Delta l/L} \\] <p>which is the tangent of the above graph.</p>"},{"location":"Ctrl/ProcessControl/Intro/#general-object-and-its-modelling","title":"General Object and its modelling","text":"<ul> <li>Test Method</li> </ul> <p>For one order object with pure delay, we can use a phase step as an input and use the response to get its corresponding model.</p> <p>Method for One order object with pure time delay</p> <p>We use Two-Point Method usually.</p> <p>(i) Gain</p> \\[ K=\\frac{O_f-O_i}{I_f-I_i} \\] <p>with units.</p> <p>(ii) time constant \\(T\\) and time daley constant \\(\\tau\\)</p> <p>choose \\(y_{283}(t_1)\\) and \\(y_{632}(t_2)\\) and then </p> \\[ T=1.5\\times (t_2-t_1),\\quad \\tau=t_2-t_0-T    \\] <p>where \\(t_0\\) denotes the start time of input step function.</p>"},{"location":"Ctrl/ProcessControl/Intro/#pid-parameters-determination","title":"PID Parameters Determination","text":"<p>After getting a one order model of the system, i.e. \\(K\\), \\(T\\), \\(\\tau\\), then we can use them to get PID controller parameters.</p>"},{"location":"Ctrl/ProcessControl/Intro/#_1","title":"\u54cd\u5e94\u66f2\u7ebf\u6cd5","text":"<ul> <li>ZN method: only applies to \\(0&lt;\\tau&lt;T\\)</li> </ul> \u63a7\u5236\u5668\u7c7b\u578b K_C T_i T_d P \\(T/(\\tau K)\\) \\(\\infty\\) \\(0\\) PI \\(0.9T/(\\tau K)\\) \\(3.33\\tau\\) \\(0\\) PID \\(1.2T/(\\tau K)\\) \\(2.0\\tau\\) \\(0.5\\tau\\) <ul> <li>\\(\\lambda\\) method: all application</li> </ul> \u63a7\u5236\u5668\u7c7b\u578b K_C T_i T_d P \\(T/(\\tau K)\\) \\(\\infty\\) \\(0\\) PI \\(T/(2\\tau K)\\) \\(T\\) \\(0\\) PID \\(T/(1.2\\tau K)\\) \\(T\\) \\(0.5\\tau\\) <p>Both method are same for \\(T_d\\) and P controller.</p>"},{"location":"Ctrl/ProcessControl/Intro/#others","title":"Others","text":"<ul> <li> <p>\u7ecf\u9a8c\u6cd5</p> </li> <li> <p>\u4e34\u754c\u6bd4\u4f8b\u5ea6\u6cd5</p> </li> </ul> <p>\u5728\u95ed\u73af\u4e0b\uff0c\u5148\u4f7f\u7528\u7eaf\u6bd4\u4f8b\u63a7\u5236\uff0c\u4ece\u5c0f\u5230\u5927\u8c03\u8282\\(K_C\\)\uff0c\u5bf9\u4e8e\u7ed9\u5b9a\\(K_C\\).\u4f7f\u7528\u5c0f\u5e45\u5ea6\u9636\u8dc3\u8f93\u5165\uff0c\u4f7f\u5f97\u7b49\u5e45\u632f\u8361\u4e0b\uff0c\u8bb0\u5f55\u5468\u671f\\(P_u\\)\u548c\u6b64\u65f6\u7684\u6bd4\u4f8b\u589e\u76ca\\(K_{Cmax}\\)\uff0c\u6c42\u51fa\u5bf9\u5e94\u53c2\u6570\u3002</p> <p>\u6ce8: \u8be5\u65b9\u6cd5\u4ee5\u5f97\u52304\uff1a1\u8870\u51cf\u6bd4\uff0c\u5408\u9002\u8d85\u8c03\u4e3a\u76ee\u6807\u3002</p> <ul> <li>\u8870\u51cf\u66f2\u7ebf\u6cd5: \u9488\u5bf9\u7eaf\u6bd4\u4f8b\u95ed\u73af\u4e0b\u5f97\u4e0d\u5230\u7b49\u5e45\u632f\u8361\u7684\u60c5\u51b5</li> </ul> <p>\u6ce8\uff1a\u548c\u7ecf\u9a8c\u6cd5\u7ed3\u5408\u4f7f\u7528</p>"},{"location":"Ctrl/ProcessControl/Nonlinear_Compensation/","title":"Nonlinear Compensation","text":""},{"location":"Ctrl/ProcessControl/Nonlinear_Compensation/#_1","title":"\u5bf9\u8c61\u975e\u7ebf\u6027","text":""},{"location":"Ctrl/ProcessControl/Nonlinear_Compensation/#_2","title":"\u7eaf\u6ede\u540e","text":""},{"location":"Ctrl/ProcessControl/PID/","title":"PID Controller","text":""},{"location":"Ctrl/ProcessControl/PID/#basic-concept","title":"Basic concept","text":"<p>\u65f6\u57df\u6307\u6807</p> <ul> <li>\u8870\u51cf\u6bd4\\(n=\\frac{B}{B'}\\)</li> </ul> <p>\\(B\\), \\(B'\\) \u662f\u76f8\u90bb\u4e24\u4e2a\u6ce2\u5cf0\u503c\u3002\u8981\u51cf\u53bb\u7a33\u6001\u503c\\(C\\).</p> <ul> <li>\u8d85\u8c03\u91cf\u4e0e\u6700\u5927\u52a8\u6001\u504f\u5dee</li> </ul> <p>\u8d85\u8c03\u91cf\uff1a</p> \\[ \\sigma=\\frac{B}{C} \\] <p>\u6700\u5927\u52a8\u6001\u504f\u5dee\u7528\u5728 \u7a33\u6001\u503c \\(C\\) \u6bd4\u8f83\u5c0f\u65f6\u5019</p> \\[ A=|B+C| \\] <ul> <li> <p>\u4f59\u5dee: \u8bbe\u5b9a\u503c\u548c\u7a33\u6001\u503c\u7684\u5dee \\(e(\\infty)=r-c(\\infty)=r-C\\)</p> </li> <li> <p>\u8c03\u8282\u65f6\u95f4 \\(t_s\\) : \u8868\u793a\u8fdb\u5165\u7a33\u6001\u503c\u9644\u8fd1 \\(5\\%\\)/\\(2\\%\\) (\u76f8\u5bf9\u4e8e\u65e7\u7a33\u6001\u503c)\u7684\u65f6\u95f4\uff0c\u5373 \\(y_\\infty \\pm 0.05|y_\\infty-y_0|\\)</p> </li> <li> <p>\u632f\u8361\u9891\u7387: \\(\\beta=\\frac{2\\pi}{T}\\) \u662f\u8c03\u8282\u65f6\u95f4\u7684\u5012\u6570\u3002</p> </li> <li> <p>\u5cf0\u503c\u65f6\u95f4: \u7b2c\u4e00\u6b21\u5230\u8fbe\u6700\u5927\u503c/\u6700\u5c0f\u503c\u7684\u65f6\u95f4 \\(t_p\\)</p> </li> <li> <p>\u4e0a\u5347\u65f6\u95f4\uff1a \u7b2c\u4e00\u6b21\u8fbe\u5230\u7a33\u6001\u503c\u7684\u65f6\u95f4 \\(t_r\\).</p> </li> </ul> <p>\u504f\u5dee\u79ef\u5206(IE)\u6027\u80fd\u6307\u6807</p> <ul> <li> <p>IE: \\(\\int_0^\\infty e dt\\) \u5bf9\u4e8e\u7b49\u5e45\u632f\u8361\u65e0\u6548\u3002</p> </li> <li> <p>IAE: \\(\\int_0^\\infty |e| dt\\)\uff0c\u5bf9\u4e8e\u5177\u6709\u8f83\u5feb\u7684\u8fc7\u6e21\u8fc7\u7a0b\u3001\u4e0d\u5927\u8d85\u8c03\u9002\u7528</p> </li> <li> <p>ISE: \\(\\int_0^\\infty e^2 dt\\), \u6291\u5236\u5927\u504f\u5dee\uff0c\u6570\u5b66\u597d\u7528</p> </li> <li> <p>ITAE: \\(\\int_0^\\infty t |e| dt\\), \u524d\u671f\u6291\u5236\u5c0f\uff0c\u540e\u671f\u5927\uff0c\u4ece\u800c\u524d\u671f\u504f\u5dee\u5927</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#p-control","title":"P control","text":"\\[ u(t)=K_C(y_{sp}(t)-y_m(t))+u_0 \\] <p>where \\(u_0\\) \u662f\u7a33\u6001\u5de5\u4f5c\u70b9\u3002\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ G_C(s)=\\frac{U(s)}{E(s)}=K_C \\] \\(K_C\\) \u4f59\u5dee \u8870\u51cf\u6bd4 \u7a33\u5b9a\u6027 \u53d8\u5927 \u53d8\u5c0f \u53d8\u5c0f \u4ece\u8d1f\u5b9e\u6839\u5230\u865a\u6839\u5230\u590d\u6839\uff0c\u53d8\u5dee <p>\u6ce8: \u59cb\u7ec8\u5b58\u5728\u4f59\u5dee\uff0c\u589e\u76ca\u8d8a\u5927\uff0c\u4f59\u5dee\u8d8a\u5c0f\u3002</p>"},{"location":"Ctrl/ProcessControl/PID/#pi-control","title":"PI control","text":"<p>\u79ef\u5206\u4f5c\u7528\uff1a</p> \\[ u(t)=u_0+\\int_0^t e(s)ds \\] <p>\u6bd4\u4f8b\u79ef\u5206\u4f5c\u7528\uff1a</p> \\[ u(t)=u_0+K_c\\left(e(t)+\\frac{1}{T_i}\\int_0^te(s)ds\\right) \\] <p>\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ G_C(s)=K_C(1+\\frac{1}{T_i s}) \\] <ul> <li>\u53ef\u6d88\u9664\u4f59\u5dee\uff0c\u76f8\u540c\u60c5\u51b5\u4e0b\uff0c\u6709\u4e86\u79ef\u5206\uff0c\u6bd4\u4f8b\u8981\u4e0b\u964d\uff0c\u8fd9\u5bfc\u81f4\u63a7\u5236\u7cbe\u5ea6\u4e0b\u964d</li> </ul> \\(K_C\\) \\(T_I\\) \u4f59\u5dee \u8870\u51cf\u6bd4 \u7a33\u5b9a\u6027 \u4e0d\u53d8 \u53d8\u5c0f 0 \u53d8\u5dee <ul> <li> <p>\u5e94\u7528\u573a\u5408\uff1a \u7cfb\u7edf\u52a8\u6001\u7279\u6027\u6bd4\u8f83\u5feb\uff08\u4e0d\u80fd\u7528\u5fae\u5206\uff09\uff0c\u5927\u60ef\u6027\u7cfb\u7edf\uff0c\u79ef\u5206\u4f5c\u7528\u4e0d\u80fd\u592a\u5f3a</p> </li> <li> <p>\u6ce8\u610f\u79ef\u5206\u9971\u548c\u95ee\u9898\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#pid-control","title":"PID control","text":"\\[ u(t)=u_0+K_c\\left(e(t) +\\frac{1}{T_i}\\int_0^te(s)ds+ T_d\\frac{d}{dt} e(t) \\right) \\] <p>\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ G_C(s)=K_C\\left(1+\\frac{1}{T_i s}+T_ds\\right) \\] <ul> <li> <p>\u7cfb\u7edf\u65f6\u95f4\u5e38\u6570\u51cf\u5c0f\uff0c</p> </li> <li> <p>\u9ad8\u9891\u4fe1\u53f7\u4e0d\u5b9c\u4f7f\u7528\u5fae\u5206\uff0c\u4f1a\u9020\u6210\u632f\u8361\uff0c\u5efa\u8bae\u5f15\u5165\u5fae\u5206\u524d\u5148\u4f7f\u7528\u4e00\u8282\u6ee4\u6ce2\u6216\u5e73\u5747\u6ee4\u6ce2\u3002</p> </li> <li> <p>\u5fae\u5206\u5bf9\u4e8e\u7eaf\u6ede\u540e\u6ca1\u6709\u4f5c\u7528</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#pid","title":"PID \u5e94\u7528\u573a\u5408","text":"\u63a7\u5236\u5668 P\u63a7\u5236 PI\u63a7\u5236 PID\u63a7\u5236 \u5e94\u7528\u573a\u5408 \u5de5\u827a\u8981\u6c42\u4e0d\u9ad8\uff0c\u5982\u50a8\u7f50\u6db2\u4f4d\uff08\u4fdd\u6301\u5728\u4e0a\u4e0b\u754c\u5185\u90fd\u53ef\uff0c\u4e0d\u8981\u6c42\u4f59\u5dee\u4e3a0\uff09 \u6ede\u540e\u5c0f\uff0c\u52a8\u6001\u7279\u6027\u597d\uff0c\u5982\u538b\u529b\u3001\u6d41\u91cf\u63a7\u5236 \u8d1f\u8f7d\u53d8\u5316\u5927\uff0c\u65f6\u95f4\u5e38\u6570\u5927\u7684\u7cfb\u7edf\uff0c\u5982\u6e29\u5ea6\u4e0e\u6210\u5206\u63a7\u5236"},{"location":"Ctrl/ProcessControl/PID/#pid_1","title":"\u6570\u5b57PID\u63a7\u5236\u5668","text":""},{"location":"Ctrl/ProcessControl/PID/#_1","title":"\u6570\u5b57\u6ee4\u6ce2\u5668","text":"<ul> <li>\u4e2d\u4f4d\u503c\u6ee4\u6ce2: \u8fde\u7eed\u91c7\u6837\u4e09\u6b21\uff0c\u4ece\u4e2d\u9009\u62e9\u5927\u5c0f\u5c45\u4e2d\u7684\u503c\u4f5c\u4e3a\u6709\u6548\u6d4b\u91cf\u4fe1\u53f7\u3002</li> </ul> <p>\u6ce8: \u5bf9\u4e8e\u6d41\u91cf\u68c0\u6d4b\u4e0d\u9002\u7528\uff0c\u56e0\u4e3a\u6d41\u91cf\u53d8\u5316\u592a\u5feb\u3002</p> <ul> <li>\u9012\u63a8\u5e73\u5747\u6ee4\u6ce2: \u9009\u62e9\u7b2ck\u6b21\u91c7\u6837\u4e4b\u524d\u7684N\u6b21\u91c7\u6837\u7684\u5e73\u5747</li> </ul> \\[ \\overline{y}(k)=\\frac{1}{N}\\sum_{i=0}^{N-1}y(k-i) \\] <p>\u6ce8: \\(N\\) \u8d8a\u5927\uff0c\u6ee4\u6ce2\u6548\u679c\u597d\uff0c\u4f46\u4f1a\u6709\u6ede\u540e</p> <p>\u6ce8: \u4e00\u822c\u6d41\u91cf \\(N=12\\), \u538b\u529b \\(N=4\\), \u6e29\u5ea6\u4e0d\u4f7f\u7528</p> <ul> <li> <p>\u52a0\u6743\u5e73\u5747\u6ee4\u6ce2: \u52a0\u6743\u7684\u9012\u63a8\u5e73\u5747\u6ee4\u6ce2\u3002</p> </li> <li> <p>\u4e00\u9636\u6ede\u540e\u6570\u5b57\u6ee4\u6ce2: \u53ea\u6709\u4e24\u9879\u7684\u52a0\u6743\u5e73\u5747\u6ee4\u6ce2\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#pid_2","title":"\u6570\u5b57PID\u63a7\u5236\u7b97\u6cd5","text":"<ul> <li>\u4f4d\u7f6e\u5f0f\u548c\u589e\u91cf\u5f0f</li> </ul> \u63a7\u5236\u7b97\u6cd5 \u4f4d\u7f6e\u5f0f \u589e\u91cf\u5f0f \u8bef\u5dee \u5bb9\u6613\u7d2f\u8ba1\u8bef\u5dee \u4e0d\u5bb9\u6613\u4ea7\u751f\u7d2f\u8ba1\u8bef\u5dee \u79ef\u5206\u9971\u548c \u5bb9\u6613\u9650\u5e45\uff0c\u9020\u6210\u79ef\u5206\u9971\u548c \u4e0d\u5bb9\u6613 <ul> <li> <p>\u4e0d\u5b8c\u5168\u5fae\u5206\uff1a\u5c06\u4e00\u9636\u4f4e\u901a\u6ee4\u6ce2\u5668\\(\\frac{1}{1+T_is}\\)\u52a0\u5230\u5fae\u5206\u73af\u8282\uff0c\u4f7f\u5f97\u5fae\u5206\u73af\u8282\u5206\u591a\u6b21\u8f93\u51fa\uff0c\u6bcf\u6b21\u8f93\u51fa\u5e45\u503c\u8f83\u5c0f\u3002\u76ee\u6807\u662f\u9632\u6b62\u9636\u8dc3\u5f88\u5927\u65f6\u5fae\u5206\u8f93\u51fa\u5f88\u5927\u3002</p> </li> <li> <p>\u5fae\u5206\u5148\u884c\uff1a\u5c06\u5fae\u5206\u73af\u8282\u53ea\u52a0\u5728\u6d4b\u91cf\u53d8\u9001\u73af\u8282\uff0c\u4e0d\u52a0\u5728\u8bbe\u5b9a\u503c\u540e\uff0c\u53ef\u4ee5\u9632\u6b62\u8bbe\u5b9a\u503c\u7a81\u53d8\u7684\u5f71\u54cd\u3002\u5bf9\u4e8e\u6e29\u5ea6\u63a7\u5236\u5668\u6bd4\u8f83\u5e38\u7528\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/PID/#_2","title":"\u6d41\u91cf\u63a7\u5236\u56de\u8def","text":"<ul> <li> <p>\u7279\u70b9: \u54cd\u5e94\u5feb, \u6d4b\u91cf\u566a\u58f0\u5927</p> </li> <li> <p>PI\u63a7\u5236\uff0c\u4e14\u6bd4\u4f8b\u5c0f\uff0c\u79ef\u5206\u5927</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/","title":"Control Strategy","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#cascade-control","title":"Cascade Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#control-with-intension","title":"Control with Intension","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#multi-variables-control","title":"Multi-Variables Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/#feedforward-control","title":"Feedforward Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/","title":"Cascade Control","text":"<ul> <li>\u65b9\u5757\u56fe</li> </ul> <p>\u5e38\u7528\u672f\u8bed</p> <ul> <li> <p>\u526f\u56de\u8def: \u968f\u52a8\u63a7\u5236</p> </li> <li> <p>\u4e3b\u56de\u8def: \u5b9a\u5236\u63a7\u5236</p> </li> </ul> <p>\u76f8\u6bd4\u4e8e\u4e00\u822c\u63a7\u5236\u56de\u8def\uff0c\u5f15\u5165\u4e86\u4e2d\u95f4 (\u526f) \u53d8\u91cf\uff0c\u4f46\u662f\u4ecd\u7136\u662f\u4e00\u4e2a\u5b9a\u503c\u63a7\u5236\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_1","title":"\u4e32\u7ea7\u63a7\u5236\u7684\u4f18\u70b9","text":"<p>\u4e32\u7ea7\u63a7\u5236\u7cfb\u7edf\u7684\u4f18\u70b9</p> <ul> <li>\u526f\u56de\u8def\u5feb\u901f\u8c03\u8282\uff0c\u6709\u6548\u514b\u670d\u4e8c\u6b21\u6270\u52a8\u3002</li> </ul> \\[ F'_{D2}(s)=\\frac{F_{D2}(s)}{1+G_{c2}G_vG_{p2}G_{m2} } \\] <p>\u4e00\u822c\u6765\u8bf4\uff0c\\(G_{c2}G_vG_{p2}G_{m2}\\gg 1\\), \u6545\u80fd\u964d\u4f4e\u5e72\u6270.</p> <ul> <li>\u6539\u5584\u5bf9\u8c61\u7684\u52a8\u6001\u7279\u6027</li> </ul> <p>\u7ecf\u8fc7\u63a7\u5236\uff0c\u526f\u5bf9\u8c61\u7684\u4f20\u9012\u51fd\u6570\u4e3a</p> \\[ \\begin{equation} G'_{p2}(s)=\\frac{G_{c2} G_v G_{p2} }{1+G_{c2}G_vG_{p2}G_{m2} }\\label{Prime-Object} \\end{equation} \\] <p>\u5982\u679c\u539f\u526f\u5bf9\u8c61\\(G_{p2}(s)=\\frac{K_{p2}}{1+T_{p2}s}\\), \u5219</p> \\[ \\begin{align*} G'_{p2}(s)&amp;=\\frac{G_{c2} G_v K_{p2} }{(1+T_{p2}s)+(G_{c2}G_vK_{p2}G_{m2})}\\\\ &amp;=\\frac{G_{c2} G_v K_{p2} }{1+G_{c2}G_vK_{p2}G_{m2}+T_{p2}s}\\\\ &amp;=\\frac{G_{c2} G_v \\frac{K_{p2}}{1+G_{c2}G_vK_{p2}G_{m2}} }{1+\\frac{T_{p2}}{1+G_{c2}G_vK_{p2}G_{m2}}s}\\\\ \\end{align*} \\] <p>which means </p> \\[ \\begin{align*} K_{p2}'&amp;=\\frac{G_{c2} G_v}{1+G_{c2}G_vK_{p2}G_{m2}}K_{p2}\\\\ T_{p2}'&amp;=\\frac{1}{1+G_{c2}G_vK_{p2}G_{m2}}T_{p2} \\end{align*} \\] <p>Usually \\(1+G_{c2}G_vK_{p2}G_{m2}\\gg 1\\), then \\(T_{p2}'&lt;T_{p2}\\). \u65f6\u95f4\u5e38\u6570\u51cf\u5c0f\uff0c\u52a0\u5feb\u4e86\u526f\u73af\u7684\u54cd\u5e94\u901f\u5ea6\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u5de5\u4f5c\u9891\u7387\u3002</p> <ul> <li>\u9c81\u68d2\u6027\u589e\u5f3a, \u80fd\u514b\u670d\u526f\u5bf9\u8c61\u589e\u76ca\u3001\u8c03\u8282\u9600\u7279\u6027\u53d8\u5316\u5bf9\u63a7\u5236\u6027\u80fd\u7684\u5f71\u54cd</li> </ul> <p>\u5728\u5f0f \\(\\ref{Prime-Object}\\) \u4e2d\uff0c\u82e5 \\(G_{c2}G_vK_{p2}G_{m2}\\gg 1\\), \u4e8e\u662f\u6709</p> \\[ G'_{p2}\\approx\\frac{G_{c2}G_vK_{p2}}{G_{c2}G_vK_{p2}G_{m2}}=\\frac{1}{G_{m2}} \\] <p>\u4e5f\u5c31\u662f\u8bf4\uff0c\u57fa\u672c\u4e0a\u526f\u5bf9\u8c61\u548c\u8c03\u8282\u9600 \\(G_v\\) , \u526f\u5bf9\u8c61\u589e\u76ca \\(G_{p2}\\) \u65e0\u5173\u3002\u4f46\u662f\u6211\u4eec\u7684\u63a7\u5236\u7cfb\u7edf\u5bf9\u4e8e \\(G_{m2}\\) \u6ca1\u6709\u9c81\u68d2\u6027\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_2","title":"\u526f\u53c2\u6570\u7684\u9009\u62e9","text":"<ul> <li> <p>\u4e3b\u526f\u5bf9\u8c61\u65f6\u95f4\u5e38\u6570\u8981\u9519\u5f00\u3002\u5373 \\(T_{02}&lt; T_{01}\\). \u5373\u526f\u56de\u8def\u65f6\u95f4\u5e38\u6570\u8981\u5c0f\uff0c\u4e00\u822c\u9009\u62e9\u6d41\u91cf\u3002\u4f46\u9664\u4e3b\u63a7\u5236\u5bf9\u8c61\u7684\u65f6\u95f4\u5e38\u6570\u4e0d\u80fd\u53d8\u4e4b\u5916\uff0c\u526f\u56de\u8def\u5e94\u8be5\u5305\u542b\u4e00\u4e9b\u4e3b\u56de\u8def\u4e4b\u5916\u7684\u5bf9\u8c61\uff0c\u4ee5\u52a0\u5feb\u8c03\u8282\u3002\u5f53\u7136\uff0c\u526f\u73af\u5305\u542b\u8d8a\u591a\u5bf9\u8c61\uff0c\u8c03\u8282\u4f1a\u8d8a\u6162\u3002</p> </li> <li> <p>\u5305\u542b\u5e72\u6270\u7684\u53c2\u6570\u8d8a\u591a\u8d8a\u597d\uff0c\u5c06\u975e\u7ebf\u6027\u5bf9\u8c61\u5f52\u4e8e\u526f\u56de\u8def\u3002</p> </li> <li> <p>\u526f\u53c2\u6570\u611f\u53d7\u7684\u5e72\u6270\u8981\u8d8a\u5feb\u8d8a\u597d</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_3","title":"\u526f\u63a7\u5236\u5668\u9009\u62e9","text":"<ul> <li> <p>\u5141\u8bb8\u4f59\u5dee\u3002\u4e00\u822c\u7528P (\u6e29\u5ea6) , PI (\u6d41\u91cf, \u538b\u529b)\u3002</p> </li> <li> <p>PI\u63a7\u5236\u5668\u8981\u5f3a\u6bd4\u4f8b+\u5f31\u79ef\u5206\u3002</p> </li> </ul> <p>\u6ce8: \u6db2\u4f4d\u4e32\u7ea7\u63a7\u5236\u4e3b\u63a7\u5236\u5668\u4e0d\u9009\u62e9PID, \u9009\u62e9PI.</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Cascade_Control/#_4","title":"\u4e3b\u526f\u63a7\u5236\u5668\u7684\u6297\u79ef\u5206\u9971\u548c","text":"<ul> <li> <p>\u526f\u63a7\u5236\u5668\u548c\u5e38\u89c4\u63a7\u5236\u5668\u4e00\u81f4\u3002</p> </li> <li> <p>\u4e3b\u63a7\u5236\u5668\u4e0d\u91c7\u7528\u526f\u63a7\u5236\u5668\u7684\u8f93\u5165\\(r_2=u_1\\), \u800c\u662f\u91c7\u7528\u526f\u5bf9\u8c61\u6d4b\u91cf\u503c \\(y_{m2}\\) \u4f5c\u4e3a\u79ef\u5206\u53cd\u9988\u503c\u3002\u6709\u4e24\u70b9\u89e3\u91ca\u3002</p> </li> </ul> Explanation <ul> <li> <p>\u5f53\u526f\u56de\u8def\u5931\u63a7\u65f6\uff0c\\(y_{m2}\\) \u548c \\(r_2\\) \u76f8\u4e92\u72ec\u7acb\uff0c\u4f7f\u5f97\u539f\u672c\u91c7\u7528 \\(r_2\\) \u6765\u79ef\u5206\u53cd\u9988\u7684\u56de\u8def\u56e0\u91c7\u7528 \\(y_{m2}\\) \u800c\u65ad\u5f00\u79ef\u5206\u4f5c\u7528\u3002</p> </li> <li> <p>\u53e6\u5916\uff0c\u5927\u6270\u52a8\u53bb\u9664\u540e\uff0c\u4e3b\u63a7\u5236\u5668\u504f\u5dee\u56de\u5230\u96f6\uff0c\u526f\u63a7\u5236\u5668\u504f\u5dee\u4e5f\u56de\u5230\u96f6\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/","title":"Control with Intension","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#uniform-control","title":"\u5747\u5300\u63a7\u5236 | Uniform Control","text":"<ul> <li> <p>\u539f\u56e0: \u6d41\u91cf\u8c03\u8282\u8f83\u4e3a\u8fc5\u901f\u548c\u5267\u70c8\u3002</p> </li> <li> <p>\u76ee\u6807: \u517c\u987e\u6d41\u91cf\u5e73\u7a33\u3001\u6db2\u4f4d\u5728\u5141\u8bb8\u533a\u95f4\u6ce2\u52a8\u3002</p> </li> <li> <p>\u5b9e\u73b0\u624b\u6bb5: \u8c03\u8282PID\u63a7\u5236\u5668\u7684\u53c2\u6570</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_1","title":"\u5747\u5300\u63a7\u5236\u7684\u5b9e\u73b0","text":"<ul> <li>\u4e00\u822c\u6765\u8bf4\uff0c\u7eaf \"\u6db2\u4f4d-\u6d41\u91cf\" \u63a7\u5236\u91c7\u7528\u5355\u56de\u8def\u6216\u8005\u4e32\u7ea7\u63a7\u5236\u56de\u8def\u3002\u5747\u5300\u63a7\u5236\u5728\u7ed3\u6784\u4e0a\u662f\u548c\u4e00\u822c\u63a7\u5236\u4e00\u81f4\u3002</li> </ul> <p>\u63a7\u5236\u5668\u53c2\u6570\u6574\u5b9a</p> <p>\u5747\u5300\u63a7\u5236\u7684\u63a7\u5236\u5668\u63a7\u5236\u4f5c\u7528\u5f31\u3002\u53cd\u6620\u5728\u63a7\u5236\u5668\u53c2\u6570\u4e0a\uff0c\u8981\u6c42</p> <ul> <li> <p>\u5bbd\u7684\u6bd4\u4f8b\u5ea6\uff0c\u6bd4\u4f8b\u589e\u76ca\u5c3d\u53ef\u80fd\u5c0f\u3002</p> </li> <li> <p>\u5927\u7684\u79ef\u5206\u65f6\u95f4\u3002</p> </li> <li> <p>\u4e0d\u4f7f\u7528\u5fae\u5206</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_2","title":"\u53cc\u51b2\u91cf\u5747\u5300\u63a7\u5236","text":"<p>\u4e24\u4e2a\u6d4b\u91cf\u91cf\uff0c\u5e76\u4f5c\u76f8\u51cf\u8fd0\u7b97\u540e\u4f5c\u4e3a\u6d4b\u91cf\u53d8\u9001\u3002\u63a7\u5236\u7684\u662f\u4e24\u8005\u7684\u5dee\u6052\u5b9a\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#ratio-control","title":"\u6bd4\u503c\u63a7\u5236 | Ratio Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_3","title":"\u5b9a\u6bd4\u503c","text":"<p>\u5206\u7c7b</p> <p>\u6839\u636e\u95ed\u73af\u6570\u76ee\u6765\u5206\u7c7b\u3002</p> <ul> <li> <p>\u5f00\u73af\u6bd4\u503c\u63a7\u5236</p> </li> <li> <p>\u5355\u95ed\u73af\u6bd4\u503c\u63a7\u5236</p> </li> <li> <p>\u53cc\u95ed\u73af\u6bd4\u503c\u63a7\u5236</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_4","title":"\u53d8\u6bd4\u503c\u63a7\u5236","text":"<p>\u4ece\u63a7\u5236\u8d28\u91cf\u6765\u8bf4\uff0c\u5b9a\u6bd4\u503c\u63a7\u5236\u662f\u7406\u8bba\u4e0a\u7684\uff0c\u5f00\u73af\u63a7\u5236\uff0c\u6ca1\u6709\u8003\u8651\u5176\u4ed6\u5e72\u6270 (\u5982\u6e29\u5ea6\u3001\u538b\u529b\u3001\u6210\u5206\u4ee5\u53ca\u53cd\u5e94\u5668\u7684\u50ac\u5316\u5242\u8001\u5316\u95ee\u9898)</p> <p>\u6309\u7167\u67d0\u4e00\u5de5\u827a\u6307\u6807\u6765\u4fee\u6b63\u6bd4\u503c\u7cfb\u6570\u3002</p> <p>\u6ce8: \u4e5f\u53eb\u4e32\u7ea7\u6bd4\u503c\u63a7\u5236 (\u9759\u6001\u524d\u9988-\u4e32\u7ea7\u63a7\u5236\u7cfb\u7edf)\u3002</p> Explanation <ul> <li> <p>\u4e3b\u6d41\u91cf (\u53ef\u89c6\u4f5c\u5e72\u6270\u901a\u9053) \u5bf9\u526f\u6d41\u91cf\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u9759\u6001\u524d\u9988\u6d88\u9664\u3002</p> </li> <li> <p>\u526f\u6d41\u91cf\u81ea\u8eab\u7684\u5e72\u6270\uff0c\u901a\u8fc7\u526f\u56de\u8def\u52a0\u4ee5\u63a7\u5236\u3002</p> </li> <li> <p>\u6e29\u5ea6\u7684\u5f71\u54cd\uff0c\u6539\u53d8\u526f\u56de\u8def\u63a7\u5236\u5668\u7684\u8bbe\u5b9a\u503c\u3002</p> </li> </ul> <p>\u53ea\u662f\u8fd9\u91cc\u7684\u8bbe\u5b9a\u503c\u6070\u597d\u662f\u6bd4\u503c\u3002\u6545\u53c8\u53eb\u6bd4\u503c\u63a7\u5236\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_5","title":"\u4e3b\u526f\u6d41\u91cf\u9009\u62e9","text":"<ul> <li>\u4e3b\u6d41\u91cf: \u8d77\u4e3b\u5bfc\u4f5c\u7528\u3001\u53ef\u6d4b\u4e0d\u53ef\u63a7 (\u5e72\u6270)\u3001\u6602\u8d35\u7684\u7269\u6599</li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_6","title":"\u6bd4\u503c\u7cfb\u6570\u7684\u786e\u5b9a","text":"<ul> <li>\u533a\u5206: \u6d41\u91cf\u6bd4\u503c \\(k\\) \u548c\u4eea\u8868\u6bd4\u503c \\(K\\).</li> </ul> <p>\u6298\u7b97\u65b9\u6cd5</p> <p>\u4e00\u822c\u6d41\u91cf\u7684\u6d4b\u91cf\u53d8\u9001\u4e3a\u5dee\u538b\u53d8\u9001\u5668\u3002\u5982\u679c</p> <p>(i) \u7ecf\u8fc7\u5f00\u65b9\u5668\uff0c\u5219\u6d41\u91cf\u6d4b\u91cf\u53d8\u9001\u503c(\u7535\u4fe1\u53f7, \\(I\\))\u548c\u771f\u5b9e\u6d41\u91cf \\(Q\\) \u4e4b\u95f4\u4e3a\u7ebf\u6027\u5173\u7cfb\u3002\u8bb0\u53d8\u9001\u503c\u91cf\u7a0b \\([I_a, I_b]\\), \u6d41\u91cf\u91cf\u7a0b \\([Q_a,Q_b]\\), \u4e00\u822c\u8ba4\u4e3a\\(Q_a=0\\), \u5219</p> \\[ I=\\frac{Q-Q_a}{(Q_b-Q_a)}(I_b-I_a)+I_a=\\frac{Q}{Q_b}(I_b-I_a)+I_a \\] <p>\u4e3b\u526f\u6d41\u91cf\u5206\u522b\u7528\u4e0b\u6807 \\(1\\), \\(2\\) \u8868\u793a\uff0c\u5219 \u4e24\u4e2a\u5dee\u538b\u53d8\u9001\u5668\u7684\u91cf\u7a0b\u6700\u5927\u503c\u5206\u522b\u4e3a \\(Q_{b1}\\), \\(Q_{b2}\\), \u7531 \\(k\\overset{\\Delta}{=}\\frac{Q_2}{Q_1}\\) (\u4e00\u822c\u4e3a\u526f/\u4e3b\uff0c\u7531\u4e3b\u6d41\u91cf\u4e58\u4ee5\u6bd4\u503c\u7cfb\u6570),</p> \\[ \\begin{align*} K&amp;\\overset{\\Delta}{=}\\frac{I_2-I_a}{I_1-I_a}\\\\ &amp;=\\frac{Q_2}{Q_1}\\frac{Q_{b1}}{Q_{b2}}\\\\ &amp;=k\\frac{Q_{b1}}{Q_{b2}} \\end{align*} \\] <p>(ii) \u4e0d\u7ecf\u8fc7\u5f00\u65b9\u5668\uff0c\u5219\u6d41\u91cf\u6d4b\u91cf\u53d8\u9001\u503c(\u7535\u4fe1\u53f7, \\(I\\))\u548c\u771f\u5b9e\u6d41\u91cf \\(Q\\) \u4e4b\u95f4\u4e3a\u5e73\u65b9\u5173\u7cfb\uff0c\u5373</p> \\[ I=\\frac{Q^2}{Q_b^2}(I_b-I_a)+I_a \\] <p>\u6b64\u65f6\u6d41\u91cf\u6bd4\u503c\u4ecd\u7136\u662f\u4e00\u6b21\u5f0f\u4e4b\u6bd4 \\(k\\overset{\\Delta}{=}\\frac{Q_2}{Q_1}\\), \u4ece\u800c</p> \\[ \\begin{align*} K&amp;\\overset{\\Delta}{=}\\frac{I_2-I_a}{I_1-I_a}\\\\ &amp;=\\frac{Q^2_2}{Q^2_1}\\frac{Q^2_{b1}}{Q^2_{b2}}\\\\ &amp;=k^2\\frac{Q^2_{b1}}{Q^2_{b2}} \\end{align*} \\] <p>\u6b64\u65f6\u7684\u653e\u5927\u500d\u6570\\(K_m=\\frac{dI}{dQ}=2(I_b-I_a)\\frac{Q}{Q_b^2}\\), \u975e\u6052\u5b9a\u503c\u3002\u800c\u52a0\u5f00\u65b9\u5668\uff0c\u653e\u5927\u7cfb\u6570\u4e3a\u6052\u5b9a\u503c\uff0c\u4e0d\u968f\u8d1f\u8f7d\u53d8\u5316\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_7","title":"\u52a8\u6001\u8ddf\u8e2a","text":"<p>\u5982\u679c\u5e0c\u671b\u52a8\u6001\u8ddf\u8e2a\uff0c\u4f7f\u5f97\u526f\u6d41\u91cf\u80fd\u591f\u968f\u65f6\u8ddf\u968f\u4e3b\u6d41\u91cf\u7684\u53d8\u5316\u800c\u53d8\u5316\uff0c\u5219\u53ef\u4ee5\u5c1d\u8bd5\u5728\u5f00\u73af\u7ed3\u6784 (\u4e00\u822c\u4e0d\u518d\u5df2\u7ecf\u95ed\u73af\u7684\u7ed3\u6784\u91cc\u518d\u52a0\u8865\u507f\u5668, \u5f53\u7136\u53cc\u95ed\u73af\u80fd\u7528) \u4e2d\u52a0\u5165\u4e00\u4e2a\u8865\u507f\u5668 \\(G_z\\)\u3002\u5176\u4e8e\u4e3b\u6d41\u91cf\u53d8\u9001\u4fe1\u53f7\u8fdb\u5165\u4e58\u6cd5\u5668\u4e4b\u524d\u8fdb\u884c\u3002</p> <p>\u5229\u7528 \\(Q_1\\) \u5bf9 \\(Q_2\\) \u7684\u5f71\u54cd</p> \\[ \\frac{Q_2}{Q_1}=\\frac{G_{m1}G_z G_{p2}}{1+G_{m2}G_{p2}}=k \\] <p>\u4ece\u800c\u6c42\u5f97 \\(G_z\\).</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_8","title":"\u6bd4\u503c\u5b9e\u73b0\u65b9\u6848","text":"<ul> <li> <p>\u4e58\u6cd5: \u5e38\u7528</p> </li> <li> <p>\u9664\u6cd5: \u4f1a\u5177\u6709\u5f88\u5f3a\u7684\u975e\u7ebf\u6027\u3002\u5bf9\u4e8e\u5f00\u73af\u9664\u6cd5\u5b9e\u73b0\u7684\u6bd4\u503c\u63a7\u5236\uff0c\u5176\u56de\u8def (\u526f\u6d41\u91cf) \u589e\u76ca\u5728\u53d8\u5316\u3002\u5047\u8bbe\u4f7f\u7528\u4e86\u5f00\u65b9\u5668\uff0c\u5219 \\(K=\\frac{Q_2}{Q_1}\\frac{Q_{b1}}{Q_{b2}}\\)</p> </li> </ul> \\[ \\frac{dK}{dQ_2}=\\frac{1}{Q_1}\\frac{Q_{b1}}{Q_{b2}}=\\frac{k}{Q_2}\\frac{Q_{b1}}{Q_{b2}} \\] <p>\u9759\u6001\u589e\u76ca\u4f1a\u53d8\u5316\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Control_Intension/#_9","title":"\u53cc\u4ea4\u53c9\u6bd4\u503c\u63a7\u5236: \u903b\u8f91\u63d0\u964d\u95ee\u9898","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/","title":"Feedforward Control (FFC)","text":"<p>\u6838\u5fc3: \u5bf9\u5e72\u6270\u4fe1\u53f7\u505a\u4e00\u5b9a\u5904\u7406\uff0c\u9001\u5230\u8c03\u8282\u9600\u3002\u4e4b\u540e\u7684\u53d8\u79cd\u524d\u9988\u5219\u662f\u5e7f\u4e49\u4e0a\u7684\u3002\u53ea\u8981\u6ca1\u6709\u5bf9\u88ab\u63a7\u53d8\u91cf\u8fdb\u884c\u76f4\u63a5\u68c0\u6d4b\uff0c\u5c31\u662f\u5f00\u73af\u63a7\u5236\u3002</p> <p>\u4e00\u822c\u5730\uff0c\u6709</p> \\[ G_{ff}=-\\frac{G_{pd}}{G_{v}G_{pc}G_{md}} \\] <p>\u6ce8: \u9009\u62e9\u7684\u5e72\u6270\u8981\u548c\u8c03\u8282\u9600\u76f8\u72ec\u7acb\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#_1","title":"\u7406\u8bba\u57fa\u7840: \u4e0d\u53d8\u6027\u539f\u7406","text":"<p>\u4e0d\u53d8\u6027\u539f\u7406</p> <ul> <li>\u7edd\u5bf9\u4e0d\u53d8\u6027 (\u52a8\u6001\u4e0d\u53d8\u6027)</li> </ul> <p>\u4f7f\u7528\u7ebf\u6027\u524d\u9988\u6765\u5efa\u6a21\uff0c\u6709</p> \\[ G_{ff}=-\\frac{G_{pd}}{G_{v}G_{pc}G_{md}} \\] <p>\u4e14 \\(G_{pd}(s)=\\frac{K_{pd}}{1+T_{pd}s}e^{-\\tau_{pd}s}\\), \\(G_{pc}(s)=\\frac{K_{pc}}{1+T_{pc}s}e^{-\\tau_{pc}s}\\), \\(G_{v}G_{md}=K_{m}\\) \u5219</p> \\[ G_{ff}=-K_{ff}\\frac{T_{pc}s+1}{T_{pd}s+1}e^{-\\tau_{ff} s} \\] <p>\u5176\u4e2d \\(K_{ff}=\\frac{K_{pd}}{K_{pc}k_{m}}\\), \\(\\tau_{ff}=\\max \\{0,\\tau_{pd}-\\tau_{pc}\\}\\).</p> <ul> <li>\u7a33\u6001\u4e0d\u53d8\u6027: \u9759\u6001\u524d\u9988\u7cfb\u7edf</li> </ul> \\[ K_{ff}(0)=-\\frac{K_{pd}}{K_{v}K_{pc}K_{md}} \\] <p>\u5bb9\u6613\u7528\u6bd4\u503c\u5668\u3001\u52a0\u6cd5\u5668\u5b9e\u73b0</p> <ul> <li> <p>\u8bef\u5dee\u4e0d\u53d8\u6027: w\u6270\u52a8\u5b58\u5728\u65f6\uff0c\u8f93\u51fa\u548c\u9884\u5b9a\u503c\u6709\u5fae\u5c0f\u7684\u504f\u5dee</p> </li> <li> <p>\u9009\u62e9\u4e0d\u53d8\u6027</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#_2","title":"\u4e0e\u53cd\u9988\u63a7\u5236\u7684\u5bf9\u6bd4","text":"\u63a7\u5236\u65b9\u5f0f \u524d\u9988\u63a7\u5236 \u53cd\u9988\u63a7\u5236 \u68c0\u6d4b\u4fe1\u53f7 \u5e72\u6270\u91cf \u88ab\u63a7\u53d8\u91cf \u63a7\u5236\u4f5c\u7528\u53d1\u751f\u65f6\u95f4 \u5e72\u6270\u4f5c\u7528\u4e8e\u88ab\u63a7\u5bf9\u8c61\u524d \u5e72\u6270\u4f5c\u7528\u4e8e\u88ab\u63a7\u5bf9\u8c61\u4e4b\u540e \u5f00/\u95ed\u73af \u5f00\u6000 \u95ed\u73af\uff0c\u4f1a\u6709\u7a33\u5b9a\u6027\u95ee\u9898 \u611f\u53d7\u7684\u6270\u52a8\u4e2a\u6570 \u53ef\u6d4b\u91cf\u7684\u5e72\u6270\uff0c\u6709\u9650\u4e2a \u5305\u542b\u5f71\u54cd\u88ab\u63a7\u53d8\u91cf\u7684\u6240\u6709\u5e72\u6270 \u5bf9\u65f6\u53d8\u3001\u975e\u7ebf\u6027\u5bf9\u8c61\u7684\u9002\u5e94\u6027 \u4e0d\u591f\u5f3a\uff0c\u6a21\u578b\u5f80\u5f80\u65f6\u4e0d\u53d8\u3001\u7ebf\u6027\uff0c\u5bf9\u4e8e\u6b64\u7c7b\u4fe1\u53f7\u4e0d\u591f\u9c81\u68d2 \u5177\u6709\u9c81\u68d2\u6027"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#-","title":"\u524d\u9988-\u53cd\u9988\u63a7\u5236","text":"<p>\u7531\u4e8e\u6709\u53cd\u9988\u4f5c\u7528\uff0c\u6211\u4eec\u4f7f\u7528\u7ebf\u6027\u53e0\u52a0\u7684\u65b9\u6cd5\u6c42\u5f97\u4f20\u9012\u51fd\u6570. \u4f46\u6700\u540e\u7ed3\u8bba\u662f\u4e00\u81f4\u7684\u3002</p> <p> </p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Feedforward_Control/#-_1","title":"\u524d\u9988-\u4e32\u7ea7\u63a7\u5236","text":"<p>\u4e3e\u4e00\u4e2a\u6362\u70ed\u5668\u7684\u524d\u9988\u4e32\u7ea7\u63a7\u5236</p> <p> </p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/","title":"Multi-Variables Control","text":""},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#selection-control","title":"\u9009\u62e9\u63a7\u5236 | Selection Control","text":"<p>\u53c8\u53eb\u7ea6\u675f\u63a7\u5236\u3001\u8d85\u9a70\u63a7\u5236 (\u5bf9\u6781\u9650\u7684\u63a7\u5236)</p> <p>\u76ee\u6807: \u4e3a\u5b9e\u73b0\u8f6f\u4fdd\u62a4\u800c\u8bbe\u8ba1</p> <p>\u6838\u5fc3\u70b9: \u4e00\u4e2a\u64cd\u4f5c\u53d8\u91cfMV, \u4e24\u4e2a\u53ca\u4ee5\u4e0a\u7684\u88ab\u63a7\u53d8\u91cfCV. \u4e00\u4e2a\u662f\u5e38\u89c4CV, \u8981\u7cbe\u786e\u63a7\u5236, \u5269\u4f59\u7684\u662f\u8f85\u52a9CV, \u8981\u63a7\u5236\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u4ee5\u786e\u4fdd\u5b89\u5168\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_1","title":"\u9ad8\u4f4e\u9009\u62e9\u5668","text":"<p>\u6839\u636e\u8d85\u9650\u65f6\uff0c\u9600\u95e8\u8f93\u5165\u503c\u7684\u53d8\u5316\u6765\u786e\u5b9a\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_2","title":"\u9632\u79ef\u5206\u9971\u548c","text":"<p>\u603b\u6709\u4e00\u53f0\u5904\u4e8e\u5f00\u73af\u72b6\u6001\u3002</p> <p>\u6211\u4eec\u9009\u62e9 \u9650\u5e45\u6cd5\u3001\u5916\u53cd\u9988\u6cd5\u3001\u79ef\u5206\u5207\u9664\u6cd5 \u4e2d\u7684 \u5916\u53cd\u9988\u6cd5 (RFB)\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_3","title":"\u5e94\u7528","text":"<p>(i) \u88ab\u63a7\u53d8\u91cf\u7684\u6d4b\u91cf\u503c\u7684\u9009\u62e9</p> <ul> <li> <p>\u56fa\u5b9a\u5e8a\u53cd\u5e94\u5668\u7684\u70ed\u70b9\u6e29\u5ea6\uff0c\u6709\u591a\u4e2a\u6d4b\u91cf\u70b9\uff0c\u9009\u62e9\u9ad8\u8005</p> </li> <li> <p>\u4e24\u53f0\u6210\u5206\u5206\u6790\u4eea\u8868\uff0c\u9009\u62e9\u4e24\u8005\u6d4b\u91cf\u8005\u7684\u9ad8\u8005\u4f5c\u4e3a\u53cd\u9988</p> </li> </ul> <p>(ii) \u53d8\u7ed3\u6784\u63a7\u5236</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#split-range-control","title":"\u5206\u7a0b\u63a7\u5236 | Split-range Control","text":"<p>\u4e00\u4e2a\u63a7\u5236\u5668\u8f93\u51fa\u4fe1\u53f7\uff0c\u63a7\u5236\u591a\u4e2a\u9600\u95e8\uff0c\u4e0d\u540c\u7684\u4fe1\u53f7\u533a\u95f4\uff0c\u5bf9\u6620\u4e0d\u540c\u7684\u9600\u95e8\u88ab\u63a7\u5236\u3002\u4e0d\u4f1a\u540c\u65f6\u63a7\u5236\u591a\u4e2a\u9600\u95e8</p> <p>\u6838\u5fc3\u70b9: \u4e00\u4e2a\u88ab\u63a7\u53d8\u91cfCV, \u4e24\u4e2a\u53ca\u4ee5\u4e0a\u64cd\u4f5c\u53d8\u91cfMV\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_4","title":"\u53cc\u63a7\u5236\u9600\u7684\u5206\u7a0b\u7ec4\u5408","text":"<ul> <li> <p>\u8fde\u7eed\u5206\u7a0b</p> </li> <li> <p>\u95f4\u9699\u5206\u7a0b: \u6709\u6b7b\u533a\uff0c\u63a7\u5236\u5668\u6ca1\u6709\u63a7\u5236\u4efb\u4f55\u4e00\u4e2a\u9600\u95e8</p> </li> <li> <p>\u91cd\u53e0\u5206\u7a0b: \u6709\u91cd\u53e0\u533a\uff0c\u63a7\u5236\u5668\u540c\u65f6\u63a7\u5236\u4e24\u4e2a\u9600\u95e8\uff1b100%\u91cd\u53e0\u533a\uff0c\u53eb\u505a\u4e24\u4e2a\u9600\u7684\u5e76\u8054\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_5","title":"\u5b9e\u73b0","text":"<p>\u4f7f\u7528\u6bd4\u503c\u5668+\u8d1f\u8377\u5206\u914d\u5668</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#_6","title":"\u76ee\u7684","text":"<ul> <li>\u6269\u5927\u63a7\u5236\u9600\u7684\u53ef\u8c03\u8303\u56f4</li> </ul> <p>\u6d89\u53ca\u5230\u4e00\u4e2a\u589e\u76ca\u7a81\u53d8\u95ee\u9898\u3002\u9700\u8981\u6362\u7b97\u3002\u6839\u636e\u56fe\u50cf\u7684\u8d77\u70b9\u3001\u7ec8\u70b9\uff0c\u6839\u636e\u5206\u7a0b\u7eb5\u5750\u6807 \\(Q_{Amax}\\) \u6c42\u53d6\u76f4\u7ebf\u5bf9\u5e94\u70b9\u7684\u6a2a\u5750\u6807\u3002</p> <ul> <li>\u6ee1\u8db3\u64cd\u4f5c\u5de5\u827a\u7684\u7279\u6b8a\u8981\u6c42 (\u6c14\u5f00\u3001\u6c14\u5173\u7ec4\u5408)</li> </ul> <p>\u95f4\u6b47\u653e\u70ed\u53cd\u5e94\u5668: \u8003\u8651\u521d\u6001\u4fe1\u53f7\u3001\u7ec8\u6001\u4fe1\u53f7\u3002\u4e24\u8005\u589e\u76ca\u4e0d\u540c\uff0c\u9020\u6210\u975e\u7ebf\u6027\uff0c\u5982\u679c\u51b7\u5374\u90e8\u5206\u589e\u76ca\u662f\u52a0\u70ed\u90e8\u5206\u7684 \\(1/3\\)\uff0c\u5219\u53ef\u4ee5\u589e\u52a0\u51b7\u5374\u90e8\u5206\u7684\u659c\u7387\u3002</p> <p>\u50a8\u6c2e\u5206\u7a0b\u63a7\u5236\u3002\u4e3a\u907f\u514d\u4e24\u9600\u95e8\u9891\u7e41\u5f00\u95ed\uff0c\u5728\u4e2d\u95f4\u8bbe\u7f6e\u4e00\u4e2a\u95f4\u6b47\u533a (\u6b7b\u533a)\u3002</p>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#valve-control","title":"\u9600\u4f4d\u63a7\u5236 | Valve Control","text":"<p>\u4e5f\u662f\u591a\u4e2a\u64cd\u7eb5\u53d8\u91cf\uff0c\u4e00\u4e2a\u88ab\u63a7\u53d8\u91cf\u3002</p> <p>\u4e3b\u8981\u662f\u8981\u8003\u8651\u4e24\u4e2a\u9600\u95e8\u5730\u4f4d\u7684\u4e0d\u5747\u7b49\u3002\u65e2\u8003\u8651\u7ecf\u6d4e\u6027\u3001\u5408\u7406\u6027 (\u4e00\u4e2a\u64cd\u7eb5\u53d8\u91cf)\uff0c\u53c8\u8003\u8651\u5feb\u901f\u6027\u548c\u6709\u6548\u6027 (\u53e6\u4e00\u4e2a\u64cd\u7eb5\u53d8\u91cf)\u3002</p> <ul> <li> <p>\u4e3b\u56de\u8def\u9009\u62e9: \u9009\u62e9\u63a7\u5236\u54c1\u8d28\u597d\u7684\uff0c\u52a8\u6001\u6548\u679c\u597d\u7684\u9600\u95e8\u4f5c\u4e3a\u4e3b\u56de\u8def\u63a7\u5236\u5668\u7684\u8f93\u51fa\u63a7\u5236\u9600\u3002\u8bbe\u5b9a\u503c\u662f\u6e29\u5ea6\u6307\u6807\u3002</p> </li> <li> <p>\u9600\u4f4d\u63a7\u5236\u5668: \u9009\u62e9\u5408\u7406\u3001\u7ecf\u6d4e\u7684\u9600\u4f5c\u4e3a\u9600\u4f4d\u63a7\u5236\u5668\u7684\u63a7\u5236\u9600\u3002\u8ba9\u4e3b\u56de\u8def\u7684\u9600\u95e8\u7684\u5f00\u5ea6\u6210\u4e3a\u9600\u4f4d\u63a7\u5236\u5668\u7684\u88ab\u63a7\u53d8\u91cf (\u8981\u4f5c\u4e3a\u6d4b\u91cf\u53d8\u9001\u8f93\u5165), \u8bbe\u5b9a\u503c\u662f\u4e3b\u56de\u8def\u9600\u95e8\u7684\u8bbe\u5b9a\u503c\uff0c\u4e00\u822c\u6bd4\u8f83\u5c0f\uff0c\u4ee5\u5b9e\u73b0\u7ecf\u6d4e\u6027\u3002\u901a\u8fc7\u9600\u4f4d\u63a7\u5236\u5668\u7684\u53c2\u6570\u6574\u5b9a\uff0c\u7528\u5bbd\u6bd4\u4f8b\u5ea6\u548c\u5927\u7684\u79ef\u5206\u65f6\u95f4\uff0c\u5b9e\u73b0\u7f13\u6162\u7684\u63a7\u5236\u3002</p> </li> </ul>"},{"location":"Ctrl/ProcessControl/Control_Strategy/Multi_Control/#review-of-the-above-control-strategy","title":"Review of the above control strategy","text":"\u63a7\u5236\u65b9\u5f0f \u9009\u62e9\u63a7\u5236 \u5206\u7a0b\u63a7\u5236 \u9600\u4f4d\u63a7\u5236 \u88ab\u63a7\u53d8\u91cfCV (\u6d4b\u91cf\u53d8\u9001\u7684\u53d8\u91cf) \\(\\geq 2\\) \\(1\\) \\(1\\) \u64cd\u7eb5\u53d8\u91cfMV (\u9600\u95e8\u4e2a\u6570) \\(1\\) \\(\\geq 2\\) \u5730\u4f4d\u7b49\u4ef7 \\(\\geq 2\\) \u5730\u4f4d\u4e0d\u7b49\u4ef7"},{"location":"Ctrl/Sensing%26Detection/","title":"Sensing &amp; Detection","text":""},{"location":"Ctrl/Sensing%26Detection/#outline","title":"Outline","text":""},{"location":"Ctrl/Sensing%26Detection/#calculation","title":"Calculation","text":"<ul> <li> <p>\u5e94\u53d8\u7247\uff0c\u6e29\u5ea6\u8865\u507f</p> </li> <li> <p>\u70ed\u7535\u5076\u8865\u507f\u5bfc\u7ebf\u3001\u7535\u6865\u8ba1\u7b97</p> </li> <li> <p>\u68c0\u6d4b\u4eea\u8868\uff08\u538b\u529b\u68c0\u6d4b\u4eea\u8868\uff09\u7684\u51c6\u786e\u5ea6\u7b49\u7ea7\u3001\u91cf\u7a0b\u9009\u62e9</p> </li> <li> <p>\u7269\u4f4d\u6d4b\u91cf\uff0c\u5dee\u538b\u5f0f\u6db2\u4f4d\u8ba1\u7684\u91cf\u7a0b\u8fc1\u79fb</p> </li> <li> <p>\u8282\u6d41\u5f0f\u6d41\u91cf\u8ba1\uff0c\u5e26\u5f00\u65b9\u5668\u548c\u4e0d\u5e26\u5f00\u65b9\u5668</p> </li> </ul>"},{"location":"Ctrl/Sensing%26Detection/Midterm/","title":"Midterm Exam","text":""},{"location":"Ctrl/Sensing%26Detection/Midterm/#_1","title":"\u7b80\u7b54\u9898","text":""},{"location":"Ctrl/Sensing%26Detection/Midterm/#_2","title":"\u201c\u5dee\u52a8\u201d\u548c\u201c\u53c2\u6bd4\u201d\u8bbe\u8ba1\u65b9\u6cd5","text":"\u7b80\u8981\u6bd4\u8f83\u4e00\u4e0b\u201c\u5dee\u52a8\u5f0f\u201d\u548c\u201c\u53c2\u6bd4\u5f0f\u201d\u4e24\u79cd\u68c0\u6d4b\u4eea\u8868\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5f02\u540c\u70b9\u3002  <p>\u5f02\uff1a</p> <ul> <li>\u5dee\u52a8\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u8f6c\u6362\u5143\u4ef6\u540c\u65f6\u611f\u53d7\u654f\u611f\u5143\u4ef6\u7684\u8f93\u51fa\u91cf\uff0c\u5e76\u628a\u5b83\u8f6c\u6362\u6210\u4e24\u4e2a\u6027\u8d28\u76f8\u540c\uff0c\u4f46\u6cbf\u76f8\u53cd\u65b9\u5411\u53d8\u5316\u7684\u7269\u7406\u91cf\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u5f97\u6709\u6548\u8f93\u51fa\u4fe1\u53f7\u63d0\u9ad8\u4e00\u500d\uff0c\u4fe1\u566a\u6bd4\u5f97\u5230\u6539\u5584\uff0c\u975e\u7ebf\u6027\u8bef\u5dee\u51cf\u5c0f\uff1b\u6613\u4e8e\u5b9e\u73b0\u521d\u59cb\u72b6\u6001\uff08\u201c\u96f6\u201d\u8f93\u5165\uff09\u7684\u96f6\u8f93\u51fa\uff0c\u80fd\u6d88\u9664\u90e8\u5206\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\u3002</li> <li>\u53c2\u6bd4\u5f0f\u662f\u91c7\u7528\u4e24\u4e2a\u6027\u80fd\u5b8c\u5168\u76f8\u540c\u7684\u68c0\u6d4b\u5143\u4ef6\uff0c\u4ed6\u4eec\u540c\u65f6\u611f\u53d7\u73af\u5883\u6761\u4ef6\u91cf\uff0c\u4f46\u53ea\u6709\u4e00\u4e2a\u611f\u53d7\u88ab\u6d4b\u91cf\u3002\u5176\u4f5c\u7528\u662f\u5c06\u540c\u65f6\u4f5c\u7528\u5728\u4e24\u4e2a\u68c0\u6d4b\u5143\u4ef6\u4e0a\u7684\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5e72\u6270\u4fe1\u606f\u9664\u53bb\uff0c\u5bf9\u88ab\u6d4b\u91cf\u4fe1\u606f\u8fdb\u884c\u653e\u5927\u3002\u53c2\u6bd4\u53ef\u4ee5\u8f83\u597d\u5730\u6d88\u9664\u5e72\u6270\u6765\u6e90\u660e\u786e\u5730\u73af\u5883\u6761\u4ef6\u91cf\u7684\u5f71\u54cd\u3002</li> </ul> <p>\u540c\uff1a</p> <ul> <li>\u90fd\u80fd\u4e00\u5b9a\u7a0b\u5ea6\u514b\u670d\u73af\u5883\u5e72\u6270\u3002</li> </ul>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_3","title":"\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u3001\u538b\u78c1\u5f0f","text":"\u4ece\u5e94\u7528\u89d2\u5ea6\u8ba8\u8bba\u5e76\u5206\u6790\u538b\u963b\u5f0f\u3001\u538b\u7535\u5f0f\u548c\u538b\u78c1\u5f0f\u68c0\u6d4b\u5143\u4ef6\u5404\u6709\u4ec0\u4e48\u7279\u70b9\uff1f  <ul> <li>\u538b\u963b\u5f0f</li> </ul> <p>\u6d4b\u91cf\u8303\u56f4\u5bbd\u3001\u51c6\u786e\u5ea6\u9ad8\uff0c\u54cd\u5e94\u901f\u5ea6\u5feb\uff0c\u9002\u5408\u9759\u6001\u548c\u52a8\u6001\u6d4b\u91cf\uff0c\u4f7f\u7528\u5bff\u547d\u957f\uff0c\u6027\u80fd\u7a33\u5b9a\uff0c\u4ef7\u683c\u4fbf\u5b9c\uff0c\u53ef\u4ee5\u5728\u9ad8\u5f3a\u5ea6\u6076\u52a3\u73af\u5883\u4e0b\u5de5\u4f5c\u3002</p> <p>\u4f46\u6709\u8f93\u51fa\u4fe1\u53f7\u5fae\u5f31\uff0c\u6297\u5e72\u6270\u80fd\u529b\u5dee\uff0c\u6613\u53d7\u6e29\u5ea6\u7b49\u73af\u5883\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5927\u5e94\u53d8\u72b6\u6001\u4e0b\u6709\u8f83\u5927\u7684\u975e\u7ebf\u6027\u3002</p> <p>\u5e94\u7528\uff1a\u88ab\u7c98\u8d34\u5728\u5404\u79cd\u5f39\u6027\u5143\u4ef6\u4e0a\uff0c\u4ee5\u611f\u53d7\u538b\u529b\u53d8\u5316\u3002</p> <ul> <li>\u538b\u7535\u5f0f</li> </ul> <p>\u5177\u6709\u9891\u5e26\u5bbd\u3001\u7075\u654f\u5ea6\u9ad8\u3001\u7ed3\u6784\u7b80\u5355\u3001\u5de5\u4f5c\u53ef\u9760\u3001\u91cd\u91cf\u8f7b\u7b49\u4f18\u70b9\u3002</p> <p>\u4f46\u53ea\u9002\u5408\u52a8\u6001\u6d4b\u91cf\u3002</p> <p>\u5e94\u7528\uff1a\u53ef\u4ee5\u5b9e\u73b0\u529b\u3001\u538b\u529b\u3001\u52a0\u901f\u5ea6\u548c\u626d\u77e9\u7b49\u7269\u7406\u91cf\u7684\u6d4b\u91cf\u3002</p> <ul> <li>\u538b\u78c1\u5f0f</li> </ul> <p>\u8f93\u51fa\u529f\u7387\u5927\uff0c\u6297\u5e72\u6270\u80fd\u529b\u53ca\u8fc7\u8f7d\u80fd\u529b\u5f3a\uff0c\u4fbf\u4e8e\u5236\u9020\uff0c\u7ecf\u6d4e\u5b9e\u7528\uff0c\u5e76\u80fd\u5728\u6076\u52a3\u7684\u6761\u4ef6\u4e0b\u957f\u671f\u4f7f\u7528\u3002</p> <p>\u4f46\u6d4b\u91cf\u7cbe\u5ea6\u4e0d\u9ad8\uff0c\u53cd\u5e94\u901f\u5ea6\u8f83\u6162\u3002</p> <p>\u5e94\u7528\uff1a\u4e3b\u8981\u5e94\u7528\u4e8e\u6d4b\u529b\u3001\u79f0\u91cd\u3001\u6e29\u5ea6\u6d4b\u91cf\u53ca\u76c8\u5229\u65e0\u635f\u68c0\u6d4b\u7b49\u65b9\u9762\u3002</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_4","title":"\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u3001\u975e\u7ebf\u6027\u8865\u507f","text":"\u68c0\u6d4b\u4eea\u8868\u5e38\u7528\u7684\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u548c\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u7684\u65b9\u6cd5\u4e3b\u8981\u6709\u54ea\u4e9b\uff1f  <ul> <li>\u51cf\u5c11\u968f\u673a\u8bef\u5dee\u7684\u65b9\u6cd5\u3002</li> </ul> <p>\u63d0\u9ad8\u68c0\u6d4b\u7cfb\u7edf\u51c6\u786e\u5ea6\uff0c\u5bf9\u6d4b\u91cf\u7ed3\u679c\u8fdb\u884c\u7edf\u8ba1\u5904\u7406\uff0c\u6291\u5236\u566a\u58f0\u5e72\u6270\u3002</p> <ul> <li>\u8fdb\u884c\u975e\u7ebf\u6027\u8865\u507f\u3002</li> </ul> <p>\u76f4\u63a5\u4e32\u8054\u6cd5\uff0c\u975e\u7ebf\u6027\u8d1f\u53cd\u9988\u6cd5\uff0c\u8f6f\u4ef6\u7ebf\u6027\u5316\u6cd5</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_5","title":"\u5f00\u73af\u3001\u95ed\u73af\u7ed3\u6784\u4eea\u8868","text":"\u6bd4\u8f83\u5206\u6790\u4e00\u4e0b\u5f00\u73af\u7ed3\u6784\u548c\u95ed\u73af\u7ed3\u6784\u4eea\u8868\u5404\u81ea\u7684\u7279\u70b9\u3002(\u53cb\u60c5\u63d0\u9192:\u7b2c\u4e00\u7ae0\u548c\u7b2c\u4e09\u7ae0\u7684\u76f8\u5173\u5185\u5bb9\u8981\u4e00\u8d77\u8003\u8651\uff0c\u5e76\u4ece\u81ea\u52a8\u63a7\u5236\u539f\u7406\u7684\u89d2\u5ea6\u6765\u8fdb\u884c\u5206\u6790\u3002)  <ul> <li>\u5f00\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u82e5\u5e72\u4e2a\u73af\u8282\u4e32\u8054\u7ec4\u6210\uff0c\u4eea\u8868\u7684\u4fe1\u606f\u548c\u53d8\u6362\u53ea\u6cbf\u4e00\u4e2a\u65b9\u5411\u4f20\u9012\u3002\u5176\u603b\u7684\u4f20\u9012\u51fd\u6570\u4e3a\u5404\u73af\u8282\u4f20\u9012\u51fd\u6570\u4e4b\u79ef\uff0c\u6574\u53f0\u4eea\u8868\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a\u5404\u4e2a\u73af\u8282\u7684\u76f8\u5bf9\u8bef\u5dee\u4e4b\u548c\u3002</p> <p>\u603b\u4f53\u6765\u8bf4\u7ed3\u6784\u7b80\u5355\u3002\u5f53\u7ec4\u6210\u4eea\u8868\u7684\u73af\u8282\u8f83\u591a\u65f6\uff0c\u51c6\u786e\u5ea6\u8f83\u4f4e\u3002</p> <ul> <li>\u95ed\u73af\u7ed3\u6784\u4eea\u8868</li> </ul> <p>\u7531\u6b63\u5411\u901a\u9053\u548c\u53cd\u9988\u901a\u9053\u7ec4\u6210\u3002\u5bf9\u4e8e\u4e00\u9636\u73af\u8282\\(G = \\frac{k}{1+Ts}\\)\uff0c\u82e5\u6709\u53cd\u9988\u589e\u76ca\\(\\beta\\)\uff0c\u5176\u653e\u5927\u500d\u6570\u548c\u65f6\u95f4\u5e38\u6570\u90fd\u662f\u5f00\u73af\u7ed3\u6784\u4eea\u8868\u7684\\(1/(1+k\\beta)\\)\u3002\u82e5\\(k\\beta \\rightarrow \\infty\\)\uff0c\u5219\\(G' = K_0/\\beta\\)\uff0c\u5373\u4eea\u8868\u7279\u6027\u4e3b\u8981\u53d6\u51b3\u4e8e\u53cd\u9988\u901a\u9053\u7279\u6027\u3002</p> <p>\u603b\u4f53\u7ed3\u6784\u4f1a\u590d\u6742\u4e00\u4e9b\uff0c\u7a33\u5b9a\u6027\u4f1a\u8f83\u5dee\u3002\u4f46\u662f\u53cd\u5e94\u901f\u5ea6\u5feb\uff0c\u7ebf\u6027\u597d\uff0c\u51c6\u786e\u5ea6\u9ad8\u3002</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_6","title":"\u6807\u51c6\u5dee\u4e0e\u7cfb\u7edf\u3001\u7c97\u5927\u8bef\u5dee","text":"\u8bf7\u5217\u51fa\u6d4b\u91cf\u4fe1\u53f7\u5747\u503c\u548c\u6807\u51c6\u5dee\u7684\u5b9a\u4e49\u516c\u5f0f\uff0c\u5e76\u7b80\u8981\u8bba\u8ff0\u4e00\u4e0b\u8be5\u4e24\u4e2a\u91cd\u8981\u7edf\u8ba1\u91cf\u5728\u7cfb\u7edf\u8bef\u5dee\u548c\u7c97\u5927\u8bef\u5dee\u5224\u522b\u4e2d\u7684\u4f5c\u7528\u3002  <ul> <li>\u5747\u503c</li> </ul> \\[ \\overline{x}=\\sum\\limits_{i=1}^{n}x_i \\] <ul> <li>\u6807\u51c6\u5dee</li> </ul> <p>\u5355\u6b21\u6d4b\u91cf\u503c\u7684\u6807\u51c6\u5dee(\u8d1d\u585e\u5c14Bessel\u516c\u5f0f)</p> \\[ \\sigma_B=s(x) = \\sqrt{\\frac{\\sum\\limits_{i=1}^{n}(x_i - \\overline{x})^2}{n-1}} \\] <p>\u7b97\u672f\u5e73\u5747\u503c\\(\\overline{x}\\)\u7684\u6807\u51c6\u5dee</p> \\[ s(\\overline{x}) = \\frac{s(x)}{\\sqrt{n}} \\] <p>\u53ef\u4ee5\u7528\u6807\u51c6\u5dee\u5224\u636e\uff0c\u770b\u4e24\u4e2a\u4f30\u8ba1\u6807\u51c6\u5dee\u662f\u5426\u6e10\u8fdb\u76f8\u7b49\u3002\u82e5\u5b58\u5728\u7cfb\u7edf\u8bef\u5dee\uff0c\u5219\u4e24\u8005\u76f8\u5dee\u5f88\u5927\u3002</p> <p>\\(\\sigma\\)\u6cd5\u53ef\u4ee5\u5254\u9664\u7c97\u5927\u8bef\u5dee\u3002\u5f53\u6d4b\u91cf\u503c\\(x_i\\)\u6ee1\u8db3\u62c9\u4f0a\u8fbe\u6cd5\u6216\u683c\u62c9\u5e03\u65af\u6cd5\u7684\u6761\u4ef6\u65f6\uff0c\u53ef\u4ee5\u8ba4\u5b9a\u4e3a\u7c97\u5927\u8bef\u5dee\uff08\u5982\\(3\\sigma\\)\uff09\u3002</p>"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_7","title":"\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u3001\u6807\u51c6\u5dee\u7684\u4f30\u8ba1","text":"\u73b0\u6709\u4e00\u53f0\u6e29\u5ea6\u4f20\u611f\u5668\uff0c\u91cf\u7a0b\u4e3a0~100\u00b0C\uff0c\u5176\u51c6\u786e\u5ea6(\u7cbe\u5ea6)\u7b49\u7ea7\u4e3a1.0\u3002\u8bf7\u95ee\u8be5\u4f20\u611f\u5668\u5728\u91cf\u7a0b\u8303\u56f4\u5185\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u7edd\u5bf9\u8bef\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u548c\u53ef\u80fd\u51fa\u73b0\u7684\u6700\u5927\u6807\u51c6\u5dee\u7684\u5408\u7406\u4f30\u8ba1\u503c\u5206\u522b\u4e3a\u591a\u5c11?\u4e3a\u4ec0\u4e48?(\u53cb\u60c5\u63d0\u9192:\u4ed4\u7ec6\u770b\u4e00\u4e0b\u6559\u6750P13\u9875)  <ul> <li>\u6700\u5927\u7edd\u5bf9\u8bef\u5dee</li> </ul> \\[ e = 100 \\times 1\\% = 1 \u00b0C \\] <ul> <li>\u6700\u5927\u6807\u51c6\u5dee</li> </ul> \\[ 3\\sigma = 1 \u00b0C\\Rightarrow \\sigma = 1/3\u00b0C \\]"},{"location":"Ctrl/Sensing%26Detection/Midterm/#_8","title":"\u8ba1\u7b97\u9898","text":"<p>\u4e3a\u4e86\u4fdd\u8bc1\u6d4b\u91cf\u51c6\u786e\u5ea6\uff0c\u5728\u538b\u529b\u68c0\u6d4b\u8868\u9009\u578b\u65f6\uff0c\u4e00\u822c\u8981\u6c42\u6700\u5927\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u8d85\u8fc7\u4eea\u8868\u6ee1\u91cf\u7a0b\u7684\\(3/4\\)\uff0c\u6700\u5c0f\u5de5\u4f5c\u538b\u529b\u4e0d\u5e94\u4f4e\u4e8e\u6ee1\u91cf\u7a0b\u7684\\(1/3\\)\u3002\u76ee\u524d\u6211\u56fd\u51fa\u5382\u7684\u538b\u529b(\u5305\u62ec\u5dee\u538b)\u68c0\u6d4b\u4eea\u8868\u6709\u7edf\u4e00\u7684\u91cf\u7a0b\u7cfb\u5217\uff0c\u5b83\u4eec\u662f\\(1\\)\u3001\\(1.6\\)\u3001\\(2.5\\)\u3001\\(4.0\\)\u3001\\(6.0kPa\\) \u4ee5\u53ca\u5b83\u4eec\u7684\\(10^n\\)\u500d\u6570(\\(n\\)\u4e3a\u6574\u6570)\u3002</p> <p>\u67d0\u538b\u529b\u5bb9\u5668\u6b63\u5e38\u5de5\u4f5c\u65f6\u538b\u529b\u8303\u56f4\u4e3a\\(1.0~1.5MPa\\),\u8981\u6c42\u6d4b\u91cf\u8bef\u5dee\u4e0d\u5927\u4e8e\u88ab\u6d4b\u538b\u529b\u76845%,\u8bd5\u786e\u5b9a\u8be5\u8868\u7684\u91cf\u7a0b\u548c\u51c6\u786e\u5ea6\u7b49\u7ea7\u3002</p> <ul> <li>\u89e3\uff1a</li> </ul> <p>\u8bbe\u91cf\u7a0b\u4e3a\\(A (MPa)\\)\uff0c\u5219\u6ee1\u8db3</p> \\[ \\begin{cases} \\frac{1}{3}A \\leq 1.0 \\\\ 1.5 \\leq \\frac{3}{4}A \\end{cases} \\] <p>\u89e3\u5f97\\(2.0\\leq A \\leq 3.0\\)\uff0c\u6545\u53d6\\(A=2.5 MPa\\)</p> <p>\u7531\u9898\u610f\uff0c\u4eea\u8868\u57fa\u672c\u8bef\u5dee\\(e \\leq 5\\% P\\)\uff0c\u6545\u5fc5\u987b\u8981</p> \\[ e \\leq 5\\% \\min{P} = 0.05\\times 1.0 = 0.05MPa \\] <p>\u6240\u4ee5\u4eea\u8868\u6ee1\u91cf\u7a0b\u57fa\u672c\u8bef\u5dee</p> \\[ e' \\leq \\frac{e}{A} = 2\\% \\] <p>\u6545\u9009\u62e9\u51c6\u786e\u5ea6\\(1.5\\)\u3002</p>"},{"location":"Math/","title":"Mathematical Basis","text":"<p>This is a part for mathematical basis, comprising courses mainly set up in School of Mathematical Sciences, Zhejiang University.</p>"},{"location":"Math/#algebra","title":"Algebra","text":""},{"location":"Math/#lie-algebra","title":"Lie Algebra","text":"<p>To be continued...</p>"},{"location":"Math/#linear-algebra","title":"Linear Algebra","text":"<p>To be continued...</p>"},{"location":"Math/#geometry-topology","title":"Geometry &amp; Topology","text":""},{"location":"Math/#basic-topology","title":"Basic Topology","text":"<p>To be continued...</p>"},{"location":"Math/#differential-geometry","title":"Differential Geometry","text":"<p>To be continued...</p>"},{"location":"Math/#analysis","title":"Analysis","text":""},{"location":"Math/#complex-analysis","title":"Complex Analysis","text":"<p>Completed.</p>"},{"location":"Math/#functional-analysis","title":"Functional Analysis","text":"<p>To be continued...</p>"},{"location":"Math/#numerical-analysis","title":"Numerical Analysis","text":"<p>Completed.</p>"},{"location":"Math/#ordinary-differential-equation","title":"Ordinary Differential Equation","text":"<p>Completed.</p>"},{"location":"Math/#real-analysis","title":"Real Analysis","text":"<p>Completed.</p>"},{"location":"Math/#probability","title":"Probability","text":""},{"location":"Math/#stochastic-processes","title":"Stochastic Processes","text":"<p>Completed.</p>"},{"location":"Math/#mechanics","title":"Mechanics","text":""},{"location":"Math/#material-point-method","title":"Material Point Method","text":"<p>Completed.</p>"},{"location":"Math/Basic_Topology/","title":"Topology","text":"<p>Reference</p> <ul> <li> <p>\u57fa\u7840\u62d3\u6251\u5b66\u8bb2\u4e49, \u5c24\u627f\u4e1a</p> </li> <li> <p>Topology, James R.Munkres</p> </li> </ul>"},{"location":"Math/Basic_Topology/#topological-spaces","title":"Topological Spaces","text":""},{"location":"Math/Basic_Topology/#connectedness-compactness","title":"Connectedness &amp; Compactness","text":""},{"location":"Math/Basic_Topology/#countability-separation-asioms","title":"Countability &amp; Separation Asioms","text":""},{"location":"Math/Basic_Topology/Connected_Compact/","title":"Connectedness &amp; Compactness","text":""},{"location":"Math/Basic_Topology/Connected_Compact/#connectedness","title":"Connectedness","text":"<p>Connectedness defined by separations</p> <p>Suppose \\(X\\) is a topological space. A separation of \\(X\\) is a pair \\(U\\), \\(V\\) of disjoint non-empty open subsets of \\(X\\), where \\(X=U\\cup V\\). </p> <p>The space \\(X\\) is said to be connected, if there does not exist a separation of \\(X\\).</p> <p>Equivalent definition of connectedness</p> <p>The space \\(X\\) is connected, iff the only subsets of \\(X\\) that are both open and closed in \\(X\\) are \\(\\varnothing\\) and \\(X\\) itself.</p> Proof <p>We show that for a non-empty subset \\(X\\), it could not be both open and closed.</p> <ul> <li> <p>Necessary. If a non-empty subset \\(A\\) of \\(X\\) is both open and closed, then \\(B=X-A\\) is open and \\(X=A\\cup B\\), so \\(A\\), \\(B\\) are a separation of \\(X\\).</p> </li> <li> <p>Sufficient. If \\(U\\) and \\(V\\) is a separation of \\(X\\), then \\(U\\) itself is both open and closed.</p> </li> </ul> <p><p>\\(\\square\\)</p></p> <p>Example. Show that \\(\\mathbb{E}^1\\) is connected.</p> Proof"},{"location":"Math/Basic_Topology/Connected_Compact/#properties-of-connectedness","title":"Properties of connectedness","text":"<p>Separation and connected subspace</p> <p>Suppose \\(U\\subset X\\) is both open and closed, \\(A\\subset X\\) is connected, then either \\(U\\cap A = \\varnothing\\) or \\(U\\cap A=A\\), i.e. \\(A\\subset U\\).</p> <p>The above lemma could be stated as follows. Suppose \\(X=U\\cup V\\), where \\(U\\) and \\(V\\) is a separation of \\(X\\), then for a connected subspace \\(A\\subset U\\), it lie entirely either within \\(U\\) or \\(V\\).</p> Proof <p>By definition. </p> <p>Properties of connectedness</p> <p>(i) The continuous image of connected space is connected.</p> <p>(ii) Suppose \\(A\\subset X\\) is connected, and \\(A\\subset Y\\subset \\overline{A}\\), then \\(Y\\) is connected.</p> <p>(iii) Suppose \\(\\mathcal{U}\\) is a connected covering of \\(X\\), \\(A\\subset X\\) is connected, and \\(A\\cap U\\neq \\varnothing\\) for all \\(U\\in\\mathcal{U}\\), then \\(X\\) is connected. </p> Proof for (i)Proof for (ii)Proof for (iii) <p>Example. Using (iii) to show that </p> <p>(i) \\(\\mathbb{E}^2\\) is connected.</p> <p>(ii) \\(S^n\\) is conncected.</p>"},{"location":"Math/Basic_Topology/Connected_Compact/#path-connectedness","title":"Path connectedness","text":"<p>path, path connectedness</p> <p>Given points \\(x,y\\in X\\), a path in \\(X\\) from \\(x\\) to \\(y\\) is a continuous map \\(f:[a,b]\\rightarrow X\\) of some closed interval in the real line into \\(X\\), such that \\(f(a)=x\\), \\(f(b)=y\\). Specially we choose \\(a=0,b=1\\).</p> <p>\\(X\\) is said to be path connected, if every pair of points of \\(X\\) can be joined by a path in \\(X\\).</p> <p>Properties of path connectedness</p> <p>(i) A path connected space is connnected.</p> <p>(ii) Continuous image of a path connected image is path connected.</p> <p>The following example shows that connected space is necessarily a path connected space.</p> <p>Example. Topologist\u2019s sine curve. Suppose </p> \\[ A = \\left\\{x\\times \\sin \\frac{1}{x}: x\\in (0,1]\\right\\}, \\quad B=\\{0\\}\\times [-1,1], \\] <p>show that </p> <p>(i) \\(A\\) is connected, so \\(\\overline{A}=A\\cup B\\) is connected.</p> <p>(ii) \\(\\overline{A}\\) is not path connected.</p> Proof"},{"location":"Math/Basic_Topology/Connected_Compact/#compactness","title":"Compactness","text":"<p>Coverings</p> <p>A collection \\(\\mathcal{A}\\) of subsets of a space \\(X\\) is said to cover \\(X\\), or to be a covering of \\(X\\), if the union of elements of \\(\\mathcal{A}\\) equals \\(X\\), i.e. </p> \\[ X=\\bigcup_{A \\in \\mathcal{A}}A. \\] <p>If all elements of \\(\\mathcal{A}\\) is open, then \\(\\mathcal{A}\\) is called an open covering of \\(X\\).</p> <p>Compact set</p> <p>Set \\(X\\) is said to be compact, if any open covering of \\(X\\) has finite-number subcollection that also covers \\(X\\).</p> <p>Now we give some properties about compactness.</p> <p>compactness of subsets</p> <p>A collection \\(\\mathcal{U}\\) of \\(X\\), is said to be a covering of \\(A\\) in \\(X\\), if \\(A \\subset \\bigcup_{U\\in \\mathcal{U}}U\\).</p> <p>Suppose \\(A\\subset X\\). \\(A\\) is compact, iff any open covering of \\(A\\) in \\(X\\) has finite subcollection that also covers \\(A\\).</p> <p>Compactness is closely connected with the concept of closedness. When does a closed set be a compact set?</p> <p></p> <p>Compactness for closed set</p> <p>Every closed subspace of a compact space is compact.</p> <p>Like we have in continuous analysis, we have the following theorem.</p> <p></p> <p>Compactness of Image of compact set under a continuous map</p> <p>The image of a compact space under a continuous map is compact.</p>"},{"location":"Math/Basic_Topology/Connected_Compact/#compact-subspace-of-hausdorff-space","title":"Compact subspace of Hausdorff space","text":"<p>This is the result of compactness and \\(T_2\\) axiom.</p> <p></p> <p>closedness of compact subspace of Hausdorff space</p> <p>Every compact subspace of a Hausdorff space is closed. </p> <p>That is, for a compact subspace \\(A\\) of a Hausdorff space \\(X\\), choose \\(x\\not\\in A\\), then there exsit disjoint open set \\(U\\) and \\(V\\), such that \\(x\\in U\\) and \\(Y\\subset V\\).</p> Proof <p>We give a proof by showing that for all \\(x\\in A^c\\), \\(x\\) has a neighborhood \\(U\\) such that \\(U\\cap A=\\varnothing\\), i.e. \\(x\\in U\\subset A^c\\), so \\(A^c\\) is open, so \\(A\\) is closed.</p> <p>Notice for all \\(y\\in A\\), since \\(X\\) is Hausdorff, there exists open set \\(U_y\\) and \\(V_y\\) s.t. \\(x\\in U_y\\), \\(y\\in V_y\\) and \\(U_y\\cap V_y=\\varnothing\\). Also notice that \\(\\bigcup_{y\\in A}V_y\\) is an open covering of \\(A\\). Since \\(A\\) is compact, there exsits finite number \\(N\\) such that \\(Y\\subset \\bigcup_{i=1}^N V_{y_i}\\). Let</p> \\[ U_x = \\bigcap_{i=1}^N U_{y_i}, \\quad V = \\bigcup_{i=1}^N V_{y_i}, \\] <p>which are still open sets, and \\(U_x\\cap A=\\varnothing\\). \\(U_x\\) is the open set that we want.</p> <p><p>\\(\\square\\)</p></p> <p>The following theorem is a useful tool to show whether a map is a homeomorphism. The proof is done by transfering closedness and compactness.</p> <p>A sufficient condition for a homeomorphism</p> <p>Assume \\(f:X\\rightarrow Y\\) is a continuous bijection. If \\(X\\) is compact and \\(Y\\) is a Hausdorff space, then \\(f\\) is a homeomorhpism.</p> Proof <p>We only need to prove that \\(f^{-1}\\) is continuous. By relationship of open, closedness and continuous, we suffice to show that \\(f\\) is closed.</p> <p>For a closed subspace \\(A\\) of \\(X\\), since \\(X\\) is compact, by Compactness for closed set, we have \\(A\\) is compact. So is \\(f(A)\\) in \\(Y\\) by Compactness of Image of compact set under a continuous map. Since \\(Y\\) is Hausdorff, then by closedness of compact subspace of Hausdorff space, we have \\(f(A)\\) is closed.</p> <p><p>\\(\\square\\)</p></p> <p>compact Hausdorff space satisfies T3, T4 axioms</p> <p>Compact Hausdorff space satisfies \\(T_3\\), \\(T_4\\) axioms.</p> Proof <p>Suppose \\(X\\) is a Hausdorff space. We suffice to show that \\(X\\) satisfies \\(T_4\\) axiom. Choose two disjoint closed subspace \\(A\\), \\(B\\) of \\(X\\), by Compactness for closed set, \\(A\\), \\(B\\) are two disjoint compact spaces. </p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Basic_Topology/Connected_Compact/#compactness-on-product-space","title":"Compactness on product space","text":"<p>Tube Lemma</p> <p>Consider product \\(X \\times Y\\), where \\(X\\) is compact. Choose \\(y\\in Y\\). Assume \\(N\\) is an open set of \\(X\\times Y\\) that contains the slice \\(X\\times \\{y\\}\\), then \\(N\\) contains some tube \\(X\\times W\\), where \\(W\\) is a neighborhood of \\(\\{y\\}\\) in \\(Y\\).</p> Proof <p>productivity of compactness</p> <p>The product of finitely many compact spaces is compact.</p> Proof <p>We only prove for the case of two compact spaces.</p>"},{"location":"Math/Basic_Topology/Count_Sepa/","title":"Countability &amp; Separation Axioms","text":""},{"location":"Math/Basic_Topology/Count_Sepa/#separation-axioms","title":"Separation axioms","text":"<p>Separation axioms</p> <p>Suppose \\(X\\) is a topological space, we have the following four separation axioms.</p> <p>(i) \\(T_1\\) separation axiom. \\(\\forall x\\neq y\\in X\\), there exsits a neighborhood \\(U\\) of \\(x\\) such that \\(y\\not\\in U\\). </p> <p>(ii) \\(T_2\\) separation axiom. \\(\\forall x\\neq y\\in X\\), there exsit neighborhood \\(U\\), \\(V\\) of \\(x\\), \\(y\\), respectively, such that \\(U\\cap V=\\varnothing\\).</p> <p>(iii) \\(T_3\\) separation axiom (regular). \\(\\forall x\\in X\\) and closed set \\(A\\subset X\\), there exist neighborhood \\(U\\), \\(V\\) of \\(x\\) and \\(A\\) such that \\(U\\cap V=\\varnothing\\).</p> <p>(iv) \\(T_4\\) separation axiom (normal). For every two disjoint closed set \\(A,B\\subset X\\), there exist neighborhood \\(U\\), \\(V\\) of \\(A\\) and \\(B\\) such that \\(U\\cap V=\\varnothing\\). </p> <p>A space that satisfies \\(T_2\\) axiom is called a Hausdorff space, which turned out to have good properties for analysis.</p> <p>Relationship of serapation axioms</p> <p>(i) \\(T_2\\) implies \\(T_1\\).</p> <p>(ii) If \\(T_1\\) axiom holds, then \\(T_4\\) implies \\(T_3\\), \\(T_3\\) implies \\(T_2\\).</p> <p>Example. Show that \\(X\\) is \\(T_1\\), iff finite-number subset of \\(X\\) is closed.</p> <p>Now we give the definition of convergence.</p> <p>Definition of convergence</p> <p>Suppose \\(X\\) is a topological space, with \\(\\{x_n\\}\\subset X\\) is a sequence, and \\(x\\in X\\). \\(x_n\\) is said to converge to \\(x\\), if for all open neighborhood \\(U\\) of \\(x\\), there exists \\(N&gt;0\\), such that \\(x_n\\in U\\) whenever \\(n&gt;N\\).</p> <p>Example. If \\(X\\) is a Hausdorff space, then any sequence in \\(X\\) converges to at most one point.</p>"},{"location":"Math/Basic_Topology/Count_Sepa/#countability-axioms","title":"Countability axioms","text":"<p>Countability axioms</p> <p>Suppose \\(X\\) is a topological space, we have the following two countability axioms.</p> <p>(i) \\(C_1\\) countability axiom. \\(\\forall x\\in X\\), there exsits a countable collection \\(\\mathcal{B}\\) of neighborhoods of \\(x\\), such that every neighborhood of \\(x\\) contains some \\(B\\in \\mathcal{B}\\). </p> <p>We also call \\(X\\) has a countable basis at \\(x\\).</p> <p>(ii) \\(C_2\\) countability axiom. \\(X\\) has a countable basis for its topology.</p> <p>\\(C_2\\) axiom is stronger than \\(C_1\\), and some metric space could not satisfy the condition.</p> <p>Example. In \\(\\mathbb{R}\\), define metric </p> \\[ d(x,y)=\\delta_{xy}. \\] <p>Show that every point of the above space is its open set and contains uncountable basis for its topology.</p> Proof <p>Choose open ball</p> \\[ B(x, 1/2)=\\{x\\}, \\] <p>which is the smallest element of basis for its topology, and thus is uncountable.</p> <p>Example. Separable metric space is \\(C_2\\). Thus \\(\\mathbb{E}^n\\), Hilbert space \\(\\mathbb{E}^\\omega\\) are \\(C_2\\).</p> <p>Lindel\u00f6f Theorem</p> <p>If a topological space satisfies \\(C_2\\) and \\(T_3\\), then it is also \\(T_4\\). Or in some terms, A regular space with countable basis for its topology is normal.</p> Proof <p>Choose a countable basis for its topology \\(\\mathcal{B}\\). Suppose \\(F\\) and \\(F'\\) are disjoint closed subsets. </p> <p>Since the space is \\(T_3\\), so for each \\(x\\in F\\), there exist open sets \\(U_x\\) \\(V_x\\) such that \\(x\\in U_x\\), \\(F'\\subset V_x\\) and \\(U_x\\cap V_x=\\varnothing\\). Thus we have \\(\\overline{U_x}\\cap F'=\\varnothing\\). For each \\(U_x\\), choose an open set \\(B_x\\in \\mathcal{B}\\) such that \\(B_x\\subset U_x\\) and thus \\(\\overline{B_x}\\cap F'=\\varnothing\\). Since \\(\\mathcal{B}\\) is countable, we have</p> \\[ \\{B_n\\}_{n=1}^\\infty,\\quad \\{B_n'\\}_{n=1}^\\infty \\] <p>such that for each \\(n\\), \\(\\overline{B_n}\\cap F'=\\varnothing\\) and \\(\\overline{B_n}'\\cap F=\\varnothing\\), and \\(F\\subset \\bigcup_{n} B_n\\) and \\(F'\\subset \\bigcup_n B_n'\\).</p> <p>Define open sets</p> \\[ U_n=B_n-\\bigcup_{i=1}^n B_i', \\quad V_n = B_n' -\\bigcup_{i=1}^n B_i \\] <p>So we have \\(U_n\\cap V_m=\\varnothing\\) for all \\(n,m\\geq 1\\). Let</p> \\[ U:=\\bigcup_{n=1}^\\infty U_n, \\quad V:=\\bigcup_{n=1}^\\infty V_n,  \\] <p>so </p> \\[ U\\cap V = \\bigcup_{n=1}^\\infty (U_n\\cap V_n)=\\varnothing. \\] <p>Easy to show that \\(U\\) and \\(V\\) are the desired open sets that separate \\(F\\) and \\(F'\\).</p>"},{"location":"Math/Basic_Topology/Count_Sepa/#urysohn-lemma-its-applications","title":"Urysohn Lemma &amp; its applications","text":"<p>Urysohn Lemma</p> <p>Assume topological space \\(X\\) is \\(T_4\\), then for any two disjoint closed sets \\(A\\) and \\(B\\), there exists a continuous function defined on \\(X\\) such that \\(f|_A=0\\) and \\(f|_B=1\\).</p> Proof"},{"location":"Math/Basic_Topology/Topological_Spaces/","title":"Topological Space","text":"<p>Definition of Topology</p> <p>Suppose set \\(X\\), and a family set \\(\\tau\\) of \\(X\\) is said to be a topology if</p> <p>(i) \\(\\varnothing, X\\subset \\tau\\),</p> <p>(ii) if \\(\\{U_\\lambda\\}_{\\lambda\\in\\Lambda}\\subset \\tau\\), then \\(\\bigcup\\limits_{\\lambda\\in \\Lambda} U_\\lambda\\in \\tau\\),</p> <p>(iii) if \\(\\{U_n\\}_{1\\leq n\\leq N} \\subset \\tau\\), then \\(\\bigcap\\limits_{1\\leq n\\leq N}U_n\\in \\tau\\).</p> <p>The element of \\(\\tau\\) is called an open set.</p> <p>Example. Two common topologies on a set \\(X\\).</p> <p>(i) discrete topology \\(2^{X}\\).</p> <p>(ii) indiscrete/trivial topology \\(\\{\\varnothing, X\\}\\).</p> <p>Example. The topologies on \\(\\mathbb{R}\\).</p> <p>(i) Finite-complement topology </p> \\[ \\tau_f=\\{A\\subset \\mathbb{R}: |A^c|&lt;\\infty\\} \\cup \\{\\varnothing\\} \\] <p>(ii) Countable-complement topology </p> \\[ \\tau_c = \\{A\\subset \\mathbb{R}: |A^c| \\text{ is countable}\\} \\cup \\{\\varnothing\\} \\] <p>(iii) Euclidean topology </p> \\[ \\tau_e = \\{A\\subset \\mathbb{R}: A=\\bigcup_{\\lambda\\in\\Lambda} I_\\lambda, \\Lambda \\subset \\mathbb{R}\\} \\] <p>where \\(\\Lambda\\) is an index set and compose maybe finite/infinite/zero number of elements. \\(I_\\lambda\\) is an arbitrary open interval. We denote the space as \\(\\mathbb{E}^1=(\\mathbb{R}, \\tau_e)\\).</p> Proof <p>(i) and (ii) holds for condition two and three because De Morgan's formula</p> \\[ \\left(\\bigcup_{\\lambda\\in\\Lambda}A_\\lambda\\right)^c=\\bigcap_{\\lambda\\in\\Lambda}A_\\lambda^c,\\quad \\left(\\bigcap_{n=1}^N A_n\\right)^c=\\bigcup_{n=1}^N A_n^c. \\] <p>Comparison of topologies</p> <p>Suppose \\(\\tau\\), \\(\\tau'\\) are two topologies on \\(X\\). We say \\(\\tau'\\) is finer than \\(\\tau\\) (or bigger), if</p> \\[ \\tau\\subset \\tau'. \\] <p>And at this time the two topologies is comparable.</p>"},{"location":"Math/Basic_Topology/Topological_Spaces/#basis-for-a-topology","title":"Basis for a topology","text":"<p>Basis for a topology</p> <p>A family set \\(\\mathcal{B}\\) of \\(X\\) is said to be a topological basis, if</p> <p>(i) \\(\\forall x\\in X\\), \\(\\exists B\\in\\mathcal{B}\\) s.t. \\(x\\in B\\). Or equivalently, \\(X=\\bigcup\\limits_{B\\in \\mathcal{B}}B\\).</p> <p>(ii) \\(\\forall B_1,B_2\\in \\mathcal{B}\\), \\(\\forall x\\in B_1\\cap B_2\\), \\(\\exists B\\in\\mathcal{B}\\) such that \\(x\\in B\\) and \\(B\\subset B_1\\cap B_2\\).</p> <p>We now define a collection of subsets of \\(X\\) by a basis for a topology, which turns out to be a topology.    </p> <p>Topology Generated by basis</p> <p>A collection \\(\\tau_B\\) of subsets of \\(X\\) is defined as follows. A subset \\(U\\subset X\\) belongs to \\(\\tau_B\\), if \\(\\forall x\\in U\\), \\(\\exists B\\in \\mathcal{B}\\) s.t. \\(x\\in B\\) and \\(B\\subset U\\). Then \\(\\tau_B\\) is a topology on \\(X\\).</p> Proof <p>Check this by definition. Note that \\(\\varnothing\\in \\tau\\) and \\(X\\in \\tau\\) (by def (i) of basis for a topology). </p> <p>(ii) For \\(\\{U_\\lambda\\}_{\\lambda\\in\\Lambda}\\subset \\tau\\), by definition, \\(\\exists B_\\lambda\\in\\mathcal{B}\\), s.t. \\(\\forall x\\in U_\\lambda\\), \\(x\\in B_\\lambda\\) and \\(B_\\lambda\\in U_\\lambda\\). So for </p> \\[ U:=\\bigcup_{\\lambda\\in\\Lambda} U_\\lambda  \\] <p>\\(x\\in U\\), \\(\\exists \\lambda_0\\) such that \\(x\\in U_{\\lambda_0}\\), then \\(\\exists B_{\\lambda_0}\\subset U_{\\lambda_0}\\subset U\\) such that \\(x\\in B_\\lambda\\).</p> <p>(iii) Use the condition (ii) of the definition. We only prove if \\(U_1,U_2\\in \\tau\\), then \\(U_1\\cap U_2 \\in \\tau\\).</p> <p>Generation by a union of basis for a topology</p> <p>Suppose \\(\\tau\\) is a topology space generated by \\(\\mathcal{B}\\), then \\(\\forall U\\in \\tau\\), there exsits \\(\\mathcal{B}_1\\subset \\mathcal{B}\\) such that </p> \\[ U=\\bigcup_{B\\in\\mathcal{B}_1} B. \\] <p>Conversely, a collection \\(\\tau\\) defined by the above expression is a topology generated by basis \\(\\mathcal{B}\\). </p> Proof <ul> <li> <p>Sufficient. Easy to check by definition.</p> </li> <li> <p>necessary.</p> </li> </ul> <p>For each \\(U\\in \\tau\\), by definition, for each \\(x\\in U\\), there exists \\(B_x\\in \\mathcal{B}\\), s.t. \\(x\\in B_x\\subset U\\). </p> <p>So choose \\(\\mathcal{B}_1=\\{B_x: x\\in U\\}\\), and we have</p> \\[ U = \\bigcup_{B\\in\\mathcal{B}_1}B=\\bigcup_{x\\in U}B_x. \\] <p><p>\\(\\square\\)</p> </p> <p>This expression for \\(U\\) is not unique.</p> <p>Given a topology \\(\\tau\\), we could check whether it is generated by a specific topological basis.</p> <p>Lemma: find a topological basis from a topology</p> <p>Suppose \\(\\tau\\) is a topology on \\(X\\), a collection \\(\\mathcal{C}\\) of (open) subsets of \\(X\\) is said to be a topological basis of \\(\\tau\\), if for each open set \\(U\\) and each \\(x\\in U\\), there exsits \\(C\\in\\mathcal{C}\\) such that \\(x\\in C\\subset U\\). </p> <p>Actually, this topology is the same as the topology generated by basis \\(\\mathcal{C}\\).</p> Proof <ul> <li> <p>We shall check \\(\\mathcal{C}\\) is a basis for a topology.</p> </li> <li> <p>Let \\(\\tau'\\) to denote the topology generated by the basis \\(\\mathcal{C}\\). By definition, we have \\(\\tau'\\subset \\tau\\) (Readers could check). For the other direction, we have for each open set \\(U\\), choose \\(\\mathcal{C}_1=\\{C_x: x\\in C_x\\subset \\mathcal{C}\\}\\), then </p> </li> </ul> \\[ U=\\bigcup_{C \\in \\mathcal{C}_1}C\\subset \\tau'. \\] <p>Now we could give a criterion for comparing two topologies generated by two basis.</p> <p>Lemma: criterion for comparing two topologies generated by two basis</p> <p>Suppose \\(\\mathcal{B}\\) and \\(\\mathcal{B}'\\) are two basis for topologies \\(\\tau\\) and \\(\\tau'\\), respectively. Then \\(\\tau'\\) is finer than \\(\\tau\\), iff for each \\(x\\in X\\) and \\(B\\in\\mathcal{B}\\) with \\(x\\in B\\), there exists \\(B'\\in\\mathcal{B}'\\) such that \\(x\\in B'\\subset B\\).</p> Proof <ul> <li>Show by definition.</li> </ul> <p><p>\\(\\square\\)</p></p> <p>Apply the above lemma to show the following result.</p> <p>Example. Ball basis and triangular basis generate the same topology on \\(\\mathbb{R}^2\\).</p>"},{"location":"Math/Basic_Topology/Topological_Spaces/#product-topology","title":"Product Topology","text":"<p>Definition of product topology on \\(X\\times Y\\)</p> <p>Suppose \\(X\\) and \\(Y\\) are topological spaces, the product topology on \\(X\\times Y\\) is a collection of subsets of \\(X\\) generated by basis \\(\\mathcal{B}\\), where \\(\\mathcal{B}=\\{U\\times V: U\\in\\tau_X, V\\in \\tau_Y\\}\\). </p> <p>We shall show that the above \\(\\mathcal{B}\\) is a topological basis.</p> Proof <p>using </p> \\[ (U_1\\times V_1)\\cap (U_2\\times V_2)=(U_1\\cap U_2)\\times (V_1\\cap V_2). \\] <p>Note that \\(\\mathcal{B}\\) itself is not a topology of \\(X\\times Y\\).</p> <p>When does a collection of subsets of \\(X\\times Y\\) becomes a basis for the topology on \\(X\\times Y\\)?</p> <p>Basis for a topology on \\(X\\times Y\\)</p> <p>Suppose \\(\\mathcal{B}\\), \\(\\mathcal{C}\\) are topological basis on \\(X\\) and \\(Y\\), respectively. Then </p> \\[ \\mathcal{D}=\\{B\\times C: B\\in \\mathcal{B}, C\\in\\mathcal{C}\\} \\] <p>is a basis for a topology on \\(X\\times Y\\).</p> Proof <p>Now we make use of projection function to show some relationships.</p> <p>Projections</p> <p>Suppose \\(X\\) and \\(Y\\) are two non-empty sets. Define a map \\(\\pi_1:X\\times Y\\rightarrow X\\) by</p> \\[ \\pi_1(x,y)=x \\] <p>and \\(\\pi_2: X\\times Y\\rightarrow Y\\) by</p> \\[ \\pi_2(x,y)=y. \\] <p>They are called the projections of \\(X\\times Y\\) onto its first and second factor, respectively.</p> <p>Projections are surjective by definition. We would show that it is continuous naturally in the following chapter.</p> <p>topology generated by projections</p> <p>Collection</p> \\[ \\mathcal{S}=\\{\\pi_1^{-1} (U): U \\text{ is open in } X\\}\\cup \\{\\pi_2^{-1} (V): V \\text{ is open in } Y\\} \\] <p>is a sub-basis of the product topology of \\(X\\times Y\\).</p> Proof <p>By </p> \\[ (U\\times V)=(U\\times Y) \\cap (X\\times V). \\] <p>Show that product topology is equal to the topology generated by \\(\\mathcal{S}\\).</p> <p><p> </p> </p>"},{"location":"Math/Basic_Topology/Topological_Spaces/#subspace-topology","title":"Subspace Topology","text":"<p>Definition of Subspace topology</p> <p>Suppose \\(X\\) is a topological space with \\(\\tau\\). If \\(Y\\subset X\\), then the collection </p> \\[ \\tau_Y=\\{U\\cap Y: U\\in\\tau\\}. \\] <p>is a topology on \\(Y\\), called the subspace topology. With this topology, \\(Y\\) is called a subspace of \\(X\\).</p> Proof <p>Easy to have</p> \\[ \\varnothing=\\varnothing\\cap Y,\\quad Y=X\\cap Y. \\] <p>and for condition two</p> \\[ \\bigcup_{\\lambda\\in\\Lambda}(U_\\lambda \\cap Y) = \\left(\\bigcup_{\\lambda\\in\\Lambda}U_\\lambda \\right)\\cap Y  \\] \\[ \\bigcap_{n=1}^N(U_n \\cap Y) = \\left(\\bigcap_{n=1}^N U_n \\right)\\cap Y  \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Basic_Topology/Topological_Spaces/#quotient-topology","title":"Quotient Topology","text":"<p>Definition of quotient map</p> <p>Suppose \\(X\\) and \\(Y\\) are topological spaces, \\(p:X\\rightarrow Y\\) is called a quotient map if </p> <p>(i) \\(p\\) is surjective, </p> <p>(ii) \\(V\\subset Y\\) is an open set in \\(Y\\), iff \\(f^{-1}(V)\\) is open in \\(X\\).</p> <p>The (ii) could be partitioned into two conditions: \\(p\\) is continuous, and for \\(V\\subset Y\\), \\(p^{-1}(V)\\) is open in \\(X\\), implies \\(V\\) is open in \\(Y\\).</p> <p>Now we use quotient map to construct a topology, namely quotient topology.</p> <p>Definition of quotient topology</p> <p>Suppose \\(X\\) is a topological space, \\(A\\) is a set and \\(p:X\\rightarrow A\\) is a surjective map. Then there exsits exactly one topology \\(\\tau\\) on \\(A\\) such that \\(p\\) becomes a quotient map. It is called the quotient topology induced by \\(p\\).</p> Proof <p>The open set \\(U\\in\\tau\\) such that \\(p^{-1}(U)\\) is open in \\(X\\). This guarantees the continuity of \\(p\\), and actually makes the topology the finest topology that makes \\(p\\) continuous. We claim that this construction gives a topology.</p> <p>We shall show that \\(\\tau\\) is a topology. Apparently, \\(\\varnothing\\) and \\(A\\) are in \\(\\tau\\) since \\(p^{-1}(\\varnothing)=\\varnothing\\), \\(p^{-1}(A)=X\\). And</p> \\[ p^{-1}\\left(\\bigcup_{\\alpha\\in A}U_\\alpha\\right)=\\bigcup_{\\alpha\\in A} p^{-1}(U_\\alpha),\\quad p^{-1}\\left(\\bigcap_{n=1}^N U_n\\right)=\\bigcap_{n=1}^N p^{-1}(U_n)  \\] <p>A special case for quotient topology is defined as follows.</p> <p>Special case of quotient space</p> <p>Suppose \\(X\\) is a topological space, \\(X^*\\) is a partition of \\(X\\) into disjoint subsets whose union is \\(X\\). Let \\(p:X\\rightarrow X^*\\) be a surjective map that carries each point of \\(X\\) into the element of \\(X^*\\) containing it. In the quotient topology induced by \\(p\\), \\(X^*\\) is called quotient space of \\(X\\).</p> <p>We can describe the topology of \\(X^\u2217\\) in another way. A subset \\(U\\) of \\(X^\u2217\\) is a collection of equivalence classes, and the set \\(p^{\u22121}(U)\\) is just the union of the equivalence classes belonging to \\(U\\). Thus the typical open set of \\(X^\u2217\\) is a collection of equivalence classes whose union is an open set of \\(X\\).</p>"},{"location":"Math/Basic_Topology/Topological_Spaces/#continuous-functions","title":"Continuous functions","text":"<p>Definition of Continuous functions</p> <p>Suppose \\(X\\), \\(Y\\) are topological spaces, \\(f:X\\rightarrow Y\\) is a map. \\(f\\) is said to be continuous, if for all \\(U\\in \\tau_Y\\), we have \\(f^{-1}(U)\\in \\tau_X\\). </p> <p>Note that for \\(V\\subset Y\\), if \\(V\\cap f(X)=\\varnothing\\), then \\(f^{-1}(V)=\\varnothing\\).</p> <p>Example. Suppose \\(f:X\\rightarrow Y\\) is a bijection. Show that the following statements are equivalent.</p> <p>(i) \\(f^{-1}\\) is continuous,</p> <p>(ii) \\(f\\) is open,</p> <p>(iii) \\(f\\) is closed.</p> Proof <p>(i) implies (ii) is by definition.</p> <p>(ii) implies (iii) is by \\(f(U^c)=[f(U)]^c\\).</p> <p>Rules for constructing continuous functions</p> <p>Suppose \\(X, Y, Z\\) are topological spaces. </p> <p>(i) </p> Proof <p>Pasting lemma (\u9ecf\u7ed3\u5f15\u7406)</p> <p>Maps into products</p> <p>Assume \\(f: A\\rightarrow X\\times Y\\) is defined by</p> \\[ f(a)=(f_1(a), f_2(a)). \\] <p>Then \\(f\\) is continuous iff \\(f_1\\) and \\(f_2\\) are continuous. </p> <p>Here \\(f_1, f_2\\) are called the coordinate functions of \\(f\\).</p> <p>Note that if the domain is product space, then the aboe proposition is not ture.</p> <p>Example. Suppose \\(F: \\mathbb{E}\\times \\mathbb{E}\\rightarrow \\mathbb{E}\\) defined by</p> \\[ F(x,y)=\\begin{cases} xy/(x^2+y^2),\\quad &amp;(x,y)\\neq (0,0),\\\\ 0, \\quad &amp; (x,y)=(0,0). \\end{cases} \\] <p>Show that \\(F_1, F_2\\) is continuous but \\(F\\) itself is not continuous.</p>"},{"location":"Math/Basic_Topology/Topological_Spaces/#homeomorphism","title":"Homeomorphism","text":"<p>Definition of Hemeomorphism</p> <p>Denoted by \\(\\simeq\\). Suppose \\(X\\), \\(Y\\) is a topological space, and \\(f:X\\rightarrow Y\\) is a map. \\(f\\) is called homeomorphism, if \\(f\\) is bijective, \\(f\\) and \\(f^{-1}\\) are both continuous. </p> <p>For given \\(X\\) and \\(Y\\), we call them homeomorphic if there exsits a homeomorphism \\(f:X\\rightarrow Y\\). </p> <p>Example. Some homeomorphisms.</p> <p>(i) open intervals is homeomorphic with \\(\\mathbb{E}^1\\).  </p> <p>(ii) open unit balls in \\(\\mathbb{E}^n\\) is homeomorphic with \\(\\mathbb{E}^n\\).</p> <p>(iii) Any convex polygon is homeomorphic with a unit disc.</p> <p>(iv) \\(z\\mapsto z+1\\) is analytic at \\(\\infty\\).</p> Proof <p>(i) using \\(x\\mapsto \\tan x\\) from \\((-\\pi/2, \\pi/2)\\) to \\(\\mathbb{R}\\).</p> <p>(ii) using \\(x\\mapsto x+\\frac{x}{\\|x\\|}\\).</p> <p>Topological imbedding</p> <p>Suppose \\(f:X\\rightarrow Y\\) is a continuous and injective map. If restricted on \\(f(X)\\), \\(f\\) is a hemeomorphism, then we call \\(f\\) is an embedding of \\(X\\) in \\(Y\\).</p> <p>Example. </p> <p>(i) \\(\\mathbb{R}^m\\rightarrow \\mathbb{R}^n\\), \\(m &lt; n\\), defined by</p> \\[ (x_1,\\cdots,x_m)\\mapsto (x_1,\\cdots,x_m,0,\\cdots, 0) \\] <p>Example. Suppose \\(X\\simeq X'\\), \\(Y\\simeq Y'\\), then \\(X\\times Y\\simeq X'\\times Y'\\).</p> Proof <p>From a homeomorphism \\(f:X\\rightarrow X'\\), \\(g:Y\\rightarrow Y'\\), define </p> \\[ (f\\times g)(x,y)=(f(x),g(y)). \\] <p>which is a bijection, whose inverse is \\(f^{-1}\\times g^{-1}\\).</p> <p>Prove \\(f\\times g\\) is continuous.</p> <p>Example. Heegaard decomposition of \\(S^3\\).</p> <p>(i) \\(S^3 - S^1\\simeq \\mathbb{R}^3-\\mathbb{R}\\), </p> <p></p> <p>Example. Assume \\(f: X\\rightarrow Y\\) is a bijection, then the following statement is equivalent.</p> <p>(i) \\(f\\) is open.</p> <p>(ii) \\(f\\) is closed.</p> <p>(iii) \\(f^{-1}\\) is continuous.</p>"},{"location":"Math/Complex_Analysis/","title":"Complex Analysis","text":"<p>Reference</p> <ul> <li>Complex Analysis, Elias M.Stein</li> </ul>"},{"location":"Math/Complex_Analysis/#preliminary","title":"Preliminary","text":""},{"location":"Math/Complex_Analysis/#cauchys-integral-theorem","title":"Cauchy's Integral Theorem","text":""},{"location":"Math/Complex_Analysis/#meromorphic-function","title":"Meromorphic Function","text":""},{"location":"Math/Complex_Analysis/#entire-function","title":"Entire Function","text":""},{"location":"Math/Complex_Analysis/#conformal-mappings","title":"Conformal Mappings","text":""},{"location":"Math/Complex_Analysis/#gamma-zeta-function","title":"Gamma &amp; Zeta Function","text":""},{"location":"Math/Complex_Analysis/Cauchy_Inte/","title":"Cauchy's Integral Theorem","text":""},{"location":"Math/Complex_Analysis/Cauchy_Inte/#integration-along-curves","title":"Integration along curves","text":"<p>Parameterized Curve</p> <p>A complex function \\(z(t)\\) is said to be parametrized curve, if it maps a closed interval \\([a,b]\\subset \\mathbb{R}\\) to \\(D\\subset \\mathbb{C}\\).</p> <p>A parametrized curve is said to be smooth, if \\(z'(t)\\) exists, \\(z'(t)\\neq 0, \\forall t\\in [a,b]\\) and is continuous on \\([a,b]\\). At point \\(t=a,b\\), we intepret continuity and differentiability as one-side limits. (here readers could just view \\(z\\) as a function vector with respect to variable \\(t\\)). Similar situation for piecewise-smooth.</p> <p>Equivalence of parametrized curve</p> <p>Two parametrized curves \\(z:[a,b]\\mapsto \\mathbb{C}\\) and \\(\\tilde{z}:[c,d]\\mapsto \\mathbb{C}\\) are said to ba equivalent, if there exsits a continuously differentiable bijection \\(t\\) \\((t'(s)&gt;0)\\) which maps \\([c,d]\\) to \\([a,b]\\), such that </p> \\[ \\tilde{z}(s)=z(t(s)). \\] <p>The above equivalence defines an equivalent family of \\(z(t)\\), the difference of which are the definition domains, varying among any closed interval in \\(\\mathbb{R}\\). They are all the same in the range, so we denote the whole of them as \\(\\gamma\\subset \\mathbb{C}\\), called smooth curve. Thus, a smooth curve could determine incountable parametrized curves.</p> <p>The condition \\((t'(s)&gt;0)\\) guarantee the direction of the curve. Later we would see that it determines the sign of integral.</p> <p>Reversing the direction</p> <p>For a given smooth curve \\(\\gamma\\), and its one parametrized curve \\(z:[a,b]\\mapsto \\mathbb{C}\\), we could obtain one of its reverse curve \\(\\gamma^-\\), by </p> \\[ z^-=z(b+a-t). \\] <p>so \\(z^-:[a,b]\\mapsto \\mathbb{R}\\). In fact, we could formulate infinite number of these equivalent curve, while the above one guarantees the definition domain is kept same as \\(z\\).</p> <p>Simple curve, closed curve</p> <p>A smooth curve is said to be simple, if it is not self-intersecting. Or in mathematical language, \\(z(t)\\neq z(s)\\) unless \\(s=t\\).</p> <p>A smooth curve is said to be closed, if \\(z(a)=z(b)\\) for any of its parametrizations.</p> <p>For the following parts, we call the above piecewise-smooth curve a curve for brevity.</p> <p>Example. Some examples of curves.</p> <p>(i) Circle. Set like \\(C_r(z_0)=\\{z\\in\\mathbb{C}:|z-z_0|=r\\}.\\) are called circle, with its usual parametrization (positive and negative orientation respectively)</p> \\[ z(t)=z_0+re^{i\\theta},\\quad t\\in [0,2\\pi], \\] \\[ z(t)=z_0+re^{-i\\theta},\\quad t\\in [0,2\\pi]. \\] <p>In the following chapters, we denote \\(C\\) as a general positive oriented circle.</p> <p>Riemann integral of complex function f</p> <p>Assume complex function \\(f\\) is defined on \\(\\gamma\\), where \\(\\gamma\\) is not necessarily smooth, but must be length-calculatable. For one parametrization \\(z:[a,b]\\mapsto \\mathbb{C}\\), we have partition </p> \\[ \\Delta_n: z(a)=z_0&lt;z_1&lt;\\cdots&lt;z_n=z(b) \\] <p>Choose an arbitrary point \\(\\xi_k\\) on arc from \\(z_{k-1}\\) to \\(z_k\\), \\((k=1,\\cdots,n)\\) we have Riemann summation </p> \\[ \\sum_{k=1}^n f(\\xi_k)(z_k-z_{k-1}). \\] <p>Denote \\(s_k\\) to be the length of arc from \\(z_{k-1}\\) to \\(z_k\\). If \\(\\lambda=\\max\\{s_k: 1\\leq k\\leq n\\}\\rightarrow 0\\), the above summation has a unique limit, then we call complex function \\(f\\) Riemann-integrable. And we denote the integral of \\(f\\) along \\(\\gamma\\) as </p> \\[ \\int_\\gamma f(z)dz=\\lim_{\\lambda\\rightarrow 0}\\sum_{k=1}^n f(\\xi_k)(z_k-z_{k-1}). \\] <p>Sufficient condition for Riemann integrability</p> <p>If complex function \\(f=u+iv\\) is continuous on \\(\\gamma\\) which is length-integrable, then it is Riemann-integrable on \\(\\gamma\\), and </p> \\[ \\int_\\gamma f(z)dz=\\int_\\gamma (u dx-vdy)+i\\int_\\gamma (vdx+udy). \\] Proof <p>Just compose complex number into two real parts. Denote \\(z_k=x_k+iy_k\\), \\(\\xi_k=\\eta_k+i\\zeta_k\\), \\(\\Delta x_k=x_k-x_{k-1}, \\Delta y_k=y_k-y_{k-1}\\) then \\(f(\\xi_k)=u(\\eta_k,\\zeta_k)+iv(\\eta_k,\\zeta_k)\\), so Riemann summation</p> \\[ \\begin{align*} \\sum_{k=1}^n f(\\xi_k)(z_k-z_{k-1})=&amp;\\sum_{k-1}^n [u(\\eta_k,\\zeta_k)+iv(\\eta_k,\\zeta_k)]\\cdot [(x_k-x_{k-1})+i(y_k-y_{k-1})]\\\\ =&amp;\\sum_{k=1}^n\\{ [u(\\eta_k,\\zeta_k)\\Delta x_k-v(\\eta_k,\\zeta_k)\\Delta y_k]+\\\\ &amp;i [u(\\eta_k,\\zeta_k)\\Delta y_k+v(\\eta_k,\\zeta_k)\\Delta x_k]\\} \\end{align*} \\] <p>when \\(u, v\\) is continuous on \\(\\gamma\\), the above summation converges to the needed formula.</p> <p>The above formula is easy to know by</p> \\[ f(z)dz=(u+iv)(dx+idy)=udx-vdy+i(udy+vdx). \\] <p>Integral along a smooth curve</p> <p>Assume smooth curve \\(\\gamma\\subset \\mathbb{C}\\) parametrized by \\(z: [a,b]\\mapsto \\mathbb{C}\\), and \\(f\\) is a continuous function on \\(\\gamma\\). Then the integral of \\(f\\) along \\(\\gamma\\) </p> \\[ \\int_\\gamma f(z)dz=\\int_a^b f(z(t))z'(t)dt. \\] Proof <p>The logic is simple, even a little rude: Parametrize the four real integral along \\(\\gamma\\) and then sum them up. That is, \\(f=u+iv\\), \\(z=x+iy\\), then \\(z'(t)=x'(t)+iv'(t)\\).</p> <p>Since the real and imaginary parts of integral are</p> \\[ \\int_\\gamma udx-vdy=\\int_a^b [u(x(t),y(t))x'(t)-v(x(t),y(t))y'(t)]dt \\] \\[ \\int_\\gamma vdx+udy=\\int_a^b [v(x(t),y(t))x'(t)+u(x(t),y(t))y'(t)]dt \\] <p>we have</p> \\[ \\int_\\gamma f(z)dz=\\int_a^b [u(x(t),y(t))+iv(x(t),y(t))][x'(t)+iy(t)]dt=\\int_a^b f(z(t))z'(t)dt.  \\] <p></p> <p>Example. Calculate </p> <p>(i)</p> \\[ \\int_\\gamma dz=z(b)-z(a)\\neq \\int_a^b |z'(t)|dt=\\text{length}(\\gamma) \\] <p>(ii)</p> \\[ \\int_\\gamma \\frac{dz}{(z-z_0)^n}, \\] <p>where \\(n\\in \\mathbb{Z}\\), \\(\\gamma=C_r(z_0)\\).</p> Answer for (ii) <p>(i) \\(n=1\\).</p> <p>(ii) \\(n\\neq 1\\).</p> <p>Properties for Integral of complex function</p> <p>If complex function \\(f\\) and \\(g\\) is continuous on \\(\\gamma\\) which is length-calculatable, then</p> <p>(i) \\(\\gamma^-\\) is the reverse of \\(\\gamma\\), </p> \\[ \\int_{\\gamma^-}f(z)dz=-\\int_{\\gamma}f(z)dz. \\] <p>(ii) For all \\(\\alpha, \\beta\\in \\mathbb{C}\\), </p> \\[ \\int_{\\gamma} (\\alpha f(z)+\\beta g(z))dz=\\alpha\\int_{\\gamma}f(z)dz+\\beta \\int_{\\gamma}g(z)dz. \\] <p>(iii) Inequation for absolute estimation</p> \\[ \\left|\\int_\\gamma f(z)dz\\right|\\leq \\sup_{z\\in \\gamma}|f(z)|\\cdot \\text{length}(\\gamma) \\]"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#cauchys-theorem","title":"Cauchy's Theorem","text":"<p>Definition of Primitive function</p> <p>Assume \\(f\\) is a function on the open set \\(\\Omega\\). A Primitive for \\(f\\) on \\(\\Omega\\) is a function \\(F\\) that is holomorphic on \\(\\Omega\\), such that \\(F'(z)=f(z),\\forall z\\in \\Omega\\).</p> <p>Cauchy's Theorem</p> <p>If a continuous function \\(f\\) has a primitive \\(F\\) on open set \\(\\Omega\\), and \\(\\gamma\\) is a piecewise-smooth curve begins at \\(\\omega_1\\) and ends at \\(\\omega_2\\), then </p> \\[ \\int_\\gamma f(z)dz=F(\\omega_2)-F(\\omega_1). \\] Proof <p>(i) For \\(\\gamma\\) is smooth.</p> <p>Choose a parametrization \\(z: [a,b]\\mapsto \\mathbb{C}\\) of \\(\\gamma\\), such that \\(z(a)=\\omega_1, z(b)=\\omega_2\\).</p> \\[ \\begin{align*} \\int_\\gamma f(z)dz&amp;=\\int_a^b f(z(t))z'(t)dt\\\\ &amp;=\\int_a^b F'(z(t))z'(t)dt\\\\ &amp;=\\int_a^b \\frac{d}{dt}F(z(t))dt\\\\ &amp;=F(z(b))-F(z(a))=F(\\omega_2)-F(\\omega_1). \\end{align*} \\] <p>where the third \"\\(=\\)\" is by chain rule, and the fouth \"\\(=\\)\" is by fundamental theorem of calculas.</p> <p>(ii) For \\(\\gamma\\) is piecewise-smooth, just add these smooth part up.</p> <p>The condition and result of the above theorem could be replaced by the following statement.</p> Another language <p>\\(F\\) is holomorphic and \\(F'\\) is continuous on \\(\\Omega\\), then </p> \\[ \\int_\\gamma F'(z)dz=F(\\omega_2)-F(\\omega_1). \\] <p> </p> <p>Corollary: Cauchy's closed curve theorem</p> <p>If \\(\\gamma\\) is a closed smooth curve in an open set \\(\\Omega\\), and \\(f\\) is continuous and has a primitive in \\(\\Omega\\), then </p> \\[ \\int_\\gamma f(z)dz=0. \\] <p>The condition and result of the above theorem could be replaced by the following statement.</p> Another language <p>\\(F\\) is holomorphic and \\(F'\\) is continuous on \\(\\Omega\\), then </p> \\[ \\int_\\gamma F'(z)dz=0. \\] <p></p> <p>Corollary 2: zero-derivatives means constant function</p> <p>If \\(f\\) is holomorphic in a region \\(\\Omega\\), and \\(f'=0\\), then \\(f\\) is a constant.</p> Proof <p>\\(\\forall \\omega_0\\in \\Omega\\), \\(\\forall \\omega\\),</p> \\[ f(\\omega)-f(\\omega_0)=\\int_\\gamma f'(z)dz=0. \\] <p>so \\(f(\\omega)=f(\\omega_0)\\).</p> <p>Actually the above theorems also hold for general curves.</p>"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#goursat-theorem","title":"Goursat Theorem","text":"<p>Goursat's Theorem</p> <p>Assume \\(\\Omega\\) is an open set in \\(\\mathbb{C}\\). If \\(f\\in H(\\Omega)\\), then for any a triangle \\(T\\subset \\Omega\\) whose interior is also contained in \\(\\Omega\\),  </p> \\[ \\int_T f(z)dz=0. \\] Proof <p>Bisect \\(\\mathcal{T}^0\\), we have a sequence of triangles</p> \\[ \\mathcal{T}^0\\supset \\mathcal{T}^1\\supset\\cdots  \\] <p>use nested compact set theorem to get a point \\(z_0\\) in these subtriangles.</p> <p>Use properties of holomorphic function, </p> \\[ f(z)=f(z_0)+f'(z_0)(z-z_0)+\\psi(z)(z-z_0) \\] <p>where \\(\\lim\\limits_{z\\rightarrow z_0}\\psi(z)=0\\). The first and second items of the rightside equation equals zero after integrating over a closed curve, while last item is bounded by a small \\(\\varepsilon_n=\\sup_{z\\in T^{n}}|\\psi(z)|\\). That is,</p> \\[ \\left|\\int_{T^n}fdz\\right|=\\left|\\int_{T^n}\\psi (z-z_0) dz\\right|\\leq \\varepsilon_n d^{(n)}p^{(n)}. \\] <p>where \\(d^{(n)}, p^{(n)}\\) denote the diameter and perimeter of the \\(n\\)th triangles. Each time after bisection, we have</p> \\[ d^{(n+1)}\\leq \\frac{1}{2}d^{(n)},\\quad p^{(n+1)}\\leq \\frac{1}{2}p^{(n)} \\] <p>and the integral along \\(T^{(n)}\\), must hold for some \\(j\\in \\{0,1,2,3\\}\\)</p> \\[ \\left|\\int_{T^{(n)}} f(z)dz\\right|\\leq 4\\left|\\int_{T^{(n+1)}_j} f(z)dz\\right| \\] <p>So </p> \\[ \\begin{align*} \\left|\\int_{T^{(0)}} f(z)dz\\right|&amp;\\leq 4^n \\left|\\int_{T^{(n)}_j} f(z)dz\\right|\\\\ &amp;\\leq 4^n \\varepsilon_n d^{(n)}p^{(n)}\\\\ &amp;\\leq 4^n \\varepsilon_n 4^{-n} d^{(0)}p^{(0)}=\\varepsilon_n\\rightarrow 0(n\\rightarrow \\infty) \\end{align*} \\] <p>Corollary: integral in Rectangle curve</p> <p>Assume \\(f\\in H(\\Omega)\\). For any rectangle \\(R\\subset \\Omega\\) whose interior is also contained in \\(\\Omega\\), </p> \\[ \\int_T f(z)dz=0. \\]"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#local-existence-of-primitive","title":"Local existence of Primitive","text":"<p>By constructing method.</p> <p>A holomorphic function has a primitive</p> <p>Assume \\(f\\) is a holomorphic function in an open disc, then \\(f\\) has a primitive in the disc.</p> <p>Actually we use Goursat's Theorem and continuity of \\(f\\), without utilizing Holomorphism of \\(f\\). That is, a continuous function on an open set must have a primitive, if it satisfies that its integral on every triangle on \\(\\Omega\\) equals zero. In the following parts, we will show that this equals to holomorphic.</p> Proof <p>Define </p> \\[ F(z)=\\int_{\\gamma_z} f(\\omega)d\\omega, \\] <p>where \\(\\gamma_z\\) is a polygomal line from \\(0\\) to \\(z\\).</p> <p>by Goursat's Theorem, we can show that</p> \\[ \\begin{align} F(z+h)-F(z)=\\int_\\eta f(\\omega)d\\omega,\\label{primitive} \\end{align} \\] <p>where \\(\\eta\\) is a straight line from \\(z\\) to \\(z+h\\).</p> <p>Then use the continuity of \\(f\\) in \\(\\Omega\\), i.e at point \\(z\\), </p> \\[ f(\\omega)=f(z)+\\psi(\\omega) \\] <p>where \\(\\lim\\limits_{\\omega\\rightarrow z}\\psi(\\omega)=0\\). Substitute in equation \\(\\ref{primitive}\\), we have</p> \\[ \\begin{align*} F(z+h)-F(z)&amp;=f(z)\\int_\\eta d\\omega + \\int_\\eta \\psi(\\omega)d\\omega.\\\\ \\frac{F(z+h)-F(z)}{h}&amp;=f(z)+\\frac{\\int_\\eta \\psi(\\omega)d\\omega}{h} \\end{align*} \\] <p>the latter integral goes to \\(0\\) as \\(h\\) tend to \\(0\\) because</p> \\[ \\left|\\frac{\\int_\\eta \\psi(\\omega)d\\omega}{h} \\right|\\leq \\frac{\\sup\\limits_{\\omega\\in \\eta}|\\psi(\\omega)|\\cdot |h|}{|h|} \\] <p>So</p> \\[ \\lim_{h\\rightarrow 0}\\frac{F(z+h)-F(z)}{h}=f(z). \\] <p>Corollary: Cauchy's Theorem for a disc</p> <p>Assume \\(D\\) is an open disc, and \\(f\\in H(D)\\). Then for closed curve \\(\\gamma\\),</p> \\[ \\int_\\gamma f(z)dz=0 \\] Proof <p>By using Corollary: Cauchy's closed curve theorem.</p> <p>Actually, Cauchy's Theorem could apply to any closed curve with unambiguious interior, not just a disc, whose interior is clear.</p>"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#cauchys-integral-formula","title":"Cauchy's Integral Formula","text":"<p>Using the following integral formula, we could show that holomorphic function has derivatives of arbitrary order \\(n\\), and could be expanded by power series.</p> <p>Cauchy's integral formula</p> <p>Assume \\(f\\) is holomorphic in an open set that contains the closure of a disc \\(D\\). If \\(C\\) denotes the boundary circle of this disc with the positive orientation, then for any point \\(z\\in D\\), </p> \\[ f(z)=\\frac{1}{2\\pi i}\\int_C \\frac{f(\\zeta)}{\\zeta-z}d\\zeta. \\] Proof <p>We could use a \"key hole\" to prove. Look the following image.</p> <p><p> </p></p> <p>Let \\(F(z)=\\frac{f(\\zeta)}{\\zeta-z}\\), then it is holomorphic away from point \\(z\\), so</p> \\[ \\int_{\\gamma} F(\\zeta)d\\zeta=0=\\int_{C_\\varepsilon^+} F(\\zeta)d\\zeta + \\int_{\\Gamma^+}F(\\zeta)d\\zeta +\\int_{\\Gamma^-}F(\\zeta)d\\zeta +\\int_{C_r^-}F(\\zeta)d\\zeta. \\] <p>Let \\(\\delta\\rightarrow 0\\), we have \\(\\int_{\\Gamma^+}F(\\zeta)d\\zeta +\\int_{\\Gamma^-}F(\\zeta)d\\zeta=0\\). If we rewrite </p> \\[ F(z)=\\frac{f(\\zeta)-f(z)}{\\zeta-z}+\\frac{f(z)}{\\zeta-z}, \\] <p>and by the example commen integral, we know</p> \\[ \\int_{C_\\varepsilon^+}\\frac{f(z)}{\\zeta-z}d\\zeta=2\\pi i f(z), \\] <p>and \\(\\frac{f(\\zeta)-f(z)}{\\zeta-z}\\) is bounded, whose integral on \\(C_\\varepsilon^+\\) converges to \\(0\\). Actually \\(\\frac{f(\\zeta)-f(z)}{\\zeta-z}\\) has a removable singularity at \\(z\\), we could extend it into a holomorphic function </p> \\[ g(\\zeta)=\\begin{cases} \\frac{f(\\zeta)-f(z)}{\\zeta-z},\\quad &amp;\\zeta\\neq z\\\\ f'(z),\\quad &amp;\\zeta=z \\end{cases}. \\] <p>so its integral vanishes. So </p> \\[ \\int_{C_\\varepsilon^+}F(\\zeta)d\\zeta = 2\\pi i f(z). \\] <p>and we are done.</p> <p>The following theorem is amazing, called regularity.</p> <p></p> <p>Corollary: Cauchy integral formula with Infinitely many derivatives</p> <p>If \\(f\\) is holomorphic in an open set \\(\\Omega\\), then \\(f\\) has infinitely many complex derivatives in \\(\\Omega\\).</p> <p>Moreover, if \\(C\\subset \\Omega\\) is a circle whose interier is also contained in \\(\\Omega\\), then </p> \\[ f^{(n)}(z)=\\frac{n!}{2\\pi i}\\int_C \\frac{f(\\zeta)}{(\\zeta-z)^{n+1}}d\\zeta. \\] <p>for all \\(z\\) in the interior of \\(C\\)</p> Proof <p>By induction on \\(n\\). The theorem holds for \\(n=0\\) by Cauchy's integral formula.</p> <p>Assume it holds for \\(n=k\\), i.e.</p> \\[ f^{(k)}(z)=\\frac{k!}{2\\pi i}\\int_C \\frac{f(\\zeta)}{(\\zeta-z)^{k+1}}d\\zeta. \\] <p>Then we have its quotient</p> \\[ \\begin{align} \\frac{f^{(k)}(z+h)-f^{(k)}(z)}{h}&amp;=\\frac{k!}{2\\pi i h}\\int_C f(\\zeta) \\left[\\frac{1}{(\\zeta-z-h)^{k+1}} -\\frac{1}{(\\zeta-z)^{k+1}}\\right] d\\zeta.\\label{quotient} \\end{align} \\] <p>Notice that \\(A^{k+1}-B^{k+1}=(A-B)\\left(\\sum_{i=0}^{k} A^i B^{k-i}\\right)\\), with </p> \\[ A-B = \\frac{1}{\\zeta-z-h} -\\frac{1}{\\zeta-z}=\\frac{h}{(\\zeta-z-h)(\\zeta-z)} \\] <p>so if we choose \\(h\\) small enough, then equation \\(\\ref{quotient}\\) becomes</p> \\[ \\begin{align*} \\lim_{h\\rightarrow 0}\\frac{f^{(k)}(z+h)-f^{(k)}(z)}{h}&amp;=\\lim_{h\\rightarrow 0}\\frac{k!}{2\\pi i h}\\int_C f(\\zeta) \\frac{h}{(\\zeta-z-h)(\\zeta-z)} \\left[\\sum_{i=0}^{k} \\left(\\frac{1}{\\zeta-z-h}\\right)^i \\left(\\frac{1}{\\zeta-z}\\right)^{k-i}\\right] d\\zeta\\\\ &amp;=\\lim_{h\\rightarrow 0}\\frac{k!}{2\\pi i}\\int_C f(\\zeta) \\frac{1}{(\\zeta-z-h)(\\zeta-z)} \\left[\\sum_{i=0}^{k} \\left(\\frac{1}{\\zeta-z-h}\\right)^i \\left(\\frac{1}{\\zeta-z}\\right)^{k-i}\\right] d\\zeta\\\\ &amp;=\\frac{k!}{2\\pi i}\\int_C \\lim_{h\\rightarrow 0}f(\\zeta) \\frac{1}{(\\zeta-z-h)(\\zeta-z)} \\left[\\sum_{i=0}^{k} \\left(\\frac{1}{\\zeta-z-h}\\right)^i \\left(\\frac{1}{\\zeta-z}\\right)^{k-i}\\right] d\\zeta\\\\ &amp;=\\frac{k!}{2\\pi i}\\int_C \\frac{1}{(\\zeta-z)^2} \\frac{k+1}{(\\zeta-z)^{k}}d\\zeta\\\\ &amp;=\\frac{(k+1)!}{2\\pi i}\\int_C \\frac{f(\\zeta)}{(\\zeta-z)^{k+2}}d\\zeta. \\end{align*} \\] <p>The third \"\\(=\\)\" holds because when \\(h\\rightarrow 0\\), the integrated function converges uniformly whenever \\(\\zeta\\in C\\).</p> <p></p> <p>Corollary: Cauchy Inequalities</p> <p>If \\(f\\) is holomorphic in an open set that contains the closure of a disc \\(D\\) centered at \\(z\\) and of radius \\(R\\) (or \\(D_R(z)\\)), then</p> \\[ |f^{(n)}(z)|\\leq \\frac{n!\\|f\\|_{\\partial D}}{R^n}, \\] <p>where \\(\\|f\\|_{\\partial D}=\\sup\\limits_{z\\in \\partial D}|f(z)|\\) denotes the supremum of \\(|f|\\) on the boundary circle \\(\\partial D\\).</p> Proof <p>Using Cauchy integral formula to estimate</p> \\[ \\begin{align*} |f^{(n)}(z)|&amp;\\leq \\frac{n!}{2\\pi}\\left|\\int_{\\partial D} \\frac{f(\\zeta)}{(\\zeta-z)^{n+1}}d\\zeta\\right|\\\\ &amp;=\\frac{n!}{2\\pi} \\left| \\int_{\\partial D} \\frac{f(\\zeta)}{R^{n+1}}d\\zeta\\right|\\\\ &amp;\\leq \\frac{n!}{2\\pi} \\frac{\\sup\\limits_{\\zeta\\in \\partial D}|f(\\zeta)|}{R^{n+1}} \\left|\\int_{\\partial D} d\\zeta \\right|\\\\ &amp;=\\frac{n!\\|f\\|_{\\partial D}}{2\\pi R^{n+1}} \\cdot 2\\pi R\\\\ &amp;=\\frac{n!\\|f\\|_{\\partial D}}{R^n}.   \\end{align*} \\]"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#power-series-expansion","title":"Power Series Expansion","text":"<p>Theorem of holomorphic function with power series expansion</p> <p>Assume \\(f\\) is holomorphic in an open set \\(\\Omega\\). If \\(D_R(z_0)\\) is a disc centered at \\(z_0\\), whose closure is contained in \\(\\Omega\\), then \\(f\\) has a power series expansion at \\(z_0\\)</p> \\[ f(z)=\\sum_{n=0}^\\infty a_n (z-z_0)^n,\\quad \\forall z\\in D_R(z_0) \\] <p>with coefficients given by</p> \\[ a_n=\\frac{f^{(n)}(z_0)}{n!},\\quad n\\geq 0. \\] Proof <p>Fix \\(z\\in D\\). From Cauchy's integral formula, we have</p> \\[ f(z)=\\frac{1}{2\\pi i}\\int_{\\partial D} \\frac{f(\\zeta)}{\\zeta-z}d\\zeta. \\] <p>The following deduction is classic.</p> \\[ \\frac{1}{\\zeta-z}=\\frac{1}{(\\zeta-z_0)-(z-z_0)}=\\frac{1}{(\\zeta-z_0)}\\frac{1}{1-\\frac{z-z_0}{\\zeta-z_0}} \\] <p>the last item could be expanded by geometric series expansion, which is uniformly convengent for \\(\\zeta\\in \\partial D\\) because for fixed \\(z\\) and \\(z_0\\), \\(\\exists r\\in (0,1)\\), s.t. \\(\\frac{|z-z_0|}{|\\zeta-z_0|}&lt;r\\). Therefore</p> \\[ \\frac{1}{1-\\frac{z-z_0}{\\zeta-z_0}}=\\sum_{n=0}^\\infty \\left(\\frac{z-z_0}{\\zeta-z_0}\\right)^n \\] <p>so</p> \\[ \\begin{align*} f(z)&amp;=\\frac{1}{2\\pi i}\\int_{\\partial D}\\frac{f(\\zeta)}{(\\zeta-z_0)} \\sum_{n=0}^\\infty \\left(\\frac{z-z_0}{\\zeta-z_0}\\right)^n d\\zeta\\\\ &amp;=\\sum_{n=0}^\\infty \\frac{1}{2\\pi i}\\int_{\\partial D}\\frac{f(\\zeta)}{(\\zeta-z_0)}  \\left(\\frac{z-z_0}{\\zeta-z_0}\\right)^n d\\zeta\\quad \\text{by series convergence}\\\\ &amp;=\\sum_{n=0}^\\infty\\left( \\frac{1}{2\\pi i}\\int_{\\partial D}\\frac{f(\\zeta)}{(\\zeta-z_0)^{n+1}}d\\zeta \\right) (z-z_0)^n, \\end{align*} \\] <p>and we are done.</p> <p>This gives another perspective to see that holomorphic function has infinitely many derivatives in terms of power series.</p> <p>The above proof also tells us the power series converges in where \\(f\\) is holomorphic. If \\(f\\) is holomorphic on \\(\\mathbb{C}\\), then the power series converges on whole \\(\\mathbb{C}\\).</p> <p>Liouville's Theorem</p> <p>If \\(f\\) is entire and bounded, then \\(f\\) is constant.</p> Proof <p>For arbitrary \\(z\\in \\mathbb{C}\\) and arbitrary \\(R&gt;0\\) (D_R(z)), using Cauchy Inequalities</p> \\[ |f'(z)|\\leq \\frac{B}{R}\\rightarrow 0,\\quad R\\rightarrow\\infty. \\] <p>So \\(f'=0\\), then by zero-derivatives means constant function, we are done.</p>"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#analytic-continuation","title":"Analytic Continuation","text":"<p>The genetic code of a holomorphic function is determined if we know its values on appropriate arbitrarily small subsets. </p> <p>Theorem for zero accumulation</p> <p>Assume \\(f\\) is a holomorphic function in a region \\(\\Omega\\), and \\(f\\) vanishes on a sequence of distinct points with a limit point in \\(\\Omega\\), then \\(f\\) is identically zero.</p> Proof <p>Suppose \\(z_0\\in \\Omega\\) is a limit point for sequence \\(w_n\\) which satisfies \\(f(w_n)=0\\). </p> <ul> <li>We first show that \\(f(z)=0\\) for \\(z\\in B_r(z_0)\\), where \\(r\\) is a small enough number. </li> </ul> <p>Easy to have choose \\(r\\) such that \\(B_r(z_0)\\subset \\Omega\\), and we could use power series expansion of \\(f\\)</p> \\[ f(z)=\\sum_{n=0}^\\infty a_n (z-z_0)^n,\\quad \\forall z\\in D_r(z_0). \\] <p>If \\(f(z)\\) does not vanish in \\(D_r(z_0)\\), then there must exists a smallest integer \\(m\\) such that \\(a_m\\neq 0\\). So expansion becomes</p> \\[ f(z)=\\sum_{n=m}^\\infty a_n (z-z_0)^n=a_m(z-z_0)^m(1+g(z-z_0)). \\] <p>where \\(g(z-z_0)\\rightarrow 0 (z\\rightarrow z_0)\\). Since \\(z_0\\) is a limit point, then no matter how small \\(r\\) is, \\(\\exists w_K\\in B_r(z_0)\\), such that \\(w_K\\neq z_0\\), \\(f(w_K)=0\\), but \\(w_K-z_0\\neq 0\\) and \\(1+g(w_K-z_0)\\neq 0\\), contradicting!</p> <p>Actually, \\(f(z)=(z-z_0)^m h(z)\\), here \\(h(z_0)=a_m\\neq 0\\). By continuity of \\(f\\) in \\(D_r(z_0)\\), we have \\(f(z)\\neq 0\\) for \\(z\\in D_\\delta(z_0)\\). But we could always find a point of \\(w_K\\in D_\\delta(z_0)\\) such that \\(f(w_K)=0\\) which contradicts!</p> <ul> <li>Extend to the whole region \\(\\Omega\\) using connectedness. One method is by iterating the above process, since after one round, we have more points which satisfies \\(f(z)=0\\). We continue this extension until the boundary of the region. Another method is by the following trick.</li> </ul> <p>We define \\(U\\) to be the interior of sets of points \\(z\\) which satisfies \\(f(z)=0\\). It is open by definition. It is also closed, which is not easy to see, since choose \\(z_n\\rightarrow z\\), and \\(f(z_n)=0\\), then \\(f(z)=0\\) by continuity. So \\(f\\) vanishes in the neighborhood of \\(z\\) as we have discussed above. So \\(z\\in U\\). Define \\(V=\\Omega-U\\) is open, i.e. we find two disjoint open set to compose \\(\\Omega\\). However since \\(\\Omega\\) is connected, it contradicts!</p> <p>Corollary</p> <p>Assume \\(f\\) and \\(g\\) are holomorphic in a region \\(\\Omega\\) and \\(f(z)=g(z)\\) for all \\(z\\) in some non-empty open subset of \\(\\Omega\\), then \\(f(z)=g(z)\\) throughout \\(\\Omega\\).</p> <p>Definition of Analytic Continuation</p> <p>Given a pair of functions \\(f\\) and \\(f\\), which are holomorphic in region \\(\\Omega\\) and \\(\\Omega'\\), respectively. If \\(f=F\\) on \\(\\Omega\\), then we say \\(F\\) is a analytic continuation of \\(f\\) into the region \\(\\Omega'\\).</p> <p>In the following chapters, we would formulate a equation of a function. If the righthand formula is defined larger then the lefthand formula, then we could say the righthand function is an analytic continuation of the lefthand formula.</p>"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#applications","title":"Applications","text":""},{"location":"Math/Complex_Analysis/Cauchy_Inte/#moreras-theorem","title":"Morera's Theorem","text":"<p>Morera's Theorem</p> <p>Suppose \\(f\\) is a continuous function on a open disc \\(D\\) such that for any triangle \\(T\\) contained in \\(D\\),</p> \\[ \\int_T f(z)dz=0, \\] <p>then \\(f\\) is holomorphic on \\(D\\).</p> Proof <p>Just use the proof process in Local existence of Primitive.</p>"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#sequence-of-holomorphic-functions","title":"Sequence of holomorphic functions","text":"<p>Note we only talk about the uniform convergence on a compact subset of an open set \\(\\Omega\\).</p> <p>Sequence of holomorphic functions</p> <p>Assume \\(\\{f_n\\}\\) is a sequence of holomorphic functions that converges uniformly to a function \\(f\\) in every compact subset of \\(\\Omega\\), then \\(f\\) is holomorphic in \\(\\Omega\\).</p> <p>Moreover, sequence \\(\\{f'_n\\}\\) converges uniformly to \\(f'\\) on every compact subset of \\(\\Omega\\).</p> Proof <p>Notice \"compact\" guarantees \\(f\\) is also in \\(\\Omega\\).</p> <p>Use Goursat's Theorem, we have for any triangle \\(T\\) in a closed disc \\(D\\) contained in \\(\\Omega\\) </p> \\[ \\int_T f_n(z)dz=0. \\] <p>By mathematical analysis, since \\(f_n\\rightrightarrows f\\), \\(f\\) is continuous, we could change the order of limit and integral</p> \\[ \\int_T f(z)dz =\\lim_{n\\rightarrow \\infty}\\int_T f_n(z)dz=0. \\] <p>By Morera's Theorem, \\(f\\) is holomorphic.</p> <p>For additional parts, we have to show that </p> \\[ \\sup_{z\\in \\Omega_\\delta}|F'(z)|\\leq \\frac{1}{\\delta}\\sup_{\\xi\\in\\Omega}|F(\\xi)| \\] <p>where \\(\\Omega_\\delta=\\{z\\in \\Omega: \\overline{D}_\\delta(z)\\in \\Omega\\}\\). Since for every compact subset \\(K\\) of \\(\\Omega\\), there exsits \\(\\delta\\) such that \\(\\Omega_\\delta\\supset K\\).</p> <p>We usually construct a holomorphic function using</p> \\[ F(z)=\\sum_{i=1}^\\infty f_i(n). \\] <p>If \\(f_i\\) is holomorphic, and the above series converges uniformly to \\(F\\), then \\(F\\) is holomorphic.</p>"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#in-terms-of-integrals","title":"In terms of Integrals","text":"<p>Holomorphic function in terms of integrals</p> <p>Assume \\(F(z,s)\\) defined on \\(\\Omega\\times [0,1]\\), satisfies</p> <p>(i) \\(F(z,s)\\) is holomorphic with respect to \\(z\\) for each fixed \\(s\\);</p> <p>(ii) \\(F(z,s)\\) is continuous in \\(\\Omega\\times [0,1]\\);</p> <p>then a function \\(f\\) defined by</p> \\[ f=\\int_0^1 F(z,s)ds \\] <p>is holomorphic on \\(\\Omega\\).</p> Proof <p>We could use Sequence of holomorphic functions.</p> <p>Define Riemann summation</p> \\[ f_n(z):=\\frac{1}{n}\\sum_{i=1}^n F(z,\\frac{i}{n}) \\] <p>which is holomorphic on \\(\\Omega\\). We claim \\(f\\) is a uniform convergence of \\(f_n\\). We could use continuous function in a compact set is uniformly continuous, i.e. \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\forall |s_1-s_2|\\leq \\delta\\), \\(\\forall\\) disc \\(D\\subset \\Omega\\),</p> \\[ |F(z, s_1)-F(z,s_2)|&lt;\\varepsilon,\\quad \\forall z\\in D. \\] <p>Actually</p> \\[ \\begin{align*} \\left|f_n-f\\right|&amp;=\\left|\\sum_{i=1}^{n}\\int_\\frac{i-1}{n}^\\frac{i}{n} \\left[F\\left(z,\\frac{i}{n}\\right)-F(z,s)\\right]ds\\right|\\\\ &amp;\\leq \\sum_{i=1}^{n}\\int_\\frac{i-1}{n}^\\frac{i}{n} \\left|F\\left(z,\\frac{i}{n}\\right)-F(z,s)\\right|ds\\\\ &amp;\\leq \\sum_{i=1}^{n} \\frac{\\varepsilon}{n}=\\varepsilon. \\end{align*} \\]"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#homotopies-and-simply-connected-domains","title":"Homotopies and simply connected domains","text":"<p>Definition of Homotopies</p> <p>Assume two continuous curves \\(\\gamma_0\\), \\(\\gamma_1\\) defined on an open set \\(\\Omega\\), are given by a parametrization \\(\\gamma_0(t)\\) and \\(\\gamma_1(t)\\), \\(t\\in [a,b]\\), with common end-points</p> \\[ \\gamma_0(a)=\\gamma_1(a)=\\alpha,\\quad \\gamma_0(b)=\\gamma_1(b)=\\beta \\] <p>then the above curve are said to be homotopic if \\(\\forall s\\in [0,1]\\), there exsits another curve \\(\\gamma_s\\subset \\Omega\\), parametrized with \\(\\gamma_s(t)\\), \\(t\\in [a,b]\\), such that</p> \\[ \\gamma_s(a)=\\alpha, \\quad \\gamma_s(b)=\\beta, \\] <p>and \\(\\gamma_s(t)|_{s=0}=\\gamma_0(t)\\), and \\(\\gamma_s(t)|_{s=1}=\\gamma_1(t)\\). Moreover, \\(\\gamma_s(t)\\) is continuous jointly on \\(s\\in [0,1]\\) and \\(t\\in [a,b]\\).</p> <p>Loosely speaking, two curves are homotopic if one curve could be continuously deformed into the other without leaving \\(\\Omega\\).</p> <p>Theorem for homotopic curves</p> <p>Assume two curves \\(\\gamma_0\\) and \\(\\gamma_1\\) with same end-points are homotopic in an open set \\(\\Omega\\), \\(f\\) is holomorphic on \\(\\Omega\\), then </p> \\[ \\int_{\\gamma_0}f(z)dz=\\int_{\\gamma_1}f(z)dz. \\] <p>Definition of Simply connected region</p> <p>A region (connected) \\(\\Omega\\) is said to be simply connected, if for all two curves \\(\\gamma_0,\\gamma_1\\subset \\Omega\\) with same end-points, they are homotopic.</p> <p></p> <p>Theorem for primitives on simply connected region</p> <p>Every function on a simply connected region has a primitive.</p> Proof <p>just the same logic as our proof in Local existence of Primitive, where we use rectangular to formulate a primitive. Here we just use a continuous curve.</p> <p>The punctured plane is not simply connected, since on which \\(f=\\frac{1}{z}\\) is holomorphic, however \\(\\int_{C_\\varepsilon}\\frac{1}{z}dz=2\\pi i\\neq 0\\). This means \\(f\\) does not have a primitive on \\(D_\\delta(0)-\\{0\\}\\), which implies it is not simply connected.</p>"},{"location":"Math/Complex_Analysis/Cauchy_Inte/#the-complex-logarithm","title":"The complex Logarithm","text":"<p>Simply connected region guarantees the uniqueness of complex logarithm,</p> \\[ \\log z=\\log r+ i\\theta. \\] <p>with \\(z=re^{i\\theta}\\).</p> <p>The following theorem makes use of the inspiration that \\(\\log z\\) is a primitive of \\(\\frac{1}{z}\\) on a simply connected region. So we could construct a logarithm function.</p> <p>Theorem for complex logarithm</p> <p>Assume \\(\\Omega\\) is a simply connected region with \\(1\\in \\Omega\\) and \\(0\\not\\in \\Omega\\). Then there exsits a holomorphic function \\(F(z)=\\log (z)\\), such that</p> <p>(i) \\(F\\) is holomorphic on \\(\\Omega\\);</p> <p>(ii) \\(e^{F(z)}=z\\) for all \\(z\\in \\Omega\\),</p> <p>(iii) \\(F(r)=\\log r, \\forall r\\) whenever \\(r\\) is a real number and near \\(1\\).</p> Proof <p>(i) Assume \\(\\gamma\\subset \\Omega\\) starts from \\(1\\) and ends at \\(z\\), define </p> \\[ F(z):=\\int_\\gamma \\frac{1}{w}dw. \\] <p>arguing as Theorem for primitives on simply connected region, since \\(\\frac{1}{z}\\) is holomorphic on \\(\\Omega\\), we have \\(F\\) is a primitive of \\(\\frac{1}{z}\\) and </p> \\[ F'=\\frac{1}{z}. \\] <p>(ii) Equivalently, we have to prove </p> \\[ e^{-F(z)}z=1. \\] <p>Actually, \\(\\frac{d}{dz}e^{-F(z)}z=e^{-F(z)}(1-F'(z)z)=0\\), so by zero-derivatives means constant function, we have \\(e^{-F(z)}z=C\\). Given the initial value \\(F(1)=0\\), we have \\(c=1\\) and we prove (ii).</p> <p>(iii) Still by definition \\(F(r)=\\int_1^r\\frac{1}{w}dw=\\log r\\).</p> <p>Choose \\(\\Omega=\\mathbb{C}-(-\\infty, 0]\\), then we have the principle branch of logarithm</p> \\[ \\log z=\\log r+i\\theta \\] <p>with \\(|\\theta|&lt;\\pi\\). We define it by a specific curve, which starts from \\(1\\), firstly goes straight to \\(r\\) and then rotate about \\(\\theta\\) positively.</p> \\[ \\log z=\\int_1^r\\frac{1}{w}dw+\\int_0^\\theta \\frac{1}{re^{it}}rie^{it}dt=\\log r+o\\theta. \\] <p>If \\(g=\\log f\\), then \\(g'=\\frac{f'}{f}\\). So we could construct a primitive using integral of \\(\\frac{f'}{f}\\) to get \\(g\\) from a arbitrary function which is holomorphic on a simply connected region.</p> <p></p> <p>Theorem for generating logarithm of a function</p> <p>Assume \\(\\Omega\\) is a simply connected region, \\(f\\) is holomorphic on \\(\\Omega\\) and does not vanish, then there exists a holomorphic function such that \\(e^{g(z)}=f(z)\\).</p> Proof <p>Still the same logic of constructing a primitive of \\(\\frac{f'}{f}\\).</p> <p>Define</p> \\[ g(z)=\\int_\\gamma \\frac{f'(w)}{f(w)}dw+c_0 \\] <p>where \\(\\gamma\\subset \\Omega\\) starts from \\(z_0\\in \\Omega\\) and ends at \\(z\\), and \\(c_0\\) is used for compensation such that \\(e^g(z_0)=f(z_0)=e^{c_0}\\). Arguing as Theorem for primitives on simply connected region, we have \\(g(z)\\) is holomorphic and</p> \\[ g'(z)=\\frac{f'(z)}{f(z)}. \\] <p>To prove \\(e^{g(z)}=f(z)\\) we only need to prove \\(f(z)e^{-g(z)}=1\\). Actually</p> \\[ \\frac{d}{dz}\\left[f(z)e^{-g(z)}\\right]=e^{-g(z)}(f'(z)-g'(z)f(z))=0 \\] <p>so \\(f(z)e^{-g(z)}=c\\). Notice that \\(g(z_0)=c_0\\), so we have \\(c=f(z_0)e^{-c_0}=1\\). So \\(e^{g(z)}=f(z)\\).</p> <p>Example. Assume \\(f(z)=1+z\\), use</p> \\[ \\frac{f'}{f}=\\frac{1}{1+z}=\\sum_{n=0}^\\infty (-1)^n z^n. \\] <p>near \\(z=0\\) to show that</p> \\[ \\log(1+z)=\\sum_{n=0}^\\infty \\frac{(-1)^n z^{n+1}}{n+1}. \\] Proof <p>We know from the above theorem there exists a function \\(g\\) such that \\(e^{g(z)}=1+z\\). We have to prove that \\(\\log(1+z)\\) is primitive of \\(\\frac{1}{1+z}\\). Just take a derivative on both sides of the desired equaiton and show that they equal by condition. So they actually differ by a constant. Then use initial value at \\(z=0\\) to show that the constant is \\(0\\).</p>"},{"location":"Math/Complex_Analysis/Conf_Map/","title":"Conformal Mappings","text":"<p>A bijective holomorphic function \\(f: U\\rightarrow V\\) is called a conformal map or biholomorphism. Given such a mapping \\(f\\), we call \\(U\\) and \\(V\\) are conformally equivalent, or simply biholomorphic.</p> <p>If A holomorphic function is injective, whose inverse is automatically holomorphic.</p> <p></p> <p>Holomorphism of the inverse of a conformal mapping</p> <p>Assume a holomorphic function \\(f:U\\rightarrow V\\) is injective, then \\(f'(z)\\neq 0\\) for all \\(z\\in U\\). Moreover, the inverse of \\(f\\) defined on its range is holomorphic.</p> HintsProof <p>Using power series expansion and contradiction of injection.</p> <ul> <li>Show the first proposition. Show by contrsdiction.</li> </ul> <p>If not, there exists \\(z_0\\in U\\) such that \\(f'(z_0)=0\\) and assume \\(f'(z)\\neq 0\\) for all \\(z\\in U\\) except \\(z_0\\). Then expand \\(f\\) using power series</p> \\[ f(z)-f(z_0)=a(z-z_0)^k +G(z) \\] <p>where \\(a\\neq 0, k\\geq 2\\) and \\(G(z)\\) vanishes to order \\(k+1\\) at \\(z_0\\). For sufficiently small \\(w\\) (significant, for two distinct zeros, not multiple roots), write</p> \\[ f(z)-f(z_0) - w :=F(z)+G(z), \\quad F(z):=a(z-z_0)^k-w \\] <p>Choose a small enough circle centered at \\(z_0\\) such that \\(|F(z)|&gt;|G(z)|\\). Since \\(F(z)\\) has at least \\(2\\) zeros inside the circle, by Rouch\u00e9 Theorem \\(f(z)-f(z_0)-w\\) has at least \\(2\\) zeros inside the circle. We claim the zeros of \\(f(z)-f(z_0)-w\\) is distinct, hence \\(f\\) is not injective. Indeed, since \\(f'(z)\\neq z\\) for all \\(z\\neq z_0\\), \\(z_0\\) is not a zero of \\(f(z)-f(z_0)-w\\), so sufficiently close to \\(z_0\\), it follows \\(f(z)-f(z_0)-w\\) has two distinct roots, hence \\(f\\) is not injective.</p> <ul> <li>Prove inverse of \\(f\\) defined on its range is holomorphic.</li> </ul> <p>Let \\(g=f^{-1}\\) be the inverse of \\(f\\) defined on \\(f(U)\\), which we assume is \\(V\\). Then for \\(w_0\\in V\\), \\(w\\) is close to \\(w_0\\). Write \\(w=f(z)\\) and \\(w_0=f(z_0)\\). So</p> \\[ \\frac{g(w)-g(w_0)}{w-w_0}=\\frac{1}{\\frac{f(z)-f(z_0)}{z-z_0}} \\] <p>let \\(z\\rightarrow z_0\\), we have \\(w\\rightarrow w_0\\), so \\(g'(w_0)=1/f'(g(w_0))\\).</p> <p><p>\\(\\square\\)</p></p> <p>From the above theorem, we could show that </p> <p></p> <p>Corollary: method of proving conformal equivalence of two open sets</p> <p>Two open sets \\(U\\) and \\(V\\) are conformally equivalent if and only if there exsits two holomorphic and injective functions \\(f:U\\rightarrow V\\) and \\(g: V\\rightarrow U\\) with \\(g(f(z))=z\\) and \\(f(g(w))=w\\) for all \\(z\\in U\\) and \\(w\\in V\\).</p> <p>Actually some books define a conformal map \\(f:U\\rightarrow V\\) if \\(f'(z)\\neq 0\\) for all \\(z\\in U\\). This is a weaker condition, since \\(f(z)=z^2\\) is not injective but satisfies \\(f(z)\\neq 0\\) for all \\(z\\in \\mathbb{C}-\\{0\\}\\). See the following example for further description.</p> <p>Example. A holomorphic function \\(f: U\\rightarrow V\\) is a local bijection on \\(U\\) if for all \\(z\\in U\\), there exists a disc centered at \\(z\\) such that \\(f:D\\rightarrow f(D)\\) is bijective (actually \\(f\\) is injective on \\(D\\)). </p> <p>Prove \\(f\\) is a local bijection on \\(U\\) if and only if \\(f'(z)\\neq 0\\) for all \\(z\\in U\\). </p> Proof <p>The same logic as we show in the proof of Holomorphism of the inverse of a conformal mapping.</p> <p>A holomorphic function satisfying the above condition preserve angles. Check the following example.</p> <p>Example. A holomorphic function \\(f\\) defined near \\(z_0\\) is said to preserve angles at \\(z_0\\) if for any smooth curves \\(\\gamma\\) and \\(\\eta\\) intersecting at \\(z_0\\), the angle formed between \\(\\gamma\\) and \\(\\eta\\) at \\(z_0\\) equals the angle formed between \\(f\\circ \\gamma\\) and \\(f\\circ \\eta\\) at \\(f(z_0)\\).</p> <p>Prove a complex-valued function \\(f:\\Omega \\rightarrow \\mathbb{C}\\) is holomorphic and \\(f'(z_0)\\neq 0\\) if and only if \\(f\\) preserves angles. </p>"},{"location":"Math/Complex_Analysis/Conf_Map/#the-upper-half-plane-disc","title":"The Upper half-plane &amp; disc","text":"<p>We give a demonstration of the conformal equivalence between the unit disc and the upper plane.</p> <p>Denote the upper half-plane \\(\\mathbb{H}\\) to be</p> \\[ \\mathbb{H}=\\{z\\in \\mathbb{C}:\\text{Im}(z)&gt;0\\} \\] <p></p> <p>Conformal function between disc &amp; upper half-plane</p> <p>Let</p> \\[ F(z)=\\frac{i-z}{i+z},\\quad G(w)=i\\frac{1-w}{1+w} \\] <p>Then \\(F: \\mathbb{H}\\rightarrow \\mathbb{D}\\) is a conformal map with inverse \\(G: \\mathbb{D}\\rightarrow \\mathbb{H}\\).</p> Proof <ul> <li>Show that both are holomorphic in their respective domains. Easy to show that \\(|F(z)|&lt;1\\) so the range of \\(F\\) is \\(\\mathbb{D}\\). To show that \\(G\\) maps \\(\\mathbb{D}\\) into \\(\\mathbb{H}\\), let \\(w=u+iv\\), and calculate the imaginary part</li> </ul> \\[ \\begin{align*} G(w)&amp;=i\\frac{1-u-iv}{1+u+iv}\\\\ &amp;=i\\frac{(1-u-iv)(1+u-iv)}{|1+w|^2}\\\\ &amp;=\\frac{2v+i(1-u^2-v^2)}{|1+w|^2}. \\end{align*} \\] <p>so \\(\\text{Im}(G(w))&gt;0\\). And </p> \\[ F(G(w))=\\frac{i-i\\frac{1-w}{1+w}}{i+i\\frac{1-w}{1+w}}=\\frac{2w}{2}=w,\\quad \\forall w\\in \\mathbb{D}, \\] \\[ G(F(z))=i\\frac{1-\\frac{i-z}{i+z}}{1+\\frac{i-z}{i+z}}=i\\frac{2z}{2i}=z,\\quad \\forall z\\in \\mathbb{H}. \\] <p>and we are done by Corollary: method of proving conformal equivalence of two open sets.</p> <p><p>\\(\\square\\)</p></p> <p>Mappings of the form </p> \\[ z\\mapsto \\frac{az+b}{cz+d} \\] <p>where \\(a,b,c,d\\) are complex-valued number, and \\(bc\\neq ad\\), are called fractional linear transformations.</p>"},{"location":"Math/Complex_Analysis/Conf_Map/#schwarz-lemma","title":"Schwarz Lemma","text":"<p>Schwarz Lemma</p> <p>Assume \\(f:\\mathbb{D} \\rightarrow \\mathbb{D}\\) and \\(f(0)=0\\), then</p> <p>(i) \\(|f(z)|\\leq |z|\\) holds for all \\(z\\in \\mathbb{D}\\),</p> <p>(ii) If there exists \\(z_0\\in \\mathbb{D}\\) such that \\(f(z_0)=z_0\\), then \\(f\\) is a rotation.</p> <p>(iii) \\(|f'(0)|\\leq 1\\), and if the equation holds, then \\(f\\) is a rotation. </p> Proof <p>(i) Expand \\(f\\) using power series</p> \\[ f(z)=a_1z + \\cdots \\] <p>\\(f(z)/z\\) has removable singularity at \\(0\\), so </p> \\[ \\left|\\frac{f(z)}{z}\\right|\\leq \\frac{1}{r},\\quad |z|=r&lt;1,  \\] <p>By Maximum modulus principle, the above holds for \\(|z|\\leq r\\). Let \\(r\\rightarrow 1\\) and we have the result.</p> <p>(ii) By assuption, \\(f(z)/z\\) reaches the maximum modulus inside the circle, so \\(f(z)/z\\) is a constant, i.e. \\(f(z)=cz\\). Substituting \\((z_0, f(z_0))\\) and taking the absolute value, we have \\(|c|=1\\), so \\(c=e^{i\\theta}\\), that is, \\(f\\) is a rotation.</p> <p>(iii) Let \\(g(z)=f(z)/z\\), then \\(|g(z)|\\leq 1\\) for all \\(z\\in \\mathbb{D}\\). Since \\(f'(0)=\\lim\\limits_{z\\rightarrow 0}\\frac{f(z)-f(0)}{z-0}=\\lim\\limits_{z\\rightarrow 0}\\frac{f(z)}{z}=g(0)\\), so \\(|f'(0)|=|g(0)|\\leq 1\\). </p> <p>Moreover, if \\(|f'(0)|=1\\), then still by maximum modulus principle, \\(g\\) is a constant, and is also a rotation.</p> <p><p>\\(\\square\\)</p></p> <p>A conformal map from an open set \\(\\Omega\\) to itself is called Automorphism. The set of all automorphisim of \\(\\Omega\\) is called \\(Aut(\\Omega)\\), and carries the structure of group. Recall definition of group in Definition of group. </p> <p>Here the group operation is the composition of map \"\\(\\circ\\)\", the identity is the map \\(z\\mapsto z\\), and the inverses are simply the inverse function. For inverse and composition, we have</p> \\[ (f\\circ g)^{-1}=g^{-1}\\circ f^{-1}. \\]"},{"location":"Math/Complex_Analysis/Conf_Map/#automorphisms-of-the-disc","title":"Automorphisms of the disc","text":"<p>In homework, we have known that the \\(\\psi\\) function</p> \\[ \\psi_\\alpha (z)=\\frac{\\alpha -z}{1-\\overline{\\alpha}z},\\quad \\alpha \\in \\mathbb{D} \\] <p>is a bijection from \\(\\mathbb{D}\\) to itself, thus is an automorphism of \\(\\mathbb{D}\\). Interestingly, \\(\\psi_\\alpha\\) interchanges \\(0\\) and \\(\\alpha\\) by</p> \\[ \\psi_\\alpha(0)=\\alpha,\\quad \\psi_\\alpha(\\alpha)=0. \\] <p></p> <p>Expression of all automorphism of disc</p> <p>If \\(f\\) is an automorphism of the disc, then there exist \\(\\theta\\in \\mathbb{R}\\) and \\(\\alpha \\in \\mathbb{D}\\), such that </p> \\[ f(z)=e^{i\\theta}\\frac{\\alpha -z}{1-\\overline{\\alpha }z}. \\] HintsProof <p>Using Schwarz Lemma.</p> <p>By property of automorphism, there exists a unique \\(\\alpha\\) such that \\(f(\\alpha)=0\\). So by operation of group, define another automorphism \\(g=f\\circ \\psi_\\alpha\\), which satisfies \\(g(0)=0\\), and \\(g^{-1}(0)=0\\) i.e. normalization. Applying Schwarz Lemma to \\(g\\) and \\(g^{-1}\\), we have</p> \\[ |g(z)|\\leq |z|, |g^{-1}(w)|\\leq |w|,\\quad \\forall z,w\\in \\mathbb{D} \\] <p>which gives \\(|g(z)|\\leq|z|=|g^{-1}(w)|\\leq |w|=|g(z)|\\). So also by Schwarz Lemma, \\(g\\) is a rotation, i.e. \\(g=e^{i\\theta}z\\). Replacing \\(z\\) by \\(\\psi_\\alpha(z)\\) and we have \\(g\\circ \\psi_\\alpha =f\\circ\\psi_\\alpha\\circ\\psi_\\alpha=f\\), which gives the result.</p> <p><p>\\(\\square\\)</p></p> <p></p> <p>Corollary: automorphisms that fix the origin</p> <p>The only automorphism of the unit disc that fix the origin is rotation.</p> Proof <p>Setting \\(\\alpha=0\\) in Expression of all automorphism of disc.</p> <p>We could see that the group of automorphism acts transitively, in the sense that given \\(\\alpha, \\beta\\in \\mathbb{D}\\), there is an automorphism \\(\\psi\\) mapping \\(\\alpha\\) to \\(\\beta\\) by \\(\\psi=\\psi_\\beta\\circ \\psi_\\alpha\\).</p>"},{"location":"Math/Complex_Analysis/Conf_Map/#automorphism-of-the-upper-half-plane","title":"Automorphism of the upper half-plane","text":"<p>By using \\(F:\\mathbb{H}\\rightarrow \\mathbb{D}\\) in Conformal function between disc &amp; upper half-plane and automorphism of the disc, we could determine the automorphism of the upper half-plane \\(\\text{Aut}(\\mathbb{H})\\).</p> <p>Construction of automorphism of the upper plane</p> <p>Consider map</p> \\[ \\Gamma: \\text{Aut}(\\mathbb{D})\\rightarrow \\text{Aut}(\\mathbb{H}) \\] <p>defined by</p> \\[ \\Gamma(\\varphi) = F^{-1}\\circ \\varphi\\circ F, \\quad \\varphi\\in \\text{Aut}(\\mathbb{D}). \\] <p>Actually \\(\\Gamma\\) is an isomorphism between \\(\\text{Aut}(\\mathbb{D})\\) and \\(\\text{Aut}(\\mathbb{H})\\), that is,</p> \\[ \\begin{align*} \\Gamma(\\varphi_1\\circ \\varphi_2)&amp;=F^{-1}\\circ \\varphi_1\\circ \\varphi_2\\circ F\\\\ &amp;=F^{-1}\\circ \\varphi_1\\circ F\\circ F^{-1}\\circ \\varphi_2\\circ F\\\\ &amp;=\\Gamma(\\varphi_1)\\circ \\Gamma(\\varphi_2) \\end{align*} \\] <p>and \\(\\phi\\) itself is a bijection.</p> <p>Now we give a description of automorphism of the upper plane.</p> <p>Denote \\(SL_2(\\mathbb{R})\\) as the group of all \\(2\\times 2\\) matrices with real entries and determinant \\(1\\), namely </p> \\[ SL_2(\\mathbb{R}) = \\left\\{ M=\\left[\\begin{array}{cc} a&amp; b\\\\ c&amp;d\\end{array}\\right]:\\quad a,b,c,d\\in \\mathbb{R}, \\quad ad-bc=1\\right\\} \\] <p>which is called the special linear group.</p> <p>Given a matrix \\(M\\in SL_2(\\mathbb{R})\\), define the mapping \\(f_M\\) by</p> \\[ f_M = \\frac{az+b}{cz+d}. \\] <p>Expression of automorphism of the upper plane</p> <p>Every automorphism of \\(\\mathbb{H}\\) takes the form \\(f_M\\) for some \\(M\\in SL_2(\\mathbb{R})\\). Conversely, every map of his form is an automorphism on \\(\\mathbb{H}\\).</p> Proof <ul> <li> <p>Show that \\(\\forall M\\in SL_2(\\mathbb{R})\\), \\(f_M\\) maps to from \\(\\mathbb{H}\\) to itself.</p> </li> <li> <p>Show that \\(f_M \\circ f_{M'}=f_{MM'}\\).</p> </li> <li> <p>Show that using elements in \\(SL_2(\\mathbb{R})\\), we could map any \\(z\\in \\mathbb{H}\\) to \\(i\\).</p> </li> <li> <p>Show that </p> </li> </ul> \\[ M_\\theta=\\left[\\begin{array}{cc} \\cos \\theta &amp; -\\sin \\theta\\\\ \\sin\\theta &amp; \\cos \\theta \\end{array}\\right] \\] <p>belongs to \\(SL_2(\\mathbb{R})\\), \\(F\\circ f_{M_\\theta}\\circ F^{-1}\\) corresponds to the rotation of angle \\(-2\\theta\\) in the disc.</p> <ul> <li>Complete the proof.</li> </ul> <p>Suppose that \\(f\\) is a automorphism of \\(\\mathbb{H}\\), then there exists a unique \\(\\beta\\) such that \\(f(\\beta)=i\\), and consider a matrix \\(N\\in SL_2(\\mathbb{R})\\) such that \\(f_N(i)=\\beta\\), then \\(g=f\\circ f_N\\) satisfies \\(g(i)=i\\), therefore \\(F\\circ g\\circ F^{-1}\\) is a automorphism of the unit disc that fixes origin. So \\(F\\circ g\\circ F^{-1}\\) is a rotation, by step 4 there exists \\(\\theta\\) such that</p> \\[ F\\circ f_{M_\\theta}\\circ F^{-1}=F\\circ g\\circ F^{-1}. \\] <p>Hence \\(g = f_{M_\\theta}\\), so \\(f=g\\circ f^{-1}_{N}=f_{M_\\theta N^{-1}}\\), which is of the desired form.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Conf_Map/#riemann-mapping-theorem","title":"Riemann Mapping Theorem","text":"<p>The basic problem lies in determining the condition for existence of \\(F:\\Omega\\rightarrow \\mathbb{D}\\). Here we give some observations.</p> <p>Firstly, \\(\\Omega\\neq \\mathbb{C}\\). Since, otherwise \\(F\\) is entire and bounded (by definition), then by Liouville's theorem \\(F\\) is constant, thus is not conformal. Then, since \\(\\mathbb{D}\\) is connected, then we must also impose the requirement that \\(\\Omega\\) is connected. The following example gives another condition.</p> <p>Example. Suppose \\(U\\) and \\(V\\) are conformally equivalent. Prove that if \\(U\\) is simply connected, then so is \\(V\\).</p> <p>For brevity, we shall call \\(\\Omega\\) proper, if it is non-empty and not the whole of \\(\\mathbb{C}\\).</p> <p>Riemann Mapping Theorem</p> <p>Suppose \\(\\Omega\\) is proper, and simply connected. If \\(z_0\\in \\Omega\\), then there exists a unique conformal map \\(F:\\Omega \\rightarrow \\mathbb{D}\\) such that </p> \\[ F(z_0)=0,\\quad F'(z_0)&gt;0. \\] <p></p> Montel's TheoremLemma for Montal's Theorem <p>Check relative compect set description. Here we only need uniform boundedness and equicontinuity. That is, </p> <p>Suppose a famliy \\(\\mathcal{F}\\) of holomorphic functions on an open set \\(\\Omega\\). If it is uniformly bounded on every compact subset of \\(\\Omega\\), then it is equicontinuous and thus normal(every sequence of \\(\\mathcal{F}\\) has a subsequence that converges uniformly on every compect subset of \\(\\Omega\\).)</p> <p>Similar to our proof in Ascoli-Arzel\u00e0 Theorem.</p> <p>We give a definition of exhaustion. Assume \\(\\Omega\\) is an open set. A sequence \\(\\{K_l\\}_{l=1}^\\infty\\) of compact subsets of \\(\\Omega\\) is called an exhaustion of \\(\\Omega\\), if </p> <p>(i) \\(K_l\\) is contained in the interior of \\(K_{l+1}\\), for each \\(l=1,2,\\cdots\\)</p> <p>(ii) Every compact subset of \\(\\Omega\\) is contained in some \\(K_l\\) for some \\(l\\). In particular \\(\\Omega = \\bigcup_{l=1}^\\infty K_l\\).</p> <p>Now we give a lemma for open subset \\(\\Omega\\) of the complex plane, that is, Any open set \\(\\Omega\\) in the complex plane has an exhaustion.</p> <p>The proof is by construction. If \\(\\Omega\\) is bounded, we let \\(K_l\\) denote the set of all points in \\(\\Omega\\) at distance \\(\\geq 1/l\\) from the boundary of \\(\\Omega\\). If not, the above construction would cause \\(K_l\\) to be non-compact, so we add for each \\(l\\), \\(K_l'=K_l\\cap \\{z:|z|\\leq l\\}\\).</p> <p></p> PropositionHintsProof for RMT <p>If \\(\\Omega\\) is a connected open subset of \\(\\mathbb{C}\\), and \\(\\{f_n\\}\\) a sequence of injective holomorphic functions on \\(\\Omega\\) that converges uniformly on every compact subset of \\(\\Omega\\) to a holomorphic function \\(f\\), then \\(f\\) is either injective or constant.</p> <p>The proof is by contradiction. If the convergent function \\(f\\) is not injective, so there exists two complex number \\(z_1,z_2\\) such that \\(f(z_1)=f(z_2)\\). Now we define a sequence of functions \\(g_n(z)=f_n(z)-f_n(z_1)\\), which has no other zeros besides \\(z_1\\), since \\(\\{f_n\\}\\) is injective. By condition, \\(\\{g_n\\}\\) converges uniformly on every compact subset of \\(\\Omega\\) to \\(g(z)=f(z)-f(z_1)\\). If \\(g(z)\\) is not identically zero, then \\(z_2\\) is another isolated zero. Using Augument Principle, </p> \\[ \\frac{1}{2\\pi i}\\int_\\gamma \\frac{g'(\\zeta)}{g(\\zeta)}d\\zeta = 1 \\] <p>for \\(\\gamma\\) is a small circle centered at \\(z_2\\) such that \\(g\\) does not vanish on \\(\\gamma\\) and its interior. Therefore, \\(1/g_n\\) converges uniformly to \\(1/g\\) on \\(\\gamma\\). Since \\(g'_n\\) converges uniformly to \\(g'\\) on \\(\\gamma\\) we have</p> \\[ \\frac{1}{2\\pi i}\\int_\\gamma \\frac{g'_n(\\zeta)}{g_n(\\zeta)}d\\zeta \\rightarrow \\frac{1}{2\\pi i}\\int_\\gamma \\frac{g'(\\zeta)}{g(\\zeta)}d\\zeta \\] <p>However the leeft hand side equals \\(0\\) since \\(g_n\\) contains no zero inside \\(\\gamma\\). </p> <ul> <li> <p>Consider all injective holomorphic functions \\(F:\\Omega\\rightarrow \\mathbb{D}\\) with \\(f(z_0)=0\\), and find an \\(f\\) from these functions whose image fills out all of \\(\\mathbb{D}\\), by making \\(f'(z_0)\\) as large as possible.</p> </li> <li> <p>Extract this function as a limit from a sequence of functions.</p> </li> </ul> <ul> <li> <p>Prove Uniqueness. Let \\(F,G\\) satisfies the above conditions. Then \\(H=F\\circ G^{-1}\\) is an automorphism of \\(\\mathbb{D}\\) that fix the origin. Therefore by Corollary: automorphisms that fix the origin, \\(H(z)=e^{-i\\theta}z\\). Since \\(H'(0)&gt;0\\) we must have \\(e^{i\\theta}=1\\), which gives \\(F=G\\).</p> </li> <li> <p>Suppose \\(\\Omega\\) is a proper simply connected open subset of \\(\\mathbb{C}\\). We claim that \\(\\Omega\\) is conformally equivalent to a open subset of \\(\\mathbb{D}\\) that contains the origin.</p> </li> </ul> <p>Indeed, choose a complex number \\(\\alpha \\in \\mathbb{C}-\\Omega\\), then \\(z-\\alpha\\) does not vanish in \\(\\Omega\\), so define</p> \\[ f(z)=\\log (z-\\alpha) \\] <p>so \\(e^{f(z)}=z-\\alpha\\), which is apparently injective. So we claim that choose \\(w\\in \\Omega\\), </p> \\[ f(z)\\neq f(w)+2\\pi i,\\quad \\forall z\\in \\Omega.  \\] <p>Since it is injective. So there exists a small circle centered at \\(f(w)+2\\pi i\\) of radius \\(r\\) such that \\(S_r(f(w)+2\\pi i)\\) does not contain \\(f(\\Omega)\\). Otherwise choose a sequence \\(\\{z_n\\}\\) such that \\(f(z_n)\\rightarrow f(w)+2\\pi i\\), then exponentiating it we have \\(z_n \\rightarrow w\\), so we have \\(f(z_n)\\rightarrow w\\).</p> <p>Define </p> \\[ F(z)=\\frac{1}{f(z)-(f(w)+2\\pi i)},\\quad z\\in \\Omega, \\] <p>which is a conformal function that maps \\(\\Omega\\) to \\(F(\\Omega)\\), a subset of \\(\\mathbb{D}\\). Since \\(f\\) is injective, so is \\(F\\). \\(F\\) is uniformly bounded on \\(\\Omega\\), so we could translate and rescale \\(F\\) in order to get the result function \\(F'\\).</p> <ul> <li>By step one, we assume \\(\\Omega\\) is an open subset of \\(\\mathbb{D}\\) with \\(0\\in \\Omega\\). Consider the family \\(\\mathcal{F}\\) of all injective holomorphic functions on \\(\\Omega\\) that map into \\(\\mathbb{D}\\) and fix the origin, i.e.</li> </ul> \\[ \\mathcal{F}=\\{f: f\\in H(\\Omega)\\text{ and injective}, \\quad f(0)=0 \\}. \\] <p>and extract function out the desired function. Note the set is non-empty since \\(z\\mapsto z\\) is contained. By Cauchy's integral formula, we have</p> \\[ |f'(0)|=\\frac{1}{2\\pi}\\left|\\int_\\gamma \\frac{f(\\zeta)}{\\zeta^2}d\\zeta\\right|\\leq \\frac{1}{r} \\] <p>for some circle \\(\\gamma\\) centered at the origin with radius \\(r\\), so \\(|f'(0)|\\) is uniformly bounded for all \\(f\\in \\mathcal{F}\\).</p> <p>Define</p> \\[ s=\\sup_{f\\in \\mathcal{F}}|f'(0)| \\] <p>Choose a sequence of \\(\\{f_n\\}\\in \\mathcal{F}\\) such that \\(|f_n'(0)|\\rightarrow s\\). \\(f_n\\) is uniformly bounded on compact subset of \\(\\mathbb{D}\\), so by Montel's Theorem, we have a subsequence \\(f_{n_k}\\) converges uniformly to \\(f\\) on the sompact subset. Since \\(|f_n'(0)|\\geq 1\\) (\\(z\\mapsto z\\) gives the minimum value of \\(s\\)), so \\(|f'(0)|\\geq 1\\), and \\(f\\) is non-constant. By Lemma 2 for limit function of injective holomorphic functions, \\(f\\) is injective. </p> <p>Apparantly \\(f(0)=0\\). And \\(|f(z)|\\leq 1\\) since it is the limit function, and by maximum modulus principle, \\(|f(z)|&lt;1\\). So \\(f\\in \\mathcal{F}\\), then \\(|f_n'(0)|=s\\).</p> <ul> <li>Prove the limit function \\(f\\) is a conformal function from \\(\\Omega\\) to \\(\\mathbb{D}\\), i.e. prove \\(f\\) is surjective. We prove by contradiction. </li> </ul> <p>If \\(f\\) is not surjective, then there exsits \\(\\alpha \\in \\mathbb{D}\\) such that \\(f(z)\\neq \\alpha\\) for all \\(z\\in \\Omega\\). Choose \\(\\psi_\\alpha\\), an automorphism of \\(\\mathbb{D}\\), such that \\(\\psi_\\alpha\\circ f\\) does not contain the origin. Use square root function \\(g(z)=e^{\\frac{1}{2}\\log z}\\) to define</p> \\[ F=\\psi_{g(\\alpha)}\\circ g\\circ \\psi_\\alpha \\circ f, \\] <p>which is injective and maps into \\(\\mathbb{D}\\), since all the components are injective and map into \\(\\mathbb{D}\\), and satisfies \\(F(0)=0\\). Let \\(h(z)=z^2\\) to be the inverse of \\(g\\), then </p> \\[ f=\\psi_\\alpha^{-1}\\circ h\\circ \\psi_{g(\\alpha)}^{-1}\\circ F:=\\Phi\\circ F \\] <p>\\(\\Phi\\) also maps \\(\\mathbb{D}\\) into \\(\\mathbb{D}\\), \\(\\Phi(0)=0\\). But \\(\\Phi\\) is not injective since \\(h\\) is not, so it is not a rotation, then by Schwarz Lemma, \\(|\\Phi'(0)|&lt;1\\). So </p> \\[ |f'(0)|=|\\Phi'(F(0))|\\cdot |F'(0)|&lt;|F'(0)| \\] <p>which contradicts! (\\(|f'(0)|=s\\) matters!)</p> <p>Then let \\(\\beta\\) to be the augument of \\(f'(0)\\), and let \\(f_{rot}=e^{-i\\beta}f\\) and we have \\(f_{rot}'(0)&gt;0\\). </p> <p>Note that simple-connectivity occurs in using the logarithm.</p> <p>Corollary of RMT</p> <p>Any proper simply connected open subsets in \\(\\mathbb{C}\\) is conformally equivalent.</p>"},{"location":"Math/Complex_Analysis/Conf_Map/#conformal-mappings-onto-polygons","title":"Conformal mappings onto Polygons","text":"<p>Example. Conformal mapping \\(f(z)=z^\\alpha\\) from \\(\\mathbb{H}\\) to sector, i.e.</p> \\[ \\{z: 0&lt; \\arg z&lt;\\alpha \\pi\\},\\quad \\alpha\\in (0,2). \\] <p>Write it as</p> \\[ z^\\alpha=f(z)=\\int_0^z f'(\\zeta)d\\zeta=\\alpha \\int_0^z \\zeta^{-\\beta}d\\zeta. \\] <p>where \\(\\alpha+\\beta=1\\), and the integral is taken along any path in the upper half-plane.</p> <p>Example. Conformal mapping </p> \\[ f(z)=\\int_0^z \\frac{d\\zeta}{(1-\\zeta^2)^{1/2}},\\quad z\\in \\mathbb{H}. \\] <p>where the integral is taken from \\(0\\) to \\(z\\) along any path in the closed upper half-plane. We choose the branch for square root of \\((1-\\zeta^2)\\) that makes it holomorphic in the upper half-plane and positive when \\(\\zeta\\in (-1,1)\\). This is actually the inverse of \\(\\sin z\\).</p> <p>Example. Consider Elliptic integrals (varients of this kind occur in the calculation of arc-length of an ellipse) </p> \\[ f(z)=\\int_0^z \\frac{d\\zeta}{[(1-\\zeta^2)(1-k^2\\zeta^2)]^{1/2}},\\quad z\\in \\mathbb{H}, \\] <p>where \\(k\\in (0,1)\\) and the branch is chosen to make it positive when \\(\\zeta\\in (-1,1)\\).</p> <p>Using the above examples, we could define the Schwarz-Christoffel integral and prove it maps the real line to a polygonal line.</p> <p>General Schwarz-Christoffel integral</p> <p>Define Schwarz-Christoffel integral as</p> \\[ \\begin{align} S(z)=\\int_0^z \\frac{d\\zeta}{(\\zeta-A_1)^{\\beta_1}\\cdots(\\zeta-A_n)^{\\beta_n}},\\label{SC-integral} \\end{align} \\] <p>where \\(A_1&lt;\\cdots&lt;A_n\\) are \\(n\\) distinct points on the real axis. The exponents \\(\\beta_k\\) will be assumed to satisfy the conditions \\(\\beta_k&lt;1\\) for each \\(k\\) and \\(1&lt;\\sum_{k=1}^n \\beta_k\\). </p> <p>The integrand in \\(\\ref{SC-integral}\\) is defined as follows. \\((z-A_k)^{\\beta_k}\\) is that branch which is positive when \\(z=x\\) is real and \\(x&gt;A_k\\). That is, </p> \\[ (z-A_k)^{-\\beta_k}= \\begin{cases}(x-A_k)^{-\\beta_k},\\quad &amp;x&gt;A_k\\\\ (A_k - x)^{-\\beta_k}e^{-i\\pi \\beta_k} ,\\quad &amp;x&lt;A_k\\end{cases} \\] <p>\\(\\beta_k&lt;1\\) allows \\((x-A_k)^{-\\beta_k}\\) is integral near \\(A_k\\), so the integral \\(S(x)\\) is continuous on the whole real axis. The simple connectivity of \\(S'(z)\\) in \\(\\mathbb{C}-\\bigcup_{k=1}^n\\{A_k+iy: y\\leq 0\\}\\) allows it to have primitive and \\(S(z)\\) is holomorphic in the region. The continuity of \\(S\\) implies the integral path can be taken along any path in \\(\\mathbb{C}-\\bigcup_{k=1}^n\\{A_k+iy: y&lt; 0\\}\\).</p> <p>And estimation of growth order gives</p> \\[ \\left|\\prod_{k=1}^n\\frac{1}{(z-A_k)^{-\\beta_k}}\\right|\\leq c |z|^{-\\sum_{k=1}^n \\beta_k} \\] <p>so the condition \\(\\sum \\beta_k&gt;1\\) allows the integral to converge at infinity, which is also independent of argument \\(\\theta\\). We call the limit \\(a_\\infty\\). Define \\(a_k=S(A_k)\\) for each \\(k\\).</p> <p>Image of Schwarz-Christoffel integral</p> <p>For the above integral \\(\\ref{SC-integral}\\), we have</p> <p>(i) If \\(\\sum \\beta_k =2\\), then the image is the polygonal region \\(P\\) with polygon \\(\\mathfrak{p}\\) whose vertices are \\(a_1,\\cdots,a_n\\), and \\(a_\\infty\\) lies on \\([a_n, a_1]\\). Moreover, the interior angle at \\(a_k\\) is \\(\\pi \\alpha_k\\) with \\(\\alpha_k=\\pi(1-\\beta_k)\\).</p> <p>(ii) If \\(\\sum \\beta_k \\in (1,2)\\), then the image is the polygonal region \\(P\\) with polygon \\(\\mathfrak{p}\\) whose vertices are \\(a_1,\\cdots,a_n,a_\\infty\\). The interior angle at \\(a_k\\) for \\(k=1,\\cdots,n\\) are the same as above, and the interior angle at \\(a_\\infty\\) is \\(\\pi \\alpha_\\infty\\) with \\(\\alpha_\\infty=2-\\sum \\beta_k\\).</p> Proof <p>We prove for \\(\\sum_k \\beta_k=2\\). For \\(x\\in [A_k,A_{k+1}]\\), \\(k=1,\\cdots,k-1\\), we consider the argument of \\(S'(x)\\), that is, </p> \\[ \\arg S'(x)=\\arg \\left(\\prod_{j&gt;k} (x-A_k)^{-\\beta_k}\\right)=\\arg e^{-i \\pi\\sum \\beta_k}=-\\pi \\sum \\beta_k. \\] <p>So the image of \\(S\\) on \\([A_k,A_{k+1}]\\) is the linear line \\([a_k, a_{k+1}]\\) with argument \\(-\\pi \\sum \\beta_k\\). As for \\(x&gt;A_n\\), the above argument equals \\(-2\\pi\\), so \\([A_n,\\infty]\\) is mapped into a linear line parallel to \\(x\\) axis. Similar for \\(x\\in [-\\infty, A_1]\\).</p> <p>Same logic for \\(\\sum \\beta_k&lt;2\\).</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Conf_Map/#boundary-behavior","title":"Boundary behavior","text":"<p>We consider a polygonal region \\(P\\), namely a bounded, simply connected open set whose boundary is a polygonal line \\(\\mathfrak{p}\\). We refer to this \\(\\mathfrak{p}\\) as a polygon. To study conformal maps from \\(\\mathbb{H}\\) to \\(P\\), we first consider the conformal maps from \\(\\mathbb{D}\\) to \\(P\\) and their boundary behavior.</p> <p></p> <p>Theorem of the boundary behavior of conformal map from the unit disc to polygon</p> <p>If \\(F:\\mathbb{D}\\rightarrow P\\) is a conformal mapping, then \\(F\\) extends to a continuous bijection from the closure \\(\\overline{\\mathbb{D}}\\) of the disc to the closure \\(\\overline{P}\\) of the polygon. In particular, \\(F\\) gives rise to a bijection from the boundary of \\(\\mathbb{D}\\) to the boundary of \\(P\\).</p> <p>To prove this, we have to make use of the following lemma.</p> <p>Lemma 1: </p> <p>Suppose \\(z_0\\) is on the unit circle, for all small \\(r\\in (0,1/2)\\), define \\(C_r\\) be the circle centered at \\(z_0\\) with radius \\(r\\). Given two points \\(z_r\\) and \\(z_r'\\) on \\(C_r\\) and contained in the unit disc, let \\(\\rho(r)=|f(z_r)-f(z_r')|\\), then there exsits a sequence of \\(\\{r_n\\}\\) of radii tending to zero, and \\(\\lim\\limits_{n\\rightarrow \\infty}\\rho(r_n)=0\\).</p> Proof <p>Show by contradiction.</p> <p>If not, then there exsit \\(c&gt;0\\) and \\(R\\in (0,1/2)\\), such that \\(\\rho(r)&gt;c\\) for all \\(r\\in (0,R]\\). Choose a \\(r\\), and have a corresponding \\(z_r\\) and \\(z_r'\\) on the circle \\(C_r\\) and contained in the unit disc. Define \\(\\alpha\\subset C_r\\) to be the arc connecting \\(z_r\\) and \\(z_r'\\), and parameterize it as \\(z=z_0+re^{i\\theta}\\), with \\(\\theta\\in (\\theta_1(r), \\theta_2(r))\\), we have</p> \\[ f(z_r)-f(z_r')=\\int_\\alpha f'(\\zeta)d\\zeta=\\int_{\\theta_1{r}}^{\\theta_2(r)} |f'(z_0+re^{i\\theta})|re^{i\\theta}d\\theta.  \\] <p>Take an absolute value and by Cauchy-Schwarz inequation</p> \\[ \\begin{align*} \\rho(r)&amp;\\leq \\int_{\\theta_1{r}}^{\\theta_2(r)} |f'(z)|rd\\theta\\\\ &amp; \\leq \\left(\\int_{\\theta_1{r}}^{\\theta_2(r)} |f'(z)|^2 rd\\theta\\right)^{1/2}\\left(\\int_{\\theta_1{r}}^{\\theta_2(r)} rd\\theta\\right)^{1/2}\\\\ \\frac{\\rho(r)^2}{r} &amp;\\leq \\left(\\int_{\\theta_1{r}}^{\\theta_2(r)} |f'(z)|^2 d\\theta\\right)\\left(\\int_{\\theta_1{r}}^{\\theta_2(r)} d\\theta\\right)\\\\ &amp;\\leq 2\\pi \\int_{\\theta_1{r}}^{\\theta_2(r)} |f'(z)|^2 d\\theta\\\\ \\int_0^R \\frac{\\rho(r)^2}{r} dr &amp;\\leq 2\\pi \\int_0^Rdr\\int_{\\theta_1{r}}^{\\theta_2(r)} |f'(z)|^2 d\\theta\\\\ &amp;\\leq 2\\pi \\int_0^Rdr\\int_0^{2\\pi} |f'(z)|^2 d\\theta=\\iint_{f(D_r)}dxdy. \\end{align*} \\] <p>The left hand side is larger than \\(c^2\\int_0^R \\frac{1}{r}dr=\\infty\\), while the right hand side is a finite number (actually the area of \\(f(D_r)\\)), which contradicts!</p> <p><p>\\(\\square\\)</p></p> <p>Lemma 2: Existence of value on the boundary of unit disc</p> <p>Suppose \\(z_0\\) is on the unit circle, and \\(\\{z_n\\}_{n\\geq 1}\\subset \\mathbb{D}\\) tend to \\(z_0\\), then \\(\\{F(z_n)\\}\\) has a limit on the boundary of \\(P\\).</p> Proof <p>Show by contradiction.</p> <p>If not, then there exist \\(\\{z_n\\}\\) and \\(\\{z_n'\\}\\) that both converges to \\(z_0\\) but \\(\\{F(z_n)\\}\\) and \\(\\{F(z_n')\\}\\) converges to two distinct values, denoted by \\(\\zeta\\) and \\(\\zeta'\\) respectively. Since \\(F\\) is conformal, then \\(\\zeta\\) and \\(\\zeta'\\) lie on the boundary of \\(P\\). Choose two two small isolated disc \\(D\\) and \\(D'\\) with a distance \\(d&gt;0\\) that centered at \\(\\zeta\\) and \\(\\zeta'\\). For \\(n\\) large enough, \\(F(z_n)\\in D\\) and \\(F(z_n')\\in D'\\). There must exist two smooth curve \\(\\gamma\\) which connect all \\(F(z_n)\\) and ends at \\(\\zeta\\), and similar to curve \\(\\gamma'\\). </p> <p>Let \\(\\lambda=F^{-1}(\\gamma)\\) and \\(\\lambda'=F^{-1}(\\gamma')\\), both of which are smooth and contain infinitely many points of \\(\\{z_n\\}\\) and \\(\\{z_n'\\}\\) respectively. For small enough \\(r\\), let \\(z_r=\\lambda \\cap C_r\\), \\(z_r'=\\lambda'\\cap C_r\\), so by lemma 1, we have \\(\\rho(r)\\rightarrow 0\\) as \\(r\\rightarrow 0\\), which contradicts \\(|F(z_n)-F(z_n')|&gt;d\\) for large \\(n\\).</p> <p><p>\\(\\square\\)</p></p> <p>Now we have prove the existence of </p> \\[ \\lim_{z\\rightarrow z_0}F(z) \\] <p>for \\(z_0\\) on the unit circle. Then we define \\(F(z_0)\\) to be the above limit value.</p> <p>Lemma 3</p> <p>The conformal mapping \\(F\\) extends to a continuous function from \\(\\overline{\\mathbb{D}}\\) to \\(\\overline{P}\\).</p> Proof <p>We only consider the continuity of \\(F\\) from \\(\\overline{\\mathbb{D}}\\) to \\(\\overline{P}\\). By continuity of \\(F\\) on \\(\\mathbb{D}\\), \\(\\forall z_0\\) on the unit circle, \\(\\forall \\epsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that \\(|F(z_0)-F(z)|&lt;\\varepsilon/2\\) whenever \\(z\\in \\mathbb{D}\\cap D_\\delta(z_0)\\). So for \\(z\\) on the boundary of the disc, we also choose \\(z\\in \\partial \\mathbb{D}\\cap D_\\delta(z_0)\\), then there must exist \\(w\\in \\mathbb{D}\\cap D_\\delta(z_0)\\cap D_\\delta(z)\\), such that \\(|F(z)-F(w)|&lt;\\varepsilon/2\\) and \\(|F(z_0)-F(w)|&lt;\\varepsilon\\). Then by triangle inequation we complete the proof.</p> <p>Now we give the proof of Theorem of the boundary behavior of conformal map from the unit disc to polygon.</p> Proof for Theorem of the boundary behavior of conformal map from the unit disc to polygon <p>We could apply the same logic to the inverse \\(G\\) of \\(F\\), to show that \\(G\\) could extend to a continuous bijection from \\(\\overline{P}\\) to \\(\\overline{\\mathbb{D}}\\). Then we only need to show that the extension of \\(F\\) and \\(G\\) are the inverse of each other. Given \\(z_0\\in \\partial \\mathbb{D}\\), there exsit \\(\\{z_k\\}\\subset \\mathbb{D}\\), such that \\(z_k\\rightarrow z_0\\), and we have \\(G(F(z_k))=z_k\\), and then take the limit we have \\(G(f(z_0))=z_0\\). The same logic for \\(F(G(w_k))\\rightarrow F(G(w_0))=w_0\\).</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Conf_Map/#the-mapping-formula","title":"The mapping formula","text":"<p>Now we turn to the actual formula for a conformal map from \\(\\mathbb{H}\\) to \\(P\\). The existence for \\(F\\) is guaranteed by Riemann Mapping Theorem. For \\(f:\\mathbb{D}\\rightarrow \\mathbb{H}\\), \\(f(w)=i(1-w)/(1+w)\\), the point at \\(-1\\) on the unit circle corresponds to \\(\\infty\\) on the upper half-plane. Here known info is the given \\(P\\) with its boundary \\(\\mathfrak{p}\\), whose vertices ordered consecutively \\(a_1,\\cdots, a_n\\), with \\(n\\geq 3\\).</p> <p>In Boundary behavior, we have discuss the boundary behavior of a conformal mapping from \\(\\mathbb{H}\\rightarrow P\\). We first consider \\(P\\) polygon \\(\\mathfrak{p}\\), of which none of the vertices correspond to the point at infinity. Therefore, there exist real number \\(A_1,\\cdots, A_n\\) such that \\(F(A_k)=a_k\\). Since</p> <p>Formula</p> <p>There exist complex number \\(c_1\\) and \\(c_2\\) such that for a known conformal map \\(F:\\mathbb{H}\\rightarrow P\\) could be given by</p> \\[ F(z)=c_1S(z)+c_2, \\] <p>where \\(S(z)\\) is the Schwarz-Christoffel integral formula \\(\\ref{SC-integral}\\).</p> Proof <p>Using Schwarz reflection principle to extend the upper half-plane to the two-way strip. We aim to formulate a linear line on real axis. </p> <p>For two segment \\([A_{k-1}, A_{k}]\\) and \\([A_{k}, A_{k+1}]\\) on the real axis, their image of \\(F\\) is two line with vertices \\(a_{k-1}, a_{k}\\) and \\(a_{k+1}\\) with inner angle \\(\\alpha_k\\) at \\(a_k\\). Use</p> \\[ h_k(z)=(F(z)-a_k)^{1/\\alpha_k}. \\] <p>Then this function maps \\([A_{k-1}, A_{k}]\\) and \\([A_{k}, A_{k+1}]\\) to a single line \\(L_k\\). By reflection, \\(h_k\\) is holomorphic in the whole strip \\(\\text{Re}(A_{k-1})&lt;\\text{Re}(z)&lt;\\text{Re}(A_k+1)\\). We claim first \\(h_k'\\) does not vanish on the whole strip. Since \\(F'(z)\\neq 0\\) on the upper strip, then by</p> \\[ h_k'(z)=1/\\alpha_k (F(z)-a_k)^{1/\\alpha_k-1}F'(z), \\] <p>\\(h'(z)\\neq 0\\) holds for all \\(z\\) in the upper strip. Since \\(F\\) is injective up to the real axis, so is \\(h_k\\), which guarantees \\(h'(x)\\neq 0\\) on real line \\([A_{k-1}, A_{k+1}]\\). </p> <p>By taking derivative, we have \\(F=h_k^{\\alpha_k}+a_k\\), \\(F'=\\alpha_k h_k^{\\alpha_k-1}h'_k\\), \\(F'' =\\alpha_k h_k^{\\alpha_k-1}h''_k+\\alpha_k  (\\alpha_k-1) h_k^{\\alpha_k-2}h'^2_k\\).</p> <p>so </p> \\[ \\frac{F''(z)}{F'(z)}=\\frac{h''_k}{h'_k} + \\frac{(\\alpha_k-1)h'_k}{h_k} \\] <p>we rewrite it as (single pole at \\(A_k\\) and use argument principle)</p> \\[ \\frac{F''(z)}{F'(z)}=\\frac{\\alpha_k-1}{z-A_k} + E_k(z) \\] <p>where \\(E_k(z)\\) is a holomorphic function on the corresponding strip. So </p> \\[ \\frac{F''(z)}{F'(z)} + \\sum_{k=1}^n\\frac{1-\\alpha_k}{z-A_k} \\] <p>is an entire function, then by Liouville's theorem, we have </p> \\[ \\frac{F''(z)}{F'(z)} = - \\sum_{k=1}^n\\frac{\\beta_k}{z-A_k} \\] <p>Denote \\(Q(z)=c(z-A_1)^{-\\beta_1}\\cdots (z-A_n)^{-\\beta_n}\\), then \\(\\frac{Q'(z)}{Q(z)}\\) also equal the right hand side of the above equation. So</p> \\[ \\frac{d}{dz}\\left(\\frac{F'(z)}{Q(z)}\\right)=0 \\] <p>And an integration gives the result.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Entire_func/","title":"Entire function","text":"<p>Could an entire function be determined by its zeros?</p> <p>This is the core problem we would focus in this chapter. </p>"},{"location":"Math/Complex_Analysis/Entire_func/#jensens-formula","title":"Jensen's Formula","text":"<p>Mean value Theorem</p> <p>Assume \\(f\\) is holomorphic on a disc \\(D_R(z_0)\\), and vanishes nowhere on the circle \\(C_R\\), then </p> \\[ f(z_0)=\\frac{1}{2\\pi}\\int_0^{2\\pi} f(z_0+Re^{i\\theta})d\\theta. \\] <p>Since taking a real part and integration could be exchanged, so </p> \\[ \\text{Re } f(z_0)=\\frac{1}{2\\pi}\\int_0^{2\\pi} \\text{Re } f(z_0+Re^{i\\theta})d\\theta. \\] Proof <p>Rewrite the Cauchy's integral formula with \\(\\zeta=z_0+Re^{i\\theta}\\).</p> <p><p>\\(\\square\\)</p></p> <p>Corollary</p> <p>Assume a holomorphic function \\(g\\) on \\(D_R(z)\\) that vanishes nowhere in the closure of \\(D_R(z_0)\\) (not vanish on the integral circle), then </p> \\[ \\log |g(z_0)|=\\frac{1}{2\\pi}\\int_0^{2\\pi}\\log |g(z_0+Re^{i\\theta})|d\\theta. \\] Proof <p>Just by Theorem for generating logarithm of a function, and we have \\(g(z)=e^{h(z)}\\), apply Mean value Theorem to \\({\\text{Re }h(z)}\\). Notice \\(\\log |g(z)|={\\text{Re }h(z)}\\).</p> <p><p>\\(\\square\\)</p></p> <p>Jensen's Formula</p> <p>Let \\(\\Omega\\) be an open set that contains the closure of a disc \\(D_R\\) and suppose \\(f\\) is holomorphic in \\(\\Omega\\), \\(f(0)\\neq 0\\) and \\(f\\) vanishes nowhere on the circle \\(C_R\\). If \\(z_1,\\cdots, z_n\\) denote the zeros of \\(f\\) inside the disc (counted with multiplicities), then </p> \\[ \\log |f(0)|=\\sum_{k=1}^N \\log \\left(\\frac{|z_k|}{R}\\right)+\\frac{1}{2\\pi}\\int_0^{2\\pi} \\log|f(Re^{i\\theta})|d\\theta. \\] Proof <ul> <li> <p>Show that if \\(f_1\\) and \\(f_2\\) satisfies the above theorem, then \\(f_1f_2\\) also satisfies. </p> </li> <li> <p>Decompose \\(f\\) into </p> </li> </ul> \\[ f(z)=\\prod_{k=1}^n(z-z_k) g(z) \\] <p>where \\(g(z)\\) vanishes nowhere in the closure of \\(D_R(z_0)\\). Show that Jensen's Formula holds for function \\(g\\) and \\(z-z_k\\).</p> <ul> <li> <p>For \\(g(z)\\), apply the above corollary with the same radius.</p> </li> <li> <p>For \\(F(z)=z-w\\), where \\(|w|&lt;|R|\\). </p> </li> </ul> <p>We only need to prove that </p> \\[ 0=\\int_{0}^{2\\pi} \\log |e^{i\\theta}-a|d\\theta=\\int_{0}^{2\\pi} \\log |1-ae^{i\\theta}|d\\theta \\] <p>where \\(a=w/R\\). Define \\(h(z)=1-az\\), which vanishes nowhere in the closure of \\(D_R(z_0)\\). Apply the above corollary to it with integration path of unit circle and find that \\(\\log |h(0)|=0\\).</p> <p><p>\\(\\square\\)</p> </p> <p>The Jensen's Formula connects the number of zeros of a holomorphic function \\(f\\) (in disc \\(D_R\\)) inside a circle. We define \\(\\mathscr{n} (r)\\) the number of zeros of \\(f\\) inside the circle \\(C_r\\) where \\(0&lt;r&lt;R\\). Apparently, \\(\\mathscr{n}(r)\\) is non-decreasing of \\(r\\).</p> <p>Lemma: number of zeros expressed by Characteristic function</p> <p>If \\(z_1,\\cdots,z_N\\) are the zeros of \\(f\\) inside the disc \\(D_R\\), then</p> \\[ \\int_0^R \\mathscr{n}(r)\\frac{dr}{r}=\\sum_{k=1}^N \\log \\left|\\frac{R}{z_k}\\right|. \\] Proof <p>Note the real logarithm and change the integral domain</p> \\[ \\begin{align*} \\sum_{k=1}^N \\log \\left|\\frac{R}{z_k}\\right|&amp;=\\sum_{k=1}^N \\log R - \\log |z_k|\\\\ &amp;=\\sum_{k=1}^N \\int_{|z_k|}^R \\frac{dr}{r}\\\\ &amp;=\\sum_{k=1}^N \\int_0^R 1_{\\{r&gt;z_k\\}}\\frac{dr}{r}\\\\ &amp;=\\int_0^R \\sum_{k=1}^N 1_{\\{r&gt;z_k\\}}\\frac{dr}{r}\\\\ &amp;=\\int_0^R \\mathscr{n}(r)\\frac{dr}{r}. \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>By the above lemma and Jensen's formula, we have</p> \\[ \\begin{align} \\int_0^R \\mathscr{n}(r)\\frac{dr}{r}=\\frac{1}{2\\pi}\\int_0^{2\\pi}f(Re^{i\\theta})d\\theta - \\log|f(0)|.\\label{Jensen-number of zeros} \\end{align} \\] <p>for \\(f(0)\\neq 0\\) and \\(f\\) does not vanish on the circle \\(C_R\\). Readers could check the following estimate for \\(\\mathscr{n}(r)\\).</p>"},{"location":"Math/Complex_Analysis/Entire_func/#function-of-finite-order","title":"Function of finite order","text":"<p>Definition of finite order</p> <p>Let \\(f\\) be an entire function. If there exists a positive number \\(\\rho\\) and constants \\(A,B&gt;0\\), such that </p> \\[ |f(z)|\\leq Ae^{B|z|^\\rho},\\quad \\forall z\\in \\mathbb{C} \\] <p>then \\(f\\) is called to have an order of growth \\(\\leq \\rho\\). Its order of growth id defined by</p> \\[ \\rho_f =\\inf \\rho. \\] <p>Theorem </p> <p>If \\(f\\) is an entire function that has an order of growth \\(\\leq \\rho\\), then</p> <p>(i) \\(\\mathscr{n}(r)\\leq Cr^\\rho\\) for some \\(C&gt;0\\) and all sufficiently large \\(r\\).</p> <p>(ii) If \\(\\{z_k\\}_{k\\geq 1}\\) denotes the zeros of \\(f\\), with \\(z_k\\neq 0\\), then for all \\(s&gt;\\rho\\), we have</p> \\[ \\sum_{k=1}^\\infty \\frac{1}{|z_k|^s}&lt;\\infty. \\] Proof <p>(i) By equation \\(\\ref{Jensen-number of zeros}\\), if \\(f(0)\\neq 0\\), we have </p> \\[ \\int_0^R\\mathscr{n}(x)\\frac{dx}{x}=\\frac{1}{2\\pi}\\int_0^{2\\pi} \\log |f(Re^{i\\theta})|d\\theta - \\log |f(0)| \\] <p>If \\(f(0)=0\\), we let \\(F(z)=\\frac{f(z)}{z^l}\\), where \\(l\\) is the order of zeros at \\(z=0\\), then \\(F(0)\\neq 0\\), and we could still use the above formula.</p> <p>The following is a common trick. For a fixed \\(r\\), let \\(R=2r\\), then we have</p> \\[ \\int_0^R\\mathscr{n}(x)\\frac{dx}{x}\\geq \\int_r^{2r}\\mathscr{n}(x)\\frac{dx}{x}\\geq \\mathscr{n}(r)\\int_r^{2r}\\frac{dx}{x}=\\mathscr{n}(r)\\log 2 . \\] <p>Then on the other hand, since \\(|f(z)|\\leq A e^{B|z|^\\rho}\\), we have</p> \\[ \\begin{align*} \\frac{1}{2\\pi}\\int_0^{2\\pi} \\log |f(Re^{i\\theta})|d\\theta&amp;\\leq \\frac{1}{2\\pi}\\int_0^{2\\pi} \\log |Ae^{BR^\\rho}|d\\theta=\\log |A|+B2^\\rho r^\\rho\\leq C' r^\\rho. \\end{align*} \\] <p>Combining the above two inequations, we are done.</p> <p>(ii) We use a bisection method like we used in proving the convergence of \\(\\sum_{k=1}^\\infty \\frac{1}{k^2}\\).</p> \\[ \\begin{align*} \\sum_{k=1}^\\infty \\frac{1}{|z_k|^s}&amp;=\\sum_{j=0}^\\infty\\left(\\sum_{2^j\\leq |z_k|&lt; 2^{j+1}} \\frac{1}{|z_k|^s}\\right)\\\\ &amp;\\leq \\sum_{j=0}^\\infty\\left(\\sum_{2^j\\leq |z_k|&lt; 2^{j+1}} \\frac{1}{|2^j|^s}\\right)\\\\ &amp;\\leq \\sum_{j=0}^\\infty\\left(\\sum_{|z_k|&lt; 2^{j+1}} \\frac{1}{2^{js}}\\right)\\\\ &amp;\\leq \\sum_{j=0}^\\infty\\left(\\mathscr{n}(2^{j+1})\\frac{1}{2^{js}}\\right)\\\\ &amp;\\leq \\sum_{j=0}^\\infty \\left(C2^{(j+1)\\rho}\\frac{1}{2^{js}}\\right)\\\\ &amp;\\leq C 2^\\rho\\sum_{j=0}^\\infty 2^{(\\rho-s)j}&lt;\\infty.\\\\ \\end{align*} \\]"},{"location":"Math/Complex_Analysis/Entire_func/#infinite-product","title":"Infinite Product","text":"<p>Now let us focus on constructing a function with given zeros \\(\\{z_k\\}\\), where \\(\\lim\\limits_{k\\rightarrow\\infty}|z_k|=\\infty\\) (otherwise zeros would accumulate and cause the function to be zero identically). A naive guess is to define</p> \\[ \\begin{align} f(z)=\\prod_{k=1}^\\infty (z-z_k).\\label{naive-version} \\end{align} \\] <p>Weierstrass gave a condition, and added an factor into the infinite product, for which to be convergent and no more zeros introduced.</p>"},{"location":"Math/Complex_Analysis/Entire_func/#generalities","title":"Generalities","text":"<p>We first link the convergence of infinite products with that of series. In standard language, we focus on </p> \\[ \\prod_{n=1}^\\infty (1+a_n). \\] <p>Convergence of infinite product of standard form</p> <p>If \\(\\sum_n|a_n|&lt;\\infty\\), then the product </p> \\[ \\prod_{n=1}^\\infty (1+a_n) \\] <p>converges. Moreover, it converges to \\(0\\) iff one of its factors is \\(0\\).</p> Proof <p>Just use the logarithm to transfer the product into series. For \\(1+a_n\\neq 0\\), we have</p> \\[ \\prod_{n=1}^N(1+a_n)=\\prod_{n=1}^N e^{\\log (1+a_n)}=\\exp \\left(\\sum_{n=1}^N \\log (1+a_n)\\right) \\] <p>Since exponential function is continuous, we only need to show the convergence of \\(\\sum\\limits_{n=1}^N \\log (1+a_n)\\). Note that </p> \\[ \\log |1+a_n|\\leq |\\log (1+a_n)| \\] <p>we could not use \\(\\log |1+a_n|\\leq |a_n|\\) to prove. Rather, we have to show that</p> \\[ \\begin{align} |\\log (1+a_n)|\\leq C|a_n|\\label{tangent scaling} \\end{align}\\] <p>for some \\(a_n\\). Then the above infinite product converges. Actually, if we dig further, we find that </p> \\[ \\begin{align*} |\\log(1+z)|&amp;=\\left|-\\sum_{k=1}^\\infty (-1)^k\\frac{z^k}{k}\\right|\\\\ &amp;\\leq\\sum_{k=1}^\\infty \\frac{|z|^k}{k}\\quad \\text{triangle inequation}\\\\ &amp;=|z|\\sum_{k=0}^\\infty \\frac{|z|^k}{k+1}\\\\ &amp;\\leq |z|\\sum_{k=0}^\\infty |z|^k\\\\ \\end{align*} \\] <p>we try to let the rest series to converge, so just let \\(|z|\\leq \\delta&lt;1\\) and we have \\(|\\log(1+z)|\\leq C |z|\\) for \\(C=\\frac{1}{1-\\delta}\\). In the above case, for large \\(n\\), we must have \\(|a_n|&lt;\\delta\\). So inequation \\(\\ref{tangent scaling}\\) holds, if we just disregard the finitely many terms.</p> <p><p>\\(\\square\\)</p></p> <p>More generally, we could consider the convergence of products of holomorphic functions. Note its convergence is also uniform, i.e. regardless of \\(z\\).</p> <p></p> <p>Products of holomorphic functions</p> <p>Assume a sequence of holomorphic functions \\(\\{F_n\\}\\) on an open set \\(\\Omega\\). If \\(\\exists c_n&gt;0\\) such that \\(\\sum_{n}c_n&lt;\\infty\\) and</p> \\[ |F_n-1|\\leq c_n,\\quad \\forall n, \\] <p>then </p> \\[ \\prod_{n=1}^\\infty F_n(z) \\] <p>converges uniformly in \\(\\Omega\\) to a holomorphic function \\(F(z)\\). Moreover, if for each \\(n\\), \\(F_n(z)\\) does not vanish, then we have the logarithm derivative additivity holds</p> \\[ \\frac{F'(z)}{F(z)}=\\sum_{n=1}^\\infty \\frac{F_n'(z)}{F_n(z)}. \\] Proof <p>The uniform convergence is apparent according to Weierstrass test. Check that \\(a_n(z)=F_n(z)-1\\), </p> \\[ \\begin{align*} \\left|\\prod_{n=1}^N F_n(z)\\right|&amp;\\leq \\exp\\left|\\sum_{n=1}^N \\log F_n(z)\\right|\\\\ &amp;\\leq \\exp\\left(\\sum_{n=1}^N \\left|\\log F_n(z) \\right|\\right)\\\\ &amp;\\leq \\exp \\left(\\sum_{n=1}^N \\left|F_n(z)-1 \\right|\\right)\\\\ &amp;\\leq \\exp \\left(\\sum_{n=1}^N |a_n|\\right)\\leq  \\exp \\left(\\sum_{n=1}^N c_n\\right)&lt;\\infty,\\quad \\forall z\\in \\Omega. \\end{align*} \\] <p>Define </p> \\[ G_N(z)=\\prod_{n=1}^N f_n(z) \\] <p>then \\(G_N(z)\\) converges uniformly to \\(F(z)\\) in \\(\\Omega\\). So for an arbitrary compact subset \\(K\\subset \\Omega\\), by Sequence of holomorphic functions, \\(G_N'(z)\\) converges to \\(F'(z)\\) in \\(K\\). So \\(\\frac{G_N'}{G_N}\\) converges to \\(\\frac{F'}{F}\\) in \\(K\\). Expand \\(\\frac{G_N'}{G_N}\\) by additicity of logarithm, i.e.</p> \\[ \\frac{G_N'(z)}{G_N(z)}=\\sum_{n=1}^N \\frac{F_n'(z)}{F_n(z)} \\] <p>Let \\(N\\rightarrow \\infty\\) and we are done.</p> <p><p>\\(\\square\\)</p></p> <p>Example. Show that</p> \\[ \\frac{\\sin \\pi z}{\\pi}=z \\prod_{n=1}^\\infty \\left(1-\\frac{z^2}{n^2}\\right). \\]"},{"location":"Math/Complex_Analysis/Entire_func/#weierstass-infinite-product","title":"Weierstass Infinite Product","text":"<p>We fucos on adding the following factors into the vaive version of infinite product \\(\\ref{naive-version}\\).</p> <p>Canonical Factors</p> <p>For each \\(k\\geq 0\\), we define the Canonical factors to be</p> \\[ E_0(z)=1-z, E_k(z)=(1-z)\\exp\\left(\\sum_{n=1}^k z^n/n\\right). \\] <p></p> <p>Lemma: order esstimate of canonical factors</p> <p>If \\(|z|&lt;\\frac{1}{2}\\), then \\(|1-E_k(z)|\\leq c|z|^{k+1}\\) for some constant \\(c\\).</p> Proof <p>Similar as we talked in Products of holomorphic functions. Using Taylor expansion of \\(\\log (1-z)\\), we have</p> \\[ \\begin{align*} (1-z)\\exp\\left(\\sum_{n=1}^k z^n/n\\right)&amp;=\\exp\\left(\\log (1-z)+\\sum_{n=1}^k z^n/n\\right)\\\\ &amp;=\\exp\\left(-\\sum_{n=1}^\\infty z^n/n+\\sum_{n=1}^k z^n/n\\right)\\\\ &amp;=\\exp\\left(-\\sum_{n=k+1}^\\infty z^n/n\\right) \\end{align*} \\] <p>Define \\(w=\\log (1-z)+\\sum_{n=1}^k z^n/n\\). So</p> \\[ \\begin{align*} \\left|w\\right|&amp;=\\left|\\left(-\\sum_{n=k+1}^\\infty z^n/n\\right)\\right|\\\\ &amp;\\leq \\left|-\\sum_{n=k+1}^\\infty z^n/n\\right|\\\\ &amp;\\leq \\left(\\sum_{n=k+1}^\\infty |z|^n/n\\right)\\\\ &amp;\\leq \\left(|z|^{k+1}\\sum_{n=0}^\\infty |z|^n\\right)\\\\ &amp;\\leq {2|z|^{k+1}}\\quad \\text{using } |z|&lt;\\frac{1}{2}&lt;1 \\end{align*} \\] <p>And apply Taylor's expansion to \\(1-e^w\\) again</p> \\[ \\begin{align*} |1-E_k(z)|&amp;=|1-e^w|\\\\ &amp;=\\left|1-\\sum_{n=0}^\\infty w^n/n!\\right|\\\\ &amp;=\\left|-\\sum_{n=1}^\\infty w^n/n!\\right|\\\\ &amp;\\leq \\sum_{n=1}^\\infty |w|^n/n!\\\\ &amp;\\leq |w|\\sum_{n=0}^\\infty |w|^n/(n+1)!&lt;e|w|,\\quad \\text{for } |w|&lt;1.\\\\ &amp;\\leq 2e|z|^{k+1}. \\end{align*} \\] <p>where \\(c=2e\\). </p> <p><p>\\(\\square\\)</p></p> <p>Weierstass Infinite Product</p> <p>Given any sequence \\(\\{a_n\\}\\) of complex number with \\(|a_n|\\rightarrow \\infty\\) as \\(n\\rightarrow\\infty\\), there exists an entire function \\(f\\) that vanishes at all \\(z=a_n\\) and nowhere else. Any other such function is of the form \\(f(z)e^{g(z)}\\), where \\(g(z)\\) is an entire function.</p> Proof <p>Suppose \\(f\\) has an order \\(m\\) of zeros at \\(z=0\\), and other zeros \\(\\{z_n\\}\\), define Weierstrass infinite product</p> \\[ f(z)=z^m \\prod_{n=1}^\\infty E_n(z/z_n). \\] <p>We only need to show that it is convergent, which vanishes nowhere else. The Proof is a common trick, which partition the product into two separate parts. We only need to prove the convergence in open set \\(\\{|z|&lt;R\\}\\) for each fixed \\(R\\). </p> \\[ \\prod_{n=1}^\\infty E_n(z/z_n)=\\prod_{|a_n|&lt; 2R}E_n(z/z_n)\\cdot \\prod_{|a_n|\\geq 2R} E_n(z/z_n). \\] <p>For the former part of the righthand side, we have finite many terms, which converges, and has a zeros of the desired.</p> <p>For the latter part of the righthand side, we have \\(|z|/|a_n|&lt;\\frac{R}{2R}&lt;\\frac{1}{2}\\), and by Lemma: order esstimate of canonical factors, \\(|1-E_n(z/a_n)|\\leq c|z/a_n|^{n+1}&lt;c (1/2)^{n+1}\\), by Products of holomorphic functions, it converges to a holomorphic function.</p> <ul> <li>For two functions that satisfies the above condition, denoted by \\(f_1\\), \\(f_2\\), then \\(f_1/f_2\\) has removable singularies at \\(\\{z_n\\}\\), so it is entire and vanish nowhere, by Theorem for generating logarithm of a function, \\(\\exists g(z)\\) that is holomorphic on \\(\\mathbb{C}\\)(thus entire), such that </li> </ul> \\[ \\frac{f_1}{f_2}=e^{g(z)}. \\] <p>So \\(f_1=f_2e^{g(z)}\\).</p>"},{"location":"Math/Complex_Analysis/Entire_func/#hadamards-factorization-theorem","title":"Hadamard's factorization theorem","text":"<p>Hadamard gives a refinement of Weierstrass's Theorem about infinite products by fixing the degree of canonical factors and prove \\(g(z)\\) is a polynomial when \\(f\\) is of finite order of growth. </p> <p>Hadamard's factorization theorem</p> <p>Assume \\(\\{a_n\\}\\) are a sequence of points with \\(\\lim\\limits_{n\\rightarrow \\infty}|a_n|=\\infty\\). For an entire function \\(f\\) with zeros \\(\\{a_n\\}\\) and order of growth \\(\\rho_0\\), there exsits \\(k\\) such that \\(k\\leq \\rho_0&lt;k+1\\), and </p> \\[ f(z)=e^{g(z)}z^m\\prod_{n=1}^\\infty E_k(z/a_n) \\] <p>where \\(g(z)\\) is a polynomial of degree less then \\(k\\) and \\(m\\) is the order of the zero of \\(f\\) at \\(z=0\\).</p>"},{"location":"Math/Complex_Analysis/Gamma_Zeta/","title":"Gamma &amp; Zeta Function","text":"<p>This chapter we would make use of the previous conclusions to give an analysis of typical functions.</p>"},{"location":"Math/Complex_Analysis/Gamma_Zeta/#gamma-funcion","title":"Gamma Funcion","text":"<p>Definition of Gamma Function</p> <p>For \\(s&gt;0\\), define Gamma function as </p> \\[ \\Gamma(s)=\\int_0^\\infty e^{-t}t^{s-1}dt. \\] <p>\\(s-1&gt;-1\\) guarantees the convergence near \\(0\\) and \\(e^{-1}\\) guarantees the convergence at \\(\\infty\\).</p> <p>Now we use the analytic continuation to extend the definition of \\(\\Gamma(s)\\) from the real line \\((0,\\infty)\\) to \\(\\{\\text{Re }(s)&gt;0\\}\\).</p>"},{"location":"Math/Complex_Analysis/Gamma_Zeta/#analytic-continuation","title":"Analytic Continuation","text":"<p>Analytic continuation to \\(\\{\\text{Re }(s)&gt;0\\}\\)</p> <p>Still the above integral definition, Gamma function is defined on \\(\\{\\text{Re }(s)&gt;0\\}\\).</p> Proof <p>The convergence of integral is easy to prove. But to prove the convergent function is holomorphic, we need to use Sequence of holomorphic functions, i.e. integral converges uniformly to the so-called function in a strip (ccompact subset of \\(\\{\\text{Re }(s)&gt;0\\}\\)) </p> \\[ \\{s: \\delta &lt;\\text{Re }(s)&lt;M\\} \\] <p>for every positive \\(\\delta\\) and \\(M\\). \\(\\forall \\varepsilon\\in (0,1)\\), define </p> \\[ f_\\varepsilon(z)=\\int_\\varepsilon^{1/\\varepsilon}e^{-t}t^{s-1}dt \\] <p>which converges to \\(f(z)\\), by the beginning of this chapter. Now we prove the convergence is uniform.</p> \\[ \\begin{align*} \\left|f_\\varepsilon(z)-f(z)\\right|&amp;=\\left|\\int_0^\\varepsilon +\\int_{1/\\varepsilon}^\\infty e^{-t}t^{s-1}dt\\right|\\\\ &amp;\\leq \\int_0^\\varepsilon |e^{-t}t^{s-1}|dt + \\int_{1/\\varepsilon}^\\infty |e^{-t}t^{s-1} |dt\\\\ &amp;\\leq \\int_0^\\varepsilon t^{\\delta-1}dt + \\int_{1/\\varepsilon}^\\infty e^{-t}t^{M-1} dt\\\\ &amp;\\leq \\frac{\\varepsilon^{\\delta}}{\\delta} +  \\int_{1/\\varepsilon}^\\infty ce^{-t/2} dt\\\\ &amp;=\\frac{\\varepsilon^{\\delta}}{\\delta} -ce^{-1/2\\varepsilon}\\rightarrow 0(\\varepsilon\\rightarrow 0). \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>And next we would use recursion formula to extend the domain of definition to the entire \\(\\mathbb{C}\\) except for its poles.</p> <p>Recursion Formula of Gamma function</p> <p>Show that</p> \\[ \\Gamma(s)=\\frac{\\Gamma(s+1)}{s}. \\] Proof <p>The proof is simple, just take the derivative of the following. For \\(s&gt;0\\),</p> \\[ \\begin{align*} 0=e^{-t}t^{s}|_0^\\infty&amp;=\\int_0^\\infty \\frac{d}{dt}(e^{-t}t^{s}) dt\\\\ &amp;=\\int_0^\\infty -e^{-t}t^{s} +st^{s-1}e^{-t} dt\\\\ &amp;=-\\Gamma(s+1) + s\\Gamma(s). \\end{align*} \\] <p>and we are done. </p> <p>Corollary: integer for gamma function</p> <p>Show that</p> \\[ \\Gamma(n+1)=n!. \\] Proof <p>Since </p> \\[ \\Gamma(1)=\\int_0^\\infty e^{-t} t dt=-e^{-t}t|_0^\\infty +\\int_0^\\infty e^{-t}dt=-e^{-t}|_{0}^\\infty=1. \\] <p>By</p> \\[ \\Gamma(n+1)=n\\Gamma(n)=n(n-1)\\Gamma(n-1)=n!. \\] <p>From the above recursion formula, we could have the following extension.</p> <p>Analytic continuation to \\(\\mathbb{C}\\)</p> <p>The Gamma function \\(\\Gamma(s)\\) initially defined on \\(\\{\\text{Re }(s)&gt;0\\}\\) has an analytic continuation to a meromorphic function on \\(\\mathbb{C}\\) with poles of one order at \\(z_n=-n\\) for \\(n=0,1,\\cdots\\) And its residue formula is </p> \\[ \\text{Res}_{s=-n}(s)=\\frac{(-1)^n}{n!}. \\] ProofProof version two <p>The result could be derived directly from the above deduction.</p> <p>Use induction to prove the residue formula.</p> <p>Split the integral.</p> \\[ \\int_0^\\infty=\\int_1^\\infty + \\int_1^\\infty \\] <p>So the latter defines an entire function for \\(s\\in \\mathbb{C}\\). And the former term could be expanded with power series for \\(e^{-t}\\), and exchange the integral and summation, i.e.</p> \\[ \\begin{align*} \\int_0^1 \\sum_{n=0}^\\infty \\frac{(-1)^n t^n}{n!} t^{s-1}dt&amp;=\\int_0^1 \\sum_{n=0}^\\infty (-1)^n t^{n+s-1}/n! dt\\\\ &amp;= \\sum_{n=0}^\\infty  \\frac{(-1)^n}{n! (n+s)} \\end{align*} \\] <p>we could check the definition of Gamma function and its residue formula. To be careful, we have to prove the series converges uniformly.</p> <p>We argue as follows.</p> <p>For any given \\(R&gt;0\\), consider \\(|s|&lt;R\\), and choose \\(N&gt;2R\\). divide the series into two part</p> \\[ \\sum_{|s|&lt;2R} \\frac{(-1)^n}{n! (n+s)}+ \\sum_{|s|&gt;2R}\\frac{(-1)^n}{n! (n+s)} \\] <p>the former term converges because it has finite many number of elements. As for the latter tern, we estimate its absolute value</p> \\[ \\left|\\frac{(-1)^n}{n! (n+s)}\\right|\\leq \\left|\\frac{1}{n! R}\\right| \\] <p>which converges. Since \\(R\\) is arbitrary, we prove the uniform convergence.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Gamma_Zeta/#functional-equation","title":"Functional equation","text":"<p>Here comes the functional equation of Gamma function.</p> <p>Euler's Reflection Formula</p> <p>For all \\(s\\in \\mathbb{C}\\),</p> \\[ \\Gamma(s)\\Gamma(1-s)=\\frac{\\pi}{\\sin \\pi s} \\] Proof \\[ \\begin{align*} \\int_0^\\infty dt \\int_0^\\infty e^{-t} t^{s-1} e^{-u}u^{-s}du&amp;=\\int_0^\\infty dt \\int_0^\\infty e^{-t} t^{s-1} e^{-tv}(tv)^{-s}tdv\\quad \\text{let } u=tv\\\\ &amp;= \\int_0^\\infty dt \\int_0^\\infty e^{-t(1+v)} v^{-s}dv\\\\ &amp;= \\int_0^\\infty dv \\int_0^\\infty e^{-t(1+v)} v^{-s}dt\\\\ &amp;= \\int_0^\\infty \\frac{v^{-s}dv}{1+v}\\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{e^{-sx+1}}{1+e^x}dx\\quad \\text{let } v=e^x\\\\ &amp;= \\frac{\\pi}{\\sin \\pi (1-s)}=\\frac{\\pi}{\\sin \\pi s}. \\end{align*} \\] <p>Note we use the lemma for \\(a\\in (0,1)\\)</p> \\[ \\int_0^\\infty \\frac{v^{a-1}}{1+v}dv=\\int_{-\\infty}^\\infty \\frac{e^{ax}}{1+e^x}dx=\\frac{\\pi}{\\sin \\pi a}. \\] <p><p>\\(\\square\\)</p></p> <p>Note the poles of lefthand side of the Euler's reflection formula are \\(\\mathbb{Z}\\).</p> <p>Corollary: </p> <p>Let \\(s=\\frac{1}{2}\\), we have</p> \\[ \\Gamma(1/2)=\\sqrt{\\pi}. \\]"},{"location":"Math/Complex_Analysis/Gamma_Zeta/#properties-of-its-reciprocal","title":"Properties of its reciprocal","text":"<p>Properties of its reciprocal</p> <p>The function \\(\\Gamma\\) has the following properties.</p> <p>(i) \\(1/\\Gamma\\) is an entire function of \\(s\\) with zeros \\(0,-1,\\cdots\\) and vanishes nowhere else.</p> <p>(ii) Growth order. \\(1/\\Gamma\\) has growth \\(\\rho_f=1\\), but cannot get \\(1\\) i.e.</p> \\[ \\left|\\frac{1}{\\Gamma(s)}\\right|\\leq c_1 \\exp(c_2|s|\\log |s|) \\] <p>or in other words, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists c_1(\\varepsilon)&gt;0\\), such that</p> \\[ \\left|\\frac{1}{\\Gamma(s)}\\right|\\leq c_1(\\varepsilon) \\exp(c_2|s|^{1+\\varepsilon}) \\] Proof <p>The proof for (i) is clear.</p> <p>As for (ii) We use the power series expansion into the Euler's reflection formula</p> \\[ \\begin{align*} \\frac{1}{\\Gamma(s)}&amp;=\\Gamma(1-s)\\frac{\\sin \\pi s}{\\pi}\\\\ &amp;=\\sum_{n=0}^\\infty \\frac{(-1)^n\\sin \\pi s}{\\pi n!(n+1-s)}+\\int_1^\\infty \\frac{e^{-t}t^{-s}\\sin \\pi s}{\\pi}dt \\end{align*} \\] <ul> <li>For the latter term, we have its order estimate using \\(n&lt;|\\sigma|=|\\text{Re } s|&lt; n+1, n\\in \\mathbb{N}\\),</li> </ul> \\[ \\begin{align*} \\left|\\int_1^\\infty e^{-t}t^{-s}dt\\right| &amp;\\leq \\int_1^\\infty e^{-t}t^{\\sigma}dt\\\\ &amp;\\leq \\int_1^\\infty e^{-t}t^{-n}dt\\\\ &amp;\\leq \\int_0^\\infty e^{-t}t^{-n}dt\\\\ &amp;=n!&lt;n^n=e^{n\\log n}&lt; e^{(|\\sigma|+1) \\log (|\\sigma|+1)} \\end{align*} \\] <p>and since \\(|\\sin \\pi s|\\leq e^{\\pi |s|}\\), and we show the order of the latter.</p> <ul> <li>For the former one, we partition the possible value of \\(s\\) into \\(3\\) parts, as the following images showed.</li> </ul> <p><p> </p></p> <p>For case 1, we have \\(|\\text{Im }s|&gt;1\\), so \\(|n+1-s|&gt;|\\text{Im } s|=1\\), so </p> \\[ \\left|\\sum_{n=0}^\\infty \\frac{(-1)^n\\sin \\pi s}{\\pi n!(n+1-s)}\\right|\\leq |\\sin \\pi s|/\\pi \\sum_{n=0}^\\infty \\frac{1}{n!}&lt;ce^{\\pi |s|}. \\] <p>For Case 2, we have \\(|\\text{Im }s|\\leq 1\\) and \\(|\\text{Re }s|&lt;0\\), we still have \\(|n+1-s|&gt;|\\text{Im } s|=1\\) and it has the same order of growth as case 1.</p> <p>For Case 3, we consider each case the poles for \\(s\\). For each \\(s\\), \\(\\exists k\\) such that \\(k-\\frac{1}{2}\\leq s&lt;k+\\frac{1}{2}\\), so only one term of the series could not be small enough, i.e.</p> \\[ \\begin{align*} \\sum_{n=0}^\\infty \\frac{(-1)^n\\sin \\pi s}{\\pi n!(n+1-s)}&amp;= \\frac{(-1)^{k-1}\\sin \\pi s}{\\pi (k-1)!(k-s)}\\\\ &amp;+\\sum_{n=0\\atop n\\neq k-1}^\\infty\\frac{(-1)^n\\sin \\pi s}{\\pi n!(n+1-s)} \\end{align*} \\] <p>the former term \\(\\sin \\pi s\\) has zero \\(s=k\\) and cancels \\(k-s\\) in the denominator, which causes the whole term to be holomorphic and bounded, while the latter could be scaled in the same way as we deal in case 1 &amp; 2.</p> <p><p>\\(\\square\\)</p></p> <p>Apply the above growth order to Hadamard's factorization theorem, we have</p> <p>Corollary: Factorization of \\(1/\\Gamma(s)\\)</p> <p>Using Euler constant \\(\\gamma=\\lim\\limits_{N\\rightarrow \\infty} \\sum\\limits_{n=1}^N \\frac{1}{n}-\\log N\\), we have a factorization for \\(1/\\Gamma(s)\\)</p> \\[ \\frac{1}{\\Gamma(s)}=e^{\\gamma s}z\\prod_{n=1}^\\infty \\left(1+\\frac{z}{n}\\right)e^{-s/n}. \\] Proof <p>This is a typical method to get the coefficient of the underdetermined polynomial \\(p(s)=as+b\\).</p> <p>Note \\(1/\\Gamma(s)\\) has zeros \\(0,-1,\\cdots\\) so we have \\(s\\rightarrow 0\\), </p> \\[ e^b = \\lim_{s\\rightarrow 0}\\frac{1}{\\Gamma(s)s} =1  \\] <p>which implies \\(b=0\\). To determine \\(a\\), we let \\(s=1\\) and </p> \\[ \\begin{align*} 1&amp;=e^a\\prod_{n=1}^\\infty (1+1/n)e^{-1/n}\\\\ \\Rightarrow \\quad e^{-a}&amp;=\\exp\\left(\\sum_{n=1}^\\infty \\log \\frac{n+1}{n}-1/n\\right)\\\\ \\Rightarrow \\quad e^a&amp;=\\exp\\left(\\lim_{N\\rightarrow \\infty}\\sum_{n=1}^N 1/n-\\log (N+1) \\right)\\\\ &amp;=\\exp\\left(\\gamma +\\lim_{N\\rightarrow\\infty}\\log N-\\log (N+1)  \\right)=e^\\gamma \\end{align*} \\] <p>so \\(a=\\gamma+2k\\pi i\\), but when \\(s\\) is real, the reciprocal is real, the righthand side would be complex if \\(k\\neq 0\\). Therefore we complete the proof.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Gamma_Zeta/#zeta-function","title":"Zeta Function","text":"<p>Definition of Zeta Function</p> <p>The Riemann Zeta Function is initially defined for real \\(s&gt;1\\) by the convergent series</p> \\[ \\zeta(s)=\\sum_{n=1}^\\infty \\frac{1}{n^s}. \\] <p>As in the case of the gamma function, \\(\\zeta\\) could be extended into the complex plane. Here we give the one that relies on the functional equation.</p>"},{"location":"Math/Complex_Analysis/Gamma_Zeta/#functional-equation_1","title":"Functional equation","text":"<p>Analytic continuation to \\(\\text{Re }s&gt;1\\)</p> <p>The series defining \\(\\zeta(s)\\) converges for \\(\\text{Re }s&gt;1\\), and the function is holomorphic in this half-plane.</p> Proof <p>Let \\(s=\\sigma+it\\), then </p> \\[ |n^{-s}|=n^{-\\sigma} \\] <p>by logarithm. So convergence is controlled by the real part of \\(s\\). So \\(G_N(s)=\\sum\\limits_{n=1}^N \\frac{1}{n^s}\\) converges uniformly to \\(\\zeta(s)\\) on every compact subset of \\(\\{\\text{Re }s&gt;1\\}\\), i.e. \\(\\{s\\geq 1+\\delta\\}\\) for each \\(\\delta&gt;0\\). So the convergent function is holomorphic.</p> <p>The analytic continuation to a meromorphic function in \\(\\mathbb{C}\\) is subtle. Here we relates it with Gamma and another Theta funtion.</p> <p>Recall Theta function is </p> \\[ \\vartheta(s)=\\sum_{n=-\\infty}^\\infty e^{-\\pi n^2 s} \\] <p>with its properties as follows.</p> <p>Suppliment: Properties of Theta Function</p> <p>(i) Functional equation</p> \\[ \\vartheta(s)=s^{-1/2} \\vartheta(1/s). \\] <p>(ii) Growth order. </p> \\[ \\vartheta(s)\\leq C t^{-1/2},\\quad t\\rightarrow 0 \\] <p>and</p> \\[ |\\vartheta(s)-1|\\leq C e^{-\\pi s},\\quad C&gt;0, \\quad \\forall t\\geq 1 \\] Proof <p>Use Fourier transfer.</p> <p>We are now in a position to prove an important relation among \\(\\zeta\\), \\(\\Gamma\\) and \\(\\vartheta\\). </p> <p>Relation among \\(\\zeta\\), \\(\\Gamma\\) and \\(\\vartheta\\)</p> <p>If \\(\\text{Re }s&gt;1\\), then </p> \\[ e^{-s/2}\\Gamma(s/2)\\zeta(s)=\\frac{1}{2}\\int_0^\\infty u^{s/2-1}[\\vartheta(u)-1]du. \\] Proof <p>we make use of </p> \\[ \\frac{\\vartheta(s)-1}{2}=\\sum_{n=1}^\\infty e^{-\\pi n^2 s}, \\] <p>so</p> \\[ \\begin{align*} \\frac{1}{2}\\int_0^\\infty u^{s/2-1}[\\vartheta(u)-1]du &amp;=\\int_0^\\infty u^{s/2-1}\\sum_{n=1}^\\infty e^{-\\pi n^2 u}du\\\\ &amp;=\\sum_{n=1}^\\infty \\int_0^\\infty u^{s/2-1} e^{-\\pi n^2 u} du\\\\ &amp;=\\sum_{n=1}^\\infty \\int_0^\\infty t^{s/2-1} \\pi^{1-s/2} n^{2-s} e^{t} \\pi^{-1} n^{-2}dt, \\quad \\text{let }u=t/(\\pi n^2)\\\\ &amp;=\\sum_{n=1}^\\infty n^{-s} \\pi^{-s/2} \\Gamma(s/2)=\\pi^{-s/2}\\zeta(s)\\Gamma(s/2). \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>Now we focus on a more specific function which is more symmetric, called xi function, defined for \\(\\text{Re }x&gt;1\\),</p> \\[ \\xi(s)=\\pi^{-s/2} \\Gamma(s/2)\\zeta(s). \\] <p>Theorem for xi function</p> <p>Xi Function is holomorphic for \\(\\text{Re }s&gt;1\\) and has an analytic continuation to all of \\(\\mathbb{C}\\) as a meromorphic function with simple poles at \\(s=0\\) and \\(x=1\\). Moreover, </p> \\[ \\xi(s)=\\xi(1-s),\\quad \\forall s\\in \\mathbb{C}. \\] Proof <p><p>\\(\\square\\)</p></p> <p>Now we have the result for zeta function.</p> <p>Analytic continuation and functional equation of Zeta function</p> <p>The zeta function has a meromorphic continuation into the entire complex plane, whose singularity is a simple pole at \\(s=1\\).</p> Proof <p>From Relation among \\(\\zeta\\), \\(\\Gamma\\) and \\(\\vartheta\\), we have</p> \\[ \\zeta(s)=\\pi^{s/2}\\frac{\\xi(s)}{\\Gamma(s/2)}. \\] <p>Recall that \\(\\Gamma(s/2)\\) is entire with simple poles \\(0,-2,\\cdots\\), and the zero of \\(\\xi(s)\\) is \\(0\\) and \\(1\\), so the origin is cancelled. Therefore the singularity of \\(\\zeta\\) is a simple pole at \\(s=1\\).</p> <p>The following estimate about delta function is useful in studying the growth of \\(zeta\\) near \\(\\text{Re }s=1\\).</p> <p>Proposition of delta function</p> <p>There is a sequence of entire functions \\(\\{\\delta_n(s)\\}_{n\\geq 1}\\) that satisfies the estimate \\(|\\delta(s)|\\leq |s|/n^{\\sigma+1}\\), where \\(s=\\sigma+it\\), and such that </p> \\[ \\begin{align} \\sum_{1\\leq n&lt;N}\\frac{1}{n^s}-\\int_1^N \\frac{dx}{x^s}=\\sum_{1\\leq n&lt;N}\\delta_n(s).\\label{delta-sum} \\end{align} \\] Proof <p>Actually we shall define </p> \\[ \\begin{align} \\delta_n(s)=\\int_n^{n+1}\\left(\\frac{1}{n^s}-\\frac{1}{x^s}\\right)dx.\\label{delta} \\end{align} \\] <p>Define \\(f(x)=x^{-s}\\), \\(f'(x)=-sx^{-s-1}\\), and its mean-value theorem \\(f(x)-f(n)=f'(\\xi)(x-n)\\), where \\(n\\leq &lt;x&lt;n+1\\) and \\(\\xi\\in (n,x)\\), which gives</p> \\[ \\left|\\frac{1}{n^s}-\\frac{1}{x^s}\\right|=|f(x)-f(n)|\\leq |f'(\\xi)|=|s|/x^{\\sigma+1}&lt;\\frac{|s|}{n^{\\sigma+1}} \\] <p>where the last inequality holds for \\(f'(x)\\) is monotonically decreasing.</p> <p>So sum equation \\(\\ref{delta}\\) up over \\(n\\), we have the result.</p> <p><p>\\(\\square\\)</p></p> <p>Corollary</p> <p>For \\(\\text{Re }s&gt;0\\), we have</p> \\[ \\begin{align} \\zeta(s)-\\frac{1}{s-1}=H(s)=\\sum_{n=1}^\\infty \\delta_n(s).\\label{estimate-zeta} \\end{align} \\] <p>where \\(H(s)\\) is holomorphic in the half-plane \\(\\text{Re }s&gt;0\\).</p> Proof <p>In equation \\(\\ref{delta-sum}\\), let \\(N\\rightarrow \\infty\\), and we have the result. As for the convergent region, at first we assume \\(\\text{Re }s&gt;1\\) for summation of \\(1/n^s\\) to converge. However by estimate for the residue \\(\\delta_n(s)\\)</p> \\[ |\\delta_n(s)|\\leq |s|/n^{\\sigma+1} \\] <p>for each \\(\\delta&gt;0\\), and compact subset \\(\\{\\text{Re }s&gt;\\delta\\}\\), the righthand term converges uniformly for \\(s\\).</p> <p>Now we give a growth estimate for \\(\\zeta(s)\\) on line \\(\\{\\text{Re }s=1\\}\\).</p> <p>Proposition for estimate of growth of zeta function on line \\(\\{\\text{Re }s=1\\}\\)</p> <p>Suppose \\(s=\\sigma+it\\) with \\(\\sigma, t\\in \\mathbb{R}\\). THen for each \\(\\sigma_0\\in [0,1]\\), and \\(\\varepsilon&gt;0\\), there exsits a constant \\(c_\\varepsilon\\) so that </p> <p>(i) \\(|\\zeta(s)|\\leq c_\\varepsilon |t|^{1-\\sigma_0+\\varepsilon}\\), if \\(\\sigma_0\\leq \\sigma\\) and \\(|t|\\geq 1\\).</p> <p>(ii) Derivative. \\(\\zeta'(s)\\leq c_\\varepsilon|t|^\\varepsilon\\), if \\(1\\leq \\sigma\\), and \\(|t|\\geq 1\\).</p> Proof <ul> <li>(i) we use the combination of two estimate. One is</li> </ul> \\[ |\\delta_n(s)|\\leq |s|/n^{\\sigma+1}\\leq |s|/n^{\\sigma_0+1} \\] <p>and another is triangle inequation </p> \\[ \\begin{align*} |\\delta_n(s)|&amp;=\\left|\\int_n^{n+1} \\frac{1}{n^{-s}} - \\frac{1}{x^{-s}}dx\\right|\\\\ &amp;\\leq \\int_n^{n+1} \\left|\\frac{1}{n^{-s}} - \\frac{1}{x^{-s}}\\right|dx\\\\ &amp;\\leq \\int_n^{n+1} \\left|\\frac{1}{n^{-s}}\\right| +\\left| \\frac{1}{x^{-s}}\\right|dx\\\\ &amp;= \\int_n^{n+1} \\left|\\frac{1}{n^{-\\sigma}}\\right| +\\left| \\frac{1}{x^{-\\sigma}}\\right|dx\\\\ &amp;\\leq \\int_n^{n+1} \\left|\\frac{1}{n^{-\\sigma}}\\right| +\\left| \\frac{1}{n^{-\\sigma}}\\right|dx=\\frac{2}{n^\\sigma}\\leq \\frac{2}{n^{\\sigma_0}}\\\\ \\end{align*} \\] <p>Since for \\(\\delta\\geq 0\\), \\(A=A^\\delta A^{1-\\delta}\\), we have</p> \\[ |\\delta_n(s)|\\leq \\left(\\frac{|s|}{n^{\\sigma_0+1}}\\right)^\\delta \\left(\\frac{2}{n^{\\sigma_0}}\\right)^{1-\\delta}=\\frac{2^{1-\\delta}|s|^\\delta}{n^{\\sigma_0+\\delta}} \\] <p>let \\(\\delta=1-\\sigma_0+\\varepsilon\\), and apply the equaiton \\(\\ref{estimate-zeta}\\) we have</p> \\[ |\\zeta(s)|\\leq \\left|\\frac{1}{s-1}\\right| + 2^{\\sigma_0-\\varepsilon}|s|^{1-\\sigma_0+\\varepsilon}\\sum_{n=1}^\\infty \\frac{1}{n^{1+\\varepsilon}}=O(|t|^{1-\\sigma_0+\\varepsilon}). \\] <ul> <li>(ii) we use Cauchy's integral formula</li> </ul> \\[ \\begin{align*} |\\zeta'(s)|&amp;\\leq \\frac{1}{2\\pi \\varepsilon}\\int_0^{2\\pi} |\\zeta(s+\\varepsilon e^{i\\theta})e^{i\\theta}|d\\theta\\\\ &amp;\\leq \\frac{1}{\\varepsilon}c_\\varepsilon |t|^{1-\\sigma_0+\\varepsilon}\\\\ &amp;=c'_\\varepsilon |t|^{2\\varepsilon},\\quad \\text{let } \\sigma_0=1-\\varepsilon \\end{align*} \\] <p>the third \"\\(=\\)\" holds for \\(\\text{Re }s&gt;\\sigma-\\varepsilon\\geq 1-\\varepsilon\\).</p>"},{"location":"Math/Complex_Analysis/Meromophic/","title":"Meromorphic Function","text":"<p>We talk about singularities at the complex plane.</p> <ul> <li> <p>Removable singularities.</p> </li> <li> <p>Poles.</p> </li> <li> <p>Essential singularities.</p> </li> </ul>"},{"location":"Math/Complex_Analysis/Meromophic/#zeros-and-poles","title":"Zeros and poles","text":"<p>We start from zeros and poles.</p> <p>Actually, by the Theorem for zero accumulation, if we have an accumalation point of a sequence of zeros, then a holomorphic fucntion \\(f\\) is identically zero on region \\(\\Omega\\) (here must be connected). So for a non-trivial holomorphic function on \\(\\Omega\\), its zeros are isolated, i.e. have no limit point. To be elaborate, if \\(f(z_0)=0\\), then \\(\\exists \\delta&gt;0\\), such that \\(f(z)\\neq 0,\\forall z\\in D_\\delta(z_0)-\\{z_0\\}\\).</p> <p>Based on the above analysis, we have the following configuration for zeros with power series. </p> <p>Theorem for Zeros with power series</p> <p>Assume \\(f\\) is holomorphic on a region \\(\\Omega\\), \\(f(z_0)=0\\). For a neighborhood \\(U\\subset \\Omega\\) of \\(z_0\\), \\(f\\) does not vanish identically on \\(U-\\{z_0\\}\\). Then there exists a unique positive interger \\(n\\) such that </p> \\[ f(z)=(z-z_0)^n g(z), \\] <p>where \\(g(z)\\) is a holomorphic function which does not vanish on \\(U\\). We call \\(f\\) has zero of order \\(n\\) (multiplicity \\(n\\)) at \\(z_0\\).</p> Proof <ul> <li>Existence.</li> </ul> <p>\\(f\\) is holomorphic on \\(U\\), so we have power series at \\(z_9\\)</p> \\[ f(z)=\\sum_{m=0}^\\infty a_m (z-z_0)^m. \\] <p>Since \\(f\\) is non-trivial, there exsits a smallest number \\(n\\) such that \\(a_n\\neq 0\\), otherwise \\(f\\) is identically zero. So </p> \\[ f(z)=(z-z_0)^n\\sum_{m=n}^\\infty a_m (z-z_0)^{m-n}=(z-z_0)^n g(z) \\] <ul> <li>Uniqueness. Assume \\(\\exists n,k\\) such that \\(n&lt;k\\) and </li> </ul> \\[ \\begin{align*} f(z)=(z-z_0)^n\\sum_{m=n}^\\infty a_m (z-z_0)^{m-n}&amp;=(z-z_0)^k\\sum_{m=k}^\\infty a_m (z-z_0)^{m-k}\\\\ \\Rightarrow \\quad \\sum_{m=n}^\\infty a_m (z-z_0)^{m-n} &amp;= (z-z_0)^{k-n}\\sum_{m=k}^\\infty a_m (z-z_0)^{m-k} \\end{align*} \\] <p>then substitute in \\(z_0\\), we have the right-hand expression equal \\(0\\) while the left-hand does not.</p> <p>Define a function \\(f\\) defined on a deleted neighborhood of \\(z_0\\) is holomorphic and has a pole at \\(z_0\\), if \\(1/f\\) has a zero at \\(z_0\\) and holomorphic on all the neighborhood of \\(z_0\\).</p> <p>Theorem for Poles with power series</p> <p>Assume \\(f\\) is holomorphic on a deleted neighborhood of \\(z_0\\), and has a pole at \\(z_0\\). Then there exsits a unique positive interger \\(n\\) such that </p> \\[ f(z)=(z-z_0)^{-n} h(z) \\] <p>where \\(h(z)\\) is a non-vanishing holomorphic function on the full neighborhood of \\(z_0\\). We call \\(f\\) has poles of order \\(n\\) (multiplicity \\(n\\)) at \\(z_0\\). Moreover</p> \\[ \\begin{align} f(z)=\\sum_{k=1}^n \\frac{a_{-k}}{(z-z_0)^k} + G(z)\\label{configuration of poles} \\end{align} \\] <p>where \\(G(z)\\) is still a holomorphic function on the full neighborhood of \\(z_0\\), but might vanish at \\(z_0\\). Here \\(\\sum\\limits_{k=1}^n \\frac{a_{-k}}{(z-z_0)^k}\\) is called the principle part of \\(f\\) at the pole \\(z_0\\). </p> Proof <p>Just use Theorem for Zeros with power series, note that </p> \\[ h(z)=\\frac{1}{g(z)}\\neq 0,\\quad \\forall z\\in U. \\] <p>Since \\(h(z)\\) is holomorphic, we apply power series expansion again to \\(h(z)\\) and have the result needed.</p> <p>Naturally we want to apply the closed loop integral on the function with poles, which by the above configuration equation \\(\\ref{configuration of poles}\\), only </p> \\[ \\frac{a_{-1}}{(z-z_0)} \\] <p>would yields non-zero value, since other parts have a primitive on \\(U-\\{z_0\\}\\). Define residue to be the coefficient of this item, i.e. \\(\\text{res}_{z_0} f=a_{-1}\\). Then actually for a closed loop \\(\\gamma\\) in \\(U-\\{z_0\\}\\), </p> \\[ \\int_\\gamma f(z)dz=2\\pi i \\text{ res}_{z_0}f. \\] <p>To elaborate, we have the following theorem.</p>"},{"location":"Math/Complex_Analysis/Meromophic/#residue-formula","title":"Residue Formula","text":"<p>Theorem of Residue formula</p> <p>Assume \\(f\\) is holomorphic on a open set containing a circle \\(C\\) and its interior, except for a pole at \\(z_0\\). Then</p> \\[ \\int_C f(z)dz=2\\pi i\\text{ res}_{z_0}f. \\] <p>The above theorem could be extended to finitely many poles in a open set.</p> <p>A lot of questions occur on acquaring the residue of functions with poles.</p>"},{"location":"Math/Complex_Analysis/Meromophic/#calculation-of-residue-a-1","title":"Calculation of residue (a-1)","text":"<p>Calculation of Residue</p> <p>For simple pole, we have</p> \\[ \\text{res}_{z_0} f=\\lim_{z\\rightarrow z_0}(z-z_0)f(z), \\] <p>For pole of multiple order \\(n\\) (\\(n\\geq 1\\)), we have</p> \\[ \\text{res}_{z_0} f= \\lim_{z\\rightarrow z_0} \\frac{1}{(n-1)!} \\frac{d^{n-1}}{dz^{n-1}}(z-z_0)^nf(z). \\]"},{"location":"Math/Complex_Analysis/Meromophic/#removable-singularies","title":"Removable Singularies","text":"<p>Definition of removable singularities</p> <p>Assume \\(f\\) is holomorphic on open set \\(\\Omega\\) except for \\(z_0\\). If we could define \\(f\\) at \\(z_0\\) such that \\(f\\) is holomorphic on \\(\\Omega\\), then we call \\(z_0\\) a removable singularity for \\(f\\).</p> <p>How to define? We could use inspirations from Sequence of holomorphic functions or in terms of integrals, that is, create a function that agree with original function on \\(\\Omega-\\{z_0\\}\\) and itself has a value at \\(z_0\\). This is achieved by Cauchy's Integral formula.</p> <p>Riemann's theorem on removable singularities</p> <p>Assume \\(f\\) is holomorphic on open set \\(\\Omega\\) except for \\(z_0\\). If \\(f(z)\\) is bounded on \\(\\Omega-\\{z_0\\}\\), then \\(z_0\\) is a removable point.</p> ProofWrong version <p>Choose \\(R\\) and we have a circle \\(C_R(z_0)\\subset \\Omega\\), and an open disc \\(D_R(z_0)\\), define </p> \\[ F(z)=\\int_{C_R(z_0)}\\frac{f(\\zeta)}{(\\zeta-z)}dz ,\\quad z\\in D_R(z_0), \\] <p>which has a definition, for each \\(\\zeta\\in C_R(z_0)\\), the integrated function \\(\\frac{f(\\zeta)}{(\\zeta-z)}\\) is holomorphic for all \\(z\\in D_R(z_0)\\), so by function in terms of integrals, \\(F(z)\\) is holomorphic for all \\(z\\in D_R(z_0)\\). Then if we find out </p> \\[ \\begin{align} F(z)=f(z), \\quad \\forall z\\in D_R(z_0)\\label{analytic-part} \\end{align} \\] <p>then \\(F\\) is a analytic continuation of \\(f\\), and could define \\(f(z_0)=F(z_0)\\).</p> <ul> <li>Show that equation \\(\\ref{analytic-part}\\) holds. That is, </li> </ul> \\[ f(z)=\\int_{C_R(z_0)}\\frac{f(\\zeta)}{(\\zeta-z)}dz, \\] <p>which is similar to Cauchy's integral formula. We have to formulate a toy contour and actually</p> \\[ \\int_{C_R(z_0)}\\frac{f(\\zeta)}{(\\zeta-z)}dz=\\int_{C_\\varepsilon(z)}\\frac{f(\\zeta)}{(\\zeta-z)}dz+\\int_{C_\\varepsilon(z_0)}\\frac{f(\\zeta)}{(\\zeta-z)}dz. \\] <p>The first item is \\(2\\pi i f(z)\\) and the second item equals \\(0\\) since \\(f\\) is bounded</p> \\[ \\left|\\int_{C_\\varepsilon(z_0)}\\frac{f(\\zeta)}{(\\zeta-z)}dz\\right| \\leq \\frac{\\left|\\sup\\limits_{z\\in \\Omega-\\{z_0\\} }f(z)\\right|}{|z-z_0|/2}\\cdot 2\\pi \\varepsilon\\rightarrow 0. \\] <p>and we are done.</p> <p>We could also define </p> \\[ g(z)=\\begin{cases} (z-z_0)f(z),\\quad &amp;z\\in \\Omega-\\{z_0\\}\\\\ 0,\\quad &amp;z=z_0 \\end{cases}. \\] <p>So \\(g(z)\\) is holomorphic on \\(\\Omega\\) because at \\(z_0\\), we have a wrong version:</p> \\[ \\lim_{z\\rightarrow z_0}\\frac{g(z)-g(z_0)}{z-z_0}=\\lim_{z\\rightarrow z_0}f(z) \\] <p>cause we have not define the limit of \\(f(z)\\) at \\(z_0\\). Actually, we only have</p> \\[ |g(z)|\\leq M(z-z_0)\\rightarrow 0,\\quad (z\\rightarrow z_0) \\] <p>So \\(\\lim_{z\\rightarrow z_0}g(z)=0\\). </p> <p>However, if we already have the theorem, then we could use the above estimate to show that \\(g(z)\\) is bounded, thus holomorphic.</p> <p>Corollary: Characteristics of poles</p> <p>Assume \\(f\\) is holomorphic on a region except for \\(z_0\\). Then \\(z_0\\) is a pole iff \\(\\lim\\limits_{z\\rightarrow z_0}|f(z)|=\\infty\\).</p> Proof <ul> <li>Necessary.</li> </ul> <p>If \\(z_0\\) is a pole, then \\(1/f\\) has a zero at \\(z_0\\), i.e. \\(\\lim\\limits_{z\\rightarrow z_0}1/f(z)=0\\), i.e. \\(\\lim\\limits_{z\\rightarrow z_0}|f(z)|=\\infty\\).</p> <ul> <li>Sufficient.</li> </ul> <p>If \\(\\lim\\limits_{z\\rightarrow z_0}|f(z)|=\\infty\\), then \\(\\frac{1}{f}\\) is bounded near \\(z_0\\), so it has a removable singularity there. So define \\(1/f(z)|_{z=z_0}=\\lim\\limits_{z\\rightarrow z_0}1/|f(z)|=0\\). So it has a pole at \\(z_0\\).</p>"},{"location":"Math/Complex_Analysis/Meromophic/#essential-singularities","title":"Essential Singularities","text":"<p>Casorati-Weierstrass for essential singularities</p> <p>Suppose \\(f\\) is holomorphic in a punctured disc \\(D_r(z_0)-\\{z_0\\}\\) and has an essential singularity at \\(z_0\\). Then, the image of \\(D_r(z_0)-\\{z_0\\}\\) under \\(f\\) is dense in \\(\\mathbb{C}\\).</p> Proof <p>We argue by contradiction.</p> <p>Assume the image of \\(D_r(z_0)-\\{z_0\\}\\) is not dense in \\(\\mathbb{C}\\), then \\(\\exists w\\in \\mathbb{C}\\), \\(\\exists \\delta&gt;0\\), such that \\(\\forall z\\in D_r(z_0)-\\{z_0\\}\\), \\(|w-f(z)|&gt;\\delta\\). Define</p> \\[ g(z)=\\frac{1}{f(z)-w}. \\] <p>which is holomorphic in \\(D_r(z_0)-\\{z_0\\}\\), and bounded by \\(\\frac{1}{\\delta}\\), so it has a removable singularity at \\(z_0\\). Now we consider the exact value of \\(g(z_0)\\).</p> <p>If \\(g(z_0)=0\\), it is a pole for \\(f(z)-w\\) at \\(z_0\\), contradicts! If \\(g(z_0)\\neq 0\\), then \\(f(z_0)=w+1/g(z_0)\\), and </p> \\[ f(z)=w+1/g(z), \\quad z\\in D_r(z) \\] <p>so \\(f(z)\\) is holomorphic at \\(z_0\\), which also contradicts!</p>"},{"location":"Math/Complex_Analysis/Meromophic/#functions-with-only-poles","title":"Functions with only poles","text":"<p>Definition for meromorphic function</p> <p>A function \\(f\\) on an open set \\(\\Omega\\) is meromorphic, if there exsits a sequence of points \\(\\{z_n\\}\\) that has no limit points in \\(\\Omega\\) and such that</p> <p>(1) \\(f\\) is holomorphic on \\(\\Omega-\\{z_n\\}\\)</p> <p>(2) \\(f\\) has poles at points \\(\\{z_n\\}\\).</p> <p>Notice that the poles are isolated by analytic continuation, otherwise \\(f\\equiv 0\\). If \\(\\Omega\\) is bounded or compact, then \\(z_n\\) must be finite number. So if \\(z_n\\) has infinite many number, we must have \\(\\Omega\\) extend to \\(\\infty\\). Here comes the discussion about singularites at infinity.</p> <p>singularities at infinity</p> <p>If \\(f\\) is holomorphic for all large value of \\(z\\), define \\(F(z)=f(\\frac{1}{z})\\), which is holomorphic in a deleted neighborhood of the origin. We say that \\(f\\)</p> <p>(i) has a pole at \\(\\infty\\) if \\(F\\) has a pole at \\(0\\).</p> <p>(ii) has an essential singularity at \\(\\infty\\) if \\(F\\) has an essential singularity at \\(0\\).</p> <p>(iii) has a removable singularity at \\(\\infty\\) if \\(F\\) has a removable singularity at \\(0\\).</p> <p>A meromorphic function in the complex plane that is either holomorphic at infinity or has a pole at infinity is said to be meromorphic in the extended complex plane.</p> <p>Now we have a description for meromorphic function.</p> <p>Description of meromorphic function</p> <p>The meromorphic functions in the extended complex plane are the rational functions.</p> Proof <ul> <li> <p>For singularities at infinity. Let \\(F(z)=f(\\frac{1}{z})\\), so it could also be expressed by \\(F(z)=F_\\infty(z)+G_\\infty(z)\\) where \\(F_\\infty(z)\\) is the principle part and \\(G_\\infty(z)\\) is holomorphic at \\(0\\) (at all other points as well). Note here their forms are</p> </li> <li> <p>For singularities less than infinity, we have only finite number of poles, denoted by \\(\\{z_i\\}_{1\\leq i\\leq N}\\). This is because \\(f(\\frac{1}{z})\\) has either a pole or removable singularities there, so \\(\\exists \\delta=\\frac{1}{R}\\), such that \\(f(1/z)\\) has no other singularities in </p> </li> </ul> \\[ D_{\\delta}(0)=\\{z: |z|&lt;\\delta\\}=\\{u: |\\frac{1}{u}|&lt;\\delta\\}=\\{u: |{u}|&gt;R\\}. \\] <p>\\(f(z)\\) has no other singularities in \\(\\{u: |{u}|&gt;R\\}\\), i.e has other singularities in \\(D_R(0)\\), which is a compact set, meaning singularities in it must have finite number. We denote its principle part \\(f_i(z)\\), so we have \\(f(z)=f_i(z)+g_i(z)\\) for each \\(i\\), where \\(g_i\\) is holomorphic at \\(z_i\\) (at all other points as well).</p> \\[ F_\\infty(z)=\\sum_{k=-n_\\infty}^{-1} a_{n_\\infty}z^k,\\quad  G_\\infty(z)=\\sum_{k=0}^\\infty a_\\infty z^k \\] <ul> <li> <p>Define a function \\(h(z)=f(z)-\\sum_{i=1}^N f_i(z)-F_\\infty(\\frac{1}{z})\\), which is an entire function. </p> </li> <li> <p>Prove it is bounded. Since \\(h(1/z)\\) has a removable singularity near \\(z=0\\), thus is bounded. So \\(h\\) is bounded. By Liouville's theorem, \\(h=c\\), so</p> </li> </ul> \\[ f(z)=\\sum_{i=1}^N f_i(z)+F_\\infty(\\frac{1}{z})+c \\] <p>which is a rational function.</p> <p>Example. \\(f(z)=\\frac{1}{\\sin \\pi z}\\) has singularities at \\(z_n=n\\), which is of denumerable number. But \\(f(\\frac{1}{z})=\\frac{1}{\\sin \\pi \\frac{1}{z}}\\) has different phenomenon at \\(z=0\\). For \\(z=x\\in \\mathbb{R}\\), \\(f(\\frac{1}{x})\\rightarrow \\infty\\) as \\(x\\rightarrow 0\\). But \\(z=iy\\), \\(f(\\frac{1}{iy})\\rightarrow 0\\) as \\(y\\rightarrow 0\\). This is because \\(f(1/z)\\) has singularities at \\(z_n=\\frac{1}{n}\\) which has a limit point at \\(z=0\\) (at infinity for \\(f(z)\\)). </p>"},{"location":"Math/Complex_Analysis/Meromophic/#argument-principle-applications","title":"Argument Principle &amp; Applications","text":"<p>The basic observations are as follows. Note we have the conclusion because of the integral of a holomorphic function over a closed curve vanishes, which demonstrates the infomation of the zeros and poles inside the curve.</p> <p></p> <p>A simple fact: Additivity of Logarithm derivatives</p> <p>Assume \\(f_1,\\cdots,f_N\\) are holomorphic functions, then</p> \\[ \\frac{(f_1f_2)'}{f_1f_2}=\\frac{f_1'f_2+f_1f_2'}{f_1f_2}=\\frac{f_1'}{f_1}+\\frac{f_2'}{f_2} \\] <p>and by induction</p> \\[ \\frac{(\\prod_{k=1}^N f_k)'}{\\prod_{k=1}^N f_k}=\\sum_{k=1}^N \\frac{f_k'}{f_k}. \\] <p></p> <p>Augument Principle</p> <p>Suppose \\(f\\) is holomorphic in an open set containing a circle \\(C\\) and its interior. If \\(f\\) has no poles and never vanishes on \\(C\\) (to let the integral well-defined), denote \\(Z\\) and \\(P\\) to be the number of zeros and poles inside \\(C\\), then </p> \\[ \\frac{1}{2\\pi i}\\int_C \\frac{f'(z)}{f(z)}dz=Z-P. \\] Proof <p>We apply the above fact into an example, a holomorphic function \\(f\\) with a zero of order \\(n\\) at \\(z_0\\), i.e. \\(f(z)=(z-z_0)^n g(z)\\), where \\(g(z)\\) is holomorphic and does not vanish at \\(z_0\\). So </p> \\[ \\frac{f'}{f}=\\frac{[(z-z_0)^n]'}{(z-z_0)^n} + \\frac{g'}{g}=\\frac{n}{z-z_0}+G(z) \\] <p>where \\(G(z)\\) is still holomorphic (maybe vanish at \\(z_0\\)). Similar method applies to a function with a pole of order \\(z\\) at \\(z_0\\), i.e. \\(f(z)=(z-z_0)^{-n} h(z)\\) and we have</p> \\[ \\frac{f'}{f}=\\frac{-n}{z-z_0}+H(z) \\] <p>So silimar as we discussed in integral around a closed curve, what we have is the \\(\\frac{\\pm n}{z-z_0}\\).</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Meromophic/#rouches-theorem","title":"Rouch\u00e9's Theorem","text":"<p>As a consequence, we have the following three theorem shown by Argument Principle.</p> <p>Rouch\u00e9's Theorem: perturbance do not affect the number of zeros</p> <p>Assume \\(f\\) and \\(g\\) are holomorphic functions on an open set containing a circle \\(C\\) and its interior. If </p> \\[ |f(z)|&gt;|g(z)|,\\quad \\forall z\\in C, \\] <p>then \\(f\\) and \\(f+g\\) has the same number of zeros inside the circle \\(C\\).</p> Proof <p>The core is to formulate a continuous function of number of zeros.</p> <p>Define </p> \\[ n_t=\\frac{1}{2\\pi i}\\int_C \\frac{f_t'(z)}{f_t(z)}dz,\\quad t\\in [0,1] \\] <p>where \\(f_t(z)=f(z)+tg(z)\\). For \\(t=0\\), \\(n_0\\) equala the number of zeros of \\(f\\) inside \\(C\\), and for \\(t=1\\), \\(n_1\\) equala the number of zeros of \\(f+g\\) inside \\(C\\). We want to show that it is continuous. Since it is a integer, it must be constant.</p> <p>In fact \\(f_t\\) is jointly continuous for \\(t\\in [0,1]\\) and \\(z\\in C\\). The only consideration is that \\(f_t(z)\\) does not vanish on \\(C\\) for all \\(t\\). This is apparent, because</p> \\[ |f_t(z)|\\geq |f(z)|-|tg(z)|\\geq |f(z)|-|g(z)|&gt;0. \\] <p>and we are done.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Meromophic/#open-mapping-theorem","title":"Open Mapping Theorem","text":"<p>We could intepret a holomorphic function \\(f\\) as an mapping from \\(\\mathbb{C}\\) to \\(\\mathbb{C}\\). A mapping is said to be open if it maps an open set to an open set. So when does a holomorphic function be an open mapping?</p> <p>The following theorem interprets a mapping as a function with zeros.</p> <p>Open Mapping Theorem</p> <p>If \\(f\\) is holomorphic and non-constant in a region \\(\\Omega\\), then \\(f\\) is open.</p> Proof <p>Assume \\(w_0\\in f(\\Omega)\\), and \\(\\exists z_0\\) such that \\(f(z_0)=w_0\\). We need to prove that \\(\\exists \\varepsilon\\), \\(B_\\varepsilon(w_0)\\subset f(\\Omega)\\). Choose \\(w\\in B_\\varepsilon(w_0)\\), we must show that \\(\\exists z\\) such that \\(f(z)=w\\).</p> <p>Define \\(g(z)=f(z)-w\\) and write \\(g(z)=f(z)-w_0+(w_0-w)=F(z)+G(z)\\). If \\(|F(z)|&gt;|G(z)|\\) on a circle \\(\\{z:|z-z_0|=\\delta\\}\\), then \\(g(z)\\) has the same number of zeros as \\(F(z)\\). Since \\(f(z)=w\\) has a zero, then \\(f(z)=w_0\\) must have a zero, so \\(\\exists z\\) such that \\(f(z)=w_0\\).</p> <p>What we need is to find the appropriate \\(\\delta\\). First choose \\(\\delta&gt;0\\), such that \\(C_\\delta(z_0)=\\{z:|z-z_0|=\\delta\\}\\subset \\Omega\\) and \\(f(z)\\neq w_0\\) on the circle (because zeros are isolated by non-constant condition). So this little distinction gives us to find \\(\\varepsilon&gt;0\\), such that \\(|f(z)-w_0|\\geq\\varepsilon\\) on the circle \\(C_\\delta(z_0)\\). So if \\(|w-w_0|&lt;\\varepsilon\\), we have \\(|F(z)|&gt;|G(z)|\\). By Rouch\u00e9's Theorem, we are done.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Complex_Analysis/Meromophic/#maximum-modulus-principle","title":"Maximum modulus principle","text":"<p>This is about the range of function on a region. A holomorphic function cannot attain its maximum in the interior of a region. Actually it is a natural sequence of open mapping theorem.</p> <p>Maximum modulus principle</p> <p>If \\(f\\) is a non-constant holomorphic function in a region \\(\\Omega\\), then \\(f\\) cannot obtain a maximum in \\(\\Omega\\).</p> Proof <p>We argue by contradiction. If \\(f\\) attain its maximum at \\(z_0\\in \\Omega\\), then since \\(f\\) is a open mapping, for \\(D_\\delta(z_0)\\subset \\Omega\\), we have \\(f(D_\\delta(z_0))\\subset f(\\Omega)\\) and it is an open set. So \\(\\exists \\varepsilon&gt;0\\), \\(D_{\\varepsilon}(f(z_0))\\subset f(D_\\delta(z_0))\\), that is, \\(\\exists f(z)\\in D_{\\varepsilon}(f(z_0))\\)</p> \\[ |f(z)-f(z_0)|&gt;\\varepsilon.\\quad \\Rightarrow |f(z)|&gt;|f(z_0)|. \\] <p><p>\\(\\square\\)</p></p> <p>Corollary</p> <p>Assume \\(f\\) is holomorphic in a region \\(\\Omega\\) with a compect closure \\(\\overline{\\Omega}\\), then </p> \\[ \\sup_{z\\in \\Omega}|f(z)|\\leq \\sup_{z\\in \\overline{\\Omega}-\\Omega} |f(z)|. \\]"},{"location":"Math/Complex_Analysis/Preliminary/","title":"Preliminary","text":""},{"location":"Math/Complex_Analysis/Preliminary/#complex-number-plane","title":"Complex Number &amp; Plane","text":""},{"location":"Math/Complex_Analysis/Preliminary/#basic-concepts","title":"Basic Concepts","text":"<p>Assume \\(i^2=-1\\), then \\(\\mathbb{C}:= \\{z: z=x+iy, \\quad x,y\\in \\mathbb{R} \\}\\). From definition we know there exsits a bijection bwtween \\(\\mathbb{C}\\) and \\(\\mathbb{R}^2\\).</p> <p>It is natural to define differential operations like addition and multiplication. Its modulus is consistent with norm-2 in \\(\\mathbb{R}^2\\):</p> \\[ |z|=(x^2+y^2)^{\\frac{1}{2}}. \\] <p>which satisfies three conditions for being a metric. So from functional analysis point of view, it is a metrix space. </p> <p>Now we have some introduction specially for complex number. Define a complex conjugete of \\(z\\) as \\(\\overline{z}=x-iy.\\) Easy to see that </p> \\[ \\begin{equation} \\text{Re}(z)=\\frac{z+\\overline{z}}{2}, \\quad  \\text{Im}(z)=\\frac{z-\\overline{z}}{2i}\\label{variables-x-y-z-zbar} \\end{equation} \\] <p>Since \\(z\\overline{z}=|z|^2\\), for \\(z\\neq 0\\), we have</p> \\[ \\frac{1}{z}=\\frac{\\overline{z}}{|z|^2}. \\] <p>We also have its formula in terms of polar coordinate. That is, </p> \\[ z=re^{i\\theta}, \\] <p>where \\(r=|z|\\) and \\(\\theta\\) is argument of \\(z\\). </p>"},{"location":"Math/Complex_Analysis/Preliminary/#convergence","title":"Convergence","text":"<p>Definition of Convergence</p> <p>Assume sequence \\(\\{z_n\\}_{n\\geq 1}\\subset \\mathbb{C}\\), \\(w\\in \\mathbb{C}\\). \\(\\{z_n\\}_{n\\geq 1}\\) is said to converge to \\(w\\), if  </p> \\[ \\lim_{n\\rightarrow \\infty}|z_n-w|=0. \\] <p>With conclusion from multi-variable functions, we could easily see that \\(\\{z_n\\}_{n\\geq 1}\\) converges to \\(w\\) iff the corresponding sequence of points \\(\\{(x_n,y_n)\\}_{n \\geq 1}\\) in the complex plane converges to the point \\((x_w,y_w)\\) that corresponds to \\(w\\), also iff the sequence of real and imaginary parts of \\(\\{z_n\\}_{n\\geq 1}\\) converge to those of \\(w\\), respectively.</p> <p>Here we simply introduce Cauchy Sewquence, which is almost similar to \\(\\mathbb{R}\\).</p> <p>Cauchy Sequence</p> <p>A sequence of \\(\\{z_n\\}_{n\\geq 1}\\) is said to be a Cauchy Sequence, if </p> \\[ |z_n-z_m|\\rightarrow 0,\\quad n,m\\rightarrow \\infty. \\] <p>Or, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), s.t \\(|z_n-z_m|&lt;\\varepsilon\\) whenever \\(n,m&gt;N\\).</p> <p>Completeness of \\(\\mathbb{C}\\)</p> <p>\\(\\mathbb{C}\\) is complete.</p>"},{"location":"Math/Complex_Analysis/Preliminary/#sets-in-the-complex-plane","title":"Sets in the complex plane","text":"<p>Here most definitions are of the same as in Sets of Real Function, only we have to change \\(\\mathbb{R}^n\\) into \\(\\mathbb{C}\\). </p> <p>Now we introduce some special definitions for complex number.</p> <p>Definitions</p> <p>(i) Assume \\(\\Omega\\in \\mathbb{C}\\). If \\(\\Omega\\) is bounded, we define its diameter by</p> \\[ \\text{diam}(\\Omega)=\\sup_{z,w\\in \\Omega}|z-w|. \\] <p>(ii) An open set \\(\\Omega\\in \\mathbb{C}\\) is said to be connected, if it is not possible to find two disjoint non-empty open sets \\(\\Omega_1\\), \\(\\Omega_2\\) such that </p> \\[ \\Omega=\\Omega_1\\cup\\Omega_2. \\] <p>Or we have another equivalent definition of connectedness: \\(\\forall x,y\\in \\Omega\\), \\(\\exists\\) curve \\(\\gamma\\), its image is completely contained in \\(\\Omega\\).</p> <p>A connected open set in \\(\\mathbb{C}\\) will be called a region. </p>"},{"location":"Math/Complex_Analysis/Preliminary/#functions-on-the-complex-plane","title":"Functions on the complex plane","text":""},{"location":"Math/Complex_Analysis/Preliminary/#basic-concepts_1","title":"Basic Concepts","text":"<p>Similar to \\(\\mathbb{R}\\), we could have definition of continuity for a function defined on \\(\\mathbb{C}\\). And a continuous function defined on compact set could assess its maximum and minimum.</p>"},{"location":"Math/Complex_Analysis/Preliminary/#holomorphic-functions","title":"Holomorphic Functions","text":"<p>Definition of Holomorphic function</p> <p>Assume \\(\\Omega\\subset \\mathbb{C}\\) is an open set, and \\(f\\) is a complex-valued function on \\(\\Omega\\). \\(f\\) is holomorphic (or regular, complex differentiable) at point \\(z_0\\in \\Omega\\), if the quotient</p> \\[ \\begin{align} \\frac{f(z_0+h)-f(z_0)}{h}\\label{derivative-f-complex} \\end{align} \\] <p>converges when \\(h\\rightarrow 0\\). Here notice that \\(h\\in \\mathbb{C}\\). If the above formula has limit, we call it the derivative of \\(f\\) ar \\(z_0\\), denoted as \\(f'(z_0)\\).</p> <p>The function \\(f\\) is said to be holomorphic on \\(\\Omega\\), if \\(f\\) is holomorphic at every point of \\(\\Omega\\). For \\(f\\) defined on a closed set \\(\\Omega'\\), we say \\(f\\) is holomorphic on \\(\\Omega'\\), if f is holomorphic on some open set containing \\(\\Omega'\\). If \\(f\\) is holomorphic on \\(\\mathbb{C}\\), we call \\(f\\) is entire.</p> <p>Notice that definition of holomorphic function is resemble to definition of real differentiable function, but we will see that the former has much stronger properties than the latter.</p> <p>Quotes from textbook</p> <p>A holomorphic function of one complex variable will satisfy much stronger properties than a differentiable function of one real variable. </p> <p>For example, a holomorphic function will actually be infinitely many times complex differentiable, that is, the existence of the first derivative will guarantee the existence of derivatives of any order. This is in contrast with functions of one real variable, since there are differentiable functions that do not have two derivatives. </p> <p>In fact more is true: every holomorphic function is analytic, in the sense that it has a power series expansion near every point (power series will be discussed in the next section), and for this reason we also use the term analytic as a synonym for holomorphic. </p> <p>Again, this is in contrast with the fact that there are indefinitely differentiable functions of one real variable that cannot be expanded in a power series. (See Exercise 23.)</p> <p>Interpretation of the above</p> <p>Assume \\(\\Omega\\) is a region in \\(\\mathbb{C}\\), use \\(C(\\Omega)\\) to denote the whole continuous function on \\(\\Omega\\), and \\(H(\\Omega)\\) to denote the whole holomorphic function on \\(\\Omega\\), then \\(H(\\Omega)\\subset C(\\Omega)\\).</p> <p>In the following chapter, we will have more stronger theorems. If we use \\(C^1(\\Omega)\\) to denote the whole function whose \\(f_x, f_y\\) are continous, \\(C^k(\\Omega)\\) to denote the whole function whose \\(f^{(k)}_x,f^{(k)}_y\\) are continuous, then we have</p> \\[ H(\\Omega)\\subset C^{\\infty}(\\Omega)\\subset C^k(\\Omega)\\subset C^1(\\Omega). \\] <p>Example. </p> <p>(i) Function \\(f(z)=z\\) is holomorphic on any open set in \\(\\mathbb{C}\\) with \\(f'(z)=1\\). </p> <p>(ii) Any polynomial function</p> \\[ p(z)=a_0+a_1z+\\cdots+a_nz^n \\] <p>is holomorphic in the entire complex plane with </p> \\[ p'(z)=a_1+2a_2z^2+\\cdots+na_nz^{n-1}. \\] <p>(iii) Function \\(f(z)=\\frac{1}{z}\\) is holomorphic on \\(\\mathbb{C}-\\{0\\}\\) with \\(f'(z)=-\\frac{1}{z^2}\\).</p> <p>(iv) Function \\(f(z)=\\overline{z}\\) is not holomorphic, since its quotient</p> \\[ \\frac{f(z_0+h)-f(z_0)}{h}=\\frac{\\overline{h}}{h} \\] <p>differ when \\(h\\rightarrow 0\\) from different rays.</p> <p>Similar to Taylor's expansion in \\(\\mathbb{R}\\), we have</p> \\[ f(z_0+h)=f(z_0)+f'(z_0)h+o(h). \\] <p>Properties of holomorphic function</p> <p>If \\(f,g\\) are holomorphic in \\(\\Omega\\), then</p> <p>(i) \\(f+g\\) is holomorphic in \\(\\Omega\\), and \\((f+g)'=f'+g'\\).</p> <p>(ii) \\(fg\\) is holomorphic in \\(\\Omega\\), and \\((fg)'=f'g+fg'\\).</p> <p>(iii) If \\(g(z_0)\\neq 0\\), then \\(f/g\\) is holomorphic at \\(z_0\\), and </p> \\[ (f/g)'=\\frac{f'g-fg'}{g^2}. \\] <p>(iv) Chain rule. If \\(f:\\Omega\\rightarrow U\\) and \\(g:U\\rightarrow \\mathbb{C}\\), then</p> \\[ (g\\circ f)'(z)=g'(f(z))\\cdot f'(z),\\quad \\forall z\\in \\Omega. \\]"},{"location":"Math/Complex_Analysis/Preliminary/#relationship-between-real-imaginary-parts","title":"Relationship between real &amp; imaginary parts","text":"<p>To distinguish, we have the follwoing definition of the so-called differentiability.</p> <p>Definitions of differentiability</p> <p>(i) Differentiability of real function. Assume \\(f(z)=u(x,y)+iv(x,y)\\) is defined on \\(\\Omega\\), \\(z_0=x_0+iy_0\\in \\Omega\\). \\(f\\) is said to be real differentiable, if binary real functions \\(u(x,y), v(x,y)\\) is differentiable at \\((x_0,y_0)\\).</p> <p>Recall that a binary function \\(f\\) is differentiable at \\((x_0,y_0)\\), could implies \\(f\\) if continuous at \\((x_0,y_0)\\) and has partial derivatives from all directions.</p> <p>(ii) Differantiability of Vector function \\(\\pmb{f}: \\mathbb{R}^n\\rightarrow \\mathbb{R}^m\\) is said to be differentiable at \\(\\pmb{x}_0\\), if there exists matrix (or linear transformation) \\(\\pmb{A}_{m\\times n}(\\pmb{x_0})\\) independent of \\(\\Delta \\pmb{x}\\), such that</p> \\[ \\Delta\\pmb{y}=\\pmb{f}(\\pmb{x}+\\Delta\\pmb{x})-\\pmb{f}(\\pmb{x})=\\pmb{A}\\Delta \\pmb{x}+o(\\Delta\\pmb{x}). \\] <p>Here \\(\\pmb{A}\\) is denoted as Jacobi matrix \\(J\\).</p> <p>(iii) Differentiability of complex function \\(f\\). We could use from a vector function \\(\\mathbb{R}^2\\rightarrow \\mathbb{R}^2\\). That is, there exsits a complex number \\(w\\), such that</p> \\[ f(z_0+h)=f(z_0)+wh+o(h), \\quad h\\in \\mathbb{C}. \\] <p>Compared to Jacobi matrix with \\(\\pmb{f}=(u,v)\\) expressed by </p> \\[ J=\\left[\\begin{array}{cc} \\frac{\\partial u}{\\partial x}&amp; \\frac{\\partial u}{\\partial y}\\\\ \\frac{\\partial v}{\\partial x}&amp; \\frac{\\partial v}{\\partial y} \\end{array}\\right], \\] <p>how to connect \\(w=f'(z_0)\\in \\mathbb{C}\\) and matrix \\(J\\in \\mathbb{R}^{2\\times 2}\\)? We consider two special derivatives, real and pure imaginary ones.</p> <p>Holomorphic implies Cauchy-Riemann Equations</p> <p>In definition of \\(f'(z_0)\\), we let \\(h=h_1+ih_2\\), where \\(h_1,h_2\\in \\mathbb{R}\\), in order to change the dimension of denominator of formulation \\(\\ref{derivative-f-complex}\\).</p> <p>(i) \\(h_2=0\\), then \\(h=h_1\\in \\mathbb{R}\\), this is just a real derivative of complex function \\(f\\), i.e. </p> \\[ \\begin{equation} f'(z_0)=\\frac{\\partial f}{\\partial x}=\\frac{\\partial u}{\\partial x}+i\\frac{\\partial v}{\\partial x}\\label{x-partial} \\end{equation} \\] <p>is still a complex number.</p> <p>(ii) \\(h_1=0\\), then \\(h=ih_2\\) is a pure imaginary number. So this gives </p> \\[ \\begin{align} f'(z_0)=\\frac{1}{i}\\frac{\\partial f}{\\partial y}=-i\\frac{\\partial u}{\\partial y}+\\frac{\\partial v}{\\partial y}\\label{y-partial} \\end{align} \\] <p>is also still a complex number.</p> <p>Combine (i) and (ii) and let real and imaginary parts equals, we have</p> \\[ \\begin{align} \\frac{\\partial u}{\\partial x}=\\frac{\\partial v}{\\partial y},\\quad \\frac{\\partial u}{\\partial y}=-\\frac{\\partial v}{\\partial y}\\label{Cauchy-Riemann} \\end{align} \\] <p>which is called Cauchy-Riemann Equations.</p> <p>Using another two variables \\(z\\) and \\(\\overline{z}\\)</p> <p>Recall from formulation \\(\\ref{variables-x-y-z-zbar}\\), we could express \\(x,y\\) using \\(z,\\overline{z}\\). Then we could deduce the relationship of derivatives between \\(x,y\\) and \\(x,\\overline{z}\\).</p> <p>Notice that </p> \\[ f(x,y)=f\\left(\\frac{z+\\overline{z}}{2}, -i\\frac{z-\\overline{z}}{2}\\right), \\] <p>so </p> \\[ \\begin{align*} \\frac{\\partial f}{\\partial z}=\\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial z}+\\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial z}=\\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x}-i\\frac{\\partial f}{\\partial y}\\right),\\\\ \\frac{\\partial f}{\\partial \\overline{z}}=\\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial \\overline{z}}+\\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial \\overline{z}}=\\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x}+i\\frac{\\partial f}{\\partial y}\\right). \\end{align*} \\] <p>So we could define operator for \\(z\\) and \\(\\overline{z}\\)</p> \\[ \\frac{\\partial }{\\partial z}=\\frac{1}{2}\\left(\\frac{\\partial }{\\partial x}-i\\frac{\\partial }{\\partial y}\\right),\\quad \\frac{\\partial }{\\partial \\overline{z}}=\\left(\\frac{\\partial }{\\partial x}+i\\frac{\\partial }{\\partial y}\\right). \\] <p>The more specific relationship could be expressed in the following theorem.</p> <p>Extention of derivatives: Complex differential expressed by extended partial derivatives</p> <p>Assume \\(f\\) is holomorphic at \\(z_0\\), then </p> \\[ \\frac{\\partial f}{\\partial \\overline{z}}\\Bigg|_{z=z_0}=0 \\] <p>and </p> \\[ f'(z_0)=\\frac{\\partial f}{\\partial z}(z_0)=2\\frac{\\partial u}{\\partial z}(z_0). \\] <p>and for the corresponding vector function \\(\\pmb{F}: \\mathbb{R}^2\\rightarrow \\mathbb{R}^2\\) of complex function \\(f\\), \\(\\pmb{F}\\) is differentiable, and for \\((x_0,y_0)\\in \\mathbb{R}^2\\) and \\(z_0=x_0+iy_0\\in \\mathbb{C}\\),</p> \\[ \\text{det}J_{F}(x_0,y_0)=|f'(z_0)|^2. \\] Proof <p>By definition of differential operator,</p> \\[ \\begin{align*} \\frac{\\partial f}{\\partial \\overline{z}}\\Bigg|_{z=z_0}&amp;= \\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x} + i\\frac{\\partial f}{\\partial y} \\right)\\\\ &amp;=\\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x} -\\frac{1}{i}\\frac{\\partial f}{\\partial y} \\right)\\\\ &amp;=\\frac{1}{2}\\left(\\frac{\\partial u}{\\partial x}+i\\frac{\\partial v}{\\partial x} +i\\frac{\\partial u}{\\partial y}-\\frac{\\partial v}{\\partial y}\\right)\\\\ &amp;=0. \\end{align*} \\] <p>where the third \"\\(=\\)\" holds by equation \\(\\ref{x-partial}\\) and \\(\\ref{y-partial}\\), the forth \"\\(=\\)\" holds by Cauchy-Riemann equations \\(\\ref{Cauchy-Riemann}\\).</p> <p>Also using equation \\(\\ref{x-partial}\\) and \\(\\ref{y-partial}\\), we have an average expresstion</p> \\[ \\begin{align*} f'(z_0)&amp;=\\frac{1}{2}\\cdot 2f'(z_0)\\\\ &amp;=\\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x}+\\frac{1}{i}\\frac{\\partial f}{\\partial y}\\right)\\\\ &amp;=\\frac{1}{2}\\left(\\frac{\\partial f}{\\partial x}-i\\frac{\\partial f}{\\partial y}\\right)\\\\ &amp;=\\frac{\\partial f}{\\partial z}. \\end{align*} \\] <p>where the second \"\\(=\\)\" holds by equation \\(\\ref{x-partial}\\) and \\(\\ref{y-partial}\\), the forth \"\\(=\\)\" holds by its definition.</p> <p>and</p> \\[ \\begin{align*} \\text{det}J_{F}(x_0,y_0)&amp;=\\left|\\begin{array}{cc} \\frac{\\partial u}{\\partial x}&amp; \\frac{\\partial u}{\\partial y}\\\\ \\frac{\\partial v}{\\partial x}&amp; \\frac{\\partial v}{\\partial y} \\end{array}\\right|\\\\ &amp;=\\frac{\\partial u}{\\partial x}\\frac{\\partial v}{\\partial y}-\\frac{\\partial u}{\\partial y}\\frac{\\partial v}{\\partial x}\\\\ &amp;=\\left(\\frac{\\partial u}{\\partial x}\\right)^2+\\left(\\frac{\\partial v}{\\partial x}\\right)^2\\\\ &amp;=\\left|2\\frac{\\partial u}{\\partial x}\\right|^2=|f'(z_0)|^2 \\end{align*} \\] <p>where the last \"\\(=\\)\" holds by equation \\(\\ref{x-partial}\\).</p> <p><p>\\(\\square\\)</p></p> <p>If we consider reversibly, we could also use Cauchy-Riemann equations to deduce holomorphism of \\(f\\).</p> <p>Reverse version</p> <p>Suppose \\(f=u+iv\\) is a complex-valued function defined on an open set \\(\\Omega\\). If \\(u\\) and \\(v\\) are continuously differentiable, and satisfies Cauchy-Riemann equations on \\(\\Omega\\), then \\(f\\) is holomorphic on \\(\\Omega\\), and \\(f'(z)=\\frac{\\partial f}{\\partial z}\\).</p> Proof <p>By differential of rael functions, we have</p> \\[ \\begin{align*} u(x+h_1,y+h_2)-u(x,y)&amp;=u'_x h_1+u'_y h_2+o\\left(\\sqrt{h_1^2+h_2^2}\\right)\\\\ v(x+h_1,y+h_2)-v(x,y)&amp;=v'_x h_1+v'_y h_2+o\\left(\\sqrt{h_1^2+h_2^2}\\right). \\end{align*} \\] <p>where \\(z=x+iy\\), \\(h=h_1+ih_2\\). So</p> \\[ \\begin{align*} f(z+h)-f(z)&amp;=u(x+h_1,y+h_2)-u(x,y)+i[v(x+h_1,y+h_2)-v(x,y)]\\\\ &amp;=u'_x h_1+u'_y h_2+o\\left(\\sqrt{h_1^2+h_2^2}\\right)+i\\left[v'_x h_1+v'_y h_2+o\\left(\\sqrt{h_1^2+h_2^2}\\right)\\right]\\\\ &amp;=(u'_x+iv'_x )h_1 + (u'_y+iv'_y) h_2+o\\left(\\sqrt{h_1^2+h_2^2}\\right)\\\\ &amp;=(u'_x+iv'_x )h_1 + (-v'_x+iu'_x) h_2+o\\left(\\sqrt{h_1^2+h_2^2}\\right)\\\\ &amp;=(u'_x+iv'_x )h_1 + (i^2v'_x+iu'_x) h_2+o\\left(\\sqrt{h_1^2+h_2^2}\\right)\\\\ &amp;=(u'_x+iv'_x )(h_1 +i h_2)+o\\left(\\sqrt{h_1^2+h_2^2}\\right)\\\\ &amp;=f'(z)h+o\\left(|h|\\right) \\end{align*} \\] <p>which means \\(f\\) is holomorphic.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Differential_Geometry/","title":"Differential Geometry","text":"<p>Reference</p> <ul> <li> <p>Differential Geometry on curves and surfaces, Manfredo P.do Carmo</p> </li> <li> <p>\u6574\u4f53\u5fae\u5206\u51e0\u4f55\u521d\u6b65, \u6c88\u4e00\u5175</p> </li> </ul>"},{"location":"Math/Differential_Geometry/#curves","title":"Curves","text":""},{"location":"Math/Differential_Geometry/#surfaces","title":"Surfaces","text":""},{"location":"Math/Differential_Geometry/#gauss-map","title":"Gauss map","text":""},{"location":"Math/Differential_Geometry/Curves/","title":"Curves","text":""},{"location":"Math/Differential_Geometry/Curves/#parametrized-curves","title":"Parametrized curves","text":"<p>Definition of differentiable curve</p> <p>A parametrized differentiable curve is an infinitely differentiable map \\(\\alpha: I\\rightarrow \\mathbb{R}^3\\) of an open interval into \\(\\mathbb{R}^3\\).</p> <p>The variable \\(t\\) is called the parameter of the curve. We do not exclude \\(a=-\\infty\\) and \\(b=\\infty\\).</p> <p>The vector</p> \\[ \\alpha'(t)=(x'(t), y'(t), z'(t))\\in\\mathbb{R}^3 \\] <p>is called the tangent vector of the curve \\(\\alpha\\) at \\(t\\).</p> <p>For the study of the differential geometry of a curve, it is essential to assume that there exsits such a tangent lint at every point.</p> <p>Singular point, regular curve</p> <p>(i) A point \\(t\\in I\\) is called a singular point of \\(\\alpha\\), if \\(\\alpha'(t)=0\\).</p> <p>(ii) A parametrized differentiable curve \\(\\alpha:I\\rightarrow \\mathbb{R}^3\\) is said to be regular if \\(\\alpha'(t)\\neq 0\\) for all \\(t\\in I\\).</p> <p>A parameter called arc length is usually useful in further analysis.</p> <p>Definition of Arc length</p> <p>Given \\(t_0\\in I\\), the arc length of a regular parametrized curve \\(\\alpha:I\\rightarrow \\mathbb{R}^3\\) from the point \\(t_0\\) is defined by</p> \\[ s=\\int_{t_0}^t |\\alpha'(\\tau)|d\\tau, \\] <p>where \\(|\\alpha'(t)|=\\sqrt{x'(t)^2+y'(t)^2+z'(t)^2}\\).</p> <p>Since \\(|\\alpha'(t)|&gt;0\\) for regular curve.</p> <p>Now we talk about some invariance under reparametrization.</p>"},{"location":"Math/Differential_Geometry/Curves/#vector-product-on-e3","title":"Vector product on E3","text":"<p>Equivalence: orientation</p> <p>Suppose \\(\\{e_n\\}\\), \\(\\{f_n\\}\\) are two basis of \\(n\\)-dimensional space. They are said to have the same orientation, denoted by \\(e\\sim f\\), if the matrix of change of basis has positive determinant.</p> <p>Easy to show that orientation satisfies the equivalent relationship.</p> <p>The vector product of \\(u\\) and \\(v\\) (in that order) is the unique vector \\(u\\times v\\in \\mathbb{R}^3\\) such that</p> \\[ (u\\times v)\\cdot w=det(u,v,w),\\quad \\forall w\\in\\mathbb{R}^3. \\] <p>Easy to show that \\((u\\times v)\\cdot u=0\\) and \\((u\\times v)\\cdot v=0\\), so \\(u\\times v\\neq 0\\) is orthogonal to a plane generated by \\(u\\) and \\(v\\). To give a geometric interpretation of its norm and its direction, we proceed as follows.</p> <p>Deduction</p> <p>(i) Observe that \\((u\\times v)\\cdot (u\\times v)=|u\\times v|^2&gt;0\\), so the determinant of \\((u,v,u\\times v)\\) is positive and it could be a basis.</p> <p>(ii) Prove that</p> \\[ (u\\times v)\\cdot(x\\times y)=\\left|\\begin{array}{cc}u\\cdot x&amp; v\\cdot x\\\\u\\cdot y&amp; v\\cdot y\\end{array}\\right| \\] <p>where \\(u,v,x,y\\) are arbitrary vectors. Check for basis.</p> <p>and show that </p> \\[ |u\\times v|^2=|u|^2|v|^2(1-\\cos ^2\\theta)=A^2. \\] <p>(iii) The vector product is not associative. Because </p> \\[ (u\\times v)\\times w = (u\\cdot w)v - (v\\cdot w)u, \\] <p>and check for all basis.</p>"},{"location":"Math/Differential_Geometry/Curves/#the-local-theory-of-curves","title":"The local theory of curves","text":""},{"location":"Math/Differential_Geometry/Curves/#curvature","title":"Curvature","text":"<p>Let \\(s\\) be the arc length, and \\(\\alpha\\) be parametrized by \\(s\\). \\(\\alpha'(s)\\) has unit length, and \\(|\\alpha''(s)|\\) measures how rapid the curve pulls away from the tangent line \\(\\alpha'(s)\\), in the neighborhood of \\(s\\).</p> <p>Curvature of a curve</p> <p>Let \\(\\alpha:I\\rightarrow \\mathbb{E}^3\\) be a curve parametrized be arc length \\(s\\in I\\). The number \\(\\kappa(s):=|\\alpha''(s)|\\) is called the curvature of \\(\\alpha\\) at \\(s\\).</p> <p>Easy to show that for straight line, \\(\\alpha=us+v\\), if and only if \\(k(s)=0\\). If we have \\(k(s)\\neq 0\\) unless for \\(s=s_0\\), we could still find its curvature by leveraging the limit. If for its neighborhood, \\(k(s)=0\\), then it is a straight line.</p> <p>When change the direction, we have tangent vector changes but the curvature does not. This is because, let \\(\\beta(s)=\\alpha(-s)\\), then </p> \\[ \\frac{d\\beta(s)}{ds}=\\frac{d\\alpha(-s)}{ds}=(-1)\\frac{d\\alpha(-s)}{(-s)}. \\] <p>If \\(k(s)\\neq 0\\), then we have a unit normal vector \\(n\\) well defined by \\(\\alpha''(s)=k(s)n(s)\\). The plane composed by \\(t\\) and \\(n\\) are called the osculating plane at \\(s\\).</p> <p>Example. For \\(k(s)=0\\), check the following example.</p> \\[ \\alpha(t)=\\begin{cases}(t,0,e^{-1/t^2}), \\quad &amp;t&gt;0,\\\\ (0,0,0),\\quad &amp;t=0,\\\\ (t,e^{-1/t^2},0), \\quad &amp;t&lt;0.\\\\\\end{cases} \\] <p>To proceed with the local analysis of curves, we assume \\(\\alpha''(s)\\neq 0\\) (the singular point of order \\(1\\), and \\(\\alpha'(s)=0\\) is called the singular point of order \\(0\\)).</p> <p>For a plane curve, we have the following description.</p> <p>Example. Assume that \\(\\alpha(I) \\subset \\mathbb{E}^2\\) and give \\(k\\) a sign as in the text. </p> <p>Transport the vectors \\(t(s)\\) parallel to themselves in such a way that the origins of \\(t(s)\\) agree with the origin of \\(\\mathbb{E}^2\\); the end points of \\(t(s)\\) then describe a parametrized curve \\(s \\rightarrow t(s)\\) called the indicatrix of tangents of \\(\\alpha\\). </p> <p>Let \\(\\theta(s)\\) be the angle from \\(e_1\\) to \\(t(s)\\) in the orientation of \\(\\mathbb{E}^2\\). notice that we are assuming that \\(k \\neq 0\\). Show that</p> <p>(a) The indicatrix of tangents is a regular parametrized curve.</p> <p>(b) \\(dt/ds = (d\\theta/ds)n\\), that is, \\(k = d\\theta/ds\\).</p> Proof <p>(a) Easy to see since \\(|t'(s)|=|k(s)|\\neq 0\\).</p> <p>(b) Let </p> \\[ t(s)=[\\cos \\theta(s), \\sin \\theta(s)], \\] <p>then </p> \\[ t'(s)=[-\\sin \\theta(s), \\cos\\theta(s) ]\\theta'(s)=\\theta'(s)n, \\] <p>since we have \\(n = j t\\) (rotation by \\(\\pi/2\\)).</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Differential_Geometry/Curves/#tortion","title":"Tortion","text":"<p>Already we have \\(t'=kn\\). Now we do not check \\(n'\\), but check \\(b(s):=t(s)\\times n(s)\\), which is called binormal vector at \\(s\\), also a unit vector representing the osculating plane. \\(|b'(s)|\\) meausres the rate of change of the neighboring osculating plane. We claim that \\(b'(s)\\) is parallel with \\(n(s)\\). Indeed,</p> \\[ b' = t' \\times n + t\\times n'=t\\times n'. \\] <p>The focus is not \\(n'\\), but \\(t\\perp b'\\). With \\(b'\\perp b\\), we have the result. </p> <p>Extract the number out and define \\(b'(s)=\\tau(s)n(s)\\) to be the following concept.</p> <p>Definition of tortion</p> <p>Let \\(\\alpha:I\\rightarrow \\mathbb{E}^3\\) be a curve parametrized be arc length \\(s\\in I\\) such that \\(\\alpha''(s)\\neq 0\\). The number \\(\\tau(s):=|b'(s)|\\) is called the tortion of \\(\\alpha\\) at \\(s\\).</p> <p>Also, \\(\\alpha\\) is a plane curve, if and only if \\(|b'(s)|\\equiv 0\\). Necessarily speaking, it is easy. On the other hand, we shall show that \\(b(s):=b_0\\), take a inner product \\(\\alpha(s)\\cdot b_0\\) and check it is also a constant, which is exactly the parametrized form of plane.</p> <p>Tortion could be either positive or negative.</p>"},{"location":"Math/Differential_Geometry/Curves/#frenet-trihedron","title":"Frenet trihedron","text":"<p>We have associated three orthonomal unit vector \\(t(s), n(s), b(s)\\), which is referred to as the Frenet trihedron at \\(s\\). And we already known that \\(t'=kn\\), and \\(b'=\\tau n\\), for \\(n=b\\times t\\) we </p> \\[ n'=b'\\times t + b\\times t'=-\\tau b -k t. \\] <p>We call the above three equations the Frenet formulas.</p> <p>Now we give the core theorem of this chapter.</p> <p>Fundamental theorem of the local theory of curves</p> <p>Given differentiable functions \\(k(s)&gt;0\\) and \\(\\tau(s)\\), \\(s\\in I\\), there exsits a regular parametrized curve \\(\\alpha: I\\rightarrow \\mathbb{E}^3\\) such that \\(s\\) is the arc length, \\(k(s)\\) is the curvature, and \\(\\tau(s)\\) is the tortion of \\(\\alpha\\).</p> <p>Moreover, any other curve \\(\\overline{\\alpha}\\), which satisfies the same condition, differs from \\(\\alpha\\) by a rigid motion. That is, there exists a orthonormal map \\(\\rho\\) with positive determinant, and a translation vector \\(c\\), such that \\(\\overline{\\alpha}=\\rho \\circ \\alpha +c\\).</p> Proof for uniquenessProof for existence <p>This is by ODEs theory.</p> <p>For plane curve, we could have a simpler version of the above theorem.</p> <p>Example. Given a function \\(k(s)\\), show that the parametrized plane curve have \\(k\\) as curvature is given by </p> \\[ \\alpha(s)=\\left( \\int \\cos \\theta(s)ds +a, \\int \\sin \\theta(s)ds +b\\right) \\] <p>where \\(\\theta(s)=\\int k(s)ds + \\varphi\\). The curve is determined up to a translation of the vector \\((a,b)\\) and a rotation of the angle \\(\\varphi\\).</p>"},{"location":"Math/Differential_Geometry/Curves/#calculations","title":"Calculations","text":"<p>For general regular parametrized curve \\(\\alpha(t)\\), we have the following formula for calculating the geometric variables.</p> <p>Calculations of curvature</p> <p>(i) generally speaking, we have</p> \\[ \\kappa(t) = \\frac{|\\alpha'(t)\\times \\alpha''(t)|}{|\\alpha'(t)|^3}. \\] <p>(ii) for plane curve \\(\\alpha'(t)=[x(t), y(t)]\\), we have signed curvature</p> \\[ \\kappa(t) = \\frac{x'y''-y'x''}{|x'^2+y'^2|^{3/2}}. \\] <p>Calculation of tortion</p> <p>Generally speaking, we have</p> \\[ \\tau(t)=-\\frac{(\\alpha'(t), \\alpha''(t), \\alpha'''(t))}{|\\alpha'(t)\\times \\alpha''(t)|^2}. \\] Proof"},{"location":"Math/Differential_Geometry/Curves/#the-local-canonical-form","title":"The local canonical form","text":"<p>Let \\(\\alpha:I\\rightarrow\\mathbb{E}^3\\) be a curve parametrized by arc length without singular points of order \\(1\\). We now consider the equations in a neighborhood of \\(s_0\\) using the trihedron \\(t(s_0)\\), \\(n(s_0)\\), \\(b(s_0)\\) as a basis for \\(\\mathbb{E}^3\\). We may assume without loss of generality, that \\(s_0=0\\), and consider Taylor expansion </p> \\[ \\alpha(s)-\\alpha(0)=\\alpha'(0)s + \\frac{1}{2}\\alpha''(0)s^2 + \\frac{1}{6}\\alpha'''(0)s^3 + R(s), \\] <p>where \\(R(s)/s^3\\rightarrow \\pmb{0}\\) as \\(s\\rightarrow 0\\). Using \\(\\alpha'(0)=t\\), \\(\\alpha''(0)=kn\\), and \\(\\alpha'''(0)=(kn)'=k'n-k^2t-k\\tau b\\), we rewrite the above equation sorted by \\(t,n,b\\)</p> \\[ \\alpha(s)-\\alpha(0)=\\left(s-\\frac{1}{6}k^2s^3\\right)t + \\left(\\frac{1}{2}ks^2-\\frac{1}{6}k's^3\\right)n -\\frac{1}{6}k\\tau s^3 b+ R(s), \\] <p>with</p> \\[ \\begin{cases} x(s)&amp;=s-\\frac{1}{6}k^2s^3 + R_x\\\\ y(s)&amp;=\\frac{1}{2}ks^2-\\frac{1}{6}k's^3 + R_y\\\\ z(s)&amp;=-\\frac{1}{6}k\\tau s^3 + R_z \\end{cases} \\] <p>which is called the local canonical form of \\(\\alpha\\).</p>"},{"location":"Math/Differential_Geometry/Curves/#classical-form-of-curves","title":"Classical form of curves","text":"<p>Example. Given the parametrized curve </p> \\[ \\alpha(s)=\\left(a\\cos \\frac{s}{c}, a\\sin \\frac{s}{c}, b\\frac{s}{c}\\right) \\] <p>where \\(c^2=a^2+b^2\\). Show that</p> <p>(a) \\(s\\) is the arc length. (i.e. \\(|\\alpha'(s)|=1\\)).</p> <p>(b) </p> \\[ \\kappa(s)=\\frac{|a|}{c^2},\\quad \\tau(s)=\\frac{b}{c^2}. \\] <p>Example. A curve \\(\\alpha\\) is called a helix if the tangent line of \\(\\alpha\\) make a constant angle with a fixed direction. Assume \\(\\tau(s)\\neq 0\\), \\(s\\in I\\), show the following statements are equivalent:</p> <p>(i) \\(\\alpha\\) is a helix,</p> <p>(ii) \\(\\kappa/\\tau=const\\).</p> <p>(iii) the lines containing \\(n(s)\\) and passing \\(\\alpha(s)\\) are parallel to a fixed plane.</p> <p>(iv) the lines containing \\(b(s)\\) and passing \\(\\alpha(s)\\) make a constant angle with a fixed direction.</p>"},{"location":"Math/Differential_Geometry/Gauss_map/","title":"Gauss map","text":"<p>Given a parametrization \\(\\pmb{x}: U\\subset\\mathbb{E}^2\\rightarrow S\\) of a regular surface at a point \\(p\\in S\\), we could choose a unit normal vector at each point of \\(\\pmb{x}(U)\\) by</p> \\[ N(p)= \\frac{\\pmb{x}_u\\times \\pmb{x}_v}{|\\pmb{x}_u||\\pmb{x}_v|} (p), \\quad p\\in \\pmb{x}(U). \\] <p>Thus we have a differentiable map \\(N: U\\subset \\mathbb{E}^2\\rightarrow \\mathbb{R}^3\\). More generally, if \\(V\\in S\\) is an open set in \\(S\\) and \\(N: V\\rightarrow \\mathbb{R}^3\\) is a differentiable map which associates to each \\(p\\in V\\) a unit normal vector at \\(p\\), we say that \\(N\\) is a differentiable field of unit normal vectors on \\(V\\).</p> <p>Not all surfaces admit a differentiable field of unit vectors defined on the whole surface. For instance, the Mobius strip.</p> <p>definition of Gauss map</p> <p>Let \\(S\\subset \\mathbb{E}^3\\) be a regular surface with an orientation \\(N\\). The map \\(N: S\\rightarrow \\mathbb{R}^3\\) takes its values at the unit sphere</p> \\[ S^2 =\\{(x,y,z): x^2+y^2+z^2=1\\} \\] <p>thus \\(N:S\\rightarrow S^2\\) is called the Gauss map of \\(S\\).</p> <p>Gauss map is differentiable. The differential \\(dN_p\\) of \\(N\\) at \\(p\\in S\\) is a linear map from \\(T_p(S)\\) to \\(T_{N(p)}(S^2)\\). Since the two are the same space, \\(dN_p\\) could be looked upon as a linear map on \\(T_p(S)\\).</p> <p>Example. check the differential of \\(N\\) of each surfaces.</p> <p>(i) Plane. Norm vector is a constant, so \\(dN_p\\equiv 0\\).</p> <p>(ii) Unit Sphere. Norm vector \\(N=(x,y,z)\\) and \\(dN_p (v)=v\\).</p> <p>(iii) Cylender, i.e. \\(x^2+y^2=1\\). Norm vector \\(N=(x,y,0)\\), and </p> \\[ dN_p(v)=\\begin{cases}\\theta, \\quad v=(0,0,z)\\\\ v,\\quad v = (x,y,0). \\end{cases} \\] Proof <p>Considering a curve in the surface.</p> <p>The following is a fact about the differential of Gauss map.</p> <p>Self-adjoint map of the differential map of Guass map</p> <p>The differential \\(d N_p: T_p (S)\\rightarrow T_{p}(S)\\) of the Guass map at point \\(p\\in S\\) is a self-adjoint linear map. </p> Proof <p>We shall show that \\(\\langle dN_p (w_1), w_2\\rangle = \\langle w_1, dN_p (w_2)\\rangle\\).</p> <p>Now we assume \\(\\pmb{x}(u,v)\\) be a parametrization of \\(S\\) at \\(p\\), \\(\\{\\pmb{x}_u, \\pmb{x}_v\\}\\) is the associated basis of \\(T_p(S)\\). If \\(\\alpha (t)=\\pmb{x}(u(t), v(t))\\) is a curve in \\(S\\), with \\(\\alpha (0)=p\\), then </p> \\[ dN_p (\\alpha'(0))= N_u u'(0) + N_v v'(0). \\] <p>where \\(N_u=dN_p(\\pmb{x}_u)\\) for \\(u\\) line, and \\(N_v=dN_p (\\pmb{x}_v)\\) for \\(v\\) line. So we only need to show that \\(\\langle dN_p (\\pmb{x}_u), \\pmb{x}_v\\rangle = \\langle \\pmb{x}_u, dN_p (\\pmb{x}_v)\\rangle\\).</p> <p>Notice that \\(\\langle N, \\pmb{x}_u \\rangle =0\\), so taking its derivatives w.r.t \\(v\\) gives </p> \\[ \\langle N_v, \\pmb{x}_u\\rangle + \\langle N, \\pmb{x}_{uv}\\rangle = 0. \\] <p>taking derivatives of \\(\\langle N, \\pmb{x}_v \\rangle =0\\) w.r.t \\(u\\) gives \\(\\langle N_u, \\pmb{x}_v\\rangle + \\langle N, \\pmb{x}_{vu}\\rangle = 0\\).</p> <p>And we are done.</p> <p><p>\\(\\square\\)</p></p> <p>Given the above fact, we could associate to \\(dN_p\\) a quadratic form \\(Q\\) in \\(T_p(S)\\), namely \\(Q(v)=\\langle dN_p(v), v \\rangle\\) (according to bilinear form \\(B(v,w)=\\langle dN_p(v), w\\rangle\\) and \\(Q(v)=B(v,v)\\)).</p> <p>Second fundamental form</p> <p>The quadratic form \\(\u2161_p\\), defined in \\(T_p(S)\\) by </p> \\[ \u2161_p (v)=-\\langle dN_p(v), v\\rangle \\] <p>is called the second fundamental form of \\(S\\) at \\(p\\).</p> <p>We give a geometric interpretation of the above second fundamental form using the normal curvature.</p> <p>Definition of Normal curvature</p> <p>Let \\(C\\) be a regular curve in \\(S\\) passing through \\(p\\in S\\), \\(k\\) the curvature of \\(C\\) at \\(p\\), with \\(\\cos \\theta=\\langle n, N\\rangle\\) where \\(n\\) is the normal vector to \\(C\\) and \\(N\\) is the normal vector to \\(S\\) at \\(p\\). Then the number \\(k_n=k\\cos \\theta\\) is called the normal curvature of \\(C\\subset S\\) at \\(p\\).</p> <p>Meusnier: geometric interpretation of the second fundamental form</p> <p>All curves lying on \\(S\\) and having at a given point \\(p\\in S\\) the same tangent vector share the same normal curvature.</p> <p>The above proposition allows us to speak of the normal curvature along a given direction at \\(p\\).</p>"},{"location":"Math/Differential_Geometry/Surfaces/","title":"Surfaces","text":""},{"location":"Math/Differential_Geometry/Surfaces/#regular-surfaces","title":"Regular surfaces","text":"<p>Before stepping into the definition of regular curves, we have to define a differential of a map.</p> <p></p> <p>Definition of a differential of a map</p> <p>Let \\(F: U\\subset \\mathbb{E}^n\\rightarrow \\mathbb{E}^m\\) be a differentiable map, i.e. each component function has continuous partial derivatives w.r.t each variable. \\(p\\in U\\). A linear map \\(dF_p: \\mathbb{E}^n\\rightarrow \\mathbb{E}^m\\) is called the differential of \\(F\\) at \\(p\\), and is defined as follows.</p> <p><p> </p></p> <p>Let \\(w\\in \\mathbb{E}^n\\) and \\(\\alpha: (-\\varepsilon, +\\varepsilon)\\rightarrow U\\) is a differentiable curve such that \\(\\alpha(0)=p\\), \\(\\alpha'(0)=w\\). Then by chain rule, \\(\\beta=F\\circ \\alpha: (-\\varepsilon, +\\varepsilon)\\rightarrow \\mathbb{E}^m\\) is also differentiable. Then </p> \\[ dF_p(w) := \\beta'(0). \\] <p> </p> <p>Differential of a map is independent of choice of curves</p> <p>The above definition of \\(dF_p\\) does not depend on the choice of the curve which passes through \\(p\\) with tangent vector \\(w\\).</p> Proof <p>We porve the case when \\(n=2, m=3\\). Let \\(\\alpha(t)=(u(t), v(t))^T\\), and \\(F(u,v)=(x(u,v), y(u,v), z(u,v))^T\\), then </p> \\[ \\beta'(0)=\\left[\\begin{array}{cc} \\frac{\\partial x}{\\partial u} &amp; \\frac{\\partial x}{\\partial v}\\\\ \\frac{\\partial y}{\\partial u} &amp; \\frac{\\partial y}{\\partial v}\\\\ \\frac{\\partial z}{\\partial u} &amp; \\frac{\\partial z}{\\partial v} \\end{array}\\right] \\left[\\begin{array}{c} \\frac{\\partial u}{\\partial t} \\\\ \\frac{\\partial v}{\\partial t} \\\\ \\end{array}\\right]=dF_p(w). \\] <p>which is a linear map.</p> <p>Actually the above map has a matrix in canonical bases, which is usually called the Jacobian matrix.</p> <p></p> <p>defintion of regular surfaces</p> <p>A subset \\(S\\subset \\mathbb{E}^3\\) is a regular surface if for each \\(p\\in S\\), there exsits a neighborhood \\(V\\) in \\(\\mathbb{E}^3\\) and a map \\(\\pmb{x}: U\\rightarrow V\\cap S\\) of an open set \\(U\\in \\mathbb{E}^2\\) onto \\(V\\cap S\\subset \\mathbb{E}^3\\) such that</p> <p>(i) \\(\\pmb{x}\\) is differentiable, i.e. </p> \\[ \\pmb{x}(u,v)=(x(u,v), y(u,v), z(u,v)),\\quad (u,v)\\in U \\] <p>whose component functions have continuous partial derivatives of all orders in \\(U\\).</p> <p>(ii) \\(\\pmb{x}\\) is a homeomorphism. </p> <p>(iii) regularity condition. For each \\(q\\in U\\), the differential \\(d\\pmb{x}_q: \\mathbb{E}^2\\rightarrow \\mathbb{E}^3\\) is one-to-one.</p> <p>Condition (i) is necessary if we want to do some geometric ananlysis on \\(S\\). The homeomorphism in Condition (ii) prevents the self-intersections in regular surfaces, otherwise it would induce ambiguous tangent plane at the intersection point. Consition (iii) guarantee the existence of a tangent plane at all points of \\(S\\).  A more familiar form of condition (iii) is given as follows.</p> <p></p> <p>Interpretation of condition (iii)</p> <p>Let us compute the matrix of the linear map \\(d\\pmb{x}_q\\) in the canonical bases \\(e_1, e_2\\) of \\(\\mathbb{R}^2\\) with coordinate \\((u,v)\\) and \\(f_1, f_2, f_3\\) of \\(\\mathbb{R}^3\\) with coordinate \\((x,y,z)\\).</p> <p>Let \\(q=(u_0,v_0)\\), then \\(e_1=(1,0)\\) is tangent to the curve \\(u\\mapsto (u, v_0)\\) on \\(\\mathbb{R}^2\\) whose image is \\(u\\mapsto (x(u,v_0), y(u, v_0), z(u,v_0))\\) (This image curve is called the coordinate curve \\(v=v_0\\), or with ODE \\(dv=0\\)), which lies on \\(S\\) and has a tangent vector at \\(\\pmb{x}_q\\)</p> \\[ \\frac{\\partial \\pmb{x}}{\\partial u}=\\left(\\frac{\\partial x}{\\partial u},\\frac{\\partial y}{\\partial u}, \\frac{\\partial z}{\\partial u} \\right)^T =d\\pmb{x}_q (e_1). \\] <p>Similarly, we have </p> \\[ \\frac{\\partial \\pmb{x}}{\\partial v}=\\left(\\frac{\\partial x}{\\partial v},\\frac{\\partial y}{\\partial v}, \\frac{\\partial z}{\\partial v} \\right)^T =d\\pmb{x}_q (e_2). \\] <p>So we could write the matrix of \\(d\\pmb{x}_q\\)</p> \\[ \\displaystyle{\\left[\\begin{array}{cc} \\displaystyle\\frac{\\partial x}{\\partial u} &amp; \\displaystyle\\frac{\\partial x}{\\partial v} \\\\ \\displaystyle\\frac{\\partial y}{\\partial u} &amp; \\displaystyle\\frac{\\partial y}{\\partial v} \\\\ \\displaystyle\\frac{\\partial z}{\\partial u} &amp; \\displaystyle\\frac{\\partial z}{\\partial v} \\end{array}\\right]}. \\] <p>condition (iii) requires the matrix to be full rank. Equivalently speaking, we need \\(\\frac{\\partial \\pmb{x}}{\\partial u}\\times \\frac{\\partial \\pmb{x}}{\\partial v}\\neq 0\\); or one of the minors of order \\(2\\) of the matrix of \\(d\\pmb{x}_q\\), that is, one of the Jacobian determinants</p> \\[ \\frac{\\partial (x,y)}{\\partial (u,v)},\\quad \\frac{\\partial (y,z)}{\\partial (u,v)},\\quad \\frac{\\partial (x,z)}{\\partial (u,v)} \\] <p>does not vanish at \\(q\\).</p> <p>Condition (iii) is also of great importance for \\(\\pmb{x}^{-1}\\) to be a so-called differentiable function, that is, if we lift its range to three dimension, then the map is differentiable. Details could be found in Change of parameters.</p> <p>Actually, it would be tiresome if we test all the three conditions one by one. The following theorems gives a cheaper method to the testing by utilizing the image of a multi-variable function.</p>"},{"location":"Math/Differential_Geometry/Surfaces/#images","title":"Images","text":"<p>images implies regularity</p> <p>If \\(f:U\\rightarrow \\mathbb{E}\\in C^1(U)\\) where \\(U\\subset \\mathbb{E}^2\\) is an open set, then the graph of \\(f\\), viewed in \\(\\mathbb{E}^3\\), i.e. the subset of \\(\\mathbb{E}^3\\) given by \\((x,y,f(x,y))\\) for \\((x,y)\\in U\\) is a regular surface.</p> Proof <p>Easy to show that condition (i) and (iii) are satisfied by taking derivatives and showing that \\(\\frac{\\partial (x,y)}{\\partial (u,v)}=1\\). As for condition (ii), we only need to show that \\(\\pmb{x}^{-1}\\) is continuous, which is obvious if we check it as a projection from \\(\\mathbb{E}^3\\) onto \\(\\mathbb{E}^2\\).</p> <p><p>\\(\\square\\)</p></p> <p>Now we give some definitions about the following application.</p> <p>regular point, critical point</p> <p>Given a differentiable map \\(F:U\\subset \\mathbb{E}^n\\rightarrow \\mathbb{E}^m\\) where \\(U\\) is open, a point \\(p\\in U\\) is called a critical point of \\(F\\) if the differential \\(dF_p:\\mathbb{E}^n\\rightarrow \\mathbb{E}^m\\) is not a surjective mapping. </p> <p>The image \\(F(p)\\in\\mathbb{E}^m\\) of a critical point is called the critical value of \\(F\\). A non-critical value of \\(\\mathbb{E}^m\\) is called the regular value of \\(F\\).</p> <p>The above terminology is inspired by a real-valued function of a real variable.</p> <p></p> <p>Now particularly we consider \\(f:U\\subset \\mathbb{E}^3\\rightarrow \\mathbb{E}\\), which takes \\(m=1,n=3\\). With a similar logic as we have in Interpretation of condition (iii), for canonical bases \\(f_1,f_2,f_3\\), we have the matrix form</p> \\[ df_p = (f_x, f_y, f_z). \\] <p>In this case, \\(df_p\\) is not surjective at \\(p\\), iff \\(f_x=f_y=f_z=0\\) at \\(p\\). </p> <p>From the above multi-variable function, we could find a regular surface.</p> <p>regular surfaces by images</p> <p>If \\(f:U\\subset \\mathbb{E}^3\\rightarrow \\mathbb{E}\\) is a differentiable function and \\(a\\in f(U)\\) is a regular value of \\(f\\), then \\(f^{-1}(a)\\) is a regular surface in \\(\\mathbb{E}^3\\).</p> <p>Actually, this is a trick that we choose a plane in \\(\\mathbb{E}^3\\) to find a regular surface. The image of \\(f\\) cooresponds to a image of another function \\(h\\) which could give an arbitrary regular surface.</p> Proof <p>Let \\(p=(x_0,y_0,z_0)\\) be a point of \\(f^{-1}(a)\\). Since \\(a\\) is a regular value of \\(f\\), we may assume without loss of generality that \\(f_z\\neq 0\\) at \\(p\\). Then define a map like an image</p> \\[ F(x,y,z)=(x,y,f(x,y,z))^T \\] <p>and we indicate by \\((u,v,t)\\) the coordinates of a point in \\(\\mathbb{E}^3\\) where \\(F\\) takes its values. The matrix of the differential map \\(dF_p\\) is given by</p> \\[ dF_p=\\left[\\begin{array}{ccc}1 &amp; 0 &amp; 0\\\\ 0&amp; 1&amp; 0\\\\ f_x &amp; f_y &amp; f_z \\end{array}\\right] \\] <p>as we illustrated in special case for function on \\(\\mathbb{E}^3\\). Whence \\(det(dF_P)=f_z\\neq 0\\). We may apply the inverse function theorem, which guarantees the existence of neighborhood \\(V\\) of \\(p\\) and \\(W\\) of \\(F(p)\\) such that \\(F:V\\rightarrow W\\) is invertible, and the inverse \\(F^{-1}:W\\rightarrow V\\) is differentiable. It follows that </p> \\[ x=u, \\quad y=v, \\quad z=g(u,v,t),\\quad (u,v,t)\\in W \\] <p>are differentiable. In particular, \\(z=g(u,v,t=a)=h(x,y)\\) is differentiable defined in the projection of \\(V\\) onto \\(xy\\) plane. To use the previous proposition, we only have to show that \\(h\\) is differentiable.</p> <p><p> </p></p> <p>Since</p> \\[ F(f^{-1}(a)\\cap V)=\\{(u,v,t): t=a\\}\\cap W \\] <p>we conclude that the graph of \\(h\\) (i.e. \\(z\\)) is \\(f^{-1}(a)\\cap V\\). By images implies regularity, we have \\(f^{-1}(a)\\cap V\\) is a coordinate neighborhood of \\(p\\). Therefore, every point \\(p\\in f^{-1}(a)\\) can be covered by a coordinate neighborhood and \\(f^{-1}(a)\\) is a regular surface.</p> <p><p>\\(\\square\\)</p></p> <p>It would be good if readers could recall the implicit function theorem and its application -- inverse function theorem.</p> <p>The following proposition shows that any regular surface is locally the graph of a differeniable function.</p> <p>Find differentiable function using projection</p> <p>Let \\(S\\subset \\mathbb{E}^3\\) is a regular surface, and \\(p\\in S\\). Then there exsits a neighborhood \\(V\\) of \\(p\\) in \\(S\\) such that \\(V\\) is the graph of a differentiable function, which belongs to one of the three forms \\(z=f(x,y), y=g(x,z), x=h(y,z)\\).</p> Proof <p>Using projection and by inverse function theorem. Without generality, we assume </p> \\[ \\frac{\\partial (x,y)}{\\partial (u,v)}\\neq 0. \\] <p>We shall find an inverse funcion of \\(\\pi\\circ \\pmb{x}\\), denoted by \\((\\pi\\circ\\pmb{x})^{-1}\\) and compose it with \\(z=z(u,v)\\), we could have</p> \\[ z = z(u(x,y), v(x,y)):=f(x,y) \\] <p>which is also differentiable.</p> <p><p> </p></p> <p>Using the above proposition, we claim that, for a regular surface, and any other parametrization \\(\\pmb{x}\\), we do not need to test continuity of \\(\\pmb{x}^{-1}\\), provided that the other conditions hold.</p> <p>Example. Show that one-sheeted cone, with its vertex at the origin, i.e. </p> \\[ S=\\{(x,y,z): z^2=x^2+y^2, z\\geq 0\\} \\] <p>is not a regular surface.</p> Proof <p>The problem is at the origin. By </p> \\[ z=+\\sqrt{x^2+y^2} \\] <p>is not differentiable at \\((0,0)\\).</p> <p>omit the test of continuity of inverse map</p> <p>Let \\(p\\in S\\) be a point of a regular surface \\(S\\) and \\(\\pmb{x}:U\\subset \\mathbb{E}^2\\rightarrow \\mathbb{E}^3\\) is a parametrization with \\(p\\in \\pmb{x}(U)\\) such that condition 1 and 3 of defintion of regular surfaces hold. Assume \\(\\pmb{x}\\) is one-to-one, then \\(\\pmb{x}^{-1}\\) is continuous.</p> Proof <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Differential_Geometry/Surfaces/#change-of-parameters","title":"Change of parameters","text":"<p>Differentiability of change of parameters</p> <p>Let \\(p\\) be a point in a regular surface \\(S\\subset \\mathbb{E}^3\\), and two parametrizations \\(\\pmb{x}:U\\subset \\mathbb{E}^2\\rightarrow \\mathbb{E}^3\\) and \\(\\pmb{y}:V\\subset \\mathbb{E}^2\\rightarrow \\mathbb{E}^3\\), parametrized by \\((u,v)\\) and \\((\\xi, \\eta)\\), respectively. Suppose \\(p\\in \\pmb{x}(U)\\cap \\pmb{y}(V)=W\\). Then the change of parameters \\(h=\\pmb{x}^{-1}\\circ \\pmb{y}: \\pmb{y}^{-1}(W)\\rightarrow \\pmb{x}^{-1}(W)\\) is a diffeomorphism.</p> Proof <p>Utilizing the map</p> \\[ F(u,v, t)\\rightarrow (x(u,v), y(u,v), z(u,v)+t),\\quad (u,v)\\in U \\] <p>is a diffeomorphism by condition (iii). Restrict the map on a slice \\(U\\times {0}\\) and \\(F^{-1}\\) is differentiable.</p> <p>Check the following figure.</p> <p><p> </p></p> <p><p>\\(\\square\\)</p></p> <p>The definition of differentiability could be extended to mappings between surfaces by utilizing the differentiability of maps between plane parameters.</p> <p>differentiability of maps between surfaces</p> <p>A continuous map \\(\\varphi: V_1 \\subset S_1 \\rightarrow S_2\\) of an open set \\(V_1\\) of a regular surface \\(S_1\\) to a regular surface \\(S_2\\), is said to be differentiable at \\(p\\in V_1\\), if for given parametrization</p> \\[ \\pmb{x}_1:U_1\\subset \\mathbb{E}^2\\rightarrow S_1,\\quad \\pmb{x}_2: U_2\\subset \\mathbb{E}^2 \\rightarrow S_2, \\] <p>with \\(p\\in \\pmb{x}_1(U_1)\\) and \\(\\varphi(\\pmb{x}_1(U_1))\\subset \\pmb{x}_2(U_2)\\), the map composition</p> \\[ \\pmb{x}^{-1}_2 \\circ \\varphi \\circ \\pmb{x}_1: U_1\\rightarrow U_2 \\] <p>is differentiable at \\(q = \\pmb{x}^{-1}_1 (p)\\).</p> <p><p> </p></p> <p>Two surfaces \\(S_1\\) and \\(S_2\\) are diffeomorphic, if there exsits a differentiable map \\(\\varphi: S_1\\rightarrow S_2\\) with a differentiable inverse \\(\\varphi^{-1} S_2\\rightarrow S_1\\). Such a map \\(\\varphi\\) is called a diffeomorphism between \\(S_1\\) and \\(S_2\\).</p> <p></p> <p>Example. If \\(\\pmb{x}: U\\subset \\mathbb{E}^2\\rightarrow S\\) is a parametrization, then \\(\\pmb{x}^{-1}: \\pmb{x}(U)\\rightarrow \\mathbb{E}^2\\) is differentiable. This means every regular surface is locally diffeomorphic to a plane. This is useful in manifold learning.</p> Proof <p>Just check the differentiability of the map \\(I\\circ\\pmb{x}^{-1}\\circ \\pmb{y}\\) for any two given parametrizations.</p> <p><p>\\(\\square\\)</p></p> <p>Example. Let \\(S_1\\) and \\(S_2\\) be regular surfaces. Assume that \\(S_1\\subset V\\subset \\mathbb{E}^3\\), \\(V\\) is an open set of \\(\\mathbb{E}^3\\). Suppose \\(\\varphi: V\\rightarrow \\mathbb{E}^3\\) is a differentiable map such that \\(\\varphi(S_1)\\subset S_2\\). Then the restriction \\(\\varphi_{|_{S_1}}: S_1\\rightarrow S_2\\) is a differentiable map. The followings are some applications.</p> <p>(i) Symmetry. \\(S\\) is a symmetric surface relative to \\(xy\\) plane. Then the differentiable map \\(\\sigma: \\mathbb{E}^3\\rightarrow \\mathbb{E}^3\\) defined by</p> \\[ \\sigma(x, y, z)=(x, y, -z) \\] <p>is differentiable restricted on \\(S\\).</p> <p>(ii) Rotations.\\(S\\) is a regular surface invariant by rotation \\(R_{z,\\theta}\\), which denotes a rotation of angle \\(\\theta\\) about \\(z\\) axis. Then the restriction </p> \\[ R_{z,\\theta}: S\\rightarrow S \\] <p>is differentiable.</p> <p>(iii) Stretching operation. Let \\(\\varphi: \\mathbb{E}^3\\rightarrow \\mathbb{E}^3\\) is a stretching map given by</p> \\[ \\varphi(x,y,z)=(ax, by, cz), \\quad a, b, c \\neq 0. \\] <p>Then \\(\\varphi: S^2\\rightarrow \\text{ellipsoid}\\)</p> \\[ \\left\\{(x,y,z)\\in \\mathbb{E}^3: \\frac{x^2}{a^2}+\\frac{y^2}{b^2}+\\frac{z^2}{c^2}=1\\right\\} \\] <p>is differentiable.</p> Proof <p><p>\\(\\square\\)</p></p> <p>Now we could define a regular curve using concept of maps.</p> <p>definition of regular curve</p> <p>A regular curve in \\(\\mathbb{E}^3\\) is a subset \\(C\\subset \\mathbb{E}^3\\) with the following properties. For each \\(p\\in C\\), there exsits a neighborhood \\(V\\subset \\mathbb{E}^3\\) of \\(p\\) and a differentiable map \\(\\alpha: I\\subset \\mathbb{E}\\rightarrow C\\cap V\\) such that the differential \\(d_{\\alpha_t}\\) is one-to-one for each \\(t\\in I\\).</p> <p>It is of the same logic to show that change of parameters of curves is given by a diffeomorphism. </p> <p>By change of parameters, we could find properties independent of parameters, that is, the geometric properties.</p>"},{"location":"Math/Differential_Geometry/Surfaces/#the-tangent-plane","title":"The tangent plane","text":"<p>Note in the following, \\(q\\) is at the plane, \\(p\\) is on the regular surface in \\(\\mathbb{E}^2\\), and \\(w\\) is a velocity vector of a regular surface at \\(p\\).</p> <p>Two ways of viewing tangent plane, definition of tangent plane is independent of parameters</p> <p>Let \\(\\pmb{x}: U\\subset\\mathbb{E}^2\\rightarrow S\\) be a parametrization of a regular surface, \\(q\\in U\\) is on the plane. The vector subspace of dimension \\(2\\), </p> \\[ d\\pmb{x}_q (\\mathbb{E}^2) \\subset \\mathbb{E}^3 \\] <p>coincides with the set of tangent vectors to \\(S\\) at \\(\\pmb{x}_q\\).</p> Proof <p>Let \\(w\\) be a tangent vector at \\(\\pmb{x}(q)\\), i.e. \\(w=\\alpha'(0)\\), where \\(\\alpha: (-\\varepsilon, +\\varepsilon) \\rightarrow \\pmb{x}(U)\\subset S\\) is differentiable and \\(\\alpha (0)=\\pmb{x} (q)\\). By differentiability of parametrization, we have the composition \\(\\beta=\\pmb{x}^{-1}\\circ \\alpha: (-\\varepsilon, +\\varepsilon) \\rightarrow U\\) is differentiable. Take a differential, and we have \\(d\\pmb{x}_q (\\beta'(0))=w\\), so \\(w\\in d\\pmb{x}_q (\\mathbb{E}^2)\\).</p> <p><p> </p></p> <p>On the other hand, let \\(w=d\\pmb{x}_q (v)\\), where \\(v\\in \\mathbb{E}^2\\), which is the velocity vector of the curve \\(\\gamma: (-\\varepsilon, +\\varepsilon) \\rightarrow U\\) given by </p> \\[ \\gamma(t)=tv+q, \\quad t\\in (-\\varepsilon, +\\varepsilon). \\] <p>by Definition of a differential of a map, we have \\(d\\pmb{x}_p(v)=\\alpha'(0)\\), with \\(\\alpha=\\pmb{x}\\circ \\gamma\\).</p> <p><p>\\(\\square\\)</p></p> <p>From the above definition, the plane \\(d\\pmb{x}_q (\\mathbb{E}^2)\\) which passes \\(\\pmb{x}(q)=p\\), does not depend on the parametrization \\(\\pmb{x}\\). This plane is called the tangent plane to \\(S\\) at \\(p\\), denoted by \\(T_p (S)\\).</p> <p>Write its bases as follows</p> \\[ \\pmb{x}_u:=\\frac{\\partial \\pmb{x}}{\\partial u}, \\quad \\pmb{x}_v:=\\frac{\\partial \\pmb{x}}{\\partial v}. \\] <p>Then the parametrization of vector \\(w\\in T_p(S)\\) are determined by</p> \\[ w=\\alpha'(0)=\\frac{d(\\pmb{x}\\circ \\beta)}{dt}=\\pmb{x}_u (q) u'(0)+\\pmb{x}_v(q) v'(0).  \\] <p>Differential of a map between surfaces</p> <p>Let \\(S_1, S_2\\) be two regular surfaces and \\(\\varphi: V\\in S_1\\rightarrow S_2\\) is a differentiable map of an open set \\(V\\) of \\(S_1\\) into \\(S_2\\). Given tangent vector \\(w=\\alpha'(0)\\in T_p(S_1)\\), let \\(\\beta:\\varphi\\circ \\alpha\\) with \\(\\beta(0)=\\varphi(p)\\). Then \\(\\beta'(0)\\) does not depend on the choice of \\(alpha\\). The map \\(d\\varphi_p: T_p(S_1)\\rightarrow T_{\\varphi(p)}(S_2)\\) defined by \\(d\\varphi_p(w)=\\beta'(0)\\) is linear.</p> Proof <p>Take the partial derivatives and the proof is similar as we have in Differential of a map is independent of choice of curves.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Differential_Geometry/Surfaces/#the-first-fundamental-form","title":"The first fundamental form","text":"<p>Definition of the first fundamental form of surfaces</p> <p>The first fundamental form of the regular surface \\(S\\) at point \\(p\\in S\\) is defined by \\(I_p : T_P(S)\\rightarrow \\mathbb{R}\\)</p> \\[ I_p(w)=\\langle w, w\\rangle_p = |w|^2\\geq 0. \\] <p>For a parametrization of \\(I_p\\), assume we have \\(\\pmb{x}(u,v)\\), \\(\\alpha: (-\\varepsilon, +\\varepsilon)\\rightarrow S\\), with \\(\\alpha(0)=p\\), with \\(\\alpha'(0)=w\\), then</p> \\[ \\begin{align*} I_p(w)&amp;=\\langle w, w\\rangle_p \\\\ &amp;= \\langle \\pmb{x}_u u' + \\pmb{x}_v v', \\pmb{x}_u u' + \\pmb{x}_v v'\\rangle_p\\\\ &amp;=|\\pmb{x}_u|^2 (u')^2 +  \\pmb{x}_u \\cdot \\pmb{x}_v u' v' + |\\pmb{x}_v|^2 (v')^2\\\\ &amp;:= E(u')^2 + 2F u'v' + G(v')^2. \\end{align*} \\] <p>Now we could give some typical examples.</p> <p>Example. Calculate the first fundamental form of the regular surfaces.</p> <p>(i) Plane. A plane that passes through \\(p=(x_0,y_0,z_0)\\) and contain \\(w_1=(a_1,a_2,a_3)\\) and \\(b_2=(b_1,b_2, b_3)\\), is given by </p> \\[ \\pmb{x}(u,v)=p + u w_1+v w_2. \\] <p>where \\(U=\\mathbb{R}^2\\).</p> <p>(ii) The cylender over the circle \\(x^2+y^2=1\\), is given by </p> \\[ \\pmb{x}(u,v) = (\\cos u, \\sin u, v). \\] <p>where \\(U=\\{(u,v): u\\in (0,2\\pi), v\\in \\mathbb{R}\\}\\).</p> <p>(iii) Helicoid generated by a helix \\((\\cos u, \\sin u, a u)\\) given by </p> \\[ \\pmb{x}(u,v)=(v\\cos u,v\\sin u, au) \\] <p>where \\(u\\in (0,2\\pi), v\\in \\mathbb{R}\\).</p> <p>(iv) Sphere. A sphere given by</p> \\[ \\pmb{x}(u,v)=(\\cos u \\cos v, \\cos u \\sin v, \\sin u) \\] <p>where \\(u\\in(0,2\\pi), v\\in (0,\\pi)\\).</p> <p>Practically speaking, if we know \\(I_p\\), then we could calculate some geometric quantity.</p> <p>calculations of geometric quantity on a regular surface</p> <p>(i) arc length.</p> \\[ \\begin{align*} s(t)&amp;=\\int_0^t |\\alpha'(\\tau)|d\\tau \\\\ &amp;=\\int_0^t \\sqrt{|\\alpha'(\\tau)|^2} d\\tau\\\\ &amp;=\\int_0^t \\sqrt{I(\\alpha'(\\tau))}d\\tau \\end{align*} \\] <p>(ii) vector angle.</p> \\[ \\cos \\theta = \\frac{\\langle \\alpha'(t_0), \\beta'(t_0)\\rangle}{|\\alpha'(t_0)| |\\beta'(t_0)|}. \\] <p>(iii) Area. Let \\(R\\in S\\) be a bounded region of a regular surface contained in the coordinate neighborhood of the parametrization \\(\\pmb{x}: U\\subset \\mathbb{E}^2\\rightarrow S\\). The positive number </p> \\[ A(R):=\\iint_Q |\\pmb{x}_u\\times \\pmb{x}_v|du dv,\\quad Q=\\pmb{x}^{-1}(R). \\] <p>is called the area of \\(R\\). In actual calculations, we have</p> \\[ |\\pmb{x}_u\\times \\pmb{x}_v| = \\sqrt{|\\pmb{x}_u|^2|\\pmb{x}_v|^2-|\\pmb{x}_u\\cdot \\pmb{x}_v|^2}=\\sqrt{EG-F^2}. \\]"},{"location":"Math/Functional_Analysis/","title":"Functional Analysis","text":""},{"location":"Math/Functional_Analysis/#metric-space","title":"Metric Space","text":""},{"location":"Math/Functional_Analysis/#banach-space","title":"Banach Space","text":""},{"location":"Math/Functional_Analysis/#hilbert-space","title":"Hilbert Space","text":""},{"location":"Math/Functional_Analysis/#bounded-linear-operator","title":"Bounded Linear Operator","text":""},{"location":"Math/Functional_Analysis/BL_Operator/","title":"Bounded Linear Operator","text":"<p>We talk about the mapping from a space to another space. Because linear operator helps analysis, we fucos on normed linear space.</p> <p>Basic Concepts</p> <p>Assume \\(E\\) and \\(E_1\\) are linear sapce (number domain \\(K\\)), \\(T\\) is a mapping from \\(D\\) (a subspace of \\(E\\)) to \\(E_1\\). If for any \\(x,y\\in D\\), we have</p> \\[ T(x+y)=Tx+Ty, \\] <p>we call \\(T\\) is additive. If for all number \\(\\alpha\\in K\\), we have</p> \\[ T(\\alpha x)=\\alpha Tx, \\] <p>we call \\(T\\) is homogeneous.</p> <p>An additive and homogeneous mapping is called linear mapping or linear operator. If \\(E_1\\) is number domain, then the mapping is called functional. And it is called linear functional if it is additive andhomogeneous.</p> <p>Now we give some important concepts about continuity and boundedness.</p> <p>continuity and boundedness</p> <p>Assume \\(D\\) is a subspace of \\(E\\), \\(T\\) is a linear operator from \\(D\\) to \\(E_1\\).</p> <p>If \\(T\\) maps a bounded set in \\(D\\) into a bounded set in \\(E_1\\), then we call \\(T\\) is a bounded linear operator. Otherwise, there exsits a bounded subset \\(A\\) of \\(D\\), it is mapped to a unbounded set in \\(E_1\\) by \\(T\\), then \\(T\\) is called an unbounded linear operator.</p> <p>This result is directly from the proof in Parallelogram Law.</p> <p>Theorem</p> <p>Assume \\(E\\), \\(E_1\\) are real normed linear spaces, \\(D\\) is a subspace of \\(E\\), and \\(T\\) is a continuous additive operator from \\(D\\) to \\(E_1\\). Then \\(T\\) is homogeneous.</p> Proof <p>For any \\(x\\in D\\), define \\(f(\\alpha)=T(\\alpha x)\\), so \\(f\\) is continuous and for all \\(\\alpha_1\\), \\(alpha_2\\) </p> \\[ f(\\alpha_1+\\alpha_2)=f(\\alpha_1)+f(\\alpha_2). \\] <p>By lemma in Parallelogram Law, we have for all \\(\\alpha \\in \\mathbb{R}\\), \\(f(\\alpha)=\\alpha f(1)\\), i.e. </p> \\[ T(\\alpha x)=\\alpha T(x). \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/BL_Operator/#boundedness-continuity-for-lo","title":"Boundedness &amp; Continuity for LO","text":"<p>The following result is an equivalent proposition for bounded linear operator. It is the linearity that bridges the difference between these two.</p> <p></p> <p>Lemma: Equivalent proposition for bounded linear operator</p> <p>Assume \\(E\\), \\(E_1\\) are normed linear spaces, \\(D\\) is a subspace of \\(E\\), and \\(T\\) is a linear operator from \\(D\\) to \\(E_1\\). Then \\(T\\) is bounded iff \\(\\exists M&gt;0\\), such that </p> \\[ \\|Tx\\|\\leq M\\|x\\| \\] Proof <ul> <li>Necessary. This is a general method. Choose a unit sphere \\(S=\\{x: \\|x\\|=1\\}\\), and we have \\(S\\) is bounded set, so \\(T(S)\\) is also bounded by assumption. So \\(\\exists M&gt;0\\), such that for all \\(x\\in S\\), \\(\\|Tx\\|\\leq M\\). Then \\(\\forall x\\in D\\), \\(\\frac{x}{\\|x\\|}\\in S\\), so</li> </ul> \\[ \\left|T\\left(\\frac{x}{\\|x\\|}\\right)\\right|\\leq M \\] <p>By linear properties, we have \\(\\|Tx\\|\\leq M \\|x\\|\\).</p> <ul> <li>Sufficient.</li> </ul> <p>This is easy. Since we choose an bounded set \\(A\\subset D\\), \\(\\exists K&gt;0\\), we have \\(\\|x\\|\\leq K\\) for all \\(x\\in A\\), so by assumption, \\(\\|Tx\\|\\leq M\\|x\\|\\leq MK\\), which shows that \\(T(A)\\) is bounded in \\(E_1\\).</p> <p>Now we give a clear connection between continuity and boundedness. </p> <p>Continuity &amp; Boundedness</p> <p>Assume \\(E\\), \\(E_1\\) are normed linear spaces, \\(D\\) is a subspace of \\(E\\), and \\(T\\) is a linear operator from \\(D\\) to \\(E_1\\). Then the following statement is equivalent.</p> <p>(i) \\(T\\) is continuous.</p> <p>(ii) \\(T\\) is continuous at \\(\\theta\\).</p> <p>(iii) \\(T\\) is bounded.</p> Proof <ul> <li> <p>(i) \\(\\Rightarrow\\) (ii) is apparent.</p> </li> <li> <p>(ii) \\(\\Rightarrow\\) (i). Choose \\(\\varepsilon-\\delta\\) language. Choose \\(\\varepsilon=1\\), then \\(\\exists \\delta\\) such that whenever \\(\\|x\\|\\leq \\delta\\), we have</p> </li> </ul> \\[ \\|Tx\\|\\leq 1 \\] <p>Now we consider any \\(x\\neq \\theta\\in D\\), then \\(\\frac{\\delta x}{\\|x\\|}\\leq \\delta\\), so</p> \\[ \\begin{align*} \\|T\\frac{\\delta x}{\\|x\\|}\\|\\leq 1\\\\ \\Rightarrow \\quad \\|Tx\\|\\leq \\frac{1}{\\delta}\\|x\\|. \\end{align*} \\] <p>by Lemma: Equivalent proposition for bounded linear operator, we complete the proof. Actually the abouve inequation is Lipschitz continuity at the origin.</p> <ul> <li>(iii) \\(\\Rightarrow\\) (i). Choose a sequence \\(x_n\\rightarrow x\\), because it is bounded, we have </li> </ul> \\[ \\|Tx_n-Tx\\|\\leq \\|T(x_n-x)\\|\\leq M\\|x_n-x\\|\\rightarrow 0. \\]"},{"location":"Math/Functional_Analysis/BL_Operator/#norm-of-bounded-linear-operator","title":"Norm of Bounded linear Operator","text":"<p>Definition of Norm</p> <p>Assume \\(E,E_1\\) are normed linear space, \\(D\\) is a subspace of \\(E\\), \\(T\\) is a bounded linear mapping from \\(D\\) to \\(E_1\\). Define the norm of \\(T\\) </p> \\[ \\|T\\|=\\sup_{x\\neq \\theta\\atop x\\in D}\\frac{\\|Tx\\|}{\\|x\\|}. \\] <p>Actually, \\(\\|T\\|\\) is also given by</p> \\[ \\|T\\|=\\inf\\{M: \\|Tx\\|\\leq M\\|x\\|,\\forall x\\in D\\}. \\] <p>Corollary</p> <p>(i) for all \\(x\\in D\\), \\(\\|Tx\\|\\leq \\|T\\|\\|x\\|\\).</p> <p>(ii) Normalized definition.</p> \\[ \\|T\\|=\\sup_{\\|x\\|\\leq 1\\atop x\\in D}\\|Tx\\|=\\sup_{\\|x\\|=1 \\atop x\\in D}\\|Tx\\|. \\] <p>That is, we only need to consider the vector on the unit sphere.</p> <p>It is usually hard to consider the exact magnitude of norm of an operator. Check the following examples.</p> <p>Example. Assume \\((a_{ij}) (i,j=1,\\cdots n)\\) is a given \\(n\\times n\\) matrix, \\(a_{ij}\\in \\mathbb{R}\\), define </p> \\[ \\eta_i=\\sum_{j=1}^n a_{ij} \\xi_j,\\quad i=1,\\cdots,n \\] <p>then \\(x=(\\xi_1,\\cdots,x_n), y=(\\eta_1,\\cdots, \\eta_n) \\in\\mathbb{R}^n\\), define \\(T: \\mathbb{R}^n\\mapsto \\mathbb{R}^n\\) by \\(Tx=y\\). Show that \\(T\\) is a bounded linear operator.</p> Proof <p><p>\\(\\square\\)</p></p> <p>Example. We use \\(C(-\\infty,+\\infty)\\) to be a set of all the bounded continuous function defined on \\(\\mathbb{R}\\), with linear operator the same as \\(C[a,b]\\). Define norm in linear space \\(C(-\\infty,+\\infty)\\) by</p> \\[ \\|y\\|=\\sup_{-\\infty &lt; t &lt; \\infty}|y(t)|,\\quad y\\in (-\\infty,+\\infty), \\] <p>then we could show that \\(C(-\\infty,+\\infty)\\) is a Banach Space. Then for \\(x\\in L(-\\infty,+\\infty)\\), we define a operator \\(T\\)</p> \\[ y=Tx: y(s)=\\int_\\mathbb{R} e^{-ist}x(t)dt, \\] <p>So it is a linear operator from \\(L(-\\infty,+\\infty)\\) to \\(C(-\\infty,+\\infty)\\), whose boundedness is given by</p> \\[ |(Tx)(s)|=|y(s)|\\leq \\int_\\mathbb{R}|e^{-ist}x(t)|dt=\\int_\\mathbb{R}|x(t)|dt&lt;\\infty. \\] <p>So \\(Tx\\) is bounded and thus \\(T\\) is continuous by Continuity &amp; Boundedness. And \\(\\|T\\|\\leq 1\\).</p> <p>Example. Lagrangian Formula. As we have shown in Lagrange interpolating polynomial in Numerical Analysis.</p> <p>Given \\(x\\in C[a,b]\\) with \\(n\\) points \\((t_1,x_1),\\cdots (t_n,x_n)\\) passing it, \\(a&lt;x_1&lt;\\cdots&lt;x_n&lt;b\\) we have Lagrangian formula</p> \\[ l_k(t)=\\prod_{j=1\\atop j\\neq k}^n \\frac{t-t_j}{ t_k-t_j}. \\] <p>then \\(l_k(t)\\) is a basis of \\(C[a,b]\\), and the interpolating polynomial defined by</p> \\[ y=L_nx: y(t)=\\sum_{k=1}^n x_k l_k(t). \\] <p>Then \\(L_n\\) is a operator which maps \\(C[a,b]\\) into itself. Show that \\(L_n\\) is a bounded linear operator and its norm satisfies</p> \\[ \\|L_n\\|=\\max_{t\\in [a,b]}\\sum_{k=1}^n |l_k(t)| \\] Proof <p><p>\\(\\square\\)</p></p> <p>Example. Assume \\(K(t,s)\\) is continuous function defined on \\([a,b]\\times [a,b]\\). Define integral operator on \\(C[a,b]\\)</p> \\[ y(t)=(Tx)(t)=\\int_a^b K(t,s)x(s)ds. \\] <p>show that \\(T\\) is a bounded linear operator mapping \\(C[a,b]\\) to itself and its norm satisfies</p> \\[ \\|T\\|=\\max_{t\\in [a,b]}\\int_a^b |K(t,s)|ds. \\] Proof <p><p>\\(\\square\\)</p></p> <p>Example. Asssume</p>"},{"location":"Math/Functional_Analysis/BL_Operator/#lo-space","title":"LO Space","text":"<p>Definition of Linear operator space</p> <p>Assume \\(E\\), \\(E_1\\) are normded linear space. We use \\(\\mathscr{B}(E.E_1)\\) to be the set of linear operator which maps \\(E\\) into \\(E_1\\). If we define the following linear calculation</p> <p>(i) \\((T_1+T_2)x=T_1 x+T_2 x\\)</p> <p>(ii) \\((\\alpha T)x=\\alpha (Tx)\\)</p> <p>where \\(T_1,T_2,T\\in \\mathscr{B}(E,E_1)\\), \\(\\alpha \\in K\\). Then \\(\\mathscr{B}(E,E_1)\\) is a linear space by the above linear calculation. </p> <p>Normed linear space</p> <p>If we use norm defined in the previous part, \\(\\mathscr{B}(E,E_1)\\) is a normed linear space.</p> Proof <p>(i) Positive &amp; definitive.</p> <p>(ii) homogeneous.</p> <p>(iii) Triangle inequation.</p>"},{"location":"Math/Functional_Analysis/BL_Operator/#convergence","title":"Convergence","text":"<p>Convergence in operator norm</p> <p>Assume \\(\\{T_n\\},T\\subset \\mathscr{B}(E,E_1)\\), then we call \\(\\{T_n\\}\\) converges to \\(T\\) in operator norm, or in the uniform operator topology, if</p> \\[ \\lim_{n\\rightarrow \\infty }\\|T_n-T\\|=0. \\] <p>The above convergence implies some uniform convergence. </p> <p>Convergence in the uniform operator topology</p> <p>Assume \\(\\{T_n\\},T\\subset \\mathscr{B}(E,E_1)\\), then \\(\\{T_n\\}\\) converges to \\(T\\) in the uniform operator topology, iff \\(T_n\\) converges to \\(T\\) uniformly on any bounded subset of \\(E\\).</p> Proof \\[ \\|T_n x-Tx\\|\\leq \\|T_n-T\\| \\|x\\| \\] <p>so for a bounded \\(x\\), necessary condition is apparent.</p> <p>As for sufficient condition, we have to make use of the scaling transformation. That is, choose a unit sphere \\(\\overline{S}(\\theta,1)\\). \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N\\), we have</p> \\[ \\|T_n x-Tx\\|&lt;\\varepsilon,\\quad \\text{whenever } n&gt;N \\] <p>Now by definition of norm of operator, </p> \\[ \\|T_n-T\\|=\\sup_{\\|x\\|=1}\\|(T_n-T)x\\|\\leq \\varepsilon. \\]"},{"location":"Math/Functional_Analysis/BL_Operator/#completeness","title":"Completeness","text":"<p>Condition for completeness of \\(\\mathscr{B}(E,E_1)\\)</p> <p>Assume \\(E_1\\) is a Banach space, then \\(\\mathscr{B}(E,E_1)\\) is a Banach space.</p> Proof <p>We make use of the completeness of \\(E_1\\) to formulate the limit of Cauchy sequence in \\(\\mathscr{B}(E,E_1)\\).</p>"},{"location":"Math/Functional_Analysis/BL_Operator/#strong-convergence","title":"Strong Convergence","text":"<p>Definition of strong convergence</p> <p>Assume \\(\\{T_n\\},T\\subset \\mathscr{B}(E,E_1)\\), if for every \\(x\\in E\\),</p> \\[ \\lim_{n\\rightarrow\\infty}\\|T_n x- Tx\\|=0, \\] <p>then we call \\(\\{T_n\\}\\) converges to \\(T\\) in strong convergence, or in the strong operator topology. </p> <p>From Proof of Convergence in the uniform operator topology, we could know that convergence in the uniform operator topology implies that in the strong operator topology.</p>"},{"location":"Math/Functional_Analysis/BL_Operator/#banachs-open-mapping-theorem","title":"Banach's Open mapping theorem","text":"<p>Banach's Open mapping theorem</p> <p>Assume \\(E\\), \\(E_1\\) are banach space, and a bounded linear operator \\(T\\) maps \\(E\\) into \\(E_1\\). If the range of \\(T\\), denoted by \\(F=T(E)\\), is a set of the second category, then \\(F=E_1\\), i.e. \\(T\\) is surjective, and holds the following estimate: \\(\\exists K&gt;0\\), \\(\\forall y\\in E_1\\), \\(\\exists x\\in E\\), such that \\(Tx=y\\) and</p> \\[ \\|x\\|\\leq K\\|y\\|. \\] <p>Moreover, \\(T\\) maps any open set in \\(E\\) into an open set in \\(E_1\\).</p> Proof <p>We must use a sequence of ball \\(O_n=\\{x:\\|x\\|\\leq n,x\\in E\\}\\), and \\(M_n=T(O_n)\\). This is the form we depend on.</p> <ul> <li> <p>By definition of set of the second category. Since \\(E=\\bigcup_{n}O_n\\), so \\(F=\\bigcup_n M_n\\). Since it is not a set of the first category, \\(\\exists n_0\\), such that \\(M_{n_0}\\) is not a nowhere dense set, i.e. $\\exists $ a closed ball \\(Q(y_0,r_0)\\) in which \\(M_{n_0}\\) is dense.</p> </li> <li> <p>Move the closed ball to orinin. Show that \\(M_1\\) is dense in \\(Q(0,\\delta)\\) where \\(\\delta=r_0/n_0\\). This is done by making use of the subtraction of two point in \\(Q(y_0,r_0)\\).</p> </li> <li> <p>Show that \\(M_1\\supset Q_{\\delta/2}\\). This is by \\(T(O_{1/2^n})\\) is dense in \\(Q_{\\delta/2^n}\\). This is not a closure because the original image of any element in \\(Q_{\\delta/2}\\) is constructed by limit of a sequence.</p> </li> <li> <p>Finally, use scaling transformation to get the result.</p> </li> <li> <p>For the moreover part, we could make use of the step (iii).</p> </li> </ul> <p>Corollary: open mapping theorem</p> <p>Assume \\(E\\), \\(E_1\\) are Banach space. If a bounded linear operator \\(T \\in \\mathcal{B}(E,E_1)\\) is surjective, then it is an open mapping.</p> Proof <p>Known from the proof, an open ball \\(O_\\delta\\) is mapped into \\(T(O_\\delta)\\), which might be of arbitrary shape, but cover a smaller open ball \\(Q_{\\delta/2}\\), which means an interior point is mapped into an interior point.</p> <p><p>\\(\\square\\)</p></p> <p></p> <p>Corollary: inverse mapping theorem</p> <p>Assume \\(E\\), \\(E_1\\) are Banach space. If a bounded linear operator \\(T \\in \\mathcal{B}(E,E_1)\\) is surjective and injective, then it is inversible, and the inverse mapping is also a linear bounded operator.</p> Proof <p>Since \\(T\\) is bijective, then it has an inverse mapping, denoted \\(T^{-1}\\). By the estimate in Banach's Open mapping theorem, we have </p> \\[ \\|T^{-1}y\\|=\\|x\\|\\leq K \\|y\\|, \\] <p>which means \\(T^{-1}\\) is bounded. It is easy to show its linearity.</p> <p><p>\\(\\square\\)</p></p> <p>Corollary: Equivalence of norm</p> <p>We recall that norms \\(\\|\\cdot\\|_1\\), \\(\\|\\cdot\\|_2\\) of \\(E\\) are equivalent iff there exists \\(K_1,K_2\\) such that \\(\\forall x \\in E\\)</p> \\[ K_2\\|x\\|_1 \\leq \\|x\\|_2 \\leq K_1\\|x\\|_1. \\] <p>by Open mapping theorem, we have the following theorem.</p> <p>If there exsits \\(K\\) such that </p> \\[ \\|x\\|_2 \\leq K\\|x\\|_1, \\] <p>then the two norms are equivalent.</p> Proof <p>This is by utilizing the inverse mapping theorem. That is, let \\(I\\) to be the identity mapping from \\(E\\) to \\(E\\), with norm \\(\\|\\cdot\\|_1\\) and \\(\\|\\cdot\\|_2\\) respectively.</p> <p>so \\(I\\) is bijective, by condition \\(\\|x\\|_2=\\|Ix\\|_2\\leq K\\|x\\|_1\\), we have \\(I\\) is bounded. So it has an inverse mapping, which is also bounded, i.e. there exists \\(K'\\)</p> \\[ \\|x\\|_1=\\|I^{-1}x\\|_1\\leq K'\\|x\\|_2. \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/BL_Operator/#closed-image-theorem","title":"Closed image theorem","text":"<p>Definition of image</p> <p>For a linear operator \\(T\\in \\mathcal{L}(E,E_1)\\), define its direct sum of \\(E\\) and \\(E_1\\), and its norm </p> \\[ \\|(x,y)\\|=\\|x\\|+\\|y\\|,\\quad x\\in E, y\\in E_1. \\] <p>by definition \\(E \\oplus E_1\\) is a normed linear space. For a linear operator T which maps \\(D\\subset E\\) to \\(E_1\\), we call \\(G_t:=\\{(x,Tx): x\\in D\\} \\subset E\\oplus E_1\\) to be the image of \\(T\\), and we call \\(T\\) is closed, if \\(G_T\\) is a closed subspace. </p> <p>Description for closed linear operator</p> <p>A linear operator \\(T\\in \\mathcal{L}(D, E_1)\\) is closed, iff for a given \\((x,y)\\in E\\oplus E_1\\), there exists sequence \\(\\{x_n\\}\\subset D\\) and \\(x_n\\rightarrow x\\), \\(\\{Tx_n\\}\\rightarrow y\\), then \\(x\\in D\\) and \\(Tx=y\\).</p> Proof <ul> <li> <p>Necessary. \\(G_T\\) is closed, so given \\(x_n\\rightarrow x\\), \\(Tx_n \\rightarrow y\\), \\((x_n,Tx_n)\\rightarrow (x,y)\\), so \\((x,y) \\in G_T\\), so \\(x\\in D\\), \\(y=Tx\\).</p> </li> <li> <p>Sufficient. For \\((x,y)\\in \\overline{G_T}\\), \\(\\exists x_n \\subset D\\), \\(x_n\\rightarrow x\\) and \\(Tx_n\\rightarrow y\\), by condition, \\(x\\in D\\), and \\(y=Tx\\), so \\((x,y)\\in G_T\\).</p> </li> </ul> <p>The following theorem answers when a closed linear operator becomes bounded. In the proof we could check that property closedness is weaker then boundedness. </p> <p>Closed image theorem</p> <p>If a linear operator \\(T\\) maps \\(E\\) to \\(E_1\\), where \\(E\\) and \\(E_1\\) are Banach space, then \\(T\\) is bounded iff \\(T\\) is closed.</p> Proof <ul> <li>Sufficient.</li> </ul> <p>We still make use of inverse mapping theorem. So if \\(T\\) is closed, \\(G_T\\) is by definition a closed subspace of \\(E \\oplus E_1\\). Then define projection mapping \\(\\tilde{T}\\) from \\(G_T\\) to \\(E\\) to be \\(\\tilde{T}(x, Tx)=x\\). \\(\\tilde{T}\\) is injective also surjective apparently. Now we show that it is bounded, i.e. </p> \\[ \\|\\tilde{T}(x, Tx)\\|=\\|x\\|\\leq \\|x\\|+\\|Tx\\|=\\|(x,Tx)\\|. \\] <p>so by inverse mapping theorem, \\(\\exists \\tilde{T}^{-1}\\) such that \\((x, Tx)=\\tilde{T}^{-1}x\\), and bounded \\(\\|(x, Tx)\\|\\leq K\\|x\\|\\), which means \\(\\|Tx\\|\\leq K\\|x\\|\\).</p> <ul> <li>Necessary.</li> </ul> <p>We make use of the continuity of \\(T\\) when it is bounded. For \\((x,y)\\in E\\oplus E_1\\), \\(\\exists x_n\\subset E\\), \\(x_n \\rightarrow x\\), \\(Tx_n \\rightarrow y\\). By continuity of \\(T\\), \\(Tx_n\\rightarrow Tx\\), so by uniqueness of limit, \\(Tx=y\\), so \\(T\\) is bounded.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/BL_Operator/#dual-space","title":"Dual Space","text":"<p>For a bounded linear operator space \\(\\mathcal{B}(E,E_1)\\) is complete, if \\(E_1\\) is complete. So Let \\(E_1=K\\), the number domain, then \\(\\mathcal{B}(E,K)\\), space composed by all the bounded linear functional, is naturally complete.</p> <p>Dual Space</p> <p>Denote the dual space of \\(E\\) to be \\(E^*=\\mathcal{B}(E,K)\\).</p> <p>Now we first dive into the relationship between \\(E\\) and \\(E^{**}\\).</p> <p>Assume \\(x\\in E\\), \\(f\\in E^*\\). Define \\(x^{**}\\) to be \\(x^{**}(f)=f(x)\\), then by linearity on \\(E^*\\), \\(x^{**}\\) is also a linear functional. It is also bounded, since</p> \\[ \\begin{align} |x^{**}(f)|=|f(x)|\\leq \\|x\\|\\|f\\|\\label{boudnedness-1} \\end{align} \\] <p>so \\(x^{**}\\in E^{**}\\). For each \\(x\\in E\\), we could get a corresponding \\(x^{**}\\). We could discuss about the space spanned by \\(x^{**}\\).</p> <p>Properties of \\(x\\mapsto x^{**}\\), canonical mapping</p> <p>(i) Linearity. For \\(x_1^{**}\\), \\(x_2^{**}\\), we have</p> \\[ (\\alpha x_1+\\beta x_2)^{**}=\\alpha x_1^{**} + \\beta x_2^{**}. \\] <p>(ii) Isometry. \\(\\|x^{**}\\|=\\|x\\|\\), then \\(x^{**}\\) is injective.</p> Proof <p>(i) Easy to show.</p> <p>(ii) From the estimation \\(\\ref{boudnedness-1}\\), we have \\(\\|x^{**}\\|\\leq \\|x\\|\\). Then on the other hand, we choose \\(f_0\\in E^*\\), such that </p> \\[ \\|f_0\\|=1,|f_0(x)|=\\|x\\|. \\] <p>So the proposition holds.</p> <p><p>\\(\\square\\)</p></p> <p>If \\(T: x\\mapsto x^{**}\\) is surjective, then we call \\(E\\) is a reflexive Space.</p> <p>We could say \\(T(E)\\) is a subspace of \\(E^{**}\\), if \\(T(E)=E^{**}\\), then \\(E\\) is a reflexive space.</p> <p>Example. </p> <p>(i) The dual space of \\(L^p[a,b](1 &lt; p &lt; \\infty)\\) is \\(L^q[a,b]\\).</p> <p>(ii) The dual space of \\(l^p(1 &lt; p &lt; \\infty)\\) is \\(l^q\\).</p> Proof <p>For </p> <p>Example. (Riesz Representation Theorem) The dual space of Hilbert space \\(\\mathscr{U}\\) is itself. That is, for any bounded linear functional \\(f\\) on \\(\\mathscr{U}\\), there exists a unique element \\(u\\in \\mathscr{U}\\) such that </p> \\[ f(x)= \\langle x,u \\rangle, \\quad \\|f\\|=\\|u\\|. \\] <p>Conversely, for a given \\(u\\in \\mathscr{U}\\), \\(\\langle x,u \\rangle\\) defines a linear bounded functional on \\(\\mathscr{U}\\), and satisfies the isometric property.</p> Proof <ul> <li>Necessary. If \\(f=\\theta\\), then choose \\(x=\\theta\\) and we are done. Suppose \\(f\\neq \\theta\\), then define its kernel </li> </ul> \\[ L=\\{x: f(x)=\\theta\\}. \\] <p>which is a nonempty, pure and closed subset. (why?)</p> <p>Choose \\(x_0\\neq \\theta\\in L^\\perp\\), and let \\(\\|x_0\\|=1\\). Then by a lemma, we have a direct product decomposition,</p> \\[ \\mathscr{U}=L \\oplus \\{\\alpha x_0\\}. \\] <p>which is actually a orthogonal decomposition with \\(x=y+\\langle x, x_0\\rangle x_0\\) where \\(\\langle x, x_0\\rangle x_0\\in L^\\perp\\) is the orthogonal projection of \\(x\\) on \\(\\{alpha x_0\\}\\). Then apply \\(f\\) we have</p> \\[ f(x)=f(y)+\\langle x, x_0\\rangle f(x_0)=\\langle x, \\overline{f}(x_0) x_0\\rangle. \\] <p>so choose \\(u=\\overline{f}(x_0)x_0\\).</p> <ul> <li>Prove uniqueness.</li> </ul> <p>Assume \\(u'\\in \\mathscr{U}\\) also satisfies the condition, then \\(f(x)=\\langle x, u'\\rangle=\\langle x, u\\rangle\\) which implies \\(\\langle x, u'-u\\rangle=0\\), so \\(u'-u=\\theta\\).</p> <ul> <li>Prove isometry.</li> </ul> <p>By Schwarz inequation we have</p> \\[ |f(x)|=|\\langle x, u\\rangle|\\leq \\|x\\|\\|u\\|, \\] <p>so \\(\\|f\\|\\leq \\|u\\|\\). On the other hand, we have choose \\(x=u\\), we achieve </p> \\[ |f(u)|=|\\langle u,u \\rangle|=\\|u\\|\\|u\\| \\] <p>so \\(\\|f\\|\\geq \\|u\\|\\).</p> <ul> <li>Prove Sufficient.</li> </ul> <p>We only need to prove \\(f(x)=\\langle x,u\\rangle\\) is a linear bounded operator, which is by the lineariry of inner product and Schwarz inequation.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/BL_Operator/#existence-of-functional","title":"Existence of functional","text":"<p>Extension of functional</p> <p>Assume \\(E\\) is a vector space, and linear functionals \\(f_1,f_2\\in E^*\\) defined on subspace \\(G_1, G_2\\subset E\\) respectively. If \\(G_1\\subset G_2\\) and </p> \\[ f_1(x)=f_2(x),\\quad \\forall x\\in G_1 \\] <p>then we call \\(f_2\\) is an extension of \\(f_1\\).</p> <p>subadditive and positive-homogeneous functional (SP functional)</p> <p>A functional \\(f\\in E^*\\) is subadditive if for \\(x,y\\in E\\)</p> \\[ f(x+y)\\leq f(x)+f(y). \\] <p>It is positive-homogeneous if for positive real number \\(\\alpha\\), </p> \\[ f(\\alpha x)=\\alpha f(x). \\] <p></p> <p>Theorem for extension of SP functional</p> <p>Assume \\(G\\) is a subspace of linear space \\(E\\), \\(f\\) is a real functional on \\(G\\), and \\(p(x)\\) is a SP functional on \\(E\\), and thay satisfies</p> \\[ f(x)\\leq p(x),\\quad \\forall x\\in G. \\] <p>Then there exists a real functional \\(f_0\\) on \\(E\\) which is an extension of \\(f\\) and holds the boundedness of \\(p\\), i.e. </p> \\[ f_0(x)\\leq p(x),\\quad x\\in E. \\] Proof <ul> <li>Prove extension of one dimension.</li> </ul> <p>define \\(G_1=\\{\\alpha x\\}\\oplus G\\) for \\(x\\in E-G\\), </p> \\[ f(y+\\alpha x)=f(y)+c\\alpha  \\] <p>where \\(c\\) satisfies</p> \\[ \\sup_{x'\\in G} {-p(-x'-x)-f(x')}\\leq c\\leq \\inf_{x''\\in G} \\{p(x''+x)-f(x'')\\} \\] <p>this is because we want to let the following inequation holds</p> \\[ \\begin{align*} f(y+\\alpha x)&amp;=f(y)+c\\alpha \\leq p(y+\\alpha x)\\\\ \\alpha c&amp;\\leq p(y+\\alpha x)-f(y) \\end{align*} \\] <p>if \\(\\alpha =0\\) it holds naturally. If \\(\\alpha&gt;0\\), we have to let \\(c\\leq p(y/\\alpha +x)-f(y/\\alpha)\\). If \\(\\alpha&lt;0\\), then we have to let \\(-p(-y/\\alpha -x)-f(y/\\alpha)\\leq c\\). The above two gives the estimation. We have to show that the estimation holds because \\(\\forall y_1,y_2\\in G\\), </p> \\[ \\begin{align*} f(y_2)-f(y_1)&amp;=f(y_2-y_1)\\leq p(y_2-y_1)\\\\ &amp;= p(y_2+x-y_1-x)\\\\ &amp;\\leq p(y_2+x)+p(-y_1-x)\\\\ \\Rightarrow \\quad -p(-y_1-x)-f(y_1)&amp;\\leq p(y_2+x)-f(y_2). \\end{align*} \\] <ul> <li>Use Zorn's Lemma. It states that a partially ordered set containing upper bound for every totally ordered subset necessarily contains at least one maximal element. </li> </ul>"},{"location":"Math/Functional_Analysis/BL_Operator/#hahn-banach-theorem","title":"Hahn-Banach Theorem","text":"<p>Lemma: Complexification of functional</p> <p>Assume \\(f\\) is a complex bounded linear functional on \\(E\\), define </p> \\[ \\varphi(x)=\\text{Re}(f(x)),\\quad x\\in E, \\] <p>then \\(\\varphi\\) is a real bounded linear functional on \\(E\\) and satisfies</p> \\[ f(x)=\\varphi(x)-i\\varphi(ix). \\] Proof <p>Assume \\(f(x)=\\varphi(x)+i\\psi(x)\\), then</p> \\[ \\varphi(ix)+i\\psi(ix)=f(ix)=if(x)=i(\\varphi(x)+i\\psi(x))=-\\psi(x)+i\\varphi(x). \\] <p>so we have \\(\\psi(x)=-\\varphi(ix)\\). And for boundedness</p> \\[ |\\varphi(x)|\\leq |f(x)| \\] <p>so \\(\\|\\varphi\\|\\leq \\|f\\|\\).</p> <p>Hahn-Banach Theorem</p> <p>Assume \\(G\\) is a subset of vector space \\(E\\). \\(f\\) is a bounded linear functional on \\(G\\), then \\(f\\) could be extended to \\(E\\) and keep norm. That is, there exists  a bounded linear operator \\(f_0\\) such that </p> \\[ f_0(x)=f(x),\\quad x\\in G. \\] <p>and \\(\\|f_0\\|=\\|f\\|_G\\).</p> Proof <p>Suppose \\(f\\) is complex-valued, and we shall first talk about \\(\\varphi\\). Define control functional</p> \\[ p(x)=\\|f\\|_G \\|x\\| \\] <p>then \\(\\varphi(x)\\leq |f(x)|\\leq \\|f\\|_G \\|x\\|\\). \\(p(x)\\) is a SP functional so by Theorem for extension of SP functional, we could extend \\(\\varphi\\) to \\(E\\) and keep its norm, denoted by \\(\\varphi_0\\). By Lemma: Complexification of functional, we define </p> \\[ f_0(x)=\\varphi_0(x)-i\\varphi_0(ix). \\] <p>which is a bounded linear functional, since it satisfies </p> \\[ f_0(ix)=\\varphi_0(ix)- i \\varphi_0(-x)=\\varphi_0(ix)+i\\varphi_0(x)=i[\\varphi_0(x)-i\\varphi_0(ix)]=if(ix). \\] <p>and let \\(\\alpha\\) to be the argument of \\(f_0(x)\\), so by rotation,</p> \\[ |f_0(x)|=e^{-i\\alpha}f_0(x)= f(e^{-i\\alpha}x)=\\varphi_0(e^{-i\\alpha }x)\\leq p(e^{-i\\alpha}x)=\\|f\\|_G \\|x\\|. \\] <p>So \\(\\|f_0\\|\\leq \\|f\\|_G\\). On the other hand, since \\(f_0\\) is an extension of \\(f\\), then \\(\\|f_0\\|\\geq \\|f\\|\\).</p> <p><p>\\(\\square\\)</p></p> <p></p> <p>Corollary 1: generate functional by extend one dimension subspace</p> <p>Assume \\(G\\) is a subspace of \\(E\\), choose \\(x\\in E-G\\), if \\(d(x_0,G)=\\delta&gt;0\\), then there exists a bounded linear functional \\(f\\) such that </p> \\[ \\|f\\|=\\frac{1}{\\delta},\\quad f(x_0)=1,\\quad f(x)=0,\\forall x\\in G. \\] Proof <p>Extend \\(G\\) to \\(G_1=\\{\\alpha x_0+x: \\alpha\\in K, x\\in G\\}\\). Let</p> \\[ f(\\alpha x_0+x)=\\alpha \\] <p>which is a linear functional and satisfies \\(f(x_0)=1\\), \\(f(x)=0,\\forall x\\in G\\). It is also bounded since</p> \\[ \\|\\alpha x_0+x\\|=\\left\\|\\alpha(x_0+x/\\alpha)\\right\\|\\geq \\alpha \\delta, \\] <p>so</p> \\[ |f(\\alpha x_0+x)|=|\\alpha|\\leq\\frac{1}{\\alpha}\\|\\alpha x_0+x\\|. \\] <p>meaning \\(\\|f\\|_{G_1}\\leq \\frac{1}{\\delta}\\).</p> <p>Now we give proof of another direction. Since \\(G\\) might not be closed, we have to use a sequence of points. That is, assume \\(\\{x_n\\}\\subset G\\) and \\(\\|x_n-x_0\\|\\rightarrow \\delta\\), so</p> \\[ 1=f(x_0-x_n)\\leq \\|f\\|_{G_1}\\|x_0-x_n\\|. \\] <p>which implies \\(\\|f\\|_{G_1}\\geq \\frac{1}{\\|x_0-x_n\\|}\\) and take limit. </p> <p><p>\\(\\square\\)</p></p> <p></p> <p>Corollary 2</p> <p>Assume \\(G\\) is a subspace of \\(E\\), then \\(x_0\\in \\overline{G}\\) iff for all bounded linear functional \\(f\\) on \\(E\\) which satisfies \\(f(x)=0, \\quad \\forall x\\in G\\), \\(f(x_0)=0\\).</p> Proof <ul> <li> <p>Sufficient. Assume \\(x_0\\not\\in \\overline{G}\\), then by Corollary 1: generate functional by extend one dimension subspace, there exists a bounded linear functional \\(f\\) such that \\(f(x_0)=1\\) contradicts!</p> </li> <li> <p>Necessary. Choose \\(\\{x_n\\}\\subset G\\), such that \\(x_n\\rightarrow x_0\\), so by continuity of \\(f\\), \\(f(x_0)=\\lim\\limits_{n\\rightarrow \\infty}f(x_n)=0\\).</p> </li> </ul> <p>Corollary 3</p> <p>Assume \\(A\\) is a subset of \\(E\\), \\(x_0\\in E\\). Then \\(x_0\\) could be approximated by combination of elements in \\(A\\), iff for all bounded linear functional \\(f\\), with \\(f(x)=0\\) for \\(x\\in A\\), satisfies \\(f(x_0)=0\\).</p> Proof <p>Here we let \\(G\\) to be the subspace generated by \\(A\\).</p> <p>there exsits \\(\\{x_n\\}\\subset G\\), \\(x_n\\rightarrow x_0\\), iff \\(x_0\\in \\overline{G}\\), by Corollary 2 we are done.</p> <p>Corollary 4</p> <p>Choose \\(G=\\{\\theta\\}\\) and \\(E\\neq \\{\\theta\\}\\), then for \\(x_0\\in E-G\\), there exists bounded linear functional \\(f\\) such that</p> \\[ \\|f\\|=1, \\quad f(x_0)=d(x_0,G)=\\|x_0\\|. \\] <p>For any normed vector space, once \\(E\\neq \\{\\theta\\}\\), then it has sufficient number of functionals.</p> <p>Example. Extenstion of functional on \\(\\mathbb{R}^2\\). Show that the extensions are not unique, and the number must be no less than \\(\\aleph\\).</p>"},{"location":"Math/Functional_Analysis/BL_Operator/#uniform-boundedness-principle","title":"Uniform Boundedness Principle","text":"<p>Banach-Steinhaus Theorem: Uniform Boundedness Principle</p> <p>Assume \\(E\\) is a Banach space and \\(E_1\\) is a normed vector space. \\(\\{T_\\alpha\\}_{\\alpha \\in \\mathscr{A}}\\subset \\mathcal{B}(E,E_1)\\). If for every \\(x\\in E\\), define point-wisely</p> \\[ p(x)=\\sup_{\\alpha \\in \\mathcal{A}}\\|T_\\alpha x\\| \\] <p>which satisfies \\(p(x)&lt;\\infty\\), then \\(\\|T_\\alpha\\|&lt;\\infty\\) for all \\(\\alpha \\in \\mathcal{A}\\), i.e. \\(T_\\alpha\\) is uniformly bounded.</p> Proof <p>Now we talk about strong operator topology. The first is about its description.</p> <p></p> <p>Description of strong operator topology</p> <p>Assume \\(E\\) and \\(E_1\\) are Banach spaces, then sequence of bounded linear operators \\(\\{T_n\\}\\) converges in a sense of strong operator topology, iff</p> <p>(i) \\(\\{T_n\\}\\) is uniformly bounded,</p> <p>(ii) There exsits a subset \\(G\\) which is dense in \\(E\\), \\(\\forall x\\in G\\), \\(\\{T_n x\\}\\) is convergent in \\(E_1\\).</p> Proof <p>Now we a description about the convergent point of bounded linear operator, i.e. completeness with respect to strong operator topology.</p> <p>Completeness with respect to strong operator topology</p> <p>Assume \\(E\\) and \\(E_1\\) are banach spaces, then \\(\\mathcal{B}(E,E_1)\\) is complete with respect to strong operator topology.</p> <p>Example. Approximation of integral formula.</p> <p>Assume \\(f\\in C[0,1]\\) is an unknown function, to estimate \\(I(f)=\\int_0^1 f(t)dt\\), we use a sequence of functional on \\(C[0,1]\\)</p> \\[ F_n(f)=\\sum_{k=0}^n f(t^{(n)}_k ) A_k^{(n)} \\] <p>where \\(\\{t^{(n)}_k\\}_{0\\leq k\\leq n}\\) is a partion of \\([0,1]\\). Show that </p> \\[ F_n(f)\\rightarrow I(f),\\quad \\forall f\\in C[0,1]. \\] Proof <p>This is an example using strong operator topology. Using Description of strong operator topology.</p>"},{"location":"Math/Functional_Analysis/Banach_Space/","title":"Banach Space","text":""},{"location":"Math/Functional_Analysis/Banach_Space/#normed-linear-space","title":"Normed Linear Space","text":"<p>We have already learn the definition of linear space with its number domain \\(K\\). Now we introduce a topology in it.</p> <p>Definition of Norm</p> <p>Assume \\(E\\) is a linear space. If \\(\\forall x\\in E\\), there exists a corresponding real number, denoted as \\(\\|x\\|\\), which satisfies</p> <p>(i) Positive &amp; Definite. \\(\\|x\\|\\geq 0\\). And \\(\\|x\\|=0\\), iff \\(x=\\theta\\).</p> <p>(ii) Homogeneous. \\(\\forall \\alpha\\in K\\), \\(\\|\\alpha x\\|=|\\alpha| \\|x\\|\\).</p> <p>(iii) Triangle inequation. \\(\\forall x,y\\in E\\), </p> \\[ \\|x+y\\|\\leq \\|x\\|+\\|y\\|. \\] <p>then we call \\(\\|\\cdot\\|\\) norm and \\(E\\) is normed linear space.</p> <p>Readers could check norm of finite-dimensional linear space in Numerical Analysis.</p> <p>If we define </p> \\[ \\rho(x,y)=\\|x-y\\| \\] <p>then it is easy to see the above defines a metric in \\(E\\). With metric, all the conclusions in previous chapter Metric Space could apply here. The following is about convergence.</p> <p>Properties of Norm</p> <p>(i) As a functional \\(\\|x\\|\\) is continuous.</p> <p>(ii) If \\(\\{x_n\\}\\subset E\\) converges to \\(x\\in E\\), then \\(\\{\\|x_n\\|\\}\\) is bounded.</p> <p>The linear operations are continuous:</p> <p>(iii) Assume \\(\\{x_n\\}, \\{y_n\\}, x,y\\in E\\), and \\(x_n\\rightarrow x\\) and \\(y_n\\rightarrow y\\), respectively, then </p> \\[ x_n+y_n\\rightarrow x+y. \\] <p>(iv) Assume \\(\\{\\alpha_n\\}, \\alpha\\in K\\), and \\(\\alpha_n\\rightarrow \\alpha\\). And \\(\\{x_n\\}, x\\in E\\), and \\(x_n\\rightarrow x\\). Then we have</p> \\[ \\alpha_n x_n\\rightarrow \\alpha x. \\] Proof <p>(i) Since \\(\\{x_n\\}\\rightarrow x\\), and by triangle inequation</p> \\[ |\\|x_n\\|-\\|x\\||\\leq \\|x_n-x\\|. \\] <p>(ii) From above we have</p> \\[ \\|x_n\\|\\leq \\|x_n-x\\|+\\|x\\|&lt;M. \\] <p>(iii) By</p> \\[ \\|x_n+y_n-x-y\\|\\leq \\|x_n-x\\|+\\|y_n-y\\|. \\] <p>(iv) By \\(|\\alpha_n|\\) is bounded</p> \\[ \\|\\alpha_n x_n-\\alpha x\\|\\leq \\|\\alpha_n x_n-\\alpha_n x\\|+\\|\\alpha_n x-\\alpha x\\|\\leq |\\alpha_n| \\|x_n-x\\|+|\\alpha_n-\\alpha|\\|x\\|. \\] <p><p>\\(\\square\\)</p></p> <p>A normed linear space is called Banach Space if it is complete.</p> <p>Example. Assume space \\(C^k[a,b]\\) is composed by functions with \\(k\\)-th continuous derivatives. Define norm</p> \\[ \\|x\\|=\\sum_{i=0}^k \\max_{t\\in [a,b]}|x^{(i)}(t)|. \\] <p>Then in this norm the above space is Banach space.</p> Proof <p>It is easy to show that norm satisfies triangle inequation.</p> <p>We now show it is complete. For all Cauchy sequence \\(\\{x_n\\}\\subset C^k[a,b]\\), \\(\\forall \\varepsilon\\), \\(\\exists N&gt;0\\), \\(\\forall n,m&gt;N\\), </p> \\[ \\begin{align} \\|x_n-x_m\\|=\\sum_{i=0}^k \\max_{t\\in [a,b]}|x_n^{(i)}(t)-x_n^{(i)}(t)|&lt;\\varepsilon.\\label{Cauchy-continuous-k} \\end{align} \\] <p>So for each \\(i\\), we have  </p> \\[ |x_n^{(i)}(t)-x_m^{(i)}(t)|&lt;\\varepsilon,\\quad \\forall t\\in [a,b]. \\] <p>So \\(x_n^{(i)}\\) is a Cauchy sequence, by mathematical analysis, we have a convergent function \\(x_i(t), (i=0,\\cdots,k)\\), such that </p> \\[ x_n^{(i)}(t)\\rightrightarrows x_i(t),\\quad i=0,1,\\cdots,k \\] <p>and satisfy \\(x^{(i)}(t)=x_i(t)\\) for each \\(i\\). So \\(x(t)\\in C^k[a,b]\\). We show that \\(x_n(t)\\) converges to \\(x(t)\\) in this norm.</p> <p>In equation \\(\\ref{Cauchy-continuous-k}\\), fix \\(n\\), let \\(m\\rightarrow \\infty\\), and we have the result. </p> <p><p>\\(\\square\\)</p></p> <p>Example. \\(L^p(E)\\) is a Banach space.</p> Proof <ul> <li>From a Cauchy sequence in this metric to get a Cauchy sequence in measure. </li> </ul> <p>For all Cauchy sequence \\(\\{x_n\\}\\), \\(\\forall \\varepsilon, \\sigma&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n,m&gt;N\\),</p> \\[ \\begin{align*} \\varepsilon&gt;&amp;\\|x_n,x_m\\|\\\\ &amp;=\\left(\\int_E |x_n(t)-x_m(t)|^p dt\\right)^{\\frac{1}{p}}\\\\ &amp;\\geq \\left(\\int_{\\{|x_n(t)-x_m(t)|\\geq \\sigma\\}} |x_n(t)-x_m(t)|^p dt\\right)^{\\frac{1}{p}}\\\\ &amp;\\geq \\left(\\int_{\\{|x_n(t)-x_m(t)|\\geq \\sigma\\}} \\sigma^p dt\\right)^{\\frac{1}{p}}\\\\ &amp;\\geq \\sigma m(\\{|x_n(t)-x_m(t)|\\geq \\sigma\\})^{\\frac{1}{p}} \\end{align*} \\] <p>which means \\(x_n\\) is a Cauchy sequence in measure. </p> <ul> <li>From a Cauchy sequence in measure to get a convergent function in usual metric \\(||\\).</li> </ul> <p>One method is, by what we have learned, there exists a finite-valued function \\(x\\) a.e. on \\(E\\) such that \\(x_n\\) converges to \\(x\\) in measure. By Riesz Theorem, there exists a subsequence \\(\\{x_{n_k}\\}\\) such that \\(x_{n_k}\\) converges to \\(x\\) a.e. on \\(E\\).</p> <p>Or we take another version inspired by Lemma for summation in \\(L^p\\) space. Since \\(x_n\\) is a Cauchy sequence in \\(L^p(E)\\), \\(\\forall k&gt;0\\), there exists a subsequence \\(x_{n_k}\\) such that </p> \\[ \\left(\\int_E|x_{n_k}-x_{n_{k-1}}|^p dt\\right)^\\frac{1}{p}=\\|x_{n_k}-x_{n_{k-1}}\\|&lt;\\frac{1}{2^k} \\] <p>So by H\u00f6lder inequation, \\(1\\in L^q\\), \\(q=\\frac{p}{p-1}\\),</p> \\[ \\int_E |x_{n_k}-x_{n_{k-1}}| \\cdot 1 dt\\leq (\\int_E |x_{n_k}-x_{n_{k-1}}|^p dt)^\\frac{1}{p}m(E)^\\frac{1}{q}&lt;\\frac{1}{2^k} m(E)^\\frac{1}{q} \\] <p>So by Levy Theorem, \\(f_n=\\sum_{k=1}^n |x_{n_k}-x_{n_{k-1}}|\\) converges to \\(x\\).</p> \\[ \\begin{align*} \\int_E \\sum_{k=1}^\\infty |x_{n_k}-x_{n_{k-1}}| dt &amp;= \\lim_{k\\rightarrow \\infty}\\int_E \\sum_{k=1}^K |x_{n_k}-x_{n_{k-1}}| dt \\\\ &amp;\\leq \\lim_{k\\rightarrow \\infty} \\sum_{k=1}^K \\int_E |x_{n_k}-x_{n_{k-1}}| dt&lt;\\infty. \\end{align*} \\] <p>So \\(x_n=\\sum_{k=1}^\\infty x_{n_k}- x_{n_{k-1}}+x_i\\) converges to \\(x\\). </p> <ul> <li>Use Fatau theorem to change the order of limit &amp; integral.</li> </ul> <p>By Fatau theorem, \\(\\forall n_k&gt;N\\), we still have \\(\\left(\\int_E |x_n(t)-x_{n_k}(t)|^p dt\\right)^{\\frac{1}{p}}&lt;\\varepsilon\\), so</p> \\[ \\begin{align*} \\int_E |x_n-x|^pdt &amp;= \\int_E \\lim_{k\\rightarrow \\infty}|x_n-x_{n_k}|^pdt\\\\ &amp;\\leq \\underline{\\lim}\\limits_{k\\rightarrow \\infty} \\int_R |x_n-x_{n_k}|^pdt\\\\ &amp;\\leq \\varepsilon^p. \\end{align*} \\] <p>So \\(x_n\\) converges to \\(x\\) in the above metric. Then by Minkowski inequation, we have \\(\\|x\\|\\leq \\|x_n-x\\|+\\|x_n\\|&lt;\\infty\\), so \\(x\\in L^p(E)\\).</p>"},{"location":"Math/Functional_Analysis/Banach_Space/#quotient-space","title":"Quotient Space","text":"<p>Equivalence of two points</p> <p>Assume \\(X\\) is a linear space, \\(F\\subset X\\) is a subspace. Two points \\(x_1, x_2\\in X\\) are called equivalent modulo \\(F\\), denoted as \\(x_1\\equiv x_2 (\\text{ mod } F)\\), or simply \\(x_1\\sim x_2\\), if \\(x_1-x_2\\in F\\). </p> <p>Properties of Equivalence</p> <p>(i) Reflexive. \\(x\\sim x\\).</p> <p>(ii) Symmetric. If \\(x\\sim y\\), then \\(y\\sim x\\).</p> <p>(iii) Tansitive. If \\(x\\sim y\\), \\(y\\sim z\\), then \\(x\\sim z\\).</p> <p>The above property tells us equivalence mod \\(F\\) is an equivalence relation, meaning we could divide \\(X\\) into distinct equivalence classes mod \\(F\\), denoted as </p> \\[ \\xi:=x+F=\\{x+y: y\\in F\\},\\text{ where } x\\in \\xi. \\] <p>The whole equivalence relation compose a set \\(X / F\\). If we define proper linear operations on \\(X/F\\), we could show that it is also a linear space.</p> <p>Linear operations on Space \\(X/F\\)</p> <p>(i) \\(\\forall \\xi,\\eta\\in X/F\\), choose \\(x\\in \\xi, y\\in \\eta\\), define</p> \\[ \\xi+\\eta:=\\{x+y+z: z\\in F\\}=x+y+F. \\] <p>(ii) \\(\\forall \\xi\\in X/F\\), \\(\\forall \\alpha\\in K\\), define</p> \\[ \\alpha \\xi:=\\{\\alpha x + y: y\\in F\\}= \\alpha x+F. \\] <p>we have to show that the above definition is irrelevant with the choice of \\(x\\).</p> Proof <p>\\(\\forall x'\\in \\xi,y'\\in \\eta\\), </p> \\[ \\begin{align*} x'+y'+F&amp;=x+(x'-x)+y+(y'-y)+F\\\\ &amp;=x+y+(x'-x)+(y'-y)+F\\\\ &amp;=x+y+F. \\end{align*} \\] <p>Similar to \\(\\alpha x'\\).</p> <p><p>\\(\\square\\)</p></p> <p>Easy to see that \\(X/F\\) is a linear space based on the above linear operations. And its zero element is \\(F\\).</p> <p>Provided \\(F\\) is closed, quotient space \\(X/F\\) could be a normed linear space.</p> <p>Normed linear space for \\(X/F\\)</p> <p>Assume \\(F\\) is a closed subspace of a normed linear space \\(X\\). For any equivalence class \\(\\xi\\in X/F\\), we define</p> \\[ \\|\\xi\\|=\\inf_{x\\in \\xi}\\|x\\|. \\] <p>then the above definition is a norm and \\(X/F\\) is a normed linear space.</p> Proof <p>(i) Positive and definive.</p> <p>Positive is apparent. We prove definitive. Let \\(\\|\\xi\\|=0\\), if \\(\\xi=x_0+F\\), \\(x_0\\not\\in F\\). Then by definition of infimum, \\(\\forall \\varepsilon_n=\\frac{1}{n}\\), \\(\\exists \\{y_n\\}\\subset F\\) such that</p> \\[ \\|x_0+y_n\\|&lt;\\frac{1}{n} \\] <p>which means \\(y_n\\rightarrow -x_0\\). Since \\(F\\) is closed, \\(-x_0\\in F\\), so by linear operation, \\(x_0\\in F\\), which contradicts! So \\(\\xi=F\\).</p> <p>(ii) Homogeneous. Easy to show.</p> <p>(iii) Triangle inequation. \\(\\forall \\{x_j\\}, \\{y_j\\}\\in X/F\\), \\(\\forall \\varepsilon&gt;0\\), we have </p> \\[ \\|x_j\\|\\leq \\|\\{x_j\\}\\|+\\varepsilon, \\quad |y_j\\|\\leq \\|\\{y_j\\}\\|+\\varepsilon. \\] <p>So</p> \\[ \\|\\{x_j\\}+ \\{y_j\\}\\|\\leq \\|x_j+y_j\\|\\leq \\|x_j\\|+\\|y_J\\|\\leq \\|\\{x_j\\}\\|+ \\|\\{y_j\\}\\|+2\\varepsilon. \\] <p>Let \\(\\varepsilon\\rightarrow 0\\), and we are done. <p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/Hilbert_Space/","title":"Hilbert Space","text":"<p>We first introduce inner product in abstract space.</p> <p>Inner Product</p> <p>Assume \\(\\mathscr{U}\\) is a linear space on number field \\(\\mathbb{K}\\) (i.e. \\(\\mathbb{R}\\) or \\(\\mathbb{C}\\)). A mapping </p> \\[ (\\cdot, \\cdot): \\mathscr{U}\\times \\mathscr{U} \\mapsto \\mathbb{K} \\] <p>is said to be a Inner Product, if it satisfies that, for all \\(x,y,z\\in \\mathscr{U}\\), for all \\(\\alpha \\in \\mathbb{K}\\),</p> <p>(i) Homogeneous on the first element. \\((\\alpha x,y)=\\alpha(x,y)\\).</p> <p>(ii) Linearity on the first element. \\((x+z,y)=(x,y)+(z,y)\\).</p> <p>(iii) Conjugate inverse. \\((x,y)=\\overline{(y,x)}\\).</p> <p>(iv) Positive-definite. \\((x,x)\\geq 0\\) and \\((x,x)=0\\Leftrightarrow x=\\theta\\).</p> <p>And A linear space \\(\\mathscr{U}\\) with inner product is called Inner Product Space.</p> <p>With the above definition, we have the following properties of inner product.</p> <p>Properties of inner product</p> <p>Assume \\(\\mathscr{U}\\) is an inner product space on number field \\(\\mathbb{K}\\). Then \\(x,y,z\\in \\mathscr{U}\\), for all \\(\\alpha \\in \\mathbb{K}\\),</p> <p>(i) Conjugate Homogeneous on the second element. \\((x,\\alpha y)=\\overline{\\alpha}(x, y)\\).</p> <p>(ii) Linearity on the second element. \\((x,y+z)=(x,y)+(x,z)\\).</p> <p>(iii) Zero for null vector. \\((x,\\theta)=(x,0\\cdot x)=0(x,x)=0\\).</p> <p>(iv) Normed linear space. Define \\(\\|x\\|=\\sqrt{(x,x)}\\), then it is a norm.</p> <p>(v) \\((x,y)\\) is a continuous function with respect to both \\(x,y\\).</p> ProofProof for (v) <p>Easy to show that (i), (ii), (iii).</p> <p>We prove for (iv) and the triangle inequation. This is based on Schwarz Inequation.</p> \\[ |(x,y)|^2\\leq \\|x\\|^2 \\|y\\|^2 \\] <p>We still make use of \\(\\forall \\lambda\\in \\mathbb{K}\\)</p> \\[ \\begin{align*} 0 &amp;\\leq \\|x+\\lambda y\\|^2\\\\ &amp; = (x+\\lambda y,x+\\lambda y)\\\\ &amp;= \\|x\\|^2+ \\overline{\\lambda}(x,y)+ \\lambda(y,x) + |\\lambda|^2\\|y\\|^2\\\\ &amp;= \\|x\\|^2 + \\lambda(y,x) + \\overline{\\lambda}[ (x,y) + \\lambda \\|y\\|^2] \\end{align*} \\] <p>Assume \\((y,y)\\neq 0\\), and choose \\(\\lambda=-\\frac{(x,y)}{\\|y\\|^2}\\), we have</p> \\[ \\begin{align*} 0&amp;\\leq \\|x\\|^2 - \\frac{(x,y)}{\\|y\\|^2} (y,x)\\\\ |(x,y)|^2&amp;\\leq \\|x\\|^2 \\|y\\|^2 \\end{align*} \\] <p>With Schwarz inequation, we have</p> \\[ \\begin{align*} \\|x+y\\|^2 &amp;= |(x+y,x+y)|\\\\ &amp;=|(x+y,y)+(x+y,x)|\\\\ &amp;\\leq |(x+y,y)|+|(x+y,x)|\\\\ &amp;\\leq \\|x+y\\|\\|y\\| + \\|x+y\\|\\|x\\|\\quad \\text{Schwarz Inequation} \\end{align*} \\] <p>and we are done.</p> <p></p> <p>Polarization Identity</p> <p>(i) For \\(\\mathbb{K}=\\mathbb{R}\\), we have</p> \\[ (x,y)=\\frac{1}{4}(\\|x+y\\|^2-\\|x-y\\|^2). \\] <p>(ii) For \\(\\mathbb{K}=\\mathbb{C}\\), we have</p> \\[ (x,y)=\\frac{1}{4}(\\|x+y\\|^2-\\|x-y\\|^2 + i \\|x+iy\\|^2 - i\\|x-iy\\|^2). \\] Proof <p>(i) Just expand the righthand item in inner product.</p> <p>(ii) Still the same logic, but </p> \\[ \\|x+y\\|^2-\\|x-y\\|^2=2(x,y)+2\\overline{(x,y)}=4Re[(x,y)]. \\] <p>We notice \\(Re[(x,iy)]=Re[-i(x,y)]=Im[(x,y)]\\), so apply \\(y=iy\\) in the above equation and we have</p> \\[ 4Im[(x,y)]=4Re[(x,iy)]=\\|x+iy\\|^2-\\|x-iy\\|^2. \\] <p><p>\\(\\square\\)</p></p> <p>Hilbert Space</p> <p>If a inner product space \\(\\mathscr{U}\\) is complete, then it is called Hilbert Space.</p> <p>We have known that a inner product space must be a normed linear space, then how about the inverse? Generally speaking, a normed linear space could not be a inner product space, i.e. use the inner product to deduce a norm, unless we add the following condition.</p> <p></p> <p>Parallelogram Law</p> <p>If a norm satisfies </p> \\[ \\|x+y\\|^2+\\|x-y\\|^2=2(\\|x\\|^2+\\|y\\|^2) \\] <p>then we could define a inner product using polarization identity.</p> <p>Inversely speaking, a norm introduced by a norm satisfy the above condition, which is called Parallelogram Law.</p> ProofLemmaProof for Necessary <ul> <li>Sufficient.</li> </ul> <p>Just expand lefthand of the equation using inner product and we have the result.</p> <ul> <li>Necessary.</li> </ul> <p>We first show that the inner product defined by Polar equation satisfies linearity.</p> \\[ \\begin{align*} (x,z)+(y,z)&amp;=\\frac{1}{4}(\\|x+z\\|^2-\\|x-z\\|^2 + i \\|x+iz\\|^2 - i\\|x-iz\\|^2)\\\\ &amp;+\\frac{1}{4}(\\|y+z\\|^2-\\|y-z\\|^2 + i \\|y+iz\\|^2 - i\\|y-iz\\|^2)\\\\ &amp;=\\frac{1}{8}(\\|x+y+2z\\|^2-\\|x+y-2z\\|^2+i\\|x+y+i2z\\|^2-i \\|x+y-i2z\\|^2)\\\\ &amp;=\\frac{1}{2}(\\|\\frac{x+y}{2}+z\\|^2-\\|\\frac{x+y}{2}-z\\|^2+i\\|\\frac{x+y}{2}+iz\\|^2-i \\|\\frac{x+y}{2}-iz\\|^2)\\\\ &amp;=2(\\frac{x+y}{2},z)\\\\ &amp;=\\frac{1}{2}(x+y,2z)\\quad\\text{this is not good}. \\end{align*} \\] <p>Let \\(y=\\theta\\), we have \\((x,z)=2(\\frac{x}{2},z)\\). Choose \\(x=x+y\\), we have \\((x+y,z)=2(\\frac{x+y}{2}, z)\\). Comparing with before and we have</p> \\[ (x,z)+(y,z)=(x+y,z). \\] <p>So let \\(y=y-x\\), we have </p> \\[ (x,z)+(y-x,z)=(y,z)\\Rightarrow (y-x,z)=(y,z)-(x,z) \\] <p>which means \"+\" and \"-\" satisfy.</p> <p>This is a little tricky, and we have to show a lemma.</p> <p>For a continuous function \\(f(\\alpha)\\) defined on \\(\\mathbb{R}\\), which satisfies </p> \\[ f(\\alpha_1+\\alpha_2)=f(\\alpha_1)+f(\\alpha_2),\\quad \\forall \\alpha_1,\\alpha_2\\in \\mathbb{R}, \\] <p>then \\(\\forall \\alpha\\in \\mathbb{R}\\), we have</p> \\[ f(\\alpha)=\\alpha f(1). \\] <p>Let </p> \\[ f(\\alpha)=(\\alpha x,z). \\] <p>it is continuous because</p> \\[ \\begin{align*} |f(\\alpha)-f(\\alpha_0)|&amp;=|((\\alpha-\\alpha_0)x,z)|\\quad \\text{subtraction}\\\\ \\end{align*} \\] <p>For \\(\\alpha\\rightarrow \\alpha_0\\), \\((\\alpha-\\alpha_0)x\\rightarrow 0x=\\theta\\), so the lefthand of the above equation tends to \\(0\\).</p> <p>And </p> \\[ f(\\alpha_1+\\alpha_2)=((\\alpha_1+\\alpha_2)x, z)=(\\alpha_1 x+\\alpha_2 x, z)=(\\alpha_1 x, z)+(\\alpha_2 x, z), \\] <p>so we have</p> \\[ (\\alpha x,z)=\\alpha(x,z),\\quad \\forall \\alpha\\in \\mathbb{R}, \\] <p>which is homogeneity.</p> <p>As for conjugate property and positive-definitivity, just follow the definition.</p>"},{"location":"Math/Functional_Analysis/Hilbert_Space/#orthogonal-system","title":"Orthogonal System","text":"<p>Orthogomal System</p> <p>Assume \\(\\mathscr{U}\\) is a inner product space, \\(x,y\\in \\mathscr{U}\\). If \\((x,y)=0\\), then we call \\(x\\) is orthogamal to \\(y\\), denoted by \\(x\\perp y\\). If a subset \\(M\\subset \\mathscr{U}\\), \\(\\forall y\\in M\\), \\(x\\perp y\\), then we call \\(x\\) is orthogonal to \\(M\\), denoted by \\(x\\perp M\\). If \\(N\\subset \\mathscr{U}\\), \\(\\forall x\\in N\\), \\(x\\perp M\\), then we call \\(N\\) is orthogonal to \\(M\\), denoted by \\(N\\perp M\\).</p> <p>An orthogonal complement of \\(M\\) is denoted by</p> \\[ M^\\perp=\\{x\\in \\mathscr{U}: x\\perp M\\}. \\] <p>Property of Orthogomal</p> <p>(i) Pythagorean Theorem. For \\(x_1,\\cdots,x_n\\) which are mutually orthogonal, then \\(x:=x_1+\\cdots+x_n\\), we have</p> \\[ \\|x\\|^2=\\sum_{i=1}^n \\|x_i\\|^2. \\] <p>(ii) Dense set with Zero Vector. Assume \\(L\\subset \\mathscr{U}\\) is a dense subset, if \\(x\\in \\mathscr{U}\\) is orthogonal to \\(L\\), then \\(x=0\\). </p> <p>(iii) For \\(M\\subset \\mathscr{U}\\), \\(M^\\perp\\) is a closed subset.</p>"},{"location":"Math/Functional_Analysis/Hilbert_Space/#orthogomal-projection","title":"Orthogomal Projection","text":"<p>We introudce orthogonal projection using the best approximation element.</p> <p>Definition of best Approximation element</p> <p>Assume \\(M\\subset \\mathscr{U}\\), \\(x\\in \\mathscr{U}\\), \\(y\\in \\mathscr{U}\\) is called the best approximation element of \\(x\\) in \\(M\\), if  </p> \\[ \\|x-y\\|=\\inf_{z\\in M}\\|x-z\\| \\] <p>Here we give a sufficient condition for the existence and uniqueness of best approximation element.</p> <p>Theorem for existence and uniqueness of best approximation element</p> <p>Assume \\(M\\subset \\mathscr{U}\\), \\(x\\in \\mathscr{U}\\). If \\(M\\) is a closed convex set, then there exists a unique best approximation element \\(y\\) of \\(x\\) in \\(M\\).</p> Proof <p>This proof leverages the convex property.</p> <p>From the above theorem, A subspace is naturally convex, thus adding closedness could satisfy the above condition. So </p> <p>Orthogomal decomposition and projection</p> <p>Assume \\(M\\subset \\mathscr{U}\\) is a closed subspace, then \\(\\forall x\\in \\mathscr{U}\\), \\(\\exists! y\\in M, z\\in M^\\perp\\), such that </p> \\[ x=y+z. \\] <p>the above equation is called orthogonal decomposition, and \\(y\\) is called the orthogonal projection of \\(x\\) in \\(M\\).</p>"},{"location":"Math/Functional_Analysis/Hilbert_Space/#normalized-orthogomal-system","title":"Normalized Orthogomal System","text":"<p>Definition of Normalized Orthogomal System</p> <p>System \\(\\{e_n\\}_{n\\geq 1}\\subset \\mathscr{U}\\) is called Normalized Orthogomal System, if it satisfies</p> \\[ (e_n,e_m)=\\begin{cases}1,\\quad n=m,\\\\ 0,\\quad n\\neq m.\\end{cases} \\] <p>For a element \\(x\\in \\mathscr{U}\\), define </p> \\[ c_n=(x,e_n) \\] <p>to be the Fourier coefficient of \\(x\\). They also form a system \\(\\{c_n\\}\\).</p> <p>Now we talk a little deeper into this one.</p> <p>Explicite expression of orthogonal decomposition</p> <p>Assume \\(n\\) is given, \\(\\{e_1,\\cdots, e_n\\}\\) is a finite-number normalized orthogonal system. Let \\(M=\\text{span}(e_1,\\cdots,e_n)\\), then \\(\\forall x\\in \\mathscr{U}\\), we have the orthogonal projection of \\(x\\) on \\(M\\)</p> \\[ x=\\sum_{i=1}^n (x,e_n)e_n \\] Proof <p>First use Orthogomal decomposition and projection to show that </p> \\[ x=y+z,\\quad y\\in \\text{space}(e_1,\\cdots,e_n). \\] <p>So by property of base, \\(y=\\sum_{i=1}^n a_n e_n\\). Making a inner product with \\(e_j\\) on both sides gives</p> \\[ a_j=(y,e_j) \\] <p>So we are done.</p> <p>Bessel Inequation</p> <p>For a normalized orthogonal system \\(\\{e_n\\}\\subset \\mathscr{U}\\), \\(x\\in \\mathscr{U}\\) with its Fourier coefficients \\(\\{c_n\\}\\), we have</p> \\[ \\sum_{n=1}^\\infty \\|(x,e_n)\\|^2\\leq \\|x\\|^2. \\] <p>or \\(\\{e_n\\}\\subset l^2\\).</p> Proof <p>Use finite many elements and then let \\(n\\rightarrow\\infty\\).</p> <p>Inversely speaking, given a sequence in \\(\\{c_n\\}\\subset l^2\\), could we find a element in \\(x\\in \\mathscr{U}\\) such that \\(c_n\\) is its fourier coefficient? It turns out that we need to add another condition.</p> <p></p> <p>Riesz-Fischer Theorem</p> <p>Assume \\(\\mathscr{U}\\) is a Hilbert space, \\(\\{e_n\\}\\) are its normalized orthogonal system, number sequence \\(\\{c_n\\}\\subset l^2\\). Then there exists \\(x\\in \\mathscr{U}\\), such that for each \\(n\\), we have</p> \\[ c_n=(x, e_n) \\] <p>and Parseval equation holds</p> \\[ \\sum_{i=1}^\\infty \\|c_n\\|^2=\\sum_{i=1}^\\infty \\|(x, e_n)\\|^2= \\|x\\|^2 \\] Proof <p><p>\\(\\square\\)</p></p> <p>Now we pause here. It is interesting to see that Bessel inequation holds for all inner product space, but Parseval equation does not also hold. We dig deeper into this and this is actually related to the number of \\(\\{e_n\\}\\). For a Hilbert space, its number is enough to let \"=\" hold.</p>"},{"location":"Math/Functional_Analysis/Hilbert_Space/#completeness","title":"Completeness","text":"<p>Definition for completeness of orthogonal system</p> <p>Assume \\(\\mathscr{U}\\) is a inner product space. An normalized orthogonal system is said to be complete, if for all \\(x\\in\\mathscr{U}\\), Parseval equation holds; it is said to be total, if it is dense in \\(\\mathscr{U}\\).</p> <p>Relationship</p> <p>Assume \\(\\mathscr{U}\\) is a Hilbert space. \\(\\{e_n\\}\\) is a normalized orthogonal system. Then we have the following equivalence.</p> <p>(i) \\(\\{e_n\\}\\) is complete.</p> <p>(ii) \\(\\forall x\\in \\mathscr{U}\\), series \\(\\sum_{i=1}^n (x,e_i)e_i\\) converges to \\(x\\), i.e.</p> \\[ x=\\sum_{n=1}^\\infty (x,e_n)e_n. \\] <p>(iii) \\(\\forall x,y\\in \\mathscr{U}\\), </p> \\[ (x,y)=\\sum_{n=1}^\\infty (x,e_n)\\overline{(y,e_n)}. \\] <p>(iv) \\(\\{e_n\\}\\) is total.</p> (i) \\(\\Rightarrow\\) (ii)(ii) \\(\\Rightarrow\\) (iii)(iii) \\(\\Rightarrow\\) (iv)(iv) \\(\\Rightarrow\\) (i) <p>(i) and (ii) are equivalent because</p> \\[ \\left\\| x-\\sum_{n=1}^N (x,e_n)e_n \\right\\|^2= \\|x\\|^2-\\sum_{n=1}^N |(x,e_n)|^2 \\] <p>Using equation from (ii) we only get infinite series. We start from finite-numbers. Define </p> \\[ x_n=\\sum_{i=1}^n  (x,e_i)e_i,\\quad y_n=\\sum_{j=1}^n  (y,e_j)e_j, \\] <p>then</p> \\[ \\begin{align*} (x_n,y_n)&amp;=\\left( \\sum_{i=1}^n (x,e_i)e_i, \\sum_{j=1}^n (y,e_j)e_j\\right)\\\\ &amp;=\\sum_{i=1}^n(x, e_n)\\overline{(y,e_n)}. \\end{align*} \\] <p>Let \\(n\\rightarrow \\infty\\), since by h\u00f6lder inequation</p> \\[ \\left(\\sum_{i=1}^\\infty |(x, e_n)\\overline{(y,e_n)}|\\right)^2\\leq \\left(\\sum_{i=1}^\\infty |(x,e_n)|^2 \\right)\\left(\\sum_{i=1}^\\infty |(y,e_n)|^2\\right) \\] <p>by (ii) the above converges. By continuity of inner product, we are done. </p> <p>Assume \\(\\forall x\\in \\mathscr{U}\\), \\((x,e_n)=0\\). So \\(\\forall y\\in \\mathscr{U}\\)</p> \\[ (x,y)=\\sum_{n=1}^\\infty (x,e_n)\\overline{(y,e_n)}=0. \\] <p>Choose \\(y=x\\) and \\((x,x)=0\\) implies \\(x=\\theta\\).</p> <p>\\(\\forall x\\in \\mathscr{U}\\), by Bessel ineuqation, \\(\\sum_{n=1}^\\infty |(x,e_n)|^2&lt;\\infty\\). For number sequence \\(\\{(x,e_n)\\}\\subset l^2\\), by Riesz-Fischer Theorem, \\(\\exists y\\in \\mathscr{U}\\), such that \\((x,e_n)=(y,e_n)\\) for each \\(n\\). So \\((x-y,e_n)=0\\) for each \\(n\\), by (iv) assumption, we have \\(x-y=\\theta\\), i.e. \\(x=y\\) and \\(x\\) satisfies</p> \\[ \\|x\\|^2=\\|y\\|^2 =\\sum_{i=1}^\\infty |(x,e_n)|^2.  \\]"},{"location":"Math/Functional_Analysis/Metric_Space/","title":"Metric Space","text":""},{"location":"Math/Functional_Analysis/Metric_Space/#important-equations","title":"Important Equations","text":"<p>Lemma</p> <p>Assume positive real number \\(\\alpha\\), \\(\\beta\\) satisfies conjugate condition \\(\\alpha+\\beta=1\\), then </p> \\[ u^\\alpha v^\\beta\\leq u\\cdot\\alpha+v\\cdot\\beta. \\] Proof <p>Since \\(f(x)=x^\\alpha (0&lt;\\alpha&lt;1)\\) is concave function, so tangent function at any point on the function curve lies above it. Choose \\((1,1)\\), we have tangent function</p> \\[ y=\\alpha(x-1)+1=\\alpha x+1-\\alpha=\\alpha x+\\beta \\] <p>with \\(\\beta=1-\\alpha\\in (0,1)\\). So</p> \\[ x^\\alpha\\leq \\alpha x + \\beta. \\] <p>Let \\(x=\\frac{u}{v}\\), then </p> \\[ \\begin{align*} \\left(\\frac{u}{v}\\right)^\\alpha &amp;\\leq \\alpha \\frac{u}{v} +\\beta\\\\ u^\\alpha\\cdot v^{-\\alpha}&amp;\\leq \\alpha u \\cdot v^{-1}+\\beta\\\\ u^\\alpha\\cdot v^{1-\\alpha}&amp;\\leq \\alpha u +\\beta v\\\\ u^\\alpha\\cdot v^{\\beta}&amp;\\leq \\alpha u +\\beta v. \\end{align*} \\] <p>If we let \\(\\alpha=\\frac{1}{p}\\), \\(\\beta=\\frac{1}{q}\\) in inequation, we have</p> \\[ \\begin{align} u^{\\frac{1}{p}}\\cdot v^{\\frac{1}{q}}\\leq \\frac{1}{p}u+\\frac{1}{q}v, \\quad \\frac{1}{p}+\\frac{1}{q}=1.\\label{conjugate-inequation} \\end{align} \\] <p>H\u00f6lder Inequation</p> <p>Assume \\(p\\), \\(q\\) satisfies conjugate condition \\(\\frac{1}{p}+\\frac{1}{q}=1\\), then </p> <p>(i) For \\(\\{\\xi_n\\}_{n\\geq 1}\\) and \\(\\{\\eta_n\\}_{n\\geq 1}\\), \\(\\xi_n,\\eta_n\\in \\mathbb{C}\\) we have</p> \\[ \\sum_{n=1}^\\infty |\\xi_n\\eta_n|\\leq \\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^{\\frac{1}{p}}\\cdot \\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)^{\\frac{1}{q}}. \\] <p>(ii) For \\(f\\), \\(g\\) satisfies \\(\\int_E |f(x)|^pdx&lt;\\infty\\), \\(\\int_E |g(x)|^qdx&lt;\\infty\\), we have</p> \\[ \\int_E |f(x)g(x)|dx\\leq \\left(\\int_E |f(x)|^pdx\\right)^{\\frac{1}{p}}\\cdot \\left(\\int_E |g(x)|^qdx\\right)^{\\frac{1}{q}}. \\] Proof for (i)Proof for (ii) <p>The core is normalization of item to be summed.</p> <p>In inequation \\(\\ref{conjugate-inequation}\\), for \\(n=1,2,\\cdots\\) let </p> \\[ u=\\frac{|\\xi_n|^p}{\\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)}, \\quad  v=\\frac{|\\eta_n|^q}{\\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)}, \\] <p>we have</p> \\[ \\frac{|\\xi_n|}{\\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^{\\frac{1}{p}}}\\cdot \\frac{|\\eta_n|}{\\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)^{\\frac{1}{q}}}\\leq \\frac{1}{p}\\frac{|\\xi_n|^p}{\\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)}+\\frac{1}{q}\\frac{|\\eta_n|^q}{\\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)} \\] <p>summing up \\(n=1,2,\\cdots\\), we have</p> \\[ \\begin{align*} \\frac{\\sum_{n=1}^\\infty|\\xi_n\\eta_n|}{\\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^{\\frac{1}{p}}\\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)^{\\frac{1}{q}}} &amp;\\leq \\frac{1}{p}\\frac{\\sum_{n=1}^\\infty|\\xi_n|^p}{\\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)}+\\frac{1}{q}\\frac{\\sum_{n=1}^\\infty|\\eta_n|^q}{\\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)}\\\\ \\frac{\\sum_{n=1}^\\infty|\\xi_n\\eta_n|}{\\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^{\\frac{1}{p}}\\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)^{\\frac{1}{q}}} &amp;\\leq \\frac{1}{p}+\\frac{1}{q}=1\\\\ \\Rightarrow\\quad  \\sum_{n=1}^\\infty|\\xi_n\\eta_n|&amp; \\leq \\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^{\\frac{1}{p}}\\left(\\sum_{n=1}^\\infty |\\eta_n|^q\\right)^{\\frac{1}{q}}. \\end{align*} \\] <p>In inequation \\(\\ref{conjugate-inequation}\\), let </p> \\[ u=\\frac{|f(x)|^p}{\\left(\\int_E |f(x)|^p dx\\right)}, \\quad  v=\\frac{|g(x)|^q}{\\left(\\int_E |g(x)|^q dx\\right)}, \\] <p>we have</p> \\[ \\frac{|f(x)|}{\\left(\\int_E |f(x)|^p dx\\right)^{\\frac{1}{p}}}\\cdot \\frac{|g(x)|}{\\left(\\int_E |g(x)|^q dx\\right)^{\\frac{1}{q}}}\\leq \\frac{1}{p}\\frac{|f(x)|^p}{\\left(\\int_E |f(x)|^p dx\\right)}+\\frac{1}{q}\\frac{|g(x)|^q}{\\left(\\int_E |g(x)|^q dx\\right)} \\] <p>integrate on \\(E\\) on both sides, we have</p> \\[ \\begin{align*} \\frac{\\int_E |f(x)g(x)|dx}{\\left(\\int_E |f(x)|^p dx\\right)^{\\frac{1}{p}}\\left(\\int_E |g(x)|^q dx\\right)^{\\frac{1}{q}}}&amp;\\leq \\frac{1}{p}\\frac{\\int_E |f(x)|^pdx}{\\left(\\int_E |f(x)|^p dx\\right)}+\\frac{1}{q}\\frac{\\int_E |g(x)|^qdx}{\\left(\\int_E |g(x)|^q dx\\right)}\\\\ \\frac{\\int_E |f(x)g(x)|dx}{\\left(\\int_E |f(x)|^p dx\\right)^{\\frac{1}{p}}\\left(\\int_E |g(x)|^q dx\\right)^{\\frac{1}{q}}}&amp;\\leq \\frac{1}{p}+\\frac{1}{q}=1\\\\ \\Rightarrow \\quad \\int_E |f(x)g(x)|dx &amp;\\leq \\left(\\int_E |f(x)|^p dx\\right)^{\\frac{1}{p}}\\left(\\int_E |g(x)|^q dx\\right)^{\\frac{1}{q}}. \\end{align*} \\] <p></p> <p>Minkowski Inequation</p> <p>Assume \\(p\\), \\(q\\) satisfies conjugate condition \\(\\frac{1}{p}+\\frac{1}{q}=1\\), then </p> <p>(i) For \\(\\{\\xi_n\\}_{n\\geq 1}\\) and \\(\\{\\eta_n\\}_{n\\geq 1}\\), \\(\\xi_n,\\eta_n\\in \\mathbb{C}\\), we have</p> \\[ \\left(\\sum_{n=1}^\\infty |\\xi_n+\\eta_n|^p\\right)^{\\frac{1}{p}} \\leq \\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^{\\frac{1}{p}} + \\left(\\sum_{n=1}^\\infty |\\eta_n|^p\\right)^{\\frac{1}{p}} \\] <p>(ii) For \\(f\\), \\(g\\) satisfies \\(\\int_E |f(x)|^pdx&lt;\\infty\\), \\(\\int_E |g(x)|^qdx&lt;\\infty\\), we have</p> \\[ \\left(\\int_E |f(x)+g(x)|^pdx\\right)^{\\frac{1}{p}}\\leq \\left(\\int_E |f(x)|^pdx\\right)^{\\frac{1}{p}}+\\left(\\int_E |g(x)|^pdx\\right)^{\\frac{1}{p}} \\] Proof for (i)Proof for (ii) <p>The core is to partition one part and use triangle inequation. Conjugate indices would help in the end.</p> \\[ \\begin{align*} \\left(\\sum_{n=1}^\\infty |\\xi_n+\\eta_n|^p\\right)&amp;=\\sum_{n=1}^\\infty \\left( |\\xi_n+\\eta_n|^{p-1}\\cdot  |\\xi_n+\\eta_n|\\right)\\\\ &amp;\\leq \\sum_{n=1}^\\infty \\left[|\\xi_n+\\eta_n|^{\\frac{p}{q}}\\cdot \\left(|\\xi_n|+|\\eta_n|\\right)\\right]\\\\ &amp;=\\sum_{n=1}^\\infty \\left( |\\xi_n+\\eta_n|^{\\frac{p}{q}}\\cdot |\\xi_n|\\right) + \\sum_{n=1}^\\infty \\left( |\\xi_n+\\eta_n|^{\\frac{p}{q}}\\cdot |\\eta_n|\\right)\\\\ &amp;\\leq \\left( \\sum_{n=1}^\\infty |\\xi_n+\\eta_n|^{\\frac{p}{q}\\cdot q} \\right)^\\frac{1}{q}\\cdot \\left( \\sum_{n=1}^\\infty |\\xi_n|^p\\right)^\\frac{1}{p} + \\left( \\sum_{n=1}^\\infty |\\xi_n+\\eta_n|^{\\frac{p}{q}\\cdot q} \\right)^\\frac{1}{q}\\cdot  \\left(\\sum_{n=1}^\\infty |\\eta_n|^p\\right)^\\frac{1}{p}\\\\ \\Rightarrow \\quad \\left( \\sum_{n=1}^\\infty |\\xi_n+\\eta_n|^{\\frac{p}{q}\\cdot q} \\right)^{1-\\frac{1}{q}} &amp;\\leq \\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^\\frac{1}{p}+\\left(\\sum_{n=1}^\\infty |\\eta_n|^p\\right)^\\frac{1}{p}\\\\ \\left( \\sum_{n=1}^\\infty |\\xi_n+\\eta_n|^{p}\\right)^{\\frac{1}{p}} &amp;\\leq \\left(\\sum_{n=1}^\\infty |\\xi_n|^p\\right)^\\frac{1}{p}+\\left(\\sum_{n=1}^\\infty |\\eta_n|^p\\right)^\\frac{1}{p}\\\\ \\end{align*} \\] \\[ \\begin{align*} \\left(\\int_E |f(x)+g(x)|^p dx\\right)&amp;=\\int_E \\left( |f(x)+g(x)|^{p-1}\\cdot  |f(x)+g(x)|\\right) dx\\\\ &amp;\\leq \\int_E \\left[|f(x)+g(x)|^{\\frac{p}{q}}\\cdot \\left(|f(x)|+|g(x)|\\right)\\right] dx\\\\ &amp;=\\int_E \\left( |f(x)+g(x)|^{\\frac{p}{q}}\\cdot |f(x)|\\right) dx + \\int_E \\left( |f(x)+g(x)|^{\\frac{p}{q}}\\cdot |g(x)|\\right) dx\\\\ &amp;\\leq \\left( \\int_E |f(x)+g(x)|^{\\frac{p}{q}\\cdot q} dx\\right)^\\frac{1}{q}\\cdot \\left( \\int_E |f(x)|^pdx\\right)^\\frac{1}{p} + \\left( \\int_E |f(x)+g(x)|^{\\frac{p}{q}\\cdot q} dx\\right)^\\frac{1}{q}\\cdot  \\left(\\int_E |g(x)|^pdx\\right)^\\frac{1}{p}\\\\ \\Rightarrow \\quad \\left( \\int_E |f(x)+g(x)|^{\\frac{p}{q}\\cdot q} dx\\right)^{1-\\frac{1}{q}} &amp;\\leq \\left(\\int_E |f(x)|^pdx\\right)^\\frac{1}{p}+\\left(\\int_E |g(x)|^pdx\\right)^\\frac{1}{p}\\\\ \\left( \\int_E |f(x)+g(x)|^{p}dx\\right)^{\\frac{1}{p}} &amp;\\leq \\left(\\int_E |f(x)|^pdx\\right)^\\frac{1}{p}+\\left(\\int_E |g(x)|^pdx\\right)^\\frac{1}{p}\\\\ \\end{align*} \\]"},{"location":"Math/Functional_Analysis/Metric_Space/#basic-concepts","title":"Basic Concepts","text":"<p>\u5ea6\u91cf\u7a7a\u95f4\u7684\u5b9a\u4e49 | Definition of Metric Space</p> <p>Assume \\(M\\) is a set. If there is a function \\(\\rho: M\\times M\\mapsto \\mathbb{R}\\) defined on it, satisfies the following axioms. That is, \\(\\forall x,y,z \\in M\\)</p> <ul> <li>Positive</li> </ul> \\[ \\rho(x,y)&gt;0,\\quad x\\neq y \\] <ul> <li>Definite</li> </ul> \\[ \\rho(x, y)=0 \\Leftrightarrow x=y \\] <ul> <li>Symmetry</li> </ul> \\[ \\rho(x,y) = \\rho(y,x) \\] <ul> <li>Triangle inequality</li> </ul> \\[ \\rho(x,z) \\leq \\rho(x,y) +\\rho(y,z) \\] <p>then \\(M\\) is called metric space with metrc \\(\\rho\\), denoted as \\((M,\\rho)\\).</p> <p>Example. Cases for \\((M,\\rho)\\) with background of Mathematical analysis, show that the following \\(\\rho\\) satisfies axiom of triangle inequation.</p> <p>(i) \\(M=\\mathbb{R}^n\\), \\(\\forall x=(\\xi_1,\\cdots,\\xi_2), y=(\\eta_1,\\cdots,\\eta_n)\\in \\mathbb{R}\\), their metric is defined by </p> \\[ \\rho(x,y)=\\left(\\sum_{i=1}^n|\\xi_i-\\eta_i|^2\\right)^\\frac{1}{2} \\] <p>(ii) \\(M=C[a,b]\\), \\(\\forall f,g\\in C[a,b]\\), their metric is defined by</p> \\[ \\rho(f,g)=\\max_{t\\in[a,b]}|f(t)-g(t)|. \\] Proof for (i)Proof for (ii) <p>Use Cauchy inequation.</p> \\[ \\left(\\sum_{i=1}^n a_ib_i\\right)^2\\leq \\left(\\sum_{i=1}^n a_i^2\\right) \\left(\\sum_{i=1}^n b_i^2\\right) \\] <p>So we have</p> \\[ \\begin{align*} \\sum_{i=1}^n (a_i+b_i)^2&amp;= \\sum_{i=1}^n a_i^2 + 2\\sum_{i=1}^n a_ib_i +\\sum_{i=1}^n b_i^2 \\\\ &amp;\\leq \\sum_{i=1}^n a_i^2 + 2 \\left(\\sum_{i=1}^n a_i^2\\right)^\\frac{1}{2} \\left(\\sum_{i=1}^n b_i^2\\right)^\\frac{1}{2} +\\sum_{i=1}^n b_i^2 \\\\ &amp;=\\left[ \\left(\\sum_{i=1}^n a_i^2\\right)^\\frac{1}{2}+ \\left(\\sum_{i=1}^n b_i^2\\right)^\\frac{1}{2}\\right]^2. \\end{align*} \\] <p>So we lat \\(a_i=\\xi_i-\\zeta_i, b_i=\\zeta_i-\\eta_i\\), we have the triangle inequation.</p> <p>By</p> \\[ \\begin{align*} |x(t)+y(t)|&amp;\\leq |x(t)|+|y(t)|,\\quad t\\in [a,b]\\\\ &amp;\\leq \\max_{t\\in [a,b]} |x(t)|+\\max_{t\\in [a,b]} |y(t)|\\\\ \\Rightarrow \\quad \\max_{t\\in [a,b]} |x(t)+y(t)|&amp;\\leq\\max_{t\\in [a,b]} |x(t)|+\\max_{t\\in [a,b]} |y(t)|. \\end{align*} \\] <p>Example.  Cases for \\((M,\\rho)\\) regarding sequence space, show that the following \\(\\rho\\) satisfies axiom of triangle inequation.</p> <p>(i) \\(M=l^p(1\\leq p&lt;\\infty)\\), i.e.</p> \\[ l^p=\\left\\{ \\{\\xi_n\\}_{n\\geq 1}: \\sum_{n=1}^\\infty |\\xi_n|^p&lt;\\infty. \\right\\} \\] <p>\\(\\forall x=\\{\\xi_n\\}_{n\\geq 1}, y=\\{\\eta_n\\}_{n\\geq 1}\\in l^p\\), their metric is defined by</p> \\[ \\rho(x,y)=\\left(\\sum_{n=1}^\\infty |\\xi_n-\\eta_n|^p\\right)^{\\frac{1}{p}}. \\] <p>(ii) \\(M=l^\\infty\\), or </p> \\[ l^\\infty:=\\{\\{\\xi_n\\}_{n\\geq 1}: \\exists M&gt;0, \\text{ s.t } |\\xi_n|\\leq M,\\forall n.\\} \\] <p>\\(\\forall x=\\{\\xi_n\\}_{n\\geq 1}, y=\\{\\eta_n\\}_{n\\geq 1}\\in l^\\infty\\), their metric is defined by</p> \\[ \\rho(x,y)=\\sup_{n}|\\xi_n-\\eta_n|. \\] Proof <p>(i) Using Minkowski Inequation.</p> <p>(ii) Similar to the previous example.</p> <p>Example.  Cases for \\((M,\\rho)\\) with background of Real analysis, to be more specific, \\(L^p\\) Space, show that the following \\(\\rho\\) satisfies axiom of triangle inequation. Note that we use \"\\(=\\)\" to denote equation almost everywhere.</p> <p>(i) \\(M=L^p(E), (1\\leq p&lt;\\infty, E\\subset \\mathbb{R} \\text{ is measurable})\\), i.e.</p> \\[ L^p=\\left\\{ \\text{measurable } f : \\int_E |f(t)|^pdt&lt;\\infty. \\right\\} \\] <p>\\(\\forall f, g \\in L^p(E)\\), their metric is defined by</p> \\[ \\rho(f,g)=\\left( \\int_E |f(t)-g(t)|^p dt\\right)^{\\frac{1}{p}}. \\] <p>(ii) \\(M=L^\\infty(E)\\), or </p> \\[ L^\\infty:=\\{\\text{measurable } f : \\exists M&gt;0, \\exists E_0\\subset  E, \\text{ s.t } m(E_0)=0, \\text{ &amp; } |f(t)|\\leq M,\\forall t\\in E-E_0.\\} \\] <p>\\(\\forall f,g\\in L^\\infty(E)\\), their metric is defined by</p> \\[ \\rho(f,g)=\\inf_{m(E_0)=0,\\atop E_0\\subset E}\\left\\{\\sup_{t\\in E-E_0}|f(t)-g(t)|\\right\\}. \\] Proof <p>(i) By Minkowski Inequation.</p> <p>(ii) Here the definition of the metric is to emit the unboundedness on zero-measure set. By infimum, \\(\\forall a(t),b(t), t\\in E\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists E_1,E_2\\subset F\\) and \\(m(E_1)=m(E_2)=0\\), s.t.</p> \\[ \\begin{align*} \\sup_{t\\in E-E_1} |a(t)|\\leq \\inf_{m(E_0)=0,\\atop E_0\\subset E}\\left\\{\\sup_{t\\in E-E_0}|a(t)|\\right\\} + \\frac{\\varepsilon}{2},\\\\ \\sup_{t\\in E-E_1} |b(t)|\\leq \\inf_{m(E_0)=0,\\atop E_0\\subset E}\\left\\{\\sup_{t\\in E-E_0}|b(t)|\\right\\} + \\frac{\\varepsilon}{2}. \\end{align*} \\] <p>So since \\(m(E_1\\cup E_2)=0\\), </p> \\[ \\begin{align*} \\inf_{m(E_0)=0,\\atop E_0\\subset E}\\left\\{\\sup_{t\\in E-E_0}|a(t)+b(t)|\\right\\}&amp;\\leq\\sup_{t\\in E-(E_1+E_2)} |a(t)+b(t)|\\\\ &amp;\\leq \\sup_{t\\in E-(E_1+E_2)} |a(t)| +\\sup_{t\\in E-(E_1+E_2)} |b(t)|\\\\ &amp;\\leq \\sup_{t\\in E-E_1} |a(t)|+\\sup_{t\\in E-E_2} |b(t)|\\\\ &amp;\\leq \\inf_{m(E_0)=0,\\atop E_0\\subset E}\\left\\{\\sup_{t\\in E-E_0}|a(t)|\\right\\} +\\inf_{m(E_0)=0,\\atop E_0\\subset E}\\left\\{\\sup_{t\\in E-E_0}|b(t)|\\right\\} +\\varepsilon. \\end{align*} \\] <p>Let \\(\\varepsilon\\rightarrow 0\\), and we have the result.</p>"},{"location":"Math/Functional_Analysis/Metric_Space/#convergence","title":"Convergence","text":"<p>Readers could compare this definition to convergence in \\(L^p\\) Space.</p> <p>Definitions of Convergence</p> <p>Assume \\(\\{x_n\\}_{n\\geq 1}\\subset M\\). If there exists \\(x_0\\in M\\), such that \\(\\rho(x_n,x_0)\\rightarrow 0(n\\rightarrow \\infty)\\), then we call \\(\\{x_n\\}\\) converges to \\(x_0\\), denoted by</p> \\[ \\lim_{n\\rightarrow \\infty}x_n=x_0 \\] <p>and \\(x_0\\) is a limit of \\(\\{x_n\\}\\).</p> <p>Properties of convergence</p> <p>Assume \\(\\{x_n\\}_{n\\geq 1}\\subset M\\) is a convergent sequence, then </p> <p>(i) Limit of \\(\\{x_n\\}_{n\\geq 1}\\) exists uniquely.</p> <p>(ii) \\(\\forall y_0\\in M\\), \\(\\{\\rho(x_n,y_0)\\}_{n\\geq 1}\\) is bounded.</p> <p>(iii) Subsequence. for all subsequence \\(\\{x_{n_j}\\}\\) converges to the same limit. Conversely, if for all subsequence \\(\\{x_{n_j}\\}\\) converges, then \\(\\{x_n\\}\\) also converges.</p> Proof <p>(i) Assume there are two distinct limits \\(x_0,y_0\\in M\\),</p> \\[ \\rho(x_0,y_0)&lt;\\rho(x_0,x_n)+\\rho(x_n,y_0) \\rightarrow 0(n\\rightarrow \\infty). \\] <p>(ii) easy to see.</p> <p>(iii) The forward direction is easy. But reversely, if \\(\\{x_n\\}\\) does not converge, then there exist at least two distinct subsequential limits \\(L_1\\) and \\(L_2\\), \\(L_1\\neq L_2\\), so which contradicts the assumption!</p>"},{"location":"Math/Functional_Analysis/Metric_Space/#topology","title":"Topology","text":"<p>Definitions</p> <p>Assume metric space \\((M,\\rho)\\). </p> <p>(i) If \\(x_0\\in M\\), \\(r&gt;0\\), Define open ball (neighborhood)</p> \\[ B(x_0, r)=\\{x: \\rho(x_0,x)&lt;r\\}, \\] <p>and closed ball</p> \\[ \\overline{B}(x_0, r)=\\{x: \\rho(x_0,x)\\leq r\\}. \\] <p>(ii) Assume \\(G\\subset M\\), \\(x\\in G\\), \\(x\\) is an interior point of \\(G\\), if \\(\\exists\\) neighborhood \\(B(x,r)\\subset G\\). The kernel of a set \\( G \\) is a set made up of all its interior points, denoted as \\(G^o\\). \\(G\\) is an open set if </p> \\[ \\forall x \\in G, x \\text{ is an interior point of } G. \\] <p>Define \\(\\varnothing\\) is an open set.</p> <p>Properties of open sets</p> <p>(i) \\(M\\) is an open set.</p> <p>(ii) \\(\\{X_\\lambda\\}_{\\lambda\\in \\Lambda}\\), \\(\\bigcup_{\\lambda\\in \\Lambda} X_\\lambda\\) is an open set.</p> <p>(iii) \\(\\{X_n\\}_{1\\leq n\\leq N}\\), \\(\\bigcap_{n=1}^N X_n\\) is an open set.</p> <p>Definitions 2</p> <p>Assume metric space \\((M,\\rho)\\). \\(G\\subset M\\), \\(x_0\\in M\\). </p> <p>(i) If \\(\\forall \\varepsilon&gt;0\\), </p> \\[ B(x_0, \\varepsilon)\\cap (G-\\{x_0\\})\\neq \\varnothing, \\] <p>then \\(G\\) is an accumulation point. </p> <p>(ii) The Derived Set of \\( G \\) is defined as the set composed of all accumulation points of \\( G \\), denoted as \\( G' \\).</p> <p>(iii) The Closure of \\( G \\) is the union of \\( G \\) and its derived set, denoted as:</p> \\[ \\overline{G} = G\\cup G'. \\] <p>Properties of Closure</p> <p>Assume metric space \\((M,\\rho)\\), \\(A,B\\subset X\\), then</p> <p>(i) \\(A\\subset \\overline{A}\\), (ii) \\(\\overline{\\overline{A}}=\\overline{A}\\), (iii) \\(\\overline{A\\cup B}=\\overline{A}\\cup \\overline{B}\\), (iv) \\(\\overline{\\varnothing}=\\varnothing\\).</p>"},{"location":"Math/Functional_Analysis/Metric_Space/#density-separability","title":"Density &amp; Separability","text":"<p>Definition of Density</p> <p>Assume \\(X\\) is a metric space, \\(A,B\\subset X\\). If \\(\\overline{B}\\supset A\\), then \\(B\\) is dense in \\(A\\).</p> <p>Equivalent proposition of Density</p> <p>Assume \\(X\\) is a metric space, \\(A,B\\subset X\\). Then the following statements are equivalent.</p> <p>(i) \\(B\\) is dense in \\(A\\), </p> <p>(ii) \\(\\forall x\\in A\\), \\(\\forall \\varepsilon&gt;0\\),  \\(\\exists y\\in B\\), such that \\(x\\in B(y,\\varepsilon)\\) (or \\(\\rho(x,y)&lt;\\varepsilon\\)).</p> <p>(iii) \\(\\forall x\\in A\\), \\(\\exists \\{x_n\\}\\in B\\), such that \\(\\lim\\limits_{n\\rightarrow\\infty}x_n=x\\).</p> <p>(iv) \\(\\forall \\varepsilon&gt;0\\), \\(A\\subset \\bigcup\\limits_{x\\in B}B(x, \\varepsilon)\\).</p> Proof <ul> <li>(i) \\(\\Rightarrow\\) (ii)</li> </ul> <p>\\(\\forall x\\in A\\), since \\(A\\subset \\overline{B}\\), then \\(x\\in \\overline{B}\\), so \\(\\forall \\varepsilon&gt;0\\), \\(B(x,\\varepsilon)\\cap B\\neq\\varnothing\\). So \\(\\exists y\\in B(x,\\varepsilon)\\cap B\\), which satisfies \\(\\rho(x,y)&lt;\\varepsilon\\).</p> <ul> <li>(ii) \\(\\Rightarrow\\) (iii)</li> </ul> <p>\\(\\forall x\\in A\\), \\(\\forall \\varepsilon_k=\\frac{1}{k}&gt;0\\), \\(\\exists y_k\\in B\\), such that \\(\\rho(x,y_k)&lt;\\varepsilon_k\\). So \\(\\lim\\limits_{k\\rightarrow \\infty}y_k=x\\).</p> <ul> <li>(iii) \\(\\Rightarrow\\) (iv)</li> </ul> <p>\\(\\forall x\\in A\\), \\(\\exists \\{x_n\\}\\in B\\), such that \\(\\lim\\limits_{n\\rightarrow\\infty}x_n=x\\). So \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\rho(x,x_N)&lt;\\varepsilon\\), which means \\(x\\in B(x_N,\\varepsilon)\\). So \\(A\\subset \\bigcup\\limits_{x\\in B}B(x, \\varepsilon)\\).</p> <ul> <li>(iv) \\(\\Rightarrow\\) (i)</li> </ul> <p>\\(\\forall \\varepsilon&gt;0\\), \\(A\\subset \\bigcup\\limits_{x\\in B}B(x, \\varepsilon)\\), so \\(\\forall x\\in A\\), \\(\\exists y\\in B\\), \\(x\\in B(y,\\varepsilon)\\), which means \\(B(x,\\varepsilon)\\cap B=\\neq \\varnothing\\) (at least we have \\(y\\)), so \\(x\\in \\overline{B}\\).</p> <p>Definition of Separability</p> <p>Assume \\(X\\) is a metric space. If there exists denumerable set \\(B\\subset X\\), \\(B\\) is dense in \\(X\\), then \\(X\\) is separable. This is an essential difference between finite-dimension and infinite-dimension space.</p> <p>Example. Prove \\(L^\\infty[a,b]\\) is not separable in metric </p> \\[ \\inf_{F_0\\subset [a,b]\\atop m(F_0)=0}\\sup_{t\\in [a,b]-F_0 }|x(t)-y(t)|. \\] <p>Actually, \\(F=[a,b]\\), which means we have a non-denumerable set, we could find a set with the distance of all its elements not really small.</p> Proof <p>Choose element </p> \\[ f_s(t)=\\chi_{[a,s]},\\quad s\\in [a,b] \\] <p>So if \\(t\\neq s\\in [a,b]\\), we have </p> \\[ \\rho(f_t,f_s)=1. \\] <p>We construct set \\(A\\) to be all the functions above.</p> <p>We prove the proposition by contradiction. If it is separable, then there exists a dense denumberable subset \\(A_0\\) of \\(L^\\infty [a,b]\\), such that \\(\\forall \\varepsilon&gt;0\\), \\(L^\\infty [a,b]\\subset \\bigcup\\limits_{x\\in A_0}B(x,\\varepsilon)\\). Choose \\(\\varepsilon=\\frac{1}{3}\\). </p> <p>Since \\(A\\) has non-denumerable elements, so there exists 2 elements \\(f_t,f_s \\in A\\), \\(\\exists g_0\\in A_0\\), such that \\(f_t,f_s\\in B(g_0,\\frac{1}{3})\\), so</p> \\[ \\rho(f_t,f_s)\\leq \\rho(f_t,g_0)+\\rho(g_0,f_s)&lt;\\frac{2}{3}&lt;1, \\] <p>which contradicts!</p>"},{"location":"Math/Functional_Analysis/Metric_Space/#continuous-mapping","title":"Continuous Mapping","text":"<p>Mapping &amp; Continuous Mapping</p> <p>Assume \\(X,Y\\) are metric space, with its distance \\(\\rho\\), \\(\\rho_1\\), respectively. If \\(\\forall x\\in X\\), \\(\\exists ! y\\in Y\\), such that \\(Tx=y\\), then we call \\(T\\) is a mapping from \\(X\\) to \\(Y\\).</p> <p>If mapping \\(T\\) satisfies that for a fixed point \\(x_0\\in X\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that \\(\\rho_1(Tx,Tx_0)&lt;\\varepsilon\\) whenever \\(\\rho(x,x_0)&lt;\\delta\\), then we call \\(T\\) is a continuous mapping at \\(x_0\\).</p> <p>If the above property holds for all \\(x_0\\in X\\), then we call \\(T\\) continuous mapping on \\(X\\).</p> <p>Usually we call a mapping from metric space \\(X\\) to \\(\\mathbb{R}\\) functional.</p> <p>Example. (i) Assume \\(X\\) is a metric space, \\(x_0\\in X\\), mapping \\(T(x)=\\rho(x,x_0), (x\\in X)\\) is a continuous functional from \\(X\\) to \\(\\mathbb{R}\\).</p> <p>(ii) Assume \\(X\\) is a metric space, \\(F\\subset X\\), mapping \\(T(x)=\\inf\\limits_{y\\in F}\\rho(x,y), (x\\in X)\\) is continuous (actually uniformly continuous) functional from \\(X\\) to \\(\\mathbb{R}\\). </p> Proof for (i)Proof for (ii) <p>\\(\\forall \\varepsilon&gt;0\\), \\(\\forall x\\in X\\), choose \\(\\delta=\\varepsilon\\), \\(\\forall y\\in X\\) such that \\(\\rho(x,y)&lt;\\delta\\), we have </p> \\[ |T(x)-T(y)|=|\\rho(x,x_0)-\\rho(y,x_0)|\\leq \\rho(x,y)&lt;\\varepsilon. \\] <p><p>\\(\\square\\)</p></p> <p>\\(\\forall y\\in F\\), \\(\\forall x, z\\in X\\),</p> \\[ \\begin{align*} \\rho(x,y)&amp;\\leq \\rho(x,z)+\\rho(z,y),\\\\  \\rho(z,y)&amp;\\leq \\rho(z,x)+\\rho(x,y).\\\\ \\Rightarrow \\quad T(x)&amp;\\leq \\rho(x,z)+T(z)\\quad \\text{take } \\inf_{y\\in F}\\\\ \\Rightarrow \\quad T(z)&amp;\\leq \\rho(x,z)+T(x)\\quad \\text{take } \\inf_{y\\in F}\\\\ \\Rightarrow \\quad |T(x)-T(z)|&amp;\\leq \\rho(x,z). \\end{align*} \\] <p>\\(\\forall \\varepsilon&gt;0\\), choose \\(\\delta=\\varepsilon\\), whenever \\(\\rho(x,z)&lt;\\delta\\), </p> \\[ |T(x)-T(z)|\\leq \\rho(x,z)&lt;\\varepsilon. \\] <p><p>\\(\\square\\)</p></p> <p>Equivalent definition for continuous mapping</p> <p>Assume \\(X,Y\\) are metric space, with its distance \\(\\rho\\), \\(\\rho_1\\), respectively. </p> <p>(i) Choose a point \\(x_0\\in X\\). A mapping \\(T\\) from \\(X\\) to \\(Y\\) is continuous at \\(x_0\\), iff </p> \\[ \\forall \\{x_n\\}_{n\\geq 1}\\subset X, \\lim\\limits_{n\\rightarrow \\infty}x_n=x_0, s.t. \\lim\\limits_{n\\rightarrow \\infty}Tx_n=Tx. \\] <p>(ii) A mapping \\(T\\) from \\(X\\) to \\(Y\\) is continuous, iff one of the following two condition holds</p> \\[ \\forall \\text{ open set } G\\in Y, T^{-1}(G) \\text{ is an open set.}  \\] \\[ \\forall \\text{ closed set } F\\in Y, T^{-1}(F) \\text{ is a closed set.} \\] Proof for (i)Proof for (ii) <ul> <li>Necessary. Here we have to find a \\(N\\) for \\(\\rho(Tx_n,Tx_0)&lt;\\varepsilon\\) whenever \\(n&gt;N\\). </li> </ul> <p>Actually, since \\(T\\) is continuous, then \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that \\(\\forall \\rho(x, x_0)&lt;\\delta\\), we have \\(\\rho_1(Tx, Tx_0)&lt;\\varepsilon\\). Assume \\(\\{x_n\\}\\) converges to \\(x_0\\). For the above \\(\\delta\\), \\(\\exists N&gt;0\\), such that \\(\\rho(x_n, x_0)&lt;\\delta\\) whenever \\(n&gt;N\\). So for \\(n&gt;N\\), </p> \\[ \\rho_1(T(x_0), T(x_n))&lt;\\frac{1}{n}. \\] <ul> <li>Sufficient. Here we have to find \\(\\delta\\), such that \\(\\rho(Tx,Tx_0)&lt;\\varepsilon\\) whenever \\(\\rho(x,x_0)&lt;\\delta\\). This is a little hard, we prove by contradiction. Assume \\(T\\) is not continuous, then \\(\\exists \\varepsilon_0\\), \\(\\forall \\delta_n=\\frac{1}{n}\\), \\(\\exists x_n\\) such that \\(\\rho(x_n,x_0)&lt;\\delta_n\\), we have \\(\\rho_1(Tx_n,Tx_0)\\geq \\varepsilon_0\\). So we have a sequence \\(x_n\\rightarrow x_0\\) but \\(\\rho_1(Tx_n, Tx_0)\\not\\rightarrow 0\\), which contrsdicts!</li> </ul> <p><p>\\(\\square\\)</p></p> <ul> <li>Necessary. </li> </ul> <p>Assume \\(T\\) is continuous. we have to prove, for any open set \\(G\\in Y\\), \\(T^{-1}(G)\\) is also an open set, which means \\(\\forall x_0\\in T^{-1}(G)\\), \\(Tx_0=y_0\\), we have to find a neighborhood of \\(x_0\\) contained in \\(X\\). </p> <p>Actually, since \\(T\\) is continuous, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\rho_1(Tx,Tx_0)&lt;\\varepsilon\\) whenever \\(\\rho(x,x_0)&lt;\\delta\\). By \\(G\\) is a open set, we choose a neighborhood of radius \\(\\varepsilon\\), then \\(B_\\varepsilon(y_0)\\subset G\\). By the above assumption, \\(\\exists \\delta&gt;0\\), \\(\\forall x\\in B_\\delta(x_0)\\subset X\\), such that \\(\\rho_1(Tx, Tx_0)=\\rho_1(y,y_1)&lt;\\varepsilon\\). which means \\(Tx\\in B_\\varepsilon(y_0)\\subset G\\), so \\(x\\in T^{-1}(G)\\). By arbitrariness of \\(x\\), we know \\(B_\\delta(x_0, x)\\subset T^{-1}(G)\\), which is exactly the definition of open set.</p> <ul> <li>Sufficient.</li> </ul> <p>Assume the original image of open set \\(G\\) is open set under \\(T\\). \\(\\forall \\varepsilon&gt;0\\), \\(\\forall x_0\\in X\\), \\(y_0=Tx_0\\). Choose a specific open set \\(G=B_\\varepsilon(y_0)\\), then \\(T^{-1}(G)\\) is an open set. By its definition, \\(\\exists \\delta&gt;0\\), such that \\(B_\\delta(x_0)\\subset T^{-1}(G)\\). So \\(\\forall x\\in B_\\delta(x_0)\\), we have \\(Tx\\in B_\\varepsilon(y_0)\\), which is exactly definition of continuous mapping at \\(x_0\\). By arbitrariness of \\(x_0\\), we know \\(T\\) is continuous on \\(X\\).</p> <p><p>\\(\\square\\)</p></p> <p>Definition of inverse mapping</p> <p>Assume \\(X,Y\\) are metric space, with its distance \\(\\rho\\), \\(\\rho_1\\), respectively. If a mapping \\(T\\) from \\(X\\) to \\(Y\\) is injective and surjective, or bijective, then \\(\\forall y\\in Y\\), \\(\\exists ! x\\in X\\), such that </p> \\[ Tx=y. \\] <p>here we have a new mapping, called Inverse Mapping, deonted as \\(T^{-1}\\).</p> <p>Definition of Homeomorphic Mapping</p> <p>Assume \\(X,Y\\) are metric space, with its distance \\(\\rho\\), \\(\\rho_1\\), respectively. If a mapping \\(T\\) from \\(X\\) to \\(Y\\) is bijective, and \\(T\\), \\(T^{-1}\\) are both continuous on \\(X\\), \\(Y\\), respectively, then we call \\(T\\) is a Homeomorphic Mapping.</p> <p>If there exists a homeomorphic mapping \\(T\\) from \\(X\\) to \\(Y\\), then we call \\(X\\) and \\(Y\\) are homeomorphic.</p> <p>Definition of Isometric Mapping</p> <p>Assume \\(X,Y\\) are metric space, with its distance \\(\\rho\\), \\(\\rho_1\\), respectively. If a mapping \\(T\\) from \\(X\\) to \\(Y\\), satisfies that \\(\\forall x,y\\in X\\), \\(\\rho(x,y)=\\rho_1(Tx,Ty)\\), then we call \\(T\\) is an isometric mapping.</p> <p>If there exists a bijective and isometric mapping \\(T\\) from \\(X\\) to \\(Y\\), then we call \\(X\\) and \\(Y\\) are isometric.</p> <p>When Two metric space are homeomorphic or isometric, we view them as the same.</p>"},{"location":"Math/Functional_Analysis/Metric_Space/#completeness","title":"Completeness","text":"<p>Definition of Basic sequence (Cauchy Sequence)</p> <p>Assume \\(X\\) is a metric space with metric \\(\\rho\\). Sequence \\(\\{x_n\\}_{n\\geq 1}\\subset X\\) is said to be a Cauchy Sequence, if \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n,m&gt;N\\), such that </p> \\[ \\rho(x_n,x_m)&lt;\\varepsilon. \\] <p>The core problem of functional analysis, lies in formulating a complete space, i.e. every Cauchy sequence of \\(X\\) converge to the point in the space \\(X\\) itself.</p> <p>Example. Basic space \\(\\mathbb{R}\\) and \\(C[a,b]\\) are complete.</p> Proof <p>For \\(\\mathbb{R}\\), it is done in mathematical analysis.</p> <p>For \\(C[a,b]\\), we have prove it actually in series of functions. By definition of metric, \\(\\forall\\text{ Cauchy's Seq } \\{x_n\\}\\subset C[a,b]\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n,m&gt;N\\), </p> \\[ \\sup_{t\\in [a,b]}|x_m(t)-x_n(t)|&lt;\\varepsilon. \\] <p>So </p> \\[ \\begin{align} |x_m(t)-x_n(t)|&lt;\\varepsilon,\\quad \\forall t\\in [a,b].\\label{uniform-convergence} \\end{align} \\] <p>which means for fixed \\(t\\in [a,b]\\), \\(x_m(t), x_n(t)\\) are cauchy sequence on \\(\\mathbb{R}\\), so by completeness of \\(\\mathbb{R}\\), we have their convergent function \\(x(t)\\). Let \\(n,t\\) fixed and let \\(m\\rightarrow \\infty\\) in inequation \\(\\ref{uniform-convergence}\\), we have</p> \\[ |x_n(t)-x(t)|\\leq \\varepsilon, \\quad \\forall t\\in [a,b]. \\] <p>So \\(x_n(t)\\) converges to \\(x(t)\\) uniformly, so by conclusion of mathematical analysis, we have \\(x(t)\\) is continuous and thus \\(x(t)\\in C[a,b]\\). So from above, we take a supremum, which gives \\(\\rho(x_n, x)\\leq \\varepsilon\\). So \\(x_n\\) converges to an element in \\(C[a,b]\\).</p> <p>Example. Assume \\(S\\) is a space composed by functions defined on a finite-measure set \\(F\\), which is finite-valued almost everywhere. Define metric</p> \\[ \\rho(x,y)=\\int_F \\frac{|x(t)-y(t)|}{1+|x(t)+y(t)|}dt,\\quad x,y\\in S. \\] <p>Prove \\(S\\) is complete.</p> Proof <ul> <li>Show that \\(\\rho\\) is a metric, using </li> </ul> \\[ \\begin{align*} \\frac{|a+b|}{1+|a+b|}&amp;\\leq \\frac{|a|+|b|}{1+|a+b|}\\\\ &amp;=\\frac{|a|}{1+|a+b|}+\\frac{|b|}{1+|a+b|}\\\\ &amp;\\leq \\frac{|a|}{1+|a|}+\\frac{|b|}{1+|b|}. \\end{align*} \\] <ul> <li>We have to prove that convergence in this metric is equivalent to convergence in measure. </li> </ul> <p>Assume \\(\\{x_n\\}\\rightarrow x\\) in this metric, then \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n&gt;N\\), and cccording to the relationship, \\(\\forall \\delta&gt;0\\),</p> \\[ \\begin{align*} \\varepsilon&amp;&gt;\\int_F \\frac{|x_n(t)-x(t)|}{1+|x_n(t)+x(t)|}dt\\\\ &amp;\\geq \\int_{F(|x_n-x|\\geq \\delta)}  \\frac{|x_n(t)-x(t)|}{1+|x_n(t)+x(t)|}dt\\\\ &amp;\\geq \\frac{\\delta}{1+\\delta} m[F(|x_n-x|\\geq \\delta)] \\end{align*} \\] <p>which means \\(\\lim_{n\\rightarrow \\infty}m[F(|x_n-x|\\geq \\delta)]=0\\).</p> <p>on the other hand, assume \\(\\{x_n\\}\\rightarrow x\\) in measure, then \\(\\forall \\varepsilon&gt;0, \\delta&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n&gt;N\\), we have</p> \\[ m[F(|x_n-x|\\geq \\delta)]&lt;\\frac{\\varepsilon}{2}. \\] <p>While according to relationship</p> \\[ \\begin{align*} \\int_F \\frac{|x_n(t)-x(t)|}{1+|x_n(t)+x(t)|}dt&amp; =\\left[\\int_{F(|x_n-x|\\geq \\delta)} + \\int_{F(|x_n-x|&lt; \\delta)}\\right]\\frac{|x_n(t)-x(t)|}{1+|x_n(t)+x(t)|}dt\\\\ &amp;\\leq \\frac{\\delta}{1+\\delta} m[F(|x_n-x|&lt;\\delta)] + \\int_{F(|x_n-x|\\geq \\delta)} 1dt\\\\ &amp;=\\delta m[F(|x_n-x|&lt;\\delta)] + m[F(|x_n-x|\\geq \\delta)] \\end{align*} \\] <p>here we have to choose \\(\\delta\\) such that \\(\\delta&lt;\\frac{\\varepsilon}{2m[F(|x_n-x|&lt;\\delta)]}\\), so the above estimate is less then \\(\\varepsilon\\).</p> <ul> <li>We have to prove \\(\\forall \\text{ Cauchy sequence } \\{x_n(t)\\}\\subset S\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n,m&gt;N\\) such that </li> </ul> \\[ \\begin{align*} \\varepsilon&amp;&gt;\\int_F \\frac{|x_n(t)-x_m(t)|}{1+|x_n(t)+x_m(t)|}dt.\\\\ &amp;&gt;\\int_F \\frac{\\delta}{1+\\delta}m[F(|x_n-x_m|&gt;\\varepsilon)] \\end{align*} \\] <p>which means \\(x_n\\) is a cauchy sequence in a sense of measure. By Riesz Theorem, we have a subsequence \\(x_{n_k}\\) which is Cauchy sequence a.e. So it converges to a finite-valued measurable function \\(x(t)\\) a.e. So \\(x\\in S\\). Then for \\(n_k&gt;N\\), we have</p> \\[ \\int_F \\frac{|x_n(t)-x_{n_k}(t)|}{1+|x_n(t)+x_{n_k}(t)|}dt&lt;\\varepsilon. \\] <p>Since \\(\\left|\\frac{|x_n(t)-x_{n_k}(t)|}{1+|x_n(t)+x_{n_k}(t)|}\\right|&lt;1\\), \\(\\int_F 1dt&lt;\\infty\\), so by Lebesgue's Dominated Convergence Theorem. </p> \\[ \\begin{align*} \\rho(x,x_n)&amp;=\\int_F \\lim_{k\\rightarrow\\infty} \\frac{|x_n(t)-x_{n_k}(t)|}{1+|x_n(t)+x_{n_k}(t)|}dt\\\\ &amp;=\\lim_{k\\rightarrow \\infty}\\int_F \\frac{|x_n(t)-x_{n_k}(t)|}{1+|x_n(t)+x_{n_k}(t)|}dt \\leq \\varepsilon. \\end{align*} \\] <p>Example. Assume \\(s\\) is a space composed by sequences \\(x=\\{\\xi_1,\\cdots,\\xi_n,\\cdots\\}\\). Define metric</p> \\[ \\rho(x,y)=\\sum_{k=1}^\\infty \\frac{1}{2^k}\\frac{|\\xi_k-\\eta_k|}{1+|\\xi_k+\\eta_k|} \\] <p>Prove \\(s\\) is complete.</p> Proof <ul> <li> <p>Similar idea to prove it is a metric.</p> </li> <li> <p>Prove that convergence in the above metric is equivalent to convergence for each coordinate.</p> </li> </ul> <p>Assume \\(x_n\\rightarrow x\\) in the above metric, i.e. \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n&gt;N\\),</p> \\[ \\begin{align*} \\sum_{k=1}^\\infty \\frac{1}{2^k}\\frac{|\\xi^{(n)}_k-\\xi_k|}{1+|\\xi^{(n)}_k+\\xi_k|}&amp;&lt;\\varepsilon\\\\ \\Rightarrow \\quad \\frac{1}{2^k}\\frac{|\\xi^{(n)}_k-\\xi_k|}{1+|\\xi^{(n)}_k+\\xi_k|}&amp;&lt;\\varepsilon\\\\ \\end{align*} \\] <p>For fixed \\(k\\), let \\(\\varepsilon 2^k&lt;1\\), and \\(|\\xi^{(n)}_k-\\xi_k|&lt;\\frac{\\varepsilon}{1-\\varepsilon}&lt;\\varepsilon\\). By arbitrariness of \\(\\varepsilon&lt;\\frac{1}{2^k}\\), we have its convergence for each coordinate.</p> <p>Assume \\(\\varepsilon&gt;0\\), \\(\\exists K\\) such that \\(\\frac{1}{2^K}&lt;\\frac{\\varepsilon}{2}\\). Still the above \\(\\varepsilon\\), for each coordinate \\(k\\), \\(\\exists N_k&gt;0\\), \\(|\\xi^{(n)}_k-\\xi_k|&lt;\\varepsilon/K\\) whenever \\(n&gt;N_k\\).  For \\(k=1:K\\), choose \\(N=\\max\\{N_1,\\cdots,N_k\\}\\), let \\(n&gt;N\\), we have</p> \\[ \\begin{align*} \\sum_{k=1}^\\infty \\frac{1}{2^k}\\frac{|\\xi^{(n)}_k-\\xi_k|}{1+|\\xi^{(n)}_k+\\xi_k|}&amp;=\\left[\\sum_{k=1}^K + \\sum_{k=K+1}^\\infty\\right]\\frac{1}{2^k}\\frac{|\\xi^{(n)}_k-\\xi_k|}{1+|\\xi^{(n)}_k+\\xi_k|}\\\\ &amp;\\leq \\sum_{k=1}^K \\frac{|\\xi^{(n)}_k-\\xi_k|}{1+|\\xi^{(n)}_k+\\xi_k|} + \\sum_{k=K+1}^\\infty \\frac{1}{2^k}\\\\ &amp;\\leq \\sum_{k=1}^K |\\xi^{(n)}_k-\\xi_k| + \\frac{1}{2^K}&lt;\\varepsilon. \\end{align*} \\] <ul> <li>Prove its completeness.</li> </ul> <p>Similar to the above proof, a Cauchy sequence \\(x_n\\) in the above metric is also a Cauchy sequence for each coordinate, which is in \\(\\mathbb{R}\\). So by completeness of \\(\\mathbb{R}\\), we have \\(\\xi_k\\) \\((k=1,2,\\cdots)\\) as its convergence. So define \\(x=\\{\\xi_1,\\cdots,\\xi_k\\}\\in s\\) and prove that \\(x_n\\) converges to \\(x\\) in the above metric. This is apparent because we could change the order of limit for absolute convergent series.</p> \\[ \\begin{align*} \\rho(x_n,x)&amp;=\\sum_{k=1}^\\infty\\lim_{m\\rightarrow \\infty} \\frac{1}{2^k}\\frac{|\\xi^{(n)}_k-\\xi^{(m)}_k|}{1+|\\xi^{(n)}_k+\\xi^{(m)}_k|}\\\\ &amp;=\\lim_{m\\rightarrow \\infty}\\sum_{k=1}^\\infty \\frac{1}{2^k}\\frac{|\\xi^{(n)}_k-\\xi^{(m)}_k|}{1+|\\xi^{(n)}_k+\\xi^{(m)}_k|}\\leq \\varepsilon \\end{align*} \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/Metric_Space/#meager-sets","title":"Meager sets","text":"<p>Definition of nowhere dense sets</p> <p>Assume \\(X\\) is a metric space, \\(A\\subset X\\). \\(A\\) is said to be Nowhere Dense Set, if \\(\\forall \\text{ open set }G\\subset X\\), \\(A\\) is not dense in \\(G\\).</p> <p>Equivalent definition of nowhere dense sets</p> <p>Assume \\(X\\) is a metric space, \\(A\\subset X\\). \\(A\\) is said to be Nowhere Dense Set, iff </p> \\[ \\forall \\text{ open ball }S(x_0,r), \\exists \\text{ open ball }S(y_0,r')\\subset S(x_0,r), s.t. A\\cap S(y_0,r')=\\varnothing. \\] Proof <ul> <li>Necessary.</li> </ul> <p>Assume \\(A\\) is a nowhere dense set, since \\(\\forall \\text{ open ball } S(x,r)\\subset X\\), \\(A\\) is not dense in \\(S(x,r)\\), so \\(\\exists \\varepsilon_0&gt;0\\), \\(\\exists x_0\\in S(x,r)\\), \\(\\forall y\\in A\\), \\(\\rho(x_0,y)\\geq \\varepsilon_0\\), so \\(S(x_0,\\varepsilon)\\cap A=\\varnothing\\).</p> <p>or by definition of density, \\(S(x, r)-\\overline{A}\\) is not empty and still open, so we have a small open ball in the rest which has no common element with \\(A\\).</p> <ul> <li>Sufficient. </li> </ul> <p>\\(\\forall \\text{ open set }G \\subset X\\), \\(\\forall x\\in G\\), \\(\\exists r&gt;0\\), such that \\(S(x,r)\\subset G\\). By condition, \\(\\exists \\text{ open ball }S(y_0,r')\\subset S(x_0,r)\\) s.t. \\(A\\cap S(y_0,r')=\\varnothing\\), which means \\(\\exists r'&gt;0\\), \\(\\exists y\\in S(y_0,r')\\subset G\\), \\(\\rho(y,z)&gt;r'\\) for all \\(z\\in A\\).</p> <p><p>\\(\\square\\)</p></p> <p></p> <p>Set of the first &amp; second category</p> <p>Assume \\(X\\) is a metric space, \\(A\\subset X\\). \\(A\\) is said to be a set of the first category, if there exists a sequence of nowhere dense sets \\(\\{F_n\\}_{n\\geq 1}\\), such that </p> \\[ A=\\bigcup_{n=1}^\\infty F_n. \\] <p>If not, we call \\(A\\) a set of the second category.</p> <p></p> <p>Theorem of Nested Closed Balls</p> <p>Metric Space \\(X\\) is complete, iff \\(\\forall\\) nested closed ball \\(\\{K_n=\\overline{S}(x_n,r_n)\\}_{n\\geq 1}\\), which satisfies </p> \\[ \\overline{S}(x_1,r_1)\\supset \\overline{S}(x_2,r_2)\\supset \\cdots, \\] <p>and \\(\\lim\\limits_{n\\rightarrow \\infty}r_n=0\\), then there exists a unique point \\(x_0\\in X\\), such that \\(x_0\\in \\bigcap_{n=1}^\\infty \\overline{S}(x_n,r_n)\\).</p> Proof <ul> <li>Necessary. </li> </ul> <p>Assume \\(X\\) is complete. Choose a sequence \\(\\{x_n\\}\\), just the center of each closed ball, then for each \\(n\\geq 1\\), \\(\\forall m&gt;n\\), </p> \\[ \\begin{align} \\rho(x_n, x_{m})&lt;r_n.\\label{nested} \\end{align} \\] <p>\\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), such that \\(\\forall n&gt;N\\), \\(r_n&lt;\\varepsilon\\). Choose \\(\\forall m&gt;n&gt;N\\), \\(\\rho(x_n, x_{m})&lt;\\varepsilon\\).</p> <p>So \\(x_n\\) is a Cauchy sequence. By completeness of \\(X\\), there exists a point \\(x\\in X\\), such that \\(x_n\\rightarrow n\\). Because for each \\(n\\geq 1\\), \\(x_k\\subset \\overline{S}(x_n,r_n)\\) for \\(k\\geq n\\), so \\(x\\in K_n\\). So \\(x=\\bigcup_{n=1}^\\infty K_n\\). In inequation \\(\\ref{nested}\\), fix \\(n\\) and let \\(m\\rightarrow \\infty\\), by continuity of metric, we have</p> \\[ \\rho(x_n, n)\\leq r_n,  \\] <p>which means \\(x\\subset K_n\\) for each n.</p> <p>To prove the uniqueness, we assume there exists another point \\(y\\in \\bigcup_{n=1}^\\infty K_n\\). If \\(x\\neq y\\), then \\(\\rho(x,y)&gt;0\\), which contradicts \\(r_n\\rightarrow 0\\)! <p>\\(\\square\\)</p></p> <ul> <li>Sufficient.</li> </ul> <p>Assume \\(\\{x_n\\}\\subset X\\) is a Cauchy sequence, choose a subsequence which converges much faster. Since \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n,m&gt;N\\), we have \\(\\rho(x_n,x_m)&lt;\\varepsilon\\). Let \\(\\varepsilon_1=\\frac{1}{2}\\), \\(\\exists N_1&gt;0\\), choose \\(n_1&gt;N_1\\), and \\(\\forall m_1&gt;N_1\\), we have</p> \\[ \\rho(x_{n_1}, x_{m_1})&lt;\\frac{1}{2}. \\] <p>Choose \\(\\varepsilon_2=\\frac{1}{2^2}\\), \\(\\exists N_2&gt;0\\), choose \\(n_2&gt;\\max\\{N_1,N_2\\}\\), and \\(\\forall m_2\\), we have</p> \\[ \\rho(x_{n_2}, x_{m_2})&lt;\\frac{1}{2^2}. \\] <p>Since \\(n_2&gt;N_1\\), so we have </p> \\[ \\rho(x_{n_2},x_{n_1})&lt;\\frac{1}{2}. \\] <p>Follow the same routine, we choose \\(\\varepsilon_k=\\frac{1}{2^k}\\), then \\(\\exists N_k&gt;0\\), choose \\(n_k&gt;\\max\\{N_k,N_{k-1},\\cdots, N_1\\}\\), and we have </p> \\[ \\rho(x_{n_k},x_{n_{k-1}})&lt;\\frac{1}{2^{k-1}}. \\] <p>Construct nested closed ball \\(K_k=\\overline{S}(x_{n_k}, \\frac{1}{2^{k}})\\). \\(\\forall x\\in K_{k+1}\\), we have</p> \\[ \\rho(x_{k},x)\\leq \\rho(x_k,x_{k+1}) +\\rho(x_{k+1}, x)&lt;\\frac{1}{2^{k}}+\\frac{1}{2^{k}}=\\frac{1}{2^{k-1}}. \\] <p>So \\(x\\in K_k\\). By condition of the proposition, we have a unique point \\(x\\in \\bigcup_{k=1}^\\infty K_k\\). Since \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), s.t. \\(\\frac{1}{2^{N-1}}&lt;\\varepsilon\\), and for \\(n, n_k&gt;N\\),</p> \\[ \\rho(x_n, x)\\leq \\rho(x_n, x_{n_k})+\\rho(x_{n_k}, x)&lt;\\frac{1}{2^{N}}+\\frac{1}{2^{N}}&lt;\\varepsilon. \\] <p>So \\(x_n\\rightarrow x\\in X\\).</p> <p>Baire Theorem</p> <p>Every complete space is a set of the second category.</p> Proof <p>We prove by contradiction. Assume \\(X\\) is a complete metric space, which is a set of the first category. Then \\(X=\\bigcup_{n=1}^\\infty F_n\\) where \\(F_n\\) is nowhere dense set. We use the nested closed ball theorem. </p> <p>Since \\(F_1\\) is nowhere dense set, then choose a closed ball \\(\\overline{S}(x_0,r_0)\\subset X\\), exists another smaller closed ball \\(\\overline{S}(x_1,r_1)\\subset \\overline{S}(x_0,r_0)\\), such that \\(\\overline{S}(x_1,r_1)\\cap F_1=\\varepsilon\\). If the ball is a little large, we take a smaller part of it with \\(r_1&lt;1\\), otherwise it already satisfies. </p> <p>Since \\(F_2\\) is also a nowhere dense set, there exists another smaller ball \\(\\overline{S}(x_2,r_2)\\subset \\overline{S}(x_1,r_1)\\), such that \\(\\overline{S}(x_2,r_2)\\cap F_2=\\varnothing\\). We could also let \\(r_2&lt;\\frac{1}{2}\\). </p> <p>Continue this routine, we have a sequence of nested closed ball \\(\\{\\overline{S}(x_k,r_k)\\}\\) with \\(r_k\\frac{1}{2^k}\\), which satisfies \\(\\overline{S}(x_k,r_k)\\cap F_k=\\varnothing\\). </p> <p>By Theorem of Nested Closed Balls, since \\(X\\) is complete, there exists a unique point \\(x_0\\in \\bigcap_{k=1}^\\infty \\overline{S}(x_k,r_k)=X\\), but \\(x_0\\not\\in \\bigcup_{k=1}^\\infty F_k=X\\), which contradicts!</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/Metric_Space/#compact-sets","title":"Compact Sets","text":"<p>Here we use nested closed ball theorem to give the version of Balzano-Weierstrass Theorem. However, bounded is not sufficient for a sequence to have a limit point in general metric space, specially in infinite-dimensional metric space.</p> <p>Definition of Bounded set</p> <p>Assume \\(X\\) is a metric space, and \\(A\\subset X\\). \\(A\\) is said to be a bounded set, if there exists a open ball \\(K=S(x,r)\\) (or closed ball \\(K=\\overline{S}(x,r)\\)) such that \\(A\\subset K\\).</p> <p>Definition for relative compact set &amp; compact set</p> <p>Assume \\(X\\) is a metric space, \\(A\\subset X\\). If \\(\\forall \\{x_n\\}\\subset A\\), \\(\\exists \\{n_k\\}_{k\\geq 1}\\) and \\(x\\in X\\), such that \\(\\lim\\limits_{k\\rightarrow \\infty}x_{n_k}=x\\), then we call \\(A\\) a relative compact set. If the limit point \\(x\\in A\\), then \\(A\\) is called compact set.</p> <p>If \\(X\\) itself is a compact set, then \\(X\\) is called compact metric space.</p> <p>Properties for relative compact set &amp; compact set</p> <p>(i) Any set with finite number elements is compact set.</p> <p>(ii) Assume \\(A\\subset X\\) is a relative compact set, then \\(\\forall B\\subset A\\), \\(B\\) is also a relative compact set.</p> <p>(iii) Assume \\(A\\subset X\\) is a compact set, then \\(\\forall \\text{ closed set }B\\subset A\\), \\(B\\) is also a compact set.</p> Proof <p>(ii) By definition. \\(\\forall \\{x_n\\}_{n\\geq 1}\\subset B\\subset A\\), so \\(\\exists \\{n_k\\}_{k\\geq 1}\\) and \\(x\\in X\\), such that \\(\\lim\\limits_{k\\rightarrow \\infty}x_{n_k}=x\\), so \\(B\\) is a relative compact set.</p> <p>(iii) \\(\\forall \\{x_n\\}_{n\\geq 1}\\subset B\\subset A\\), \\(\\exists \\{n_k\\}_{k\\geq 1}\\) and \\(x\\in A\\), such that \\(\\lim\\limits_{k\\rightarrow \\infty}x_{n_k}=x\\). Since \\(B\\) is a closed set, the limit point should be in \\(B\\), i.e. \\(x\\in B\\). So \\(B\\) is a compact set.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/Metric_Space/#relative-compact-set","title":"Relative compact set","text":"<p>Now we give a description for relative compact set.</p> <p>Inspired by finding a limit point in \\(\\mathbb{R}^n\\), we find the core is to formulate a finite-number of \"net\" to cover the bounded set, and one of its piece has infinite number of points. So here comes the abstract of the specific idea.</p> <p>Definition for \\(\\varepsilon\\)-net</p> <p>Assume \\(X\\) is a metric space with metric \\(\\rho\\), \\(A,B\\subset X\\). If for a given number \\(\\varepsilon&gt;0\\), \\(\\forall x\\in A\\), \\(\\exists y\\in B\\), such that \\(\\rho(x,y)&lt;\\varepsilon\\), then we call \\(B\\) is an \\(\\varepsilon\\)-net of \\(A\\).</p> <p>Note we do not ask \\(B\\cap A\\neq \\varnothing\\).</p> <p>Definition for Totally Bounded Set</p> <p>Assume \\(X\\) is a metric space, \\(A\\subset X\\). If \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\varepsilon\\)-net with finite number of elements for \\(A\\), then we call \\(A\\) is a Totally Bounded Set.</p> <p>Properties for totally bounded set</p> <p>(i) Any set with finite number of elements is a totally bounded set.</p> <p>(ii) If \\(A\\subset X\\) is a totally bounded set, then \\(\\forall B\\subset A\\), \\(B\\) is also a totally bounded set.</p> <p>(iii) If \\(A\\subset X\\) is a totally bounded set, then \\(\\forall\\varepsilon&gt;0\\), \\(\\exists B\\subset A\\), s.t \\(B\\) is a \\(\\varepsilon\\)-net of \\(A\\). </p> Proof <p>(i) Choose the set itself as an \\(\\varepsilon\\)-net, which has finite number of elements.</p> <p>(ii) \\(\\forall \\varepsilon&gt;0\\), \\(\\exists C\\) with finite number of elements, such that \\(B\\subset A\\subset \\bigcup\\limits_{x\\in C}S(x,\\varepsilon)\\), so \\(C\\) is also an \\(\\varepsilon\\)-net of \\(B\\).</p> <p>(iii) \\(\\forall \\varepsilon&gt;0\\), \\(\\exists C=\\{y_n\\}_{1\\leq n\\leq N}\\) is an \\(\\frac{\\varepsilon}{2}\\)-net of \\(A\\). That is, \\(\\forall x\\in A\\), \\(\\exists y\\in C\\), such that \\(\\rho(x,y)&lt;\\frac{\\varepsilon}{2}\\). By assumption, \\(\\exists \\{n_k\\}_{1\\leq k\\leq K} (K\\leq N)\\), such that \\(S(y_{n_k},\\frac{\\varepsilon}{2})\\cap A\\neq \\varnothing\\), then choose \\(z_k\\in S(y_{n_k},\\frac{\\varepsilon}{2})\\cap A\\). Then define \\(B=\\bigcup_{k=1}^K \\{z_k\\}\\), which satisfies </p> \\[ \\forall x\\in A, \\exists z_k\\in B, \\rho(x,z_k)&lt;\\varepsilon. \\] <p>So \\(B\\) is a \\(\\varepsilon\\)-net of \\(A\\).</p> <p>Theorem for totally bounded set</p> <p>Totally bounded set is bounded and separable.</p> Proof <p>(i) easy to see it is bounded. Choose one element of an \\(\\varepsilon\\)-net as center of ball, estimate the sufficient radious of the ball.</p> <p>(ii) Leverage the \\(\\frac{1}{n}\\)-net to formulate a dense denumerable subsequence.</p> <p>Assume \\(B_n=\\{y_{n_k}\\}_{1\\leq k\\leq K_n}\\) is a \\(\\frac{1}{n}\\)-net for totally bounded set \\(A\\). Then define</p> \\[ B=\\bigcup_{n=1}^\\infty B_n \\] <p>which is denumerable. Now we show that \\(B\\) is dense in \\(A\\). Actually, \\(\\forall x\\in A\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists n\\) such that \\(\\frac{1}{n}&lt;\\varepsilon\\), so \\(\\exists y_{n_k}\\in B_n\\subset B\\), such that \\(\\rho(x, y_{n_k})&lt;\\frac{1}{n}&lt;\\varepsilon\\).</p> <p>Or \\(\\forall x\\in A\\), \\(\\exists x_n\\in B_n\\), such that \\(\\rho(x, y_{n_k})&lt;\\frac{1}{n}\\), \\(\\{x_n\\}\\rightarrow x\\), which means \\(B\\) is dense in \\(A\\).</p> <p><p>\\(\\square\\)</p>.</p> <p>Hausdorff's Theorem for relative compact description</p> <p>Assume \\(X\\) is a metric space with metric \\(\\rho\\), \\(A\\subset X\\) is a relative compact set, then \\(A\\) is totally bounded set.</p> <p>On the other hand, If \\(X\\) is a complete metric space, if \\(A\\subset X\\) is a totally bounded set, then \\(A\\) is also a relative compact set.</p> <p>The core relationship between relative compact set and totally bounded set lies in every sequence has a Cauchy subsequence.</p> LemmaProof <ul> <li> <p>We first prove that \\(A\\subset X\\) is a totally bounded set, iff every sequence in \\(A\\) has a Cauchy subsequence.</p> </li> <li> <p>Necessary. </p> </li> </ul> <p>If \\(A\\) is totally bounded set, then \\(\\exists\\) finite-number \\(\\varepsilon\\)-net for \\(A\\). Choose an arbitrary sequence \\(x_n\\subset A\\). </p> <p>For \\(1\\)-net \\(B_1=\\{y_{j_1}\\}_{1\\leq j_1\\leq J_1}\\), we have \\(\\bigcup\\limits_{j\\in B_1}B(y_{j_1}, 1) \\supset \\{x_n\\}\\) , so \\(\\exists y_1\\in B_1\\) such that \\(B(y_1,1)\\cap \\{x_n\\}\\) has infinitely many number. Choose \\(x_{n_1}\\in B(y_1,1)\\cap \\{x_n\\}\\), which satisfies \\(\\rho(x_{n_1}, y_1)&lt;1\\). </p> <p>For \\(\\frac{1}{2}\\)-net, \\(B_2=\\{y_{j_2}\\}_{1\\leq j_2\\leq J_2}\\), we have \\(\\bigcup\\limits_{j_2\\in B_2}B(y_{j_2}, \\frac{1}{2}) \\supset \\{x_n\\}\\) and \\(\\bigcup\\limits_{j_2\\in B_2}B(y_{j_2},\\frac{1}{2}) \\cap B(y_1,1) \\cap \\{x_n\\}\\) has infinitely many number. So \\(\\exists y_2\\in B_2\\) such that \\(B(y_2,\\frac{1}{2})\\cap B(y_1,1)\\) has infinitely many number of \\(\\{x_n\\}\\). Choose \\(n_2&gt;n_1\\), \\(x_{n_2}\\in B(y_2,\\frac{1}{2})\\cap \\{x_n\\}\\), which satisfies \\(\\rho(x_{n_2}, y_2)&lt;\\frac{1}{2}\\). </p> <p>Continue this routine, we have for each \\(k\\),</p> \\[ \\bigcap_{i=1}^k B(y_i, \\frac{1}{i}) \\] <p>has infinitely many number of \\(\\{x_n\\}\\). Choose \\(n_k&gt;n_{k-1}\\), \\(x_{n_k}\\in \\bigcap\\limits_{i=1}^k B(y_i, \\frac{1}{i})\\). So a subsequence \\(\\{x_{n_k}\\}_{k\\geq 1}\\) is chosen, which satisfies \\(\\forall n&gt;k\\)</p> \\[ \\rho(x_{n_k}, x_{n})\\leq \\rho(x_{n_k}, y_k)+\\rho(y_k,x_{n})\\leq \\frac{1}{k}+\\frac{1}{k}=\\frac{2}{k}. \\] <p>which is a Cauchy sequence.</p> <p>Actually, iterately choose point from \\(\\{x_n\\}\\) to formulate \\(\\frac{1}{k}\\)-net, which is also a totally bounded set.</p> <ul> <li>Sufficient.</li> </ul> <p>Choose a point \\(x_1\\in A\\), if \\(O(x_1,1)\\supset A\\), then \\(A\\) is a totally bounded set. Otherwise, \\(A-O(x_1,1)\\neq \\varnothing\\), choose \\(x_2\\in A-O(x_1,1)\\)\uff0cif \\(O(x_1,1)\\cup O(x_2\uff0c1) \\supset A\\), then \\(A\\) is a totally bounded set. Otherwise, \\(A-O(x_1,1)-O(x_2,1)\\neq \\varnothing\\). Continue this routine, if </p> \\[ A-\\bigcup_{k=1}^n O(x_k,1)\\neq \\varnothing, \\] <p>we choose a point \\(x_{n+1}\\) to check whether </p> \\[ \\bigcup_{k=1}^{n+1}O(x_k,1)\\supset A. \\] <p>If we could not end in finite steps, we have a sequence \\(\\{x_n\\}\\), which satisfies \\(\\rho(x_i, x_j)\\geq 1\\) for \\(i\\neq j\\). However by assumption, this sequence must have a Cauchy subsequence, which contradicts!</p> <p>In this proof we could see that if for all sequence of \\(A\\), it has a convergent subsequence, then it must be totally bounded set. </p> <p><p>\\(\\square\\)</p></p> <ul> <li>Necessary.</li> </ul> <p>If \\(A\\) is a relative compact set, then any sequence \\(\\{x_n\\}\\) must have a convergent subsequence, \\(\\{x_{n_k}\\}\\), which is also a Cauchy sequence, so by lemma \\(A\\) is totally bounded set.</p> <ul> <li>Sufficient.</li> </ul> <p>If \\(A\\) is a totally bounded set, then for any sequence \\(\\{x_n\\}\\), it must have a Cauchy subsequence \\(\\{x_{n_k}\\}\\). Since \\(X\\) is complete, \\(\\{x_{n_k}\\}\\) must converge to a point \\(x\\in X\\), which means \\(A\\) is relative compact set.</p> <p><p>\\(\\square\\)</p></p> <p>The following theorem is useful in proof. Because it is hard to find an \\(\\varepsilon\\)-net which proved to have finite number, but easy to show it is relative compact.</p> <p>Theorem for relative compact set</p> <p>Assume \\(X\\) is a complete metric space, \\(A\\subset X\\) is a relative compact set, iff \\(A\\) has a relative compact \\(\\varepsilon\\)-net.</p> Proof <ul> <li>Necessary. </li> </ul> <p>If \\(A\\) is a relative compact set, \\(A\\) itself is a relative compact \\(\\varepsilon\\)-net. Or we could do as follows.</p> <p>We know \\(A\\) is also totally bounded, so there exists a finite-number \\(\\frac{\\varepsilon}{2}\\)-net \\(B\\) for \\(A\\). Since \\(B\\) could be covered by a \\(\\varepsilon\\)-net of itself, so \\(B\\) is totally bounded, since \\(X\\) is complete, \\(B\\) is relative compact.</p> <ul> <li>Sufficient.</li> </ul> <p>If \\(A\\) has a relative compact \\(\\frac{\\varepsilon}{2}\\)-net \\(B\\), then \\(B\\) is totally bounded, so \\(B\\) could be covered by finite-number \\(\\frac{\\varepsilon}{2}\\)-net \\(B'\\subset B\\), then \\(B'\\) is a finite-number \\(\\varepsilon\\)-net for \\(A\\), so \\(A\\) is totally bounded, since \\(X\\) is complete, \\(A\\) is relative compact. </p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Functional_Analysis/Metric_Space/#compact-set-description","title":"Compact set description","text":"<p>Compact set is complete</p> <p>Assume \\(X\\) is a metric space, if \\(U\\subset X\\) is a compact set, then \\(U\\) is complete.</p> Proof <p>By definition. For all Cauchy's sequence \\(\\{x_n\\}\\in U\\), it has a subsequence \\(\\{x_{n_k}\\}\\) converging to a point \\(x\\in U\\), then since</p> \\[ |x_n-x|\\leq |x_n-x_{n_k}|+|x_{n_k}-x| \\] <p>so \\(x_n\\) converges to \\(x\\in U\\), so \\(U\\) is complete.</p> <p>Compact nested set</p> <p>Assume \\(\\{K_n\\}\\subset X\\) is a non-empty compact set sequence, which satisfies</p> \\[ K_1\\supset K_2\\supset \\cdots\\supset K_n\\supset \\cdots \\] <p>then \\(\\bigcap_{n=1}^\\infty K_n\\neq \\varnothing\\).</p> Proof <p>Choose a point \\(x_n\\in K_n\\) and we have a sequence \\(\\{x_n\\}\\subset K_1\\). Since \\(K_1\\) is a compact set, \\(\\exists \\{x_{n_k}\\}\\rightarrow x\\in K_1\\). </p> <p>For any given \\(N\\geq 1\\), choose \\(n_k\\geq N\\), then by nested property we have \\(x_{n_k}\\in K_N\\). Since \\(K_N\\) is closed (readers could prove this), we have \\(x\\in K_N\\). So</p> \\[ x\\in \\bigcap_{n=1}^\\infty K_n. \\] <p>Compact set \\(K\\), is closed, because for a limit point \\(x\\), there exists a sequence \\(x_n\\subset K\\) converges to it. By compact set, it has a subsequence \\(x_{n_k}\\) converges to \\(y\\in K\\). By </p> \\[ |x-y|\\leq |x-x_{n_k}|+|x_{n_k}-y| \\] <p>we have \\(x=y\\). So the limit point is in \\(K\\).</p> <p>In \\(\\mathbb{R}\\), bounded set is described by Balzano-Weierstrass Theorem, and closed interval is described by Heine-Borel finite-covering Theorem. Corresponding to closed interval, compact set could be described by Open-covering Theorem.</p> <p>Definition of Open Covering</p> <p>Assume \\(X\\) is a metric space, \\(A\\subset X\\). \\(\\{G_c\\}_{c\\in J}\\) is a family of open sets in \\(X\\). If </p> \\[ A\\subset \\bigcup_{c\\in J}G_c \\] <p>then \\(\\{G_c\\}_{c\\in J}\\) is called an open covering of \\(A\\). If \\(J\\) has finitely namy number, then \\(\\{G_c\\}_{c\\in J}\\) is called a finitely open covering of \\(A\\).</p> <p>Gross's Open Covering Theorem</p> <p>Assume \\(X\\) is a metric space, \\(A\\subset X\\). \\(A\\) is a comapct set, iff for any open covering \\(\\{G_c\\}_{c\\in J}\\) of \\(A\\), we could choose a finitely open covering from it.</p> Proof <ul> <li>Necessary condition.</li> </ul> <p>Assume \\(\\{G_c\\}_{c\\in J}\\) is an open covering of \\(A\\). If </p> \\[ \\begin{align} \\forall \\varepsilon&gt;0, \\exists x\\in A, \\forall c\\in J, s.t. B_\\varepsilon(x)\\not\\subset G_c,\\label{opposite} \\end{align} \\] <p>then let \\(\\varepsilon_n=\\frac{1}{2^n}\\), then we have a sequence \\(\\{x_n\\}\\in A\\), such that </p> \\[ B_{\\frac{1}{2^n}}(x_n)\\not\\subset G_c. \\] <p>Since \\(A\\) is a compact set, \\(\\exists \\{x_{n_k}\\}\\) converging to \\(x_0\\in A\\). Also because \\(A\\subset \\bigcup_{c\\in J}G_c\\), there exists \\(c_0\\in J\\), such that \\(x_0\\in G_{c_0}\\). Choose \\(N\\) such that </p> \\[ \\frac{1}{2^N}&lt;d(x_0,\\partial G_{c_0}), \\] <p>then \\(B_{\\frac{1}{2^n}}(x_n)\\subset G_{c_0}\\) whenever \\(n&gt;N\\), which contradicts the assumption \\(\\ref{opposite}\\)! So actually we have its opposite</p> \\[ \\exists \\varepsilon_0&gt;0,\\forall x\\in A, \\exists c\\in J, s.t. B_{\\varepsilon_0}(x)\\subset G_{c} \\] <p>Still because \\(A\\) is compact, it is also relative compact, or to be specific, totally bounded. So from \\(\\{B_{\\varepsilon}(x)\\}_{x\\in A}\\), we have a finitely many number \\(\\{B_{\\varepsilon}(x_l)\\}_{1\\leq l\\leq L}\\) which covers \\(A\\). So for each \\(l\\), we choose a \\(G_{c_l}\\) such that \\(B_{\\varepsilon}(x_l)\\subset G_{c_l}\\), then </p> \\[ A\\subset \\bigcup_{l=1}^L G_{c_l}. \\] <ul> <li>Sufficient condition. </li> </ul> <p>We still use contradiction. For any \\(\\{x_n\\}\\subset A\\), if \\(A\\) is not compact, then every point of \\(A\\) is not a limit point of \\(\\{x_n\\}\\), i.e.</p> \\[ \\forall y\\in A, \\exists \\delta_y, n_y&gt;0, s.t. x_n\\not\\in B_{\\delta_y}(y)\\quad \\text{ whenever } n\\geq n_y \\] <p>from above we formulate a huge open covering of \\(A\\): \\(\\bigcup_{y\\in A}B_{\\delta_y}(y)\\). By theorem condition, we could find finitely namy number \\(L\\) such that</p> \\[ A\\subset \\bigcup_{l=1}^L B_{\\delta_{y_l}}(y_l). \\] <p>Let \\(N=\\max\\{n_{y_1}, \\cdots, n_{y_L}\\}\\), then </p> \\[ x_n \\not\\in \\bigcup_{l=1}^L B_{\\delta_{y_l}}(y_l),\\quad\\text{ whenever } n\\geq N. \\] <p>which contradicts the assumption! The core is, we could choose a finite number of open covering to expel \\(x_n\\) whenever \\(n&gt;N\\).</p> <p>Here any open covering allow the open set to be strange, usually converges to a point, so we could not choose a finite-number of the sets to cover if it is only totally bounded.</p>"},{"location":"Math/Functional_Analysis/Metric_Space/#continuous-mapping-on-compact-set","title":"Continuous mapping on compact set","text":"<p>Image of continuous mapping</p> <p>Assume \\(X\\), \\(Y\\) are metric space, with metric \\(\\rho\\), \\(\\rho'\\), respectively. \\(A\\subset X\\) is a compact set, \\(T\\) is a continuous mapping from \\(X\\) to \\(Y\\), then the image of \\(T\\), denoted as \\(T(A)\\) is also a compact set in \\(Y\\).</p> <p>Moreover, the above condition could give us stronger condition, which is, continuous mapping on compact set is also uniformly continuous. That is, \\(T\\) is a uniformly continuous on \\(A\\).</p> Proof <p>\\(\\forall \\{y_n\\}\\subset Y\\), there exsits \\(\\{x_n\\}\\) such that \\(Tx_n=y_n\\). Since \\(A\\) is a compact set, \\(\\{x_n\\}\\) has a subsequence \\(\\{x_{n_k}\\}\\) converging to a point \\(x\\in X\\), then by continuity of \\(T\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\forall x_{n_k}\\in B_\\delta(x)\\),</p> \\[ \\rho'(y_{n_k}, Tx)=\\rho'(Tx_{n_k}, Tx)&lt;\\varepsilon. \\] <p>So \\(Tx\\) is a converging point of \\(\\{y_{n_k}\\}\\), and \\(Tx\\in T(A)\\). </p> <p>We could also use open sets in image, by continuous function, its preimage open sets have finite-number covering \\(A\\), so image itself has finite-number covering \\(T(A)\\).</p> <ul> <li>For uniformly continuity. We use contradiction. \\(\\exists \\varepsilon_0&gt;0\\), \\(\\forall \\delta_n=\\frac{1}{n}&gt;0\\), \\(\\exists x_n, y_n\\in A\\), such that </li> </ul> \\[ \\rho'(Tx_n, Ty_n)\\geq \\varepsilon_0,\\text{ whenever } \\rho(x_n,y_n)&lt;\\frac{1}{n}. \\] <p>since \\(A\\) is a compact set, then \\(\\{x_n\\}\\) has a subsequence \\(\\{x_{n_k}\\}\\) converging to \\(x\\in A\\). Actually, \\(y_{n_k}\\) and \\(x\\) could also be close enough, by the following inequation</p> \\[ \\rho(y_{n_k}, x_0)\\leq \\rho(y_{n_k}, x_{n_k})+\\rho(x_{n_k},x)\\rightarrow 0 (k\\rightarrow \\infty). \\] <p>However by continuous mapping</p> \\[ \\rho'(Tx_{n_k}, Ty_{n_k})\\leq \\rho'(Tx_{n_k}, Tx)+\\rho'(Tx, Ty_{n_k})\\rightarrow 0 (k\\rightarrow \\infty), \\] <p>which contradicts!</p> <p>We could also use finite-open covering to prove. That is, choose a open covering \\(\\bigcup_{x\\in A}B(x,\\varepsilon_0)\\supset A\\), then there exsits finite-number \\({x_n}_{1\\leq n\\leq N}\\) such that </p> \\[ \\bigcup_{n=1}^N B(x_n, \\varepsilon_0)\\supset A. \\] <p>\\(\\forall x,x'\\in A\\), let \\(\\rho(x,x')&lt;\\varepsilon_0\\), then they must lie in same open ball \\(B(x_{0}, \\varepsilon_0)\\). By continuous mapping, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), choose \\(\\varepsilon_0&lt;\\delta\\), we have</p> \\[ \\rho'(Tx, Tx_{0})&lt;\\frac{\\varepsilon}{2},\\quad \\rho'(Tx', Tx_{0})&lt;\\frac{\\varepsilon}{2}, \\] <p>so</p> \\[ \\rho'(Tx,Tx')&lt;\\rho'(Tx, Tx_{0})+\\rho'(Tx', Tx_{0})&lt;\\varepsilon. \\] <p>Corollary: Continuous functional</p> <p>Assume \\(X\\) is a metric space, and \\(A\\subset X\\) is a compact set. \\(f\\) is a continuous functional on \\(A\\), then \\(f\\) is bounded, and could arrive at its infimum and supremum.</p> Proof <p>\\(f\\) maps a compact set \\(A\\) into \\(T(A)\\subset \\mathbb{R}\\), which is closed and bounded.</p>"},{"location":"Math/Functional_Analysis/Metric_Space/#contraction-mapping-method","title":"Contraction Mapping Method","text":"<p>We have already give an introduction of contraction mapping in proving the existence and uniqueness of solution to an ODE. Now we give a more detailed discussion. </p> <p></p> <p>Theorem for contraction mapping</p> <p>Assume \\(X\\) is a complete metric space, with metric \\(\\rho\\). \\(T\\) is a mapping from \\(X\\) to \\(X\\), and satisfies \\(\\forall x,y\\in X\\)</p> \\[ \\rho(Tx,Ty)\\leq \\theta \\rho(x,y), \\] <p>where \\(\\theta\\in [0,1)\\) is constant. Then \\(T\\) has a unique fixed point in \\(X\\).</p> Proof <p>Actually, the above definition guarantees that starting from an arbitrary point, we could construct a Cauchy sequence. Choose \\(x_0\\in X\\), </p> \\[ x_1=Tx_0,x_2=Tx_1,\\quad x_n=Tx_{n-1}, \\] <p>easy to show that \\(\\{x_n\\}\\) is a Cauchy sequence. Let \\(k=\\rho(x_1,x_0)\\), so \\(\\rho(x_2,x_1)\\leq \\theta\\rho(x_1,x_0)=k\\theta\\), by induction we have</p> \\[ \\rho(x_n, x_{n-1})\\leq k\\theta^{n-1}. \\] <p>So \\(\\forall \\varepsilon&gt;0\\), choose \\(n\\) such that \\(k\\frac{\\theta^{n-1}}{1-\\theta}&lt;\\varepsilon\\), then </p> \\[ \\begin{align*} \\rho(x_n,x_{n+k})&amp;\\leq \\rho(x_n, x_{n+1})+\\cdots, \\rho(x_{n+k-1}, x_{n+k})\\\\ &amp;\\leq (1+\\theta+\\cdots+\\theta^{k-1})k\\theta^{n-1}\\\\ &amp;=\\frac{1-\\theta^{k}}{1-\\theta}k\\theta^{n-1}\\\\ &amp;\\leq \\frac{1}{1-\\theta}k\\theta^{n-1}&lt;\\varepsilon. \\end{align*} \\] <p>Then by \\(X\\) is complete, \\(\\{x_n\\}\\) has a convergent point \\(x\\in X\\). Now we show that it is a fixed point.</p> \\[ x=\\lim_{n\\rightarrow \\infty}x_{n+1}= \\lim_{n\\rightarrow \\infty}T^n x_0=T\\lim_{n\\rightarrow \\infty}T^n x_0=Tx. \\] <p>it is unique, since for two fixed points \\(x,y\\), we have</p> \\[ \\rho(x,y)=\\rho(Tx,Ty)\\leq \\theta\\rho(x,y) \\] <p>which means \\(\\rho(x,y)=0\\), so \\(x=y\\).</p> <p>Example. Prove the following intergal equation has a unique solution when \\(\\lambda\\) is sufficiently small.</p> \\[ x(t)=f(t)+\\lambda \\int_a^b K(t,s)x(s)ds \\] <p>where \\(f(t)\\in L^2[a,b]\\) and \\(K(t,s)\\) is kernel function defined on \\([a,b]\\times [a,b]\\), which satisfies</p> \\[ \\int_a^b\\int_a^b |K(t,s)|^2ds dt&lt;\\infty. \\] Proof <ul> <li>Show that under metric </li> </ul> \\[ \\rho(x,y)=\\left(\\int_a^b|x(t)-y(t)|^2dt\\right)^{\\frac{1}{2}} \\] <p>the mapping \\(T\\)</p> \\[ (Tx)(t)=f(t)+\\lambda \\int_a^b K(t,s)x(s)ds  \\] <p>is a mapping from \\(X\\) to \\(X\\), or to be more specific, \\(Tx\\in L^2[a,b]\\). Notice we only have to show that the latter in the integral equation is L2-integrable. Actually</p> \\[ \\begin{align*} \\int_a^b dt\\left|\\int_a^b K(t,s)x(s)ds\\right|^2&amp;\\leq \\int_a^b dt \\left[\\int_a^b |K(t,s)|^2 ds \\int_a^b |x(s)|^2 ds\\right]\\\\ &amp;=\\int_a^b |x(s)|^2 ds \\int_a^b dt \\int_a^b |K(t,s)|^2 ds&lt;\\infty. \\end{align*} \\] <p>Choose \\(\\lambda\\) such that \\(|\\lambda|\\left[\\int_a^b dt\\int_a^b |K(t,s)|^2 ds\\right]^\\frac{1}{2}&lt;1\\), then</p> \\[ \\begin{align*} \\rho(Tx,Ty)&amp;=\\left(\\int_a^b  |\\lambda|\\left|\\int_a^b K(t,s)[x(s)-y(s)]ds\\right|^2 dt\\right)^\\frac{1}{2}\\\\ &amp;\\leq \\left(\\int_a^b  |\\lambda| \\left[\\int_a^b |K(t,s)|^2 ds\\right] \\left[\\int_a^b |x(s)-y(s)|^2 ds\\right] dt\\right)^\\frac{1}{2}\\\\ &amp;= |\\lambda| \\left[\\int_a^b |x(s)-y(s)|^2 ds\\right]^\\frac{1}{2}  \\left(\\int_a^b \\left[\\int_a^b |K(t,s)|^2 ds\\right] dt\\right)^\\frac{1}{2}\\\\ &amp;\\leq |\\lambda|\\left[\\int_a^b dt\\int_a^b |K(t,s)|^2 ds\\right]^\\frac{1}{2}\\rho(x,y) \\end{align*} \\] <p>means \\(T\\) is a contraction mapping.</p> <p>Extended Contraction mapping theorem</p> <p>Assume \\(X\\) is a complete metric space, with metric \\(\\rho\\). \\(T\\) is a mapping from \\(X\\) to \\(X\\). If two constants \\(\\exists n_0&gt;0\\), \\(\\theta\\in [0,1)\\), such that</p> \\[ \\rho(T^{n_0}x,T^{n_0}y)\\leq \\theta \\rho(x,y),\\quad \\forall x,y\\in X, \\] <p>then \\(T\\) has a unique fixed point in \\(X\\).</p> Proof <p>If we view \\(T^{n_0}\\) as a mapping \\(T'\\), then apply Theorem for contraction mapping, we have a unique fixed point \\(x\\in X\\), such that \\(x=T'x\\). We show that this is also the unique fixed point of \\(T\\).</p> <p>Since \\(Tx=T T'x=T T^{n_0}x=T^{x_0+1}x=T' (Tx)\\), so \\(Tx\\) is a fixed point of \\(T'\\). By uniqueness of fixed point, we have \\(x=Tx\\). </p> <p>If \\(T\\) has another fixed point \\(y\\) of \\(T\\), then \\(Ty=y\\). So by iterating over \\(n_0\\), \\(T^{n_0}y=T^{n_0-1}Ty=T^{n_0-1}y=\\cdots=y\\), we have \\(y\\) is a fixed point of \\(T^{n_0}\\), by uniqueness of fixed point for \\(T^{n_0}\\), we have \\(x=y\\).</p>"},{"location":"Math/Lie_Algebra/","title":"Lie Algebra","text":"<p>Reference</p> <ul> <li> <p>Lie Groups, Lie Algebras, and Representations, Brian Hall.</p> </li> <li> <p>A micro Lie theory for state estimation in robotics, Joan Sol\u00e0, Jeremie Deray, Dinesh Atchuthan</p> </li> </ul> <p>Lie groups</p> <p>A Lie group \\(\\mathcal{G}\\) is a smooth manifold whose elements satisfies group axioms.</p> <p>Check the definition of groups in ODE general theory.</p> <p>We could conclude as 4 axioms: closure under \\(\\circ\\), identity, inverse and associativity.</p> <p>The smoothness of the manifold implies the existence of a unique tangent space at each point</p> <p>group actions</p> <p>Given a Lie group \\(\\mathcal{G}\\) and a set \\(\\mathcal{V}\\), we denote action \\(\\mathcal{X}\\cdot \\mathcal{v}\\) to be </p> \\[ \\cdot: \\mathcal{G}\\times \\mathcal{V}\\rightarrow \\mathcal{V}, (\\mathcal{X}, \\mathcal{v})\\mapsto \\mathcal{X}\\cdot \\mathcal{v}. \\] <p>and satisfy two axioms: identity \\(\\mathcal{E}\\cdot \\mathcal{v}=\\mathcal{v}\\) and compatibility </p> \\[ (\\mathcal{X}\\circ \\mathcal{Y})\\cdot \\mathcal{v}=\\mathcal{X}\\cdot(\\mathcal{Y}\\cdot \\mathcal{v}) \\] <p><p> </p></p> <p>Lie Algebra</p> <p>Lie algebra \\(\\mathcal{m}\\) of a Lie group \\(\\mathcal{M}\\) is defined to be the tangent space at the identity. </p> \\[ \\mathcal{m}=T_{\\mathcal{E}}\\mathcal{M} \\] <p>this is a linear space, and its elements can be identified by \\(\\mathbb{R}^m\\) where \\(m\\) is the dimension of \\(\\mathcal{m}\\). Denote the element of a Lie algebra to be \\(\\mathcal{v}^{\\land}\\). </p> <p>The structure of the Lie algebra can be found by time-differentiating the group constraint. For multiplicative group, we have </p> \\[ \\mathcal{v}^\\land= \\mathcal{X}^{-1}\\dot{\\mathcal{X}}, \\] <p>where \\(\\mathcal{X}\\) is chosen at \\(t=0\\) or identity.</p> <p>Exponential map</p> <p>\\(\\mathfrak{n}\\)</p>"},{"location":"Math/Lie_Algebra/#gaussian-mixture-model","title":"Gaussian Mixture Model","text":"<p>Use this model to estimate a relatively complex distribution of data, which could be multi-dimensional.</p> \\[ \\mathcal{N}_\\mathcal{M} (\\pmb{x}\\mid \\pmb{\\mu} , \\Sigma)=(2\\pi |\\Sigma|)^{-1/2} \\exp\\left(\\frac{1}{2}\\log_{\\pmb{\\mu}} (\\pmb{x})\\Sigma^{-1} \\log_{\\pmb{\\mu}} (\\pmb{x}) \\right) \\] <p>where \\(\\pmb{x}\\in \\mathcal{M}\\), and \\(\\pmb{\\mu} \\in \\mathcal{M}\\) is the mean of the distribution, \\(\\Sigma\\in \\mathcal{T}_{\\pmb{\\mu}}\\mathcal{M}\\) is the covariance matrix defined on the tangent space.</p> <p>A mixture model is defined by</p> \\[ p(\\pmb{x}\\mid \\theta)=\\sum_{k=1}^K \\pi_k \\mathcal{N}_\\mathcal{M}(\\pmb{x}\\mid \\pmb{\\mu}_k, \\Sigma_k). \\]"},{"location":"Math/Linear_Algebra/","title":"Linear Algebra","text":"<p>This is a review of linear algebra in actual work and study.</p> <p>Reference</p> <ul> <li>Linear algebra done right, Sheldon Axler</li> </ul>"},{"location":"Math/Linear_Algebra/#operators-on-inner-product-space","title":"Operators on Inner Product Space","text":""},{"location":"Math/Linear_Algebra/#multilinear-algebra","title":"Multilinear Algebra","text":""},{"location":"Math/Linear_Algebra/Multi_Alge/","title":"Multilinear Algebra","text":""},{"location":"Math/Linear_Algebra/Multi_Alge/#bilinear-forms","title":"Bilinear forms","text":"<p>Definition of bilinear form</p> <p>A bilinear form on \\(V\\) is a function \\(\\beta:V\\times V\\rightarrow \\mathbb{F}\\), such that </p> \\[ v \\mapsto \\beta (v,u),\\quad v\\mapsto \\beta (u,v) \\] <p>are both linear functionals on \\(V\\) for each \\(u\\in V\\).</p> <p>Example. </p> <p>(i) \\(\\mathbb{F}=\\mathbb{R}\\), inner product on \\(V\\), i.e. \\((u,v)\\mapsto \\langle u,v\\rangle\\) is a bilinear form.</p> <p>Note that for \\(\\mathbb{F}=\\mathbb{R}\\), a bilinear form differs from inner product in that inner product requires symmetry (\\(\\beta (u,v)=\\beta(v,u)\\)) and positive definiteness (\\(\\beta(v,v)&gt;0\\) for all \\(v\\in V-\\{\\theta\\}\\)), whereas these properties are not required for a bilinear form.</p> <p>Example. Show that a bilinear form \\(\\beta\\) on \\(V\\), is also a linear map on \\(V\\times V\\), then \\(\\beta=\\theta\\).</p> Proof <p><p>\\(\\square\\)</p></p> <p>For simplicity, we denote the set of all the bilinear forms on \\(V\\) by \\(V^{(2)}\\).</p> <p>Matrix form for a bilinear form</p> <p>composition of a bilinear form and an operator</p> <p>Suppose \\(\\beta\\) is a bilinear form on \\(V\\) and \\(T\\) is a linear operator on \\(V\\). Define two supplimentary bilinear form</p> \\[ \\alpha (u,v)=\\langle u, Tv\\rangle, \\quad \\rho (u,v) =\\langle Tu, v\\rangle. \\] <p>Let \\(e_1,\\cdots, e_n\\) be a basis of \\(V\\), then </p> \\[ \\mathcal{M}(\\alpha) = \\mathcal{M}(\\beta)\\mathcal{M}(T), \\quad \\mathcal{M}(\\rho) = \\mathcal{M}(T)^t \\mathcal{M}(\\beta). \\] Proof <p><p>\\(\\square\\)</p></p> <p>change-of-basis formula</p>"},{"location":"Math/Linear_Algebra/Multi_Alge/#symmetric-bilinear-form","title":"Symmetric bilinear form","text":"<p>Definition of Symmetric bilinear form</p> <p>A bilinear form \\(\\rho \\in V^{(2)}\\) is called symmetric if</p> \\[ \\rho (u,w) = \\rho (w.u) \\] <p>for all \\(u,w \\in V\\). The set of symmetric bilinear form on \\(V\\) is denoted by \\(V_{sym}^{(2)}\\).</p> <p>Example. </p> <p>(i) Suppose \\(V\\) is a real inner product space, then </p> \\[ \\rho(u,w) = \\langle u, Tw\\rangle  \\] <p>is symmetric bilinear form iff \\(T\\) is self-adjoint.</p> <p>Alternating bilinear form(\u4ea4\u9519\u53cc\u7ebf\u6027\u578b)</p> <p>A bilinear form \\(\\alpha\\) on \\(V\\) is call alternating, if</p> \\[ \\alpha(v,v)=0, \\quad \\forall v\\in V. \\] <p>The set of all alternating bilinear form is denoted by \\(V^{(2)}_{alt}\\).</p> <p>Example. </p> <p>(i) Suppose \\(n\\geq 3\\) and then \\(\\alpha:\\mathbb{F}^n \\times \\mathbb{F}^n \\rightarrow \\mathbb{F}\\) defined by </p> \\[ \\alpha ((x_1, \\cdots, x_n), (y_1, \\cdots, y_n))=x_1 y_2-x_2y_1 + x_1y_3-y_1x_3 \\] <p>is alternating.</p> <p>Characterization of alternating bilinear form</p> <p>A bilinear form \\(\\alpha\\) on \\(V\\) is alternating, iff</p> \\[ \\alpha(u,w) = -\\alpha (w,u), \\quad \\forall u,w\\in V. \\] Proof <p>Now the following theorem describes the composition of \\(V^{(2)}\\).</p> <p>Theorem</p> <p>The set \\(V^{(2)}_{sym}\\) and \\(V^{(2)}_{alt}\\) are subsets of \\(V^{(2)}\\). Furthermore, </p> \\[ V^{(2)} = V^{(2)}_{alt} \\oplus V^{(2)}_{sym}. \\] Proof"},{"location":"Math/Linear_Algebra/Multi_Alge/#quadratic-form","title":"Quadratic form","text":""},{"location":"Math/Linear_Algebra/Op_IPS/","title":"Operators on Inner Product Space","text":""},{"location":"Math/Linear_Algebra/Op_IPS/#orthogonal-complements","title":"Orthogonal Complements","text":"<p>Theorem for null space and range of \\(T^*\\)</p> <p>Suppose \\(T\\in \\mathcal{L}(V, W)\\), then </p> <p>(i) \\(N T^*=(R T)^\\perp\\).</p> <p>(ii) \\(R T^*=(NT)^\\perp\\).</p> Proof <p><p>\\(\\square\\)</p></p>"},{"location":"Math/Linear_Algebra/Op_IPS/#singular-value-decomposition","title":"Singular value decomposition","text":"<p>For a linear map \\(T \\in \\mathcal{L}(V,W)\\), we could decompose it as we have for self-adjoint operator or normal operator.</p> <p>Recall the important Riesz representation theorem in inner product space.</p> <p></p> <p>Riesz representation theorem</p> <p>Assume \\(V\\) is finite-dimensional and \\(\\phi\\) is a linear functional on \\(V\\), then there exists a unique vector \\(v\\in V\\) such that </p> \\[ \\phi(u)=\\langle v,u\\rangle,\\quad \\forall v\\in V. \\] Proof <p><p>\\(\\square\\)</p></p> <p>In functional analysis, we have actually a similar result for infinite-dimensional spaces.</p> <p>The following lemma of \\(T^*T\\) is necessary. </p> <p>Lemma</p> <p>Properties of \\(T^*T\\). Suppose \\(T \\in \\mathcal{L}(V,W)\\)</p> <p>(i) \\(T^*T\\) is a self-adjoint operator on \\(V\\). We could also check \\(TT^*\\) is a self-adjoint operator on \\(W\\).</p> <p>(ii) \\(N T^*T=N T\\).</p> <p>(iii) \\(R T^*T = R T^*\\).</p> <p>(iv) dimension. \\(\\text{Dim} R T=\\text{Dim} R T^*=\\text{Dim}R T^*\\).</p> Proof <p>(i) by definition. </p> \\[ \\langle T^*Tv, w\\rangle=\\langle Tv, Tw\\rangle=\\langle v, T^*T w\\rangle\\Rightarrow T^*T=(T^*T)^*. \\] <p>(ii) \\(N T\\subset N T^*T\\) is apparent. Assume \\(v\\in N T^*T\\), \\(T^*T v=0\\), so \\(\\langle v, T^*Tv\\rangle=0\\), so \\(\\langle Tv, Tv\\rangle=\\|Tv\\|=0\\), which means \\(Tv=0\\).</p> <p>(iii) \\(R T^*T \\subset R T^*T\\) is apparent. For another direction, we use (ii) \\(R T^*T=(N T^*T)^\\perp=(N T)^\\perp=R T^*\\).</p> <p>(iv) Use fundamental theorem of linear maps.</p> <p>Definition of singular value</p> <p>Assume a linear operator \\(T\\in \\mathcal{L}(V, W)\\), the singular values of \\(T\\) are defined as the nonnegative square roots of the eigenvalues of \\(T^*T\\), listed in decreasing order.</p> <p>(SVD) Singular value decomposition</p> <p>Assume a linear operator \\(T\\in \\mathcal{L}(V, W)\\), with its positive singular values \\(s_1,\\cdots, s_r\\). Then there exists orthonormal lists \\(e_1,\\cdots, e_r\\subset v\\), \\(f_1,\\cdots, f_r\\subset W\\), such that </p> \\[ Tv=\\sum_{k=1}^r s_k \\langle v, e_k\\rangle f_k. \\] Proof <p>Here we denote that \\(V\\) and \\(W\\) is finite-dimensional. And the proof is constructive. This method also gives info about the eigenvectors construction.</p> <p>Let \\(s_1,\\cdots,s_n\\) to be the singular value of \\(T(\\text{Dim}V=n)\\), where \\(s_{r+1},\\cdots, s_n\\) are zero singular values.</p> <ul> <li>Apply spectral thoerem to \\(T^*T\\) and there exists orthonormal basis \\(e_1,\\cdots, e_n\\subset V\\), such that </li> </ul> \\[ T^*T e_k=s_k^2 e_k,\\quad k=1,\\cdots, n. \\] <ul> <li>Define \\(f_k=\\frac{Te_k}{s_k}\\) for \\(k=1,\\cdots, r\\).</li> </ul> <p>this is actually orthonormal basis in \\(W\\). This is also inspired by \\(T=W\\Sigma V^T\\), so \\(TV=W\\Sigma\\), so \\(TV\\Sigma^{-1}=W\\), which shows a relationship of basis from \\(V\\) to \\(W\\) space.</p> <ul> <li>Prove the proposition by expressing \\(v\\) in the constructed orthonormal basis</li> </ul> \\[ \\begin{align*} Tv&amp;=T\\left(\\sum_{k=1}^n \\langle v, e_k\\rangle e_k\\right)\\\\ &amp;=\\sum_{k=1}^n  \\langle v, e_k\\rangle T  e_k\\\\ &amp;=\\sum_{k=1}^r  \\langle v, e_k\\rangle s_k f_k\\\\ \\end{align*} \\] <p>for \\(k\\geq r\\), \\(Te_k=0\\) because \\(T^*T e_k =0\\cdot e_k\\) and Property of self-adjoint \\(T^*T\\) (ii).</p> <p>We could also check that the matrix with respect to basis \\(\\{e_k\\}_{1\\leq k\\leq r}\\) and \\(\\{f_k\\}_{1\\leq k\\leq r}\\) which should be extended.</p> <p>Note we have \\(\\{e_k\\}_{1\\leq k\\leq n}\\), and from the above proof we have \\(Te_k=s_k f_k\\) for \\(k\\leq r\\) and \\(0\\) for \\(k &gt;r\\). We shall extend \\(\\{f_k\\}_{1\\leq k\\leq r}\\) to \\(\\{f_k\\}_{1\\leq k\\leq m} (\\text{Dim} W=m)\\) by utilizing \\(N T^*\\). This is because we want to solve \\(R(T)^\\perp\\), which equals \\(NT^*\\) by Theorem for null space and range of \\(T^*\\). (Readers should double check the dimension of \\(N T^*\\), which is \\(m-r\\), for \\(\\text{Dim} RT=r\\).)</p> <p><p>\\(\\square\\)</p></p> <p>Matrix version of SVD, a compact SVD form</p> <p>Assume \\(A\\) is an \\(m\\)-by-\\(n\\) matrix of rank \\(r\\geq 1\\). Then there exists an \\(m\\)-by-\\(r\\) matrix \\(W\\) with orthogonal columns, an \\(r\\)-by-\\(r\\) diagonal matrix \\(\\Sigma\\) with positive numbers on the diagonal, and an \\(n\\)-by-\\(r\\) matrix \\(V\\) with orthonormal columns such that</p> \\[ A=W\\Sigma V^*. \\] ProofAnother version <p>Let \\(T: \\mathbb{F}^n\\rightarrow \\mathbb{F}^m\\) whose matrix with respect to the standard basis equals \\(A\\). From the above proof of the SVD theorem, we have \\(\\text{Dim}RT=r\\) and </p> \\[ Tv=\\sum_{k=1}^r s_k \\langle v, e_k\\rangle f_k. \\] <p>we make use of the above structure. Let</p> <p>\\(W\\) to be the \\(m\\)-by-\\(r\\) matrix whose columns are \\(f_1,\\cdots,f_r\\),</p> <p>\\(\\Sigma\\) to be the \\(r\\)-by-\\(r\\) diagonal matrix \\(\\Sigma\\) with entries \\(s_1,\\cdots,s_r\\), </p> <p>\\(V\\) to be the \\(n\\)-by-\\(r\\) matrix whose columns are \\(e_1,\\cdots,e_r\\). </p> <p>Choose \\(u_k\\), a standard base of \\(\\mathbb{F}^m\\), then apply this matrix </p> \\[ (AV-W\\Sigma)u_k=Ae_k-Ws_k u_k=s_kf_k-s_kf_k=0. \\] <p>so \\(AV=W\\Sigma\\), multiply both sides by \\(V^*\\) and we have \\(A=W\\Sigma V^*\\). But we have to be careful.</p> <p>Here actually \\(VV^*= I\\) does not hold absolutely. We have to argue as follows. If \\(k\\leq r\\), \\(V^*e_k=u_k\\), so \\(VV^*e_k=e_k\\). Thus \\(AVV^*v=Av\\) for all \\(v\\in \\text{span}(e_1,\\cdots,e_m)\\). For \\(v\\in \\text{span}(e_1,\\cdots,e_m)^\\perp\\), we have \\(Av=0\\) and \\(V^*v=0\\), so we also have \\(AVV^*v=Av=0\\).</p> <p><p>\\(\\square\\)</p></p> <p>Denote \\(S=\\text{diag}(s_1,\\cdots,s_r)\\), \\(\\Sigma=\\left[\\begin{array}{cc} S &amp; 0 \\\\ 0&amp;0\\end{array}\\right]_{n \\times n}\\), \\(V_1=(e_1,\\cdots,e_r)\\), \\(V_2=(e_{r+1}, \\cdots,e_n)\\) where the orthonomal basis in \\(V_2\\) is with respect to eigenvalue \\(0\\). Notice</p> \\[ \\begin{align*} A^* AV_1 = S^2 V_1&amp;=V_1 S^2 \\\\ V_1^* A^* AV_1&amp;=S^2 \\\\ \\Rightarrow S^{-1} V_1^* A^* A V_1 &amp;S^{-1}=I_{r}. \\end{align*} \\] <p>define \\(W_1=A V_1 S^{-1}\\), we have \\(W_1^* W_1=I_r\\). As for \\(V_2\\), we have \\(A^*A V_2=V_2 0^2=0\\), So \\(V_2^* A^* A V_2=0\\), \\(AV_2=0\\). </p> <p>Choose \\(W_2\\) to be an orthogonal complement of \\(W_1\\), which is actually calculated from \\(NA^*\\), \\(A^* W_2=0\\). So let \\(W=(W_1,W_2)\\), we have</p> \\[ \\begin{align*} W^T A V&amp;=\\left[\\begin{array}{cc} W_1^T A V_1 &amp; W_1^T A V_2\\\\ W_2^T A V_1 &amp; W_2^T A V_2\\end{array}\\right]\\\\ &amp;=\\left[\\begin{array}{cc} W_1^T A V_1 &amp;0\\\\ W_2^T A V_1 &amp; 0\\end{array}\\right]\\quad\\text{by } AV_2=0\\\\ &amp;=\\left[\\begin{array}{cc} W_1^T W_1 S &amp;0\\\\ W_2^T W_1 S &amp; 0\\end{array}\\right]\\\\ &amp;=\\left[\\begin{array}{cc} S &amp;0\\\\ 0 &amp; 0\\end{array}\\right] \\end{align*} \\]"},{"location":"Math/Linear_Algebra/Op_IPS/#principle-component-analysis","title":"Principle Component Analysis","text":"<p>We first talk about total PCA.</p> <p>Principle Component Analysis</p> <p>Assume \\(X, Y\\in\\mathbb{R}^n\\) are random vectors. A linear map \\(T: \\mathbb{R}^n\\rightarrow \\mathbb{R}^n\\) defined by</p> \\[ Y=TX,\\quad y_i =\\alpha_i^T X,\\quad i=1,\\cdots,n. \\] <p>where \\(T\\) has an orthonormal matrix \\(A=(\\alpha_i)^T\\) with respect to standard basis, \\(\\alpha_i\\in \\mathbb{R}^n\\) and \\(\\alpha_i^T \\alpha_j=\\delta_{ij}\\). We could show that there exsits \\(\\alpha_1\\) such that after transformation, \\(y_1\\) has the maximum variance, which is called a principle component.</p> <p>Firstly, let us recall that \\(\\pmb{\\mu}=(\\mathbb{E}x_1,\\cdots,\\mathbb{E}x_n)^T\\) is the mean vector, and corresponding covaraince matrix \\(\\Sigma=(\\text{cov}(x_i,x_j))_{ij}=\\mathbb{E}(X-\\pmb{\\mu})(X-\\pmb{\\mu})^T=\\mathbb{E}X X^T-\\pmb{\\mu}\\pmb{\\mu}^T.\\)</p> <p>After transformation, we have the following property by linearity of ME.</p> <p>Property of ME after Transformation</p> <p>(i) \\(\\pmb{\\mu}_y=A\\pmb{\\mu}\\), that is, \\(\\mathbb{E}y_i=\\alpha_i^T \\pmb{\\mu}\\).</p> <p>(ii) \\(\\Sigma_y = A^T\\Sigma A\\), that is, \\(\\sigma_{ij}=\\text{cov}(x_i,x_j)=\\alpha_i^T \\Sigma \\alpha_j\\).</p> Proof <p>We prove for (ii). By definition</p> \\[ \\begin{align*} \\sigma_{ij}&amp;=\\mathbb{E}(y_i-\\alpha_i^T \\pmb{\\mu})(y_j-\\alpha_j^T \\pmb{\\mu})^T\\\\ &amp;=\\mathbb{E}(\\alpha_i^T X-\\alpha_i^T \\pmb{\\mu})(\\alpha_j^T X-\\alpha_j^T \\pmb{\\mu})\\\\ &amp;=\\mathbb{E}\\alpha_i^T (X-\\pmb{\\mu})\\alpha_j^T (X-\\pmb{\\mu})\\\\ &amp;=\\mathbb{E}\\alpha_i^T (X-\\pmb{\\mu})(X- \\pmb{\\mu})^T\\alpha_j\\quad \\text{by symmetry of inner product}\\\\ &amp;=\\alpha_i^T\\mathbb{E} [(X-\\pmb{\\mu})(X- \\pmb{\\mu})^T]\\alpha_j\\\\ &amp;=\\alpha_i^T \\Sigma \\alpha_j. \\end{align*} \\] <p>From the above property, we could explain: we ask the matrix to be orthonormal because we want the covariance matrix of Y to be diagonal, i.e. \\(y_i\\) and \\(y_j\\) are mutually irrelevant unless \\(i=j\\).</p> <p></p> <p>Theorem for principle component analysis</p> <p>The maximum of variance of \\(y_1\\) is reached when \\(\\alpha_1\\) is the eigenvector of the maximum eigenvalue \\(\\lambda_1\\) of matrix \\(\\Sigma\\), and satisfies \\(Var(y_1)=\\lambda_1\\).</p> Proof <p>To maximize \\(Var(y_1)\\), is equivalently to maximize \\(\\alpha_1^T \\Sigma \\alpha_1\\) for all possible \\(\\alpha_1\\in\\mathbb{R}^n\\). Take derivative(gradient for \\(\\alpha_1\\in \\mathbb{R}^n\\)) of the corresponding Lagrandian function with condition \\(\\alpha_1^T\\alpha_1=1\\) and we have \\(2\\Sigma \\alpha_1 -2 l (\\alpha_1)= \\theta\\). So to reach the maximum, \\(l\\) is an eigenvalue of \\(\\Sigma\\), and at this time the goal function equals </p> \\[ \\alpha_1^T \\Sigma \\alpha_1=\\alpha_1^T l \\alpha_1=l \\alpha_1^T \\alpha_1=l. \\] <p>To reach the maximum, let \\(l\\) to be the maximum eigenvalue of \\(\\Sigma\\), denoted by \\(\\lambda_1\\), and choose a eigenvector \\(\\alpha_1\\) correspondingly. </p> <p><p>\\(\\square\\)</p></p> <p>If we want to get \\(k\\) principle components, which are mutually irrelevant, i.e. \\(\\text{cov}(y_i,y_j)=0\\) unless \\(i=j\\), we could have the following conclusion.</p> <p>Theorem for \\(k\\) principle components analysis</p> <p>The \\(k\\) principle components of \\(X\\) is determined by a transformation \\(T\\) defined by </p> \\[ y_i = \\alpha_i^T X,\\quad i=1,\\cdots, k, \\] <p>where \\(\\alpha_i(i=1,\\cdots, k)\\) is the eigenvector with respect to the maximum \\(k\\) eigenvalues of \\(\\Sigma\\).</p> Proof <p>We only prove for \\(k=2\\), the other situation could be deduced by induction.</p> <p>We aim to find a vector \\(\\alpha_2\\), such that we maximize \\(\\alpha_2\\Sigma \\alpha_2\\), with a condition \\(\\alpha_2^T \\alpha_2=1, \\langle\\alpha_2,\\alpha_1 \\rangle=0\\). Take a gradient we have</p> \\[ 2\\Sigma \\alpha_2 -2l_1 \\alpha_2 -l_2 \\alpha_1 = \\theta. \\] <p>apply inner product with \\(\\alpha_1\\), we have</p> \\[ 2\\lambda_1\\alpha_2^T\\alpha_1-l_2 = 0,\\quad \\Sigma\\text{ is self-adjoint} \\] <p>so \\(l_2=0\\). Then by the same logic of Theorem for principle component analysis, we also have \\(\\lambda_2\\) to be the second largest eigenvalue of \\(\\Sigma\\). Apparently \\(l_2\\neq \\lambda_1\\) otherwise \\(\\langle\\alpha_2,\\alpha_1\\rangle\\neq 0\\).</p> <p><p>\\(\\square\\)</p></p> <p>After transformation to \\(Y\\in \\mathbb{R}^n\\), we have an amazing result of total variance of \\(Y\\)</p> \\[ \\sum_{i=1}^n Var(y_i)=\\sum_{i=1}^n \\lambda_i = \\sum_{i=1}^n \\sigma_{ii}. \\] <p>which is given by taking the trace of </p> \\[ tr(\\Sigma_y) = tr(A^T \\Sigma A) = \\sum_{i=1}^n (\\alpha_i^T \\Sigma \\alpha_i) = (\\lambda_i)  \\] <p>and making use of \\(\\Sigma = A^T \\Sigma_y A = A \\Sigma_y A^T\\).</p> <p>After choosing \\(n\\) principle components, we also want to find some relationship between \\(y_i\\) and \\(x_j\\).</p> <p>Factor of Loading</p> <p>The factor of loading for \\(y_i\\) with respect to \\(x_j\\) is defined by</p> \\[ \\rho(y_i,x_j)=\\frac{\\sqrt{\\lambda_i}\\alpha_{ji}}{\\sqrt{\\sigma_{jj}}}. \\] <p>where \\(\\alpha_{ji}\\) is the \\(j\\)th component of vector \\(\\alpha_i\\). We have to compute this element-wisely.</p> Proof <p>Just by definition.</p> \\[ \\begin{align*} \\rho(y_i, x_j)&amp;=\\frac{\\text{cov}(y_i, x_j)}{\\sqrt{\\lambda_i\\sigma_{jj}}}\\\\ &amp;=\\frac{\\text{cov}(\\alpha_i^T X, e_j^T X)}{\\sqrt{\\lambda_i\\sigma_{jj}}}\\\\ &amp;=\\frac{\\alpha_i^T\\text{cov}(X,X)e_j}{\\sqrt{\\lambda_i\\sigma_{jj}}}\\\\ &amp;=\\frac{e_j^T\\Sigma \\alpha_i}{\\sqrt{\\lambda_i\\sigma_{jj}}}\\\\ &amp;=\\frac{\\lambda_i e_j^T\\alpha_i}{\\sqrt{\\lambda_i\\sigma_{jj}}}\\\\ &amp;=\\frac{\\sqrt{\\lambda_i}\\alpha_{ji}}{\\sqrt{\\sigma_{jj}}}. \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>Properties of factor of loading</p> <p>(i) Sum over original variable. </p> \\[ \\sum_{j=1}^n \\sigma_{jj}\\rho^2(y_i,x_j)=\\lambda_i. \\] <p>(ii) Sum over all principle components</p> \\[ \\sum_{i=1}^n \\rho^2(y_i,x_j)=1. \\] Proof <p>We give proof for (ii) using outer product formula.</p> <p>Since \\(\\Sigma=A\\Sigma_y A^T=\\sum_{i=1}^n \\lambda_i \\alpha_i\\alpha_i^T\\), so</p> \\[ \\sigma_{jj}=\\sum_{i=1}^n \\lambda_i \\alpha_{ji}^2. \\]"},{"location":"Math/Linear_Algebra/Op_IPS/#normalized-version","title":"Normalized version","text":"<p>Usually different random variables have distinct values. We have to normalize them if we want to analyse them together. </p> \\[ x_i^*=\\frac{x_i-\\mathbb{E}x_i}{\\sqrt{Var(x_i)}},\\quad i=1,\\cdots,n. \\] <p>So all the content above would be the same except the folloiwng changes.</p> <p>Changes applied to normalized random vectors</p> <p>(i) \\(\\pmb{\\mu}^*=\\theta\\) and \\(\\Sigma^*=R\\), where \\(R\\) is the correlation coefficient matrix with \\(r_{jj}=\\sigma_{ii}=1\\).</p> <p>(ii) sum over variance after transformation. \\(\\sum_{i=1}^n \\lambda_i^*=n\\).</p> <p>(iii) load of factors. \\(\\rho(y_i,x_j)=\\sqrt{\\lambda_i^*}\\alpha_{ji}\\).</p>"},{"location":"Math/Linear_Algebra/Op_IPS/#truncated-principle-components","title":"Truncated principle components","text":"<p>In practice, we usually do not choose \\(n\\) principle components but rather \\(k \\ll n\\) to achieve compression of data.</p> <p>How to choose these \\(k\\) components? we based on the following criterion.</p> <p>Contribution to variance</p> <p>the contribution to total variance of principle component \\(y_i\\) is defined by</p> \\[ \\eta_i=\\frac{\\lambda_i}{\\sum_{k=1}^n \\lambda_k} \\] <p>usually we need to let \\(\\sum_{i=1}^k \\eta_i\\) to be larger than \\(70%\\).</p>"},{"location":"Math/Linear_Algebra/Op_IPS/#sampled-pca","title":"Sampled PCA","text":"<p>In actual experiments, we have to observe independently \\(m\\) times. We have to replace mean and covariance matrix with their empirical versions. Assume \\(X_1,\\cdots,X_m\\) are \\(m\\) mutually independent random vectors (samples in \\(\\mathbb{R}^n\\)), then the unbiased estimates of mean and variance are </p> \\[ \\pmb{\\mu}\\approx \\overline{X}=\\sum_{k=1}^m X_k,\\quad \\sigma_{jj}=1/(m-1)\\sum_{k=1}^m (X_k-\\overline{X})^2. \\] <p>So we have its empirical covariance matrix \\(S=\\frac{1}{m-1}\\sum_{k=1}^m (X_k-\\overline{X})(X_k-\\overline{X})^T\\). Tackle this matrix with the same method we used in PCA, then we are done.</p> <p>For actual calculation, we usually let \\(X_k=\\frac{X_k-\\overline{X}}{\\sqrt{s_{kk}}}\\) for each \\(k=1,\\cdots,m\\), and solve singular values of \\(X'=(X_1,\\cdots,X_m)_{n\\times m}\\) as \\(s_1&gt;\\cdots&gt;s_n\\), then \\(\\lambda_i=s_i^2\\) for \\(i=1,\\cdots,n\\). And \\(V=A\\) and \\(Y=V^T X\\). If we choose \\(k\\) principle components, then choose \\(k\\) columns of \\(V\\) as eigenvectors.</p>"},{"location":"Math/Numerical_Analysis/","title":"Numerical Analysis","text":"<p>Reference</p> <p>Numerical analysis, Richard L. Burden, J. Douglas Faires</p>"},{"location":"Math/Numerical_Analysis/#preliminary-errors","title":"\u8bef\u5dee | Preliminary: Errors","text":"<p>If a real number \\(x\\) is denoted as \\(0.d_1d_2d_3\\cdots \\times 10^{n}\\), then</p> <ul> <li>Truncation\uff08\u622a\u65ad\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0.d_1d_2d_3\\cdots d_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\)</p> <ul> <li>Roundoff\uff08\u820d\u5165\uff09 Error</li> </ul> <p>is induced when </p> \\[ \\hat{x}=0. \\delta_1 \\delta_2 \\delta_3 \\cdots \\delta_k \\times 10^{n}  \\] <p>for some definite \\(k&lt;\\infty\\) </p> <p>where \\(\\delta_k &gt;d_k\\) if \\(d_{k+1}&gt;=5\\).</p>"},{"location":"Math/Numerical_Analysis/#t-significant-digits","title":"t significant digits","text":"<p>The number \\(p^*\\) is said to approximate p to \\(t\\) significant digits(or figures) if \\(t\\) is the largest nonnegative integer for which the relative error </p> \\[ e = \\frac{\\Delta p}{p}=\\frac{\\|p-p^*\\|}{\\|p\\|}&lt;5\\times 10^{-t}  \\] <p>where \\(p^*\\) is the approximate number of the exact number \\(p\\).</p> <ul> <li>for Chopping:</li> </ul> \\[  \\begin{align*} e &amp;= \\left|\\frac{0.d_{k+1}d_{k+2}\\cdots}{0.d_1d_2\\cdots}\\right| \\times 10^{-k} \\\\ &amp;\\leq \\frac{1}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow\\overline{9}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\ &amp;=10^{-k+1}  \\end{align*}  \\] <ul> <li>for rounding:</li> </ul> \\[ \\begin{align*} e &amp;\\leq \\frac{0.5}{0.1} \\times 10^{-k} \\quad \\text{\"=\" for } d_{k+1}d_{k+2}\\cdots\\rightarrow 5\\overline{0}\\text{ and }d_{1}d_{2}\\cdots\\rightarrow 0 \\\\  &amp;=0.5\\times 10^{-k+1} \\end{align*}  \\]"},{"location":"Math/Numerical_Analysis/#matrix-calculation","title":"\u6570\u503c\u4ee3\u6570\uff08\u77e9\u9635\u8ba1\u7b97\uff09 | Matrix Calculation","text":""},{"location":"Math/Numerical_Analysis/#numerical-approximation","title":"\u6570\u503c\u903c\u8fd1 | Numerical Approximation","text":""},{"location":"Math/Numerical_Analysis/#numerical-solution-of-differential-equations","title":"\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3 | Numerical Solution of Differential Equations","text":""},{"location":"Math/Numerical_Analysis/DE/","title":"Numerical solution of Differential Equation","text":"<p>Reference</p> <p>\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3\uff1a\u6709\u9650\u5dee\u5206\u7406\u8bba\u65b9\u6cd5\u4e0e\u6570\u503c\u8ba1\u7b97, \u5f20\u6587\u751f</p> <p>\u5fae\u5206\u65b9\u7a0b\u6570\u503c\u89e3, \u9648\u6587\u658c</p> <p>In ordinary differential equation</p> \\[ \\frac{du}{dt}f(t,u),\\quad u(0)=u_0 \\] <p>in region \\([a,b]\\). Note that it is hard to know the function, but we know its derivatives at all points on the region. That is, for any dicrete point \\(t_n\\in [a,b]\\), we have</p> \\[ \\frac{du}{dt}\\Bigg|_{t=t_n}=f(t_n,u(t_n)) \\] <p>The following part is focused on using there infomation to get a solution of ODE.</p>"},{"location":"Math/Numerical_Analysis/DE/#numerical-differentiation","title":"\u6570\u503c\u5fae\u5206 | Numerical Differentiation","text":"<p>Denote \\(h\\) as the time step forward, then according to Taylor expansion evaluated at \\(t_n\\) and take another forward point \\(t_{n+1}\\)</p> \\[ \\begin{equation} u(t_{n+1})=u(t_n)+h u'(t_n)+\\frac{h^2}{2!}u''(t_n)+\\frac{h^3}{3!}u'''(t_n)+o(h^4) \\label{forward} \\end{equation} \\] <p>backward at point \\(t_{n-1}\\), we substitute \\(h\\) with \\(-h\\) and get</p> \\[ \\begin{equation} u(t_{n-1})=u(t_n)-h u'(t_n)+\\frac{h^2}{2!}u''(t_n)-\\frac{h^3}{3!}u'''(t_n)+o(h^4)\\label{backward} \\end{equation} \\] <p>So we have an approximation of the first order derivative from equation \\(\\ref{forward}\\), that is, the forward form of divided difference</p> \\[ \\begin{align} u'(t_n)=\\frac{u(t_{n+1})-u(t_n)}{h}-\\frac{h}{2!}u''(t_n)-\\frac{h^2}{3!}u'''(t_n)+o(h^3) \\label{Forward} \\end{align} \\] <p>or from equation \\(\\ref{backward}\\), that is, the backward form of divided difference</p> \\[ \\begin{equation} u'(t_n)=\\frac{u(t_{n})-u(t_{n-1})}{h}+\\frac{h}{2!}u''(t_n)-\\frac{h^2}{3!}u'''(t_n)+o(h^3) \\label{Backward} \\end{equation} \\] <p>Combine equation \\(\\ref{Forward}\\) and equation \\(\\ref{Backward}\\) we have the center divided difference form</p> \\[ \\begin{equation} u'(t_n)=\\frac{u(t_{n+1})-u(t_{n-1})}{2h}-\\frac{h^2}{3!}u'''(t_n)+o(h^3) \\label{center} \\end{equation} \\] <p>which is more accurate. Now we can use the above equation to get Euler' method.</p> <p>Three Point Midpoint Formula</p> \\[ f'(x_0) = \\frac{1}{2h}[f(x_0+h)-f(x_0-h)] -\\frac{h^2}{6}f'''(\\xi) \\] <p>Use Lagrange Polynomial to approximate a function. Then the \\(n\\)th derivative of Lagrange Poly approximate the \\(n\\)th derivative of the original function.</p>"},{"location":"Math/Numerical_Analysis/DE/#eulers-method","title":"Euler's Method","text":"<p>Here we introduce Euler's one step method</p> \\[ \\begin{cases} \\omega_0=\\alpha,\\\\ \\omega_{i+1}=\\omega_i+hf(t_i,\\omega_i),\\quad i=0,1,\\cdots,n-1 \\end{cases} \\] <p>From forward form of divided difference \\(\\ref{Forward}\\) we have its rounding error </p> \\[ \\tau_n=\\frac{w(t_{n+1})-w(t_n)}{h}=\\frac{h}{2}f'(\\eta),\\quad t_n\\leq \\eta\\leq t_{n+1} \\] <p>First analyze its convergence.</p> <p>Convergence of Euler's Method</p> <p>Assume solution \\(\\phi(t)\\in C^2[t_0,b]\\), then the error of the solution \\(w_n\\) acquired from Euler's method satisfies</p> \\[ \\max_{a\\leq t_n \\leq b}|w(t_n)-w_n|\\leq e^{(b-t_0)L}|e_0|+\\frac{e^{(b-t_0)L}-1}{L}\\tau(h) \\] <p>where</p> \\[ \\tau(h)=\\frac{h}{2}\\|w''\\|_{\\infty},\\quad e_0=w(t_0)-w_0 \\] <p>If \\(h\\rightarrow 0\\), s.t.</p> \\[ |w(t_0)-w_0|\\leq c_1h,\\quad c_1&gt;0 \\] <p>then \\(\\exists c&gt;0\\), such that</p> \\[ \\max_{t_0\\leq t_n\\leq b}|w(t_n)-w_n|\\leq ch \\] <p>Then we analyze its asymptotic stability.</p> <p>Definition of asymptotic stability</p> <p>We call Euler's one step method is asymptotic stable, if \\(\\exists h_0&gt;0\\), \\(\\exists C&gt;0\\), such that \\(\\forall h\\in (0,h_0]\\), </p> \\[ |z_n-w_n|\\leq C\\varepsilon,\\quad \\forall 0\\leq n\\leq N(h) \\] <p>where \\(w_n\\) and \\(z_n\\) denotes the solution from before disturbance(initial value \\(w_0\\)) and after disturbance(initial value \\(z_0\\)) using Euler's one step method.</p> <p>The above shows that Euler's method is asymptotic stable.</p>"},{"location":"Math/Numerical_Analysis/DE/#implicite-eulers-method","title":"Implicite Euler's method","text":"<p>Implicite means we have to guess the number before iterating. Usually it is used after Explicite Euler's method to improve stability.</p> \\[ \\begin{cases} \\omega_0=\\alpha, \\\\ \\omega_{i+1}=\\omega_i+hf(t_i,\\omega_{i+1}),\\quad i=0,1,\\cdots,n-1 \\end{cases} \\] <p>From Backward form of Numerical Derivative(Divided Difference) \\(\\ref{Backward}\\), we can express its rounding error </p> \\[ \\tau_n=\\frac{w(t_{n+1})-w(t_{n})}{h}=-\\frac{h}{2}f'(\\eta),\\quad t_n\\leq \\eta\\leq t_{n+1} \\]"},{"location":"Math/Numerical_Analysis/DE/#modified-eulers-method","title":"Modified Euler's Method","text":"<p>So we can choose higher order items to improve accuracy.</p> <p>This method is also called Tranpezoid Method.</p> \\[ w(t_{n+1})=w(t_n)+\\frac{h}{2}\\left[f(t_n,w(t_n))+f(t_{n+1},w(t_{n+1}))\\right] \\] <p>We have this inspiration from tranpezoidal integration. </p> From Tranpezoidal integration <p>From integration the ODE on \\([t_n, t_{n+1}]\\) we have</p> \\[ w(t_{n+1})=w(t_n)+\\int_{t_n}^{t_{n+1}}f(\\tau, w(\\tau))d\\tau \\] <p>Using Tranpezoidal Integration Formula with its Error, we have</p> \\[ w(t_{n+1})=w(t_n)+\\frac{h}{2}\\left[f(t_n,w(t_n))+f(t_{n+1},w(t_{n+1}))\\right]-\\frac{h^3}{12}f'''(\\eta_n),\\quad t_n\\leq \\eta_n\\leq t_{n+1} \\] <p>which means a higher accuracy for method</p> \\[ w(t_{n+1})=w(t_n)+\\frac{h}{2}\\left[f(t_n,w(t_n))+f(t_{n+1},w(t_{n+1}))\\right] \\]"},{"location":"Math/Numerical_Analysis/DE/#double-step-method","title":"Double-step Method","text":"<p>We have this method inspired by the center form of divided difference. </p> <p>According to center form of divided difference \\(\\ref{center}\\) we have</p> \\[ \\begin{cases} \\omega_0=\\alpha,\\\\ \\omega_1=\\omega_0+hf(t_0,\\omega_0), \\\\ \\omega_{i+1}=\\omega_{i-1}+2hf(t_i,\\omega_i), \\quad i=1,2,\\cdots,N-1 \\end{cases} \\] <p>with its rounding error expressed by</p> \\[ \\tau_n=\\frac{w_{t_{n+1}}-w(t_n)}{h}=-\\frac{h^2}{3!}f''(\\eta), \\quad t_n\\leq \\eta\\leq t_{n+1} \\]"},{"location":"Math/Numerical_Analysis/DE/#taylor-method-of-order-n","title":"Taylor method of order n","text":"<p>To improve precision, we can choose more items of Taylor's expansion for the one order derivative.</p> \\[ \\begin{cases} \\omega_0=\\alpha, \\\\ \\omega_{i+1}=\\omega_i+hT^{(n)}(t_i,\\omega_i),\\quad i=0,1,\\cdots,N-1 \\end{cases} \\] <p>where</p> \\[ T^{(n)}(t_i,\\omega_i)=f(t_i,\\omega_i)+\\frac{h}{2}f'(t_i,\\omega_i)+\\cdots+\\frac{h^{n-1}}{n!}f^{(n-1)}(t_i,\\omega_i) \\] <p>Euler's method is Tarlor's method of order one.</p>"},{"location":"Math/Numerical_Analysis/DE/#runge-kutta-method","title":"Runge-Kutta Method","text":"<p>For Taylor's method of higher order, it is trivial to calculate high order derivatives, especially when \\(p&gt;3\\) and \\(f\\) has a complex expression. So the following Runge-Kutta Method aims to calculate function \\(f\\)'s values at different points to avoid high order derivatives, reducing calculation time.</p> <ul> <li>Runge-Kutta method with order 4.</li> </ul> \\[ \\begin{cases} w_{i+1}=w_i+\\frac{1}{6}(k_1+2K_2+2k_3+k_4) w_0=\\alpha \\end{cases} \\]"},{"location":"Math/Numerical_Analysis/DE/#multistep-methods","title":"Multistep Methods","text":"<p>Derive from integration</p> \\[ y(t_{i+1})-y(t_i)=\\int_{t_i}^{t_{i+1}}f(t,y)dt \\] <p>use interpolation polynomial to replace f(t,y).</p> <ul> <li>Adams-Bashforth Four-step Explicit Method</li> </ul> \\[ w_{i+1}=w_i + \\frac{h}{24}(55f_i-59f_{i-1}+37f_{i-2}-9f_{i -3}) \\] <ul> <li>Adams-Bashforth Three-step Implicit Method</li> </ul> \\[ w_{i+1}=w_i +\\frac{h}{24}(9f_{i+1}+19f_{i}-5f_{i-1}+f_{i-2}) \\] <p>Or we derive from Taylar expansion.</p>"},{"location":"Math/Numerical_Analysis/DE/#odes","title":"ODEs","text":""},{"location":"Math/Numerical_Analysis/MC/","title":"Matrix Calculation","text":""},{"location":"Math/Numerical_Analysis/MC/#direct-methods-for-solving-linear-systems","title":"Direct Methods for Solving Linear Systems","text":"<p>We focus on solving linear system </p> \\[ A\\vec{x} = \\vec{b} \\]"},{"location":"Math/Numerical_Analysis/MC/#gasussion-elimination","title":"\u9ad8\u65af\u6d88\u5143 | Gasussion Elimination","text":"<p>Reduce A into an upper-triangular matrix, and then solve for the unknowns by a backward-substitution process</p>"},{"location":"Math/Numerical_Analysis/MC/#pivoting-stratages","title":"\u4e3b\u5143\u9009\u62e9\u7b56\u7565 | Pivoting Stratages","text":"<p>This part is to reduce the error caused by rounding/Truncation error.</p> <p>We can show that the pivoting element is of great significance.</p> Partial PivotingScaled Partial PivotingComplete Pivoting <p>(also known for not changing the columns)</p> <p>Determine the smallest \\(p\\geq k\\) (in the same column of \\(a^{(k)}_{kk}\\))such that </p> \\[ |a_{ok}^{(k)}| = \\max_{k\\leq i \\leq n}{|a_{ik}^{(k)}|} \\] <p>and perform \\((E_k) \\leftrightarrow (E_p)\\).</p> <p>For row \\(i\\), let</p> \\[ s_i = \\max_{1\\leq j\\leq n}{|a_{ij}|} \\] <p>(if \\(\\exists i, s.t. s_i=0\\), then the system has no unique root. So we assume \\(\\forall i, s_i&gt;0\\))</p> <p>For each procedure of executing \\(E_k \\leftarrow E_k - m_{k,i}E_i\\) for \\(k=i+1, \\cdots, n\\), where \\(m_{k, i} = a_{ki}/{a_{ii}}\\). let </p> \\[ p = \\arg \\max_{i\\leq k \\leq n}{\\frac{|a_{ki}|}{s_k}} \\] <p>perform \\((E_i)\\leftrightarrow(E_p)\\)</p> <p>Incorporate the interchange of both rows and columns.</p>"},{"location":"Math/Numerical_Analysis/MC/#time-cost","title":"Time Cost","text":"<p>As we all know the time expense for Gaussion elimination is</p> \\[ O(n^3) \\]"},{"location":"Math/Numerical_Analysis/MC/#lu-lu-matrix-factorization","title":"LU\u5206\u89e3 | LU Matrix Factorization","text":"<p>The idea is encouraged by Gaussion Elimination. See that a matrix \\(A\\) can be transformed into an upper-trianglar matrix \\(U\\) by primary row operations:</p> \\[ \\begin{equation}  U = M_{n-1}\\cdots M_2M_1A  \\label{eq: LU} \\end{equation}  \\] <p>where \\(M_k (k=1,2,\\cdots n-1)\\) denotes a series of row operations. There are two perspetives.</p> Version 1Version 2 <p>\\(M_k (k=1, \\cdots, n-1)\\) can be interpreted that the \\(k+1\\) row has to make its column \\(1\\) to \\(k\\) to be \\(0\\). That is,</p> \\[ E_{k+1} \\leftarrow E_{k+1} - \\sum_{j=1}^{k} m_{k+1, j}E_j \\] <p>\\(M_k (k=1,\\cdots, n-1)\\) can be defined in another way as </p> \\[ E_j \\leftarrow E_j - \\sum\\limits_{k=j}^{n}m_{j,k}E_k \\quad \\text{for } j=k+1, \\cdots n \\] <p>which is also a lower-triangular matrix. To be proved by readers. </p> <p>And we can see \\(M_k\\) formed through the above two interpretations are the same.</p> <p>If we denote \\(L_k = M_k^{-1}\\), then apply \\(L = L_1L_2\\cdots L_{n-1}\\) left to both sides of the equation \\(\\ref{eq: LU}\\), then</p> \\[ LU = L_1L_2\\cdots L_{n-1} \\cdot M_{n-1}\\cdots M_2M_1A = A \\] <p>We know that matrix \\(L_k\\) and \\(M_k\\) are lower-triangular matrix(explaned by definition, to be proved by readers), so the product of matrix L is alao a lower-triangular matrix.</p> <p>So with the triangular matrix, it can be much quicker to solve the solution. See that</p> \\[  \\begin{align*} A\\vec{x} &amp;= \\vec{b} \\\\ LU\\vec{x} &amp;= \\vec{b} \\end{align*}  \\] <p>First solve \\(L \\vec{y} = \\vec{b}\\), then solve \\(U \\vec{x} = \\vec{y}\\).</p>"},{"location":"Math/Numerical_Analysis/MC/#time-cost_1","title":"time cost","text":"<p>Eliminate \\(0.5n^2\\) elements, it needs time \\(O(0.5n^3)\\).</p> <p>Solving \\(y\\) and \\(x\\), it needs time \\(O(2n^2)\\).</p>"},{"location":"Math/Numerical_Analysis/MC/#iterative-techniques-in-matrix-algebra","title":"\u77e9\u9635\u4ee3\u6570\u7684\u8fed\u4ee3\u6cd5 | Iterative Techniques in Matrix Algebra","text":"<p>This section, we introduce the iterative thoughts from Fixed-Point Iteration\uff08\u4e0d\u52a8\u70b9\u6cd5\uff09 to solve a linear system.</p> <p>We aim to find a iterative equation like equation \\(\\pmb{x}^{k} = f(\\pmb{x}^{k-1})\\). To be more specific, a linear iterative equation like</p> \\[ \\pmb{x}^{k} = T \\pmb{x}^{k-1} + \\pmb{c} \\] <p>and its corresponding convergent relation is</p> \\[ \\pmb{x}= T \\pmb{x} + \\pmb{c} \\]"},{"location":"Math/Numerical_Analysis/MC/#preliminary-knowledge-norm-of-vectors-and-matrixes","title":"\u8303\u6570 | Preliminary knowledge: Norm of Vectors and Matrixes","text":"<p> <p>Definition of Norm of vectors</p> <p>A vector norm on \\(\\mathbb{R}^n\\) is a function, denoted as \\(\\Vert \\cdot \\Vert\\), mapping from \\(\\mathbb{R}^n\\) into \\(\\mathbb{R}\\) with the following properties for all \\(x, y \\in\\mathbb{R}^n\\) and \\(\\alpha \\in \\mathbb{C}\\).</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\vec{x} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\vec{x}\\Vert = 0 \\Leftrightarrow \\vec{x}=\\vec{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\vec{x} \\Vert = |\\alpha|\\Vert \\vec{x} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\vec{x}+\\vec{y} \\Vert \\leq \\Vert \\vec{x} \\Vert+\\Vert \\vec{y} \\Vert \\] <p>We usually use \\(p\\) norm</p> \\[ \\Vert \\vec{x} \\Vert_p = \\left(\\sum_{i=1}^n|x_i|^p\\right)^{1/p} \\] <p>with its common forms:</p> \\[ \\Vert \\vec{x} \\Vert_1 = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert \\vec{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^{n}|x_i|^2}, \\quad \\Vert \\vec{x} \\Vert_\\infty = \\max_{1\\leq i\\leq n}|x_i| \\] <p>Definition of Norm of Matrices</p> <p>A matrix norm on the set of all matrices \\(R \\in \\mathbb{R}^{n\\times n}\\) is a real-valued function, denoted as \\(\\Vert \\cdot \\Vert\\), defined on this set, satisfying for all \\(A, B \\in \\mathbb{R}^{n\\times n}\\) and all \\(\\alpha \\in \\mathbb{C}\\):</p> <ul> <li>\u6b63\u6027 | positive</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert\\geq 0 \\] <ul> <li>\u5b9a\u6027 | definite</li> </ul> \\[ \\Vert \\mathbfit{A} \\Vert = 0 \\Leftrightarrow \\mathbfit{A}=\\mathbfit{0} \\] <ul> <li>\u9f50\u6027 | homogeneous</li> </ul> \\[ \\Vert \\alpha\\mathbfit{A} \\Vert = |\\alpha|\\Vert \\mathbfit{A} \\Vert \\] <ul> <li>\u4e09\u89d2\u4e0d\u7b49\u5f0f | triangle inequality</li> </ul> \\[ \\Vert \\mathbfit{A}+\\mathbfit{B} \\Vert \\leq \\Vert \\mathbfit{A} \\Vert+\\Vert \\mathbfit{B} \\Vert \\] <ul> <li>\u4e00\u81f4\u6027 | consistent</li> </ul> \\[ \\Vert \\mathbfit{A}\\mathbfit{B} \\Vert\\leq \\Vert \\mathbfit{A} \\Vert \\cdot \\Vert \\mathbfit{B} \\Vert \\] <p>Usually we use Natural Norm:</p> \\[ \\Vert \\mathbfit{A} \\Vert = \\max_{\\vec{x}\\neq 0}\\frac{\\Vert \\mathbfit{A}\\vec{x} \\Vert_p}{\\Vert \\vec{x} \\Vert_p} = \\max_{\\Vert \\vec{x} \\Vert_p =1}\\Vert \\mathbfit{A}\\vec{x} \\Vert \\] <p>with its common forms:</p> \\[ \\begin{align*} \\Vert \\mathbfit{A} \\Vert_1 &amp;= \\max_{1\\leq i\\leq n}\\sum_{j=1}^{n}|a_{ij}| \\quad\\text{the maximum of row summation}\\\\ \\Vert \\mathbfit{A} \\Vert_2 &amp;= \\sqrt{\\max \\rho(A^T A)} \\quad\\text{the maximum of spectrum radius}\\\\ \\Vert \\mathbfit{A} \\Vert_\\infty &amp;= \\max_{1\\leq j\\leq n}\\sum_{i=1}^{n}|a_{ij}| \\quad\\text{the maximum of column summation}\\\\ \\end{align*} \\] <p>for \\(p=2\\) norm of matrix, we have special expression for special matrix:</p> <p>Sepecial Expression of Norm 2 of matrix</p> <p>If matrix \\(A\\) is symetrical, then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = \\sqrt{\\max \\rho(A)}. \\] <p>If matrix \\(A\\) is orthogonal(only rotate), then</p> \\[ \\Vert \\mathbfit{A} \\Vert_2 = 1. \\]"},{"location":"Math/Numerical_Analysis/MC/#jacobi-jacobis-method","title":"Jacobi\u65b9\u6cd5 | Jacobi's Method  <p>Here we denote \\(L\\) and \\(U\\) to be the lower-triangular and upper-triangular matrix of matrix \\(A\\) without its diagonal elements respectively. (different from \\(LU\\) factorization!) And then we denote \\(D\\) to be the diagonal elements of the matrix of \\(A\\). That is, </p> \\[ A = D - L -U \\] <p>Then</p> \\[  \\begin{align*} A\\pmb{x} &amp;= \\pmb{b} \\\\ (D-L-U)\\pmb{x} &amp;= \\pmb{b} \\\\ D\\pmb{x} &amp;= (L+U)\\pmb{x} + \\pmb{b} \\\\ \\pmb{x} &amp;= D^{-1}(L+U)\\pmb{x} + D^{-1}\\pmb{b}  \\end{align*}  \\] <p>which gives matrix form of the Jacobi iterative technique</p> \\[ \\pmb{x}^{k} = D^{-1}(L+U)\\pmb{x}^{k-1} + D^{-1}\\pmb{b} \\]","text":""},{"location":"Math/Numerical_Analysis/MC/#gauss-seidel-the-gauss-seidel-method","title":"Gauss-Seidel\u65b9\u6cd5 | The Gauss-Seidel Method <p>This method sees that a little slowness in Jacobi's Method. That is, for each itearion period(\\(\\pmb{x}^{k} \\leftarrow \\pmb{x}^{k-1}\\)), it makes use of the generated \\(\\pmb{x}^{k}_{i}\\) in the \\(i\\)th row of \\(\\pmb{x}^{k}\\) and use it to update the coressponding element in \\(\\pmb{x}^{k-1}\\).</p> <p>In matrix form, we have</p> \\[ D\\pmb{x}^{k} = U\\pmb{x}^{k-1} + L\\pmb{x}^{k}+ \\pmb{b} \\] <p>(to be proved by readers)</p> <p>then</p> \\[ \\pmb{x}^{k} = (D-L)^{-1}U\\pmb{x}^{k-1} + (D-L)^{-1}\\pmb{b} \\]","text":""},{"location":"Math/Numerical_Analysis/MC/#approximating-eigenvalues","title":"\u7279\u5f81\u503c\u903c\u8fd1 | Approximating Eigenvalues","text":""},{"location":"Math/Numerical_Analysis/MC/#the-power-method","title":"\u5e42\u6cd5 | The Power Method <p>Assume that \\(A\\) has eigenvalues \\(|\\lambda_1|&gt;|\\lambda_2|\\geq |\\lambda_3|\\geq \\cdots \\geq |\\lambda_n|\\geq 0\\), we can use the following method to make the largest \\(lambda_1\\) stand out.</p>  <p>\u5e42\u6cd5 | The Power Method</p> <p>Initialize randomly \\(\\vec{x}\\), which can be represented by \\(n\\) linearly irrelevant eigenvectors \\(\\vec{v}_1, \\vec{v}_2, \\cdots, \\vec{v}_n\\), with parameters \\(\\beta_1, \\beta_2, \\cdots, \\beta_n\\), such that</p> \\[ \\vec{x} = \\sum_{i=1}^{n}\\beta_i\\vec{v_i} \\] <p>multiply both sides by \\(A\\), according to \\(A\\vec{v_i}=\\lambda_i \\vec{v_i}\\), we get</p> \\[ A\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i \\vec{v_i} \\] <p>repeat this process for \\(n\\) times we get </p> \\[ A^n\\vec{x} = \\sum_{i=1}^{n}\\beta_i \\lambda_i^n \\vec{v_i} = \\lambda_1^n\\sum_{i=1}^{n}\\beta_i (\\frac{\\lambda_i}{\\lambda_1})^n \\vec{v_i}\\rightarrow \\lambda_1^n \\beta_1 \\vec{v_1} \\quad (n\\rightarrow \\infty) \\] <p>That is, we can neglect eigenvalues that are smaller than \\(\\lambda_1\\) through multiplying \\(A\\) and \"extract\" the biggest one.</p>  <p>To avoid divengence caused by \\(\\lambda_1&gt;0\\), we need to normalize \\(\\vec{x}^{k} = A\\vec{x}^{k-1}\\) each step after multiplying. Usually we choose \\(\\Vert\\  \\Vert_\\infty\\).</p> <p>To get the \\(\\lambda_1\\) out, we can use </p> \\[ \\frac{\\Vert\\vec{x}^{k}\\Vert}{\\Vert\\vec{x}^{k-1}\\Vert} \\approx \\lambda_1 \\quad (n\\rightarrow \\infty) \\] <p>to get \\(\\lambda_1\\).</p> <p>The next question is, naively, how about the speed of converging? Luckily, the question is easy to answer:</p> <p>rely on ratio \\(\\left|\\frac{\\lambda_2}{\\lambda_1}\\right|\\).</p>","text":""},{"location":"Math/Numerical_Analysis/MC/#inverse-power-method","title":"\u53cd\u5e42\u6cd5 | Inverse Power Method <p>This is a trick from the Power method. It comes from a question: what if we want to calculate the smallest eigenvalue of a matrix \\(A\\)?</p> <p>The answer is, by taking use of metrix inverse.</p>  <p>\u53cd\u5e42\u6cd5 | Inverse Power Method</p> <p>Matrix \\(A\\) has eigenvalues \\(|\\lambda_1| &lt; |\\lambda_2| \\leq \\cdots \\leq |\\lambda_n|\\), then matrix \\(A^{-1}\\) has eigenvalues </p> \\[ \\left|\\frac{1}{\\lambda_1}\\right| &gt; \\left|\\frac{1}{\\lambda_2}\\right| \\geq \\cdots \\geq \\left|\\frac{1}{\\lambda_n}\\right| \\] <p>Then use the power method to get \\(\\left|\\frac{1}{\\lambda_1}\\right|\\) out.</p>  <p>Actually, the above method is more often being used in situation where we have known an eigenvalue \\(\\lambda_1\\) (not neecssarily the largest or smallest) of \\(A\\) is close to a constant \\(q\\). That is, we can formulate matrix </p> \\[ (A-qI) \\] <p>which has eigenvalues </p> \\[ |\\lambda_1 - q| &lt; |\\lambda_2 - q| \\leq \\cdots \\leq \\left|\\lambda_n - q\\right| \\] <p>Then we can use the above method to get \\(\\lambda_1\\) out.</p>","text":""},{"location":"Math/Numerical_Analysis/NA/","title":"Index","text":""},{"location":"Math/Numerical_Analysis/NA/#numerical-approximation","title":"Numerical Approximation","text":"<p>Reference</p> <p>\u6570\u503c\u903c\u8fd1, \u848b\u5c14\u96c4 \u8d75\u98ce\u5149 \u82cf\u4ef0\u5cf0</p>"},{"location":"Math/Numerical_Analysis/NA/#solutions-of-equations-in-one-variables","title":"\u4e00\u5143\u51fd\u6570\u65b9\u7a0b\u6c42\u89e3 | Solutions of Equations in One Variables","text":""},{"location":"Math/Numerical_Analysis/NA/#interpolation-and-polynomial-approximation","title":"\u51fd\u6570\u63d2\u503c\u548c\u591a\u9879\u5f0f\u903c\u8fd1 | Interpolation and Polynomial Approximation","text":""},{"location":"Math/Numerical_Analysis/NA/#approximation-theory","title":"\u6700\u4f73\u903c\u8fd1\u7406\u8bba | Approximation Theory","text":""},{"location":"Math/Numerical_Analysis/NA/#numerical-integration","title":"\u6570\u503c\u79ef\u5206 | Numerical Integration","text":""},{"location":"Math/Numerical_Analysis/NA/Appro/","title":"Approximation Theory","text":""},{"location":"Math/Numerical_Analysis/NA/Appro/#best-square-approximation","title":"\u6700\u4f73\u5e73\u65b9\u903c\u8fd1 | Best Square Approximation","text":""},{"location":"Math/Numerical_Analysis/NA/Appro/#lead-in","title":"\u5f15\u5165 | Lead-in","text":"<p>We call a linear space with norm if there exists a function \\(\\|\\cdot\\|\\) that satisfies 3 properties(check here).</p> <p>So here we introduce</p> \\[ \\Delta(x,Y)=\\inf_{y\\in Y}\\|x-y\\| \\] <p>to be the best square approximation of element \\(x\\), where \\(x\\in X\\) is a linear space with norm 2 and \\(Y\\) is a subspace of \\(X\\).</p> <p>We can also introduce norm with the definition of inner product. In Euclid space, \\((\\cdot,\\cdot)\\) is defined as </p> \\[ (x,y)=x^Ty,\\quad x,y\\in \\mathbb{R}^n \\] <p>It is easy to see that \\(\\|x\\|_2=\\sqrt{(x,x)}\\). So we can give a more specific problem of inner product space. Assume \\(\\varphi_i\\), \\((i=1,2\\cdots,n)\\) are \\(n\\) linearly irrelevant elements in inner product space \\(X\\), choose \\(f\\in X\\), then subset </p> \\[ \\varPhi_n=\\text{span}(\\varphi_1,\\varphi_2,\\cdots,\\varphi_n) \\] <p>has a best approximation of \\(f\\), which is defined as </p> \\[ \\Delta(f,\\varPhi_n)=\\min_{\\varphi\\in \\varPhi_n}\\|f-\\varphi\\|_2 \\] <p>we call the \\(\\varphi\\) that enables the above equation to be the best square appromation element.</p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#properties-of-best-square-appromation-element","title":"\u6700\u4f73\u5e73\u65b9\u903c\u8fd1\u5143 | Properties of Best Square Appromation Element","text":"<p>Sufficient and Necessary Condition for best square approximation element</p> <p>Assume \\(X\\) is an inner product space, \\(f\\in X\\), \\(\\varphi^*\\in \\varPhi_0\\) is the best square approximation element, if and only if</p> \\[ (f-\\varphi^*,\\varphi_i)=0, \\quad, i=1,2\\cdots,n. \\] Proof <p>\"\\(\\Leftarrow\\)\".</p> <p>\"\\(\\Rightarrow\\)\". </p> <p>The above theorem gives a general method to solve the best square approximation element. That is, we define</p> \\[ G=\\left[\\begin{array}{cccc} (\\varphi_1,\\varphi_1) &amp; (\\varphi_1,\\varphi_2) &amp; \\cdots &amp; (\\varphi_1,\\varphi_n)\\\\ (\\varphi_2,\\varphi_1) &amp; (\\varphi_2,\\varphi_2) &amp; \\cdots &amp; (\\varphi_2,\\varphi_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (\\varphi_n,\\varphi_1) &amp; (\\varphi_n,\\varphi_2) &amp; \\cdots &amp; (\\varphi_n,\\varphi_n)\\\\ \\end{array}\\right], \\quad \\pmb{\\alpha}^*=\\left[\\begin{array}{c} \\alpha_1^*\\\\ \\alpha_2^*\\\\ \\vdots\\\\ \\alpha_n^* \\end{array}\\right], \\quad \\pmb{\\beta}=\\left[\\begin{array}{c} (\\varphi_1,f)\\\\ (\\varphi_2,f)\\\\ \\vdots\\\\ (\\varphi_n,f) \\end{array}\\right] \\] <p>then we can solve \\(G\\pmb{\\alpha}^*=\\pmb{\\beta}\\) for the best square approximation element</p> \\[ \\varphi = \\sum_{i=1}^n\\alpha^*_i \\varphi_i \\] <p>The above theorem also tells us, if \\(\\{\\varphi_i\\}_{i=1}^n\\) are group of linearly irrelevant elements, then \\(G\\) has rank \\(n\\) and the parameters \\(\\{\\alpha_i\\}_{i=1}^n\\) is unique. And it is easy to give the error estimation form</p> \\[ \\begin{align*} \\|f-\\varphi^*\\|_2^2&amp;=(f-\\varphi^*,f-\\varphi^*)\\\\ &amp;=(f-\\varphi^*, f) - (f-\\varphi^*, \\varphi^*) \\\\ &amp;=(f-\\varphi^*, f) = (f,f) - (\\varphi^*,f)\\\\ &amp;=\\|f\\|_2^2-\\sum_{i=1}^n\\alpha^*_i (\\varphi_i, f) \\end{align*} \\] <p>The geometrical meaning is also clear, if we define \\(\\Delta=f-\\varphi^*\\) and then</p> \\[ \\begin{align*} \\|f-\\varphi^*\\|_2^2&amp;=(\\Delta+\\varphi^*,\\Delta+\\varphi^*)\\\\ &amp;=(\\Delta, \\Delta) +2 (\\Delta, \\varphi^*)+(\\varphi^*,\\varphi^*) \\\\ &amp;=(\\Delta, \\Delta) +2 (f-\\varphi^*, \\varphi^*)+(\\varphi^*,\\varphi^*) \\\\ &amp;= (\\Delta, \\Delta) - (\\varphi^*,\\varphi^*) \\quad \\text{(by property of BSPE)}\\\\ \\end{align*} \\] <p>which means \\(\\varphi^*\\) is the orthogonal projection of \\(f\\) on \\(\\varPhi_n\\).</p> <p>But actually it is not so easy to solve the above linear system, so we consider another way in specific situation.</p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#best-square-approximation-on-l_rho2ab","title":"\u6709\u9650\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u6700\u4f73\u5e73\u65b9\u903c\u8fd1 | Best Square Approximation on \\(L_\\rho^2[a,b]\\)","text":"<p>Weight Function</p> <p>Assume \\(\\rho(x)\\) is Lebesgue integrable on \\([a,b]\\) and is \\(0\\) on at most only one set of measure zero, then we call \\(\\rho(x)\\) the weight function.</p> <p>If \\(\\rho(x)\\) is the weight function on \\([a,b]\\) then we denote all the measurable function \\(\\rho(x)f^2(x)\\) that is Lebesgue integrable on \\([a,b]\\), by \\(L^2_\\rho[a,b]\\), which is a linear space. The inner product is defined by</p> \\[ (f,g)=\\int_{a}^b\\rho(x)f(x)g(x)dx, \\quad f,g\\in L^2_\\rho[a,b] \\] <p>which satisfies the 4 properties of inner product definition. So we can introduce natural norm </p> \\[ \\|f\\|_2=\\left[\\int_a^b\\rho(x)f^2(x)dx\\right] \\] <p>then \\(L^2_\\rho[a,b]\\) is a linear space with norm, so we can consider the best approximation problem.</p> <p>Because \\(L^2_\\rho[a,b]\\) is a special inner product space \\(X\\),  so the former discussion about the best square approximation element can be directly applied here. That is, if \\(\\{\\varphi_i\\}_{i=1}^n\\) is a group of linearly irrelevant function in \\(L^2_\\rho[a,b]\\), then the subset </p> \\[ \\varPhi_n=\\text{span}\\{\\varphi_1,\\varphi_2, \\cdots, \\varphi_n\\} \\] <p>is call the generalized polynomial space, whose element is called generalized polynomial, or polynomial for short. The best square approximation of \\(f\\) on \\(\\varPhi_n\\) is defined by</p> \\[ \\Delta(f,\\varPhi_n)=\\min_{\\varphi\\in \\Phi}\\|f-\\varphi\\|_2 \\] <p>and so does the best square approximation element \\(\\varphi^*\\) is called the best square approximation polynomial(BSAP).</p> ps <p>There is another way to get the BSAP.</p> <p>If we choose </p> \\[ E = (P-y,P-y)=\\|P-y\\|^2 \\] <p>we can easily solve the approximation polynomial \\(\\sum\\limits_{i=0}^\\infty a_ix^i\\) by letting \\(\\frac{\\partial E}{\\partial a_k}=0\\), and get</p> \\[ \\sum_{j=0}^n(\\varphi_k, \\varphi_j)a_j=(\\varphi_k,f),\\quad k=0,1,\\cdots,n \\]"},{"location":"Math/Numerical_Analysis/NA/Appro/#orthogonal-polynomials","title":"\u6b63\u4ea4\u591a\u9879\u5f0f | Orthogonal Polynomials","text":"<p>If we limit the base function of the subspace, we can simplify the matrix \\(G\\) and simplify the solving process. Actually, we are narrawing down the condition number of \\(G\\) by using orthogonal polynomials. Readers can use exactly the same statement from abstrat inner product space and its corresponding statement of orthogonal elements.</p> <p>Orthogonal Polynomials</p> <p>If \\(\\{\\varphi_i\\}_{i=1}^n \\subset L^2_\\rho[a,b]\\) satisfy</p> \\[ (\\varphi_i,\\varphi_j)=\\int_a^b\\rho(x)\\varphi_i(x)\\varphi_j(x)dx=\\begin{cases} 0, \\quad &amp;i=j,\\\\ \\sigma, \\quad &amp; i\\neq j. \\end{cases} \\] <p>where \\(\\sigma\\) is a non-zero number, then we call \\(\\{\\varphi_i\\}_{i=1}^n\\) are orthogonal on \\([a,b]\\) according to weight \\(\\rho(x)\\). Furthermore, if we limit </p> \\[ \\int_a^b\\rho(x)\\varphi_i^2(x)dx=1 \\] <p>then we call \\(\\{\\varphi_i\\}_{i=1}^n\\) are normalized orthogonal system.</p> <p>So from above definition we can get BSAP more easily with orthogonal functions \\(\\{\\varphi_i\\}_{i=1}^n\\)</p> \\[ \\varphi^*=\\sum_{i=1}^n\\frac{(\\varphi_i,f)}{(\\varphi_i,\\varphi_i)}\\varphi_i(x) \\] <p>and its corresponding error</p> \\[ \\|f-\\varphi^*\\|^2_2=\\|f\\|^2_2-\\sum_{i=0}^n\\frac{(\\varphi_i,f)^2}{(\\varphi_i,\\varphi_i)} \\] <p>Q1. Use \\(y=2^{ax+b}\\) to approximate the following 3 points.</p> \\(x_i\\) 0 1 4 \\(f(x_i)\\) 1 2 8 \\(w\\) 1 1 1"},{"location":"Math/Numerical_Analysis/NA/Appro/#discrete-least-squares-approximation","title":"\u79bb\u6563\u6700\u5c0f\u4e8c\u4e58\u6cd5 | Discrete Least Squares Approximation","text":"<p>This problem can be discribed as the best square approximation on Euclid space. That is, if \\(y\\in \\mathbb{R}^n\\), \\(\\{\\pmb{x}_i\\}_{i=1}^m\\subset \\mathbb{R}^n\\) is a group of linearly irrelevant vectors, then </p> \\[ V=\\text{span}\\{\\pmb{x}_1,\\pmb{x}_2,\\cdots,\\pmb{x}_m\\} \\] <p>is a subspace of \\(\\mathbb{R}^n\\). </p> <p>So we can consider the best square approximation problem</p> \\[ \\Delta(\\pmb{y},V)=\\min_{\\pmb{x}\\in V}\\|\\pmb{y}-\\pmb{x}\\|_2 \\] <p>where \\(\\|\\cdot\\|_2\\) is the Euclid norm, i.e. \\(\\|\\pmb{x}\\|_2=\\pmb{x}^T\\pmb{x}\\), \\(x\\in \\mathbb{R}^n\\)</p> <p>That is, we have to solve </p> \\[ \\left[\\begin{array}{cccc} (\\pmb{x}_1,\\pmb{x}_1) &amp; (\\pmb{x}_1,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_1,\\pmb{x}_n)\\\\ (\\pmb{x}_2,\\pmb{x}_1) &amp; (\\pmb{x}_2,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_2,\\pmb{x}_n)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (\\pmb{x}_n,\\pmb{x}_1) &amp; (\\pmb{x}_n,\\pmb{x}_2) &amp; \\cdots &amp; (\\pmb{x}_n,\\pmb{x}_n)\\\\ \\end{array}\\right] \\left[\\begin{array}{c} \\alpha_1^*\\\\ \\alpha_2^*\\\\ \\vdots\\\\ \\alpha_n^* \\end{array}\\right] = \\left[\\begin{array}{c} (\\pmb{x}_1,\\pmb{y})\\\\ (\\pmb{x}_2,\\pmb{y})\\\\ \\vdots\\\\ (\\pmb{x}_n,\\pmb{y}) \\end{array}\\right] \\] <p>If we denote \\(\\pmb{x}_i=(x_{1i},x_{2i},\\cdots,x_{ni})^T\\) and </p> \\[ A=[\\pmb{x}_1,\\pmb{x}_2,\\cdots,\\pmb{x}_m]=(x_{ij})_{n\\times m}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1m}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2m}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{nm}\\\\ \\end{array}\\right] \\] <p>then it is not so hard to detect that the above equation can be rewritten as</p> \\[ A^TA\\pmb{\\alpha^*}=A^T\\pmb{y} \\] <p>Cause \\(A^TA\\) has full rank, we can get \\(\\pmb{\\alpha^*} =(A^TA)^{-1}A^T\\pmb{y}\\).</p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#best-uniform-approximation","title":"\u6700\u4f73\u4e00\u81f4\u903c\u8fd1 | Best Uniform Approximation","text":""},{"location":"Math/Numerical_Analysis/NA/Appro/#lead-in_1","title":"\u5f15\u5165 | Lead-in","text":"<p>All the continuous function defined on \\([a,b]\\) compose a infinite-dimensional linear space, denoted by \\(C_{[a,b]}\\). To simplify our discription, we introduce norm \\(\\|\\cdot\\|_\\infty\\) to denote Chebyshev Norm</p> \\[ \\|\\cdot\\|_\\infty = \\max_{[a,b]}|f(x)|, \\quad \\forall f\\in C_{[a,b]}. \\] <p>With the following theorem explored by Weierstrass, we can find an algebraic polynomial to sufficiently approximate function \\(f\\) \\(\\in C_{[a,b]}\\).</p> <p>Weierstrass First Approximation Theorem</p> <p>For all given function \\(f(x)\\in C_{[a,b]}\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists p(x)\\) which is an algebraic function, s.t.</p> \\[ \\|f(x)-p(x)\\|&lt; \\varepsilon \\] Hints <p>Prove it by Bernstein Polynomials.</p> <p>But what we care more about is whether we can find a polynomial of degree no more than \\(n\\) to approximate function \\(f(x)\\)?</p> <p>The answer to the above question results in Chebyshev Approximation.</p> <p>Consider set</p> \\[ P_n(x)=\\text{span}\\{1,x,\\cdots, x^n\\} \\] <p>It is not hard to see that \\(P_n\\) is a subspace of \\(C_{[a,b]}\\) with \\(n+1\\) dimension. And the goal is to get</p> \\[ \\Delta(f,P_n)=\\min_{p\\in P_n}\\|f(x)-p(x)\\|_\\infty \\] <p>which is called the best uniform approximation of \\(f(x)\\).</p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#characteristics-of-best-uniform-approximation","title":"\u6700\u4f73\u4e00\u81f4\u903c\u8fd1\u7684\u7279\u5f81 | Characteristics of Best Uniform Approximation","text":"<p>We can represent the best uniform approximation with the introduction of deviation points.</p> <p>Deviation Point Set</p> <p>we define</p> \\[ \\begin{cases} E^+(f)=\\{x\\in [a,b]: f(x)=\\|f\\|_\\infty\\} \\\\ E^-(f)=\\{x\\in [a,b]: f(x)=-\\|f\\|_\\infty\\} \\end{cases} \\] <p>to be positive and negative deviation point set, and define \\(E(f)=E^+(f)\\cup E^-(f)\\) to be deviation point set.</p> <p>Recall that \\(f(x)\\in C_{[a,b]}\\), so \\(E^+(f)\\) and \\(E^-(f)\\) are both bounded closed set. </p> <p>Alternating Point Set</p> <p>If set \\(\\{x_1,x_2,\\cdots,x_k\\}\\) \\(\\subset E(f)\\) satisfies</p> \\[ \\begin{cases} a\\leq x_1&lt;x_2&lt;\\cdots &lt;x_k\\leq b  \\\\ f(x_j)=-f(x_{j+1}), \\quad j=1,2\\cdots,k-1 \\end{cases} \\] <p>then we call the above set Alternating Point Set. We call them them maximal alternating point set, if there does not exist an alternating point set of size larger than \\(k\\).</p> <p>We can construct maximal alternating point set by the following way.</p> <p>Construction of Maximal Alternating Point Set</p> <p>Let \\(S_1=\\inf E^+(f)\\cup E^-(f)\\), construct \\(x_k\\) recursively</p> \\[ x_k=\\inf S_k \\] <p>where</p> \\[ S_{k+1}=\\begin{cases} E^-(f)\\cap [x_k,b],\\quad x_k\\in E^+(f)\\\\ E^+(f)\\cap [x_k,b],\\quad x_k\\in E^-(f) \\end{cases} ,\\quad k=1,2 \\cdots \\] <p>We assure \\(x_k\\in S_k\\) when we let \\(x_k=\\inf S_k\\) because \\(S_k\\) is bounded closed set. The recursion assures \\(x_{k+1}\\) is the minimal deviation point with opposite sign to \\(x_k\\), which of course assures \\(\\{x_i\\}\\) is monotonically increasing. We can prove that, if \\(f\\neq 0\\), the above constructed set is maximal and finite(to be proved by readers).</p> <p>Property of best uniform approximation on \\(C_{[a,b]}\\)</p> <p>Assume \\(f\\in C_{[a,b]}\\) and \\(f\\notin P_n\\). If \\(p(x)\\) is the best uniform approximation polynomial of \\(f(x)\\) on \\([a,b]\\), then \\(f-p\\) has at least \\(n+2\\) alternating points.</p> Proof <p>By contradiction.</p> <p>Vall\u00e9e-Poisson Theorem</p> <p>Assume \\(f\\in C_{[a,b]}\\), if there exists polynomial \\(p\\in P_n\\), such that \\(f-p\\) has at least \\(n+2\\) points \\(x_1,x_2,\\cdots,x_{n+2} \\in [a,b]\\) alternated with positive and negative, then </p> \\[ \\Delta(f,P_n)\\geq \\mu=\\min_{1\\leq i\\leq n+2}|f(x_i)-g(x_i)| \\] Proof <p>By contradiction.</p> <p></p> <p>Chebyshev Theorem</p> <p>For all \\(f\\in C_{[a,b]}\\), \\(f\\notin P_n\\), \\(p\\in P_n\\) is the best uniform approximation of \\(f\\) if and only if, \\(f-p\\) has at least \\(n+2\\) alternating points on \\([a,b]\\).</p> Proof <p>By Sequence Theorem.</p> <p>Uniqueness of best uniform approximation</p> <p>If \\(f\\in C_{[a,b]}\\), then there exists a unique polynomial \\(p\\in P_n\\) that is the best uniform approximation of \\(f\\).</p> HintsProof <p>By prove two best uniform approximation polynomials \\(p_1,p_2\\) are the same.</p> <p>Prove Uniqueness.</p> <p>Assume \\(p_1,p_2\\in P_n\\) are both the best uniform approximation polynomials of \\(f\\),i.e. </p> \\[   \\|f-p_1\\|_\\infty = \\|f-p_2\\|_\\infty = \\min_{p\\in P_n}\\|f-p\\|_\\infty = \\Delta(f,P_n) \\] <p>then define \\(p_0=(p_1+p_2)/2\\), then</p> \\[ \\|f-p_0\\|_\\infty\\leq \\frac{1}{2}(\\|f-p_1\\|_\\infty+ \\|f-p_2\\|_\\infty) =\\Delta(f,P_n) \\] <p>which means \\(p_0\\) is also the best uniform approximation polynomial of \\(f\\). By Chebyshev Theorem, there exists \\(n+2\\) alternating points \\(x_0,x_1,\\cdots,x_{n+1}\\).</p> <p>Note that </p> \\[ \\Delta(f,P_n)=|f(x_k)-p_0(x_k)|\\leq \\frac{1}{2}(|f(x_k)-p_1(x_k)|+|f(x_k)-p_2(x_k)|)\\leq \\Delta(f,P_n),\\quad k=0,1,\\cdots,n+1 \\] <p>which means \"=\" must holds, i.e.</p> \\[ |f(x_k)-p_1(x_k)| = |f(x_k)-p_2(x_k)| = \\Delta(f,P_n),\\quad k=0,1,\\cdots,n+1 \\] <p>and </p> \\[ f(x_k)-p_1(x_k) = f(x_k)-p_2(x_k),\\quad k=0,1,\\cdots,n+1 \\quad \\text{(by Triangular inequation)} \\] <p>which means \\(p_1(x_k)=p_2(x_k)\\), that is, \\(p_1-p_2\\) has \\(n+2\\) roots, so \\(p_1=p_2\\).</p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#chebyshev-first-class-chebyshev-polynomials","title":"\u7b2c\u4e00\u7c7bChebyshev\u591a\u9879\u5f0f | First Class Chebyshev Polynomials","text":"<p>Coonsider \\(n+1\\) extreme points of \\(cosn\\theta\\) on \\([0,\\pi]\\). If \\(p(x)\\) is the polynomial of best uniform  approximation of \\(f\\) on \\([a,b]\\), then \\(f-p\\) has at least \\(n+2\\) alternating points.</p> <p>We define \\(x = \\cos\\theta\\) and </p> \\[ \\begin{align*} T_n(x)&amp;=\\cos (n\\cos^{-1}x)\\\\ &amp;=\\cos(n \\theta)\\\\ &amp;=\\sum\\limits_{k=1}^n a_k(\\cos\\theta)^i\\\\ &amp;=\\sum\\limits_{k=1}^n a_k x^i \\end{align*} \\] <p>is a polynomial of degree \\(n\\), i.e. \\(T_n(x)\\in P_n\\). It is easy to see the resursive definition</p> \\[ \\begin{cases} T_0(x)=1, T_1(x)=x,\\\\  \\displaystyle T_{n+1}(x)=2xT_n(x)-T_{n-1}(x),\\quad n=1,2,\\cdots,n-1 \\end{cases} \\] <p>By definition, we have properties for first class Chebyshev Polynomials.</p> <p>Properties of First Class Chebyshev Polynomials</p> <p>(1) Prove the recursive form of definition.</p> <p>(2) the coefficient of item with the highest degree of \\(T_n(x)\\) is \\(1/2^{n-1}\\).</p> <p>(3) \\(|T_n(x)|\\leq 1\\), \\(\\forall |x|\\leq 1\\).</p> <p>(4) \\(T_n(x)\\) has \\(n\\) different real roots </p> \\[ \\cos\\left[\\frac{(2k-1)\\pi}{2n}\\right],\\quad k=1,2,\\cdots,n \\] <p>(5) \\(\\{\\cos(k\\pi/n)\\), \\(k=0,1,\\cdots,n\\}\\) are a maximal alternating point set of \\(T_n(x)\\) on \\([-1,1]\\).</p> <p>(6) \\(T_n(x)=(-1)^nT_n(-x)\\).</p> <p>(7)</p> \\[ \\int_{-1}^1\\frac{T_m(x)T_n(x)}{\\sqrt{1-x^2}}dx=\\begin{cases}\\pi, \\quad &amp;m=n=0;\\\\ \\pi/2, \\quad &amp;m=n\\neq 0;\\\\ 0,\\quad &amp;m\\neq n.\\end{cases} \\] HintsProof <p>(1) by combination property of triangular functions.</p> <p>(7) by the orthogonal property of triangular functions.</p> <p>With the above property, we can see the following theorem.</p> <p>\\(T_n(x)\\) is the is the best uniform approximation of 0</p> <p>\\(T_n(x)\\) is the best uniform approximation of function \\(f\\equiv 0\\), that is, \\(\\forall\\) Monicpolynomial \\(p\\in P_n\\)</p> \\[ \\|p\\|\\geq |T_n|/2^{n-1} = 2^{1-n} \\] <p>Use \\(n\\) Polynomial \\(P_n(x)\\) to approximate function \\(f\\) on region \\([-1,1]\\), its remainder </p> \\[ \\begin{align*} |P_n(x)-f(x)|=|R_n(x)|&amp;=\\left|\\frac{f^{(n+1)}(\\xi)}{(n+1)!}\\prod_{i=0}^n(x-x_i)\\right|\\\\ &amp;\\leq \\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|\\frac{1}{(n+1)!} \\left| \\prod_{i=0}^n(x-x_i)\\right|\\\\ &amp;\\leq \\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|\\frac{1}{(n+1)!}\\left| \\frac{T_{n+1}(x)}{2^{n}} \\right|\\\\ &amp;\\leq \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\max\\limits_{x\\in [-1,1]}\\left|f^{(n+1)}(x)\\right|  \\end{align*} \\] <ul> <li>Minimizing Approximation Error on Arbitrary Intervals</li> </ul> <p>The technique for choosing points to minimize the interpolating error is extended to a general closed interval \\([a, b]\\) by using the change of variables</p> \\[ \\tilde{x} = \\frac{1}{2}[(b-a)x+a+b] \\] <p>Q. Find the best approximating polynomial of  \\(f (x) = e^x\\) on \\([0, 1]\\) such that the absolute error is no larger than \\(0.5\\times 10^4\\).</p> Answer <p>\\(a=0\\), \\(b=1\\), so </p> \\[ x=\\frac{a+b}{2}+\\frac{b-a}{2}t = \\frac{1}{2}(t+1), \\quad t\\in [-1,1] \\] <p>So the actual function to be approximated is </p> \\[ g(t) = f\\left[\\frac{1}{2}(t+1)\\right] = e^{\\frac{1}{2}(t+1)}, \\quad t\\in [-1,1] \\] \\[ \\max\\limits_{x\\in [-1,1]}\\left|g^{(n+1)}(x)\\right| = \\max\\limits_{x\\in [-1,1]}\\left|\\frac{1}{2^{n+1}}e^{\\frac{1}{2}(t+1)}\\right|=\\frac{e}{2^{n+1}} \\] <p>So</p> \\[ \\begin{align*} |P_n(x)-f(x)|&amp;\\leq \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\max\\limits_{x\\in [-1,1]}\\left|g^{(n+1)}(x)\\right| \\\\ &amp;= \\frac{1}{(n+1)!} \\frac{1}{2^{n}}\\frac{e}{2^{n+1}} \\end{align*} \\]"},{"location":"Math/Numerical_Analysis/NA/Appro/#economization-of-power-series","title":"\u5e42\u7ea7\u6570\u7684\u964d\u7ef4 | Economization of Power series","text":"<p>This part is also called Reducing the Degree of Approximating Polynomials.</p> <p>Consider approximating an arbitrary \\(n\\)th-degree polynomial</p> \\[ P_n(x) = a_nx^n+a_{n-1}x^{n-1}+\\cdots+a_1x+a_0, \\quad x\\in [\u22121, 1] \\] <p>with a polynomial of degree at most \\(n \u2212 1\\).</p> <p>To let \\(\\max\\limits_{x\\in [-1,1]}|P_n(x)-P_{n-1}(x)|\\) to be mininal, equals to let it be</p> \\[ a_n\\tilde{T}_n(x) = a_n T_n(x)/2^{n-1} \\] <p></p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#appendix-orthogonal-polynomials-on-l2_rhoab","title":"\u9644\u5f55: \u6709\u9650\u533a\u95f4\u4e0a\u7684\u6b63\u4ea4\u591a\u9879\u5f0f | Appendix: Orthogonal polynomials on \\(L^2_\\rho[a,b]\\)","text":""},{"location":"Math/Numerical_Analysis/NA/Appro/#properties","title":"\u6027\u8d28 | Properties","text":"<p>Properties</p> <p>If \\(\\omega_0(x), \\omega_1(x),\\cdots\\) are orthogonal polynomials on space \\(L^2_\\rho[a,b]\\) by orthogonalizing power series, then is must follow</p> <p>(i) \\(\\omega_n(x)\\) is a \\(n\\)th algebraic polynomial.</p> <p>(ii) \\(\\forall p \\in P_k\\), \\(k\\leq n\\), \\(p\\) can be represented as </p> \\[ p=\\sum_{i=0}^na_i\\omega_i(x) \\] <p>(iii) \\(\\omega_n(x)\\) is orthogonal to all polynomials whose degree is less than \\(n\\), that is, </p> \\[ \\int_{a}^b\\rho(x)\\omega_n(x)p_{n-1}(x)dx=0 \\]"},{"location":"Math/Numerical_Analysis/NA/Appro/#construction-of-monic-orthogonal-polynomials","title":"\u9996\u4e00\u6b63\u4ea4\u591a\u9879\u5f0f | Construction of Monic Orthogonal Polynomials","text":"<p>Construction of Monic Orthogonal Polynomials</p> <p>Assume \\(\\{\\overline{\\omega}_i(x)\\}_{i=0}^\\infty\\) are Monic Orthogonal Polynomials, then they satisfy the following recurrence relation</p> \\[ \\overline{\\omega}_{n+1}(x)=(x-B_n)\\overline{\\omega}_n(x)-C_n\\overline{\\omega}_{n-1}, \\quad n=1,2,\\cdots \\] <p>where </p> \\[ \\begin{align*} B_n &amp;= \\frac{(x\\overline{\\omega}_{n},\\overline{\\omega}_{n})}{(\\overline{\\omega}_{n},\\overline{\\omega}_{n})} \\\\ C_n&amp;=\\frac{( \\overline{\\omega}_{n}, \\overline{\\omega}_{n})}{(\\overline{\\omega}_{n-1},\\overline{\\omega}_{n-1})} \\end{align*} \\] HintsProof <p>By using the property of orthogonal polynomials.</p> <p>To simplify the notation, we temporarily use \\(\\omega_n(x)\\) to replace \\(\\overline{\\omega}_n(x)\\).</p> <p>We focus on \\(x\\omega_n(x)\\), which is a \\(n+1\\)th polynomial, so it can be represented by \\(\\omega_0,\\omega_1,\\cdots,\\omega_n\\), i.e.</p> \\[ \\begin{equation} x\\omega_n(x)=\\omega_{n+1}(x)+\\sum_{i=0}^nc_i\\omega_i(x) \\label{eq1} \\end{equation} \\] <p>where \\(c_i\\) are parameters to be determined.</p> <p>Now we notice that \\((\\omega_n,\\omega_s)=0\\), \\(s\\leq n-1\\), so we first employ inner product on both sides of \\(\\ref{eq1}\\) with \\(\\omega_s\\) (\\(s=0,1,\\cdots,n-2\\))</p> \\[ \\begin{equation} (x\\omega_n, \\omega_s)=(\\omega_{n+1},\\omega_s)+\\sum_{i=1}^nc_i(\\omega_i,\\omega_s)\\label{eq2} \\end{equation} \\] <p>Because we have an exact meaning of inner product, that is, integral form, so we have \\((x\\omega_n, \\omega_s)=(\\omega_n, x\\omega_s)\\), then</p> <p>for \\(s=0,1,\\cdots,n-2\\), we get </p> \\[ (x\\omega_n, \\omega_s)=0,\\quad (\\omega_{n+1},\\omega_s)=0 \\] <p>and </p> \\[ (\\omega_i,\\omega_s)=0,\\quad i\\geq s+1 \\text{ or }i\\leq s-1 \\] <p>So equation \\(\\ref{eq2}\\) becomes </p> \\[ 0=0+c_s(\\omega_s,\\omega_s) \\] <p>which means \\(c_s=0\\), \\(s=0,1,\\cdots n-2\\). Then we rewrite equation \\(\\ref{eq1}\\)</p> \\[ \\begin{equation} x\\omega_n(x)=\\omega_{n+1}(x)+c_n\\omega_n(x)+c_{n-1}\\omega_{n-1}(x) \\label{eq3} \\end{equation} \\] <p>In a similar way, we try employing inner product on both sides of the above equation \\(\\ref{eq3}\\) with \\(\\omega_{n-1}\\)</p> \\[ (x\\omega_n, \\omega_{n-1})=0+c_{n-1}(\\omega_{n-1},\\omega_{n-1}) \\] <p>which gives \\(c_{n-1}=(x\\omega_n, \\omega_{n-1})/(\\omega_{n-1},\\omega_{n-1})\\). Notice </p> \\[ (x\\omega_n, \\omega_{n-1})=(\\omega_n, x\\omega_{n-1})=(\\omega_n, \\omega_{n}) \\text{(by representing } x\\omega_{n-1} \\text{ again)} \\] <p>so \\(c_{n-1}=(\\omega_n, \\omega_{n})/(\\omega_{n-1},\\omega_{n-1})\\).</p> <p>In a similar way, employing inner product on both sides of the above equation \\(\\ref{eq3}\\) with \\(\\omega_{n}\\)</p> \\[ (x\\omega_n, \\omega_{n})=0+c_{n}(\\omega_{n},\\omega_{n}) \\] <p>which gives \\(c_{n}=(x\\omega_n, \\omega_{n})/(\\omega_{n},\\omega_{n})\\).</p> <p>Rewrite equation \\(\\ref{eq3}\\) and we prove the theorem.</p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#roots-of-orthogonal-polynomials","title":"\u96f6\u70b9\u5206\u5e03 | Roots of Orthogonal Polynomials","text":"<p>Roots of Orthogonal Polynomials</p> <p>\\(n\\)th Orthogonal Polynomial \\(\\omega_n(x)\\) has \\(n\\) distinct roots on \\([a,b]\\).</p> Hints <p>By contradiction. First show that \\(\\omega_n(x)\\) must have root, and then show it is not multiple root. Finally show the number of roots must be equal to \\(n\\). All the proof can be done by making use of properties.</p>"},{"location":"Math/Numerical_Analysis/NA/Appro/#common-orthogonal-polynomials","title":"\u5e38\u89c1\u7684\u6b63\u4ea4\u591a\u9879\u5f0f | Common Orthogonal Polynomials","text":"<ul> <li> <p>Legendre Polynomial (on \\(L^2[-1,1]\\))</p> </li> <li> <p>First class Chebyshev Polynomial (on \\(L^2_\\rho[-1,1]\\) with \\(\\rho=1/\\sqrt{1-x^2}\\))</p> </li> <li> <p>Second class Chebyshev Polynomial (on \\(L^2_\\rho[-1,1]\\) with \\(\\rho=\\sqrt{1-x^2}\\))</p> </li> <li> <p>Laguerre Polynomial (on \\(L^2(0,\\infty)\\) with \\(\\rho=e^{-x}\\))</p> </li> <li> <p>Hermite Polynomial (on \\(L^2(\\infty,\\infty)\\) with \\(\\rho=e^{x^2}\\))</p> </li> </ul>"},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/","title":"Interpolation & Polynomial Approximation","text":""},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#interpolating-polynomial","title":"\u63d2\u503c\u591a\u9879\u5f0f | Interpolating Polynomial","text":""},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#lagrange-lagrange-interpolating-polynomial","title":"Lagrange\u63d2\u503c\u591a\u9879\u5f0f | Lagrange Interpolating Polynomial","text":"<p> Inspired by \u52a0\u6743\u5e73\u5747.</p> <p>There exists and only exists a \\(n\\)th Lagrange interpolating polynomial (\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570) </p> \\[  L_n(x) = \\sum_{i=0}^{n}l_i(x)y_i = \\begin{bmatrix} l_0(x) &amp; l_1(x) &amp;l_2(x) &amp; \\cdots &amp; l_n(x)  \\end{bmatrix} \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix} = \\Phi_n(x)\\vec{y}  \\] <p>such that for each pair of given points \\((x_i, y_i)\\), \\(i = 0, 1,2,\\cdots n\\), we have \\(y_i = L_n(x_i)\\).</p> <p>Here we can consider \\(l_i(x)\\) as a base of a linear space \\(\\mathcal{P}_n(x)\\), and it can be displayed by natural base \\(1, x, x^2, \\cdots x^n\\). To be more specific,</p> \\[ l_i(x) = \\prod_{j=0 \\atop j \\neq i }^{n}\\frac{(x - x_i)}{(x_i-x_j)} \\quad i=0,1,\\cdots n  \\] <p>readers can prove the above \\(n+1\\) polynomials are linearly irrelevant.</p> <p>Another form of \\(l_i(x)\\)</p> <p>If we denote \\(\\omega(x)=\\prod\\limits_{k=0}^{n}(x-x_k)\\), so the numerator of \\(l_i(x)\\) is </p> \\[ \\frac{\\omega(x)}{x-x_i} \\] <p>and its demunerator is </p> \\[ \\omega'(x)|_{x=x_i}=\\sum_{j=0}^{n}\\prod_{k=0 \\atop k\\neq j}^{n}(x-x_k)|_{x=x_i}=\\prod_{k=0\\atop k\\neq i}^{n}(x_i-x_k) \\] <p>so </p> \\[ l_i(x)=\\frac{\\omega(x)}{(x-x_i)\\omega'(x_i)} \\] <p>which satisfies </p> \\[ l_i(x_j)=\\begin{cases}1,\\quad &amp;j=i,\\\\ 0,\\quad &amp;j\\neq i.\\end{cases} \\] <p></p> <p>Q1. Calculate the Lagrange polynomial that interpolates the following 3 points.</p> \\(x_i\\) 1 2 4 \\(f(x_i)\\) 8 1 5 Answer \\[ \\begin{align*} p_2(x) &amp;= \\frac{(x-2)(x-4)}{(1-2)(1-4)}\\times 8 + \\frac{(x-1)(x-4)}{(2-1)(2-4)}\\times 1 + \\frac{(x-1)(x-2)}{(4-1)(4-2)}\\times 5\\\\ &amp;=\\frac{8}{3}(x^2-6x+8)-\\frac{1}{2}(x^2-5x+4)+\\frac{5}{6}(x^2-3x+2)\\\\ &amp;=3x^2-16x+21 \\end{align*} \\] <p>In fact, if we assume \\(P_n(x) = \\sum\\limits_{i=0}^{n}a_ix^i\\)(natural base), and to get the parameters \\(\\{a_i\\}\\) such that \\(P_n(x_i) = y_i\\), we have to solve the following linear system</p> \\[ \\begin{bmatrix} 1 &amp; x_0 &amp; x_0^2 &amp;\\cdots &amp; x_0^n \\\\  1 &amp; x_1 &amp; x_1^2 &amp;\\cdots &amp; x_1^n \\\\  \\vdots &amp; \\vdots &amp; \\vdots &amp; &amp;\\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp;\\cdots &amp; x_n^n  \\end{bmatrix} \\begin{bmatrix}  a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_n  \\end{bmatrix}=  \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_n  \\end{bmatrix}  \\] <p>which is a little tedious.</p>"},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#neville-nevilles-method","title":"Neville\u65b9\u6cd5 | Neville's Method","text":"<p>There is another way to express polynomial, which is also a weighted average.</p> <p>An interpolation polynomial for a set of points \\(A =\\{x_0, x_1, \\cdots x_n \\}\\) can be expressed by two polynomials that interpolate \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\). That is,</p> \\[ \\begin{align*} P_{0,1,\\cdots,n}(x) &amp;= \\frac{(x-x_i)P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)-(x-x_j)P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)}{(x_j-x_i)}\\\\ &amp;=\\frac{1}{(x_j-x_i)}\\left|\\begin{array}{cc} (x-x_i) &amp; P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x) \\\\ (x-x_j) &amp; P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x) \\end{array}\\right| \\end{align*} \\] <p>where \\(P_{0,1,\\cdots,i-1,i+1,\\cdots,n}(x)\\) and \\(P_{0,1,\\cdots,j-1,j+1,\\cdots,n}(x)\\) denotes the polynomial that interpolates \\(A\\) \\ \\(\\{ x_i\\}\\) and \\(A\\) \\ \\(\\{ x_j\\}\\) respectively.</p> <p>For the first item, we have</p> \\[ \\begin{align*} P_{0,1}(x) &amp;= \\frac{(x-x_0)\\times f(x_1)-(x-x_1)\\times f(x_0)}{(x_1-x_0)}\\\\ &amp;=\\frac{1}{(x_1-x_0)}\\left|\\begin{array}{cc} (x-x_0) &amp; f(x_0) \\\\ (x-x_1) &amp; f(x_1) \\end{array}\\right| \\end{align*} \\] <p>Q2. Get the polynomial \\(p_3(x)\\) that interpolates the following 4 points and estimate \\(f(5)\\) by calculate \\(p_3(5)\\).</p> \\(x_i\\) 2 4 6 8 \\(f(x_i)\\) -8 0 8 64 Answer <p>It is a little tedious if we write the form of the polynomial and then substitute \\(5\\) in. It is more suitable to list a table.</p> <p> \\(x\\) \\(f(x)\\) \\(p_1(5)\\) \\(p_2(5)\\) \\(p_3(5)\\) 2 -8 4 0 \\(\\frac{1}{(4-2)}\\left|\\begin{array}{cc}     (5-2) &amp; -8 \\\\     (5-4) &amp; 0     \\end{array}\\right|=4\\) 6 8 \\(\\frac{1}{(6-4)}\\left|\\begin{array}{cc}     (5-4) &amp; 0 \\\\     (5-6) &amp; 8     \\end{array}\\right|=4\\) \\(\\frac{1}{(6-2)}\\left|\\begin{array}{cc}     (5-2) &amp; 4 \\\\     (5-6) &amp; 4     \\end{array}\\right|=4\\) 8 64 \\(\\frac{1}{(8-6)}\\left|\\begin{array}{cc}     (5-6) &amp; -8 \\\\     (5-8) &amp; 64     \\end{array}\\right|=-20\\) \\(\\frac{1}{(8-4)}\\left|\\begin{array}{cc}     (5-4) &amp; 4 \\\\     (5-8) &amp; -20     \\end{array}\\right|=-2\\) \\(\\frac{1}{(8-2)}\\left|\\begin{array}{cc}     (5-2) &amp; 4 \\\\     (5-8) &amp; -2     \\end{array}\\right|=1\\) </p>"},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#newton-newtons-divided-difference-formula","title":"Newton\u5dee\u5546\u8868\u8fbe\u5f0f | Newton's Divided Difference Formula","text":"<p>If we rewrite the \\(n\\)th Lagrange polynomial \\(P_n(x)\\) into another form:</p> \\[ P_n(x) = a_0+a_1(x-x_0)+a_2(x-x_0)(x-x_1)+\\cdots+a_n(x-x_0)\\cdots(x-x_n) \\] <p>By letting \\(x =x_0, x_1,\\cdots, x_n\\), we get</p> \\[ \\begin{align*} P_n(x_0) = &amp; a_0\\\\ P_n(x_1) = &amp; a_0 + a_1(x_1-x_0)\\\\ P_n(x_2) = &amp; a_0+a_1(x_2-x_0)+a_2(x_2-x_0)(x_2-x_1)\\\\ &amp;\\vdots\\\\ P_n(x_n) = &amp; a_0+a_1(x_n-x_0)+a_2(x_n-x_0)(x_n-x_1)+\\\\ &amp;\\cdots+a_n(x_n-x_0)\\cdots(x_n-x_{n-1}) \\end{align*} \\] <p>Then we can define:</p> \\[ \\begin{align*} f_n[x_0] &amp;\\overset{\\Delta}{=} f(x_0) = a_0\\\\ f_n[x_0, x_1] &amp;\\overset{\\Delta}{=} \\frac{f(x_1) - f(x_0)}{x_1-x_0} = a_1\\\\ f_n[x_0, x_1, x_2] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_1)}{x_2-x_1}-\\frac{f(x_1)-f(x_0)}{x_1-x_0} }{x_2-x_0} \\\\&amp;= \\frac{\\frac{f(x_2)-f(x_0)-(f(x_1)-f(x_0))}{x_2-x_1}-a_1}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_1-x_0)}{x_2-x_1}-\\frac{a_1(x_2-x_1)}{x_2-x_1}}{x_2-x_0} \\\\ &amp;= \\frac{\\frac{f(x_2)-f(x_0)}{x_2-x_1} - \\frac{a_1(x_2-x_0)}{x_2-x_1}}{x_2-x_0}\\\\ &amp;= \\frac{f(x_2)-a_0 - a_1(x_2-x_0)}{(x_2-x_0)(x_2-x_1)}=a_2\\\\ &amp;\\vdots\\\\ f[x_0,x_1,\\cdots, x_n] &amp;\\overset{\\Delta}{=} \\frac{f[x_1,x_2,\\cdots,x_n] - f[x_0,x_1,\\cdots,x_{n-1}]}{x_n-x_0} = a_n \\end{align*} \\] <p>We can prove the above definition \\(f[x_0,x_1\\cdots,x_n]\\), which is called divided difference, to be equal to \\(a_n\\) by induction.</p> <p>We can also show that \\(a_1\\) is the coefficient of the highest item of polynomial of degree \\(1\\) that interpolates \\(x_0,x_1\\), and \\(a_2\\) is the coefficient of the highest item of polynomial of degree \\(2\\) that interpolates \\(x_0,x_1,x_2\\) ...</p> <p>This iterative method is quite useful in determining the parameters of \\(n\\)th Lagrange polynomial.</p> \\[ P_n(x) = f[x_0] + \\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(x-x_j)\\right) \\] <p>The following relation gives a slightly quicker way to compute \\(f[x_0,x_1,\\cdots,x_n]\\):</p> <p>\u8ba1\u7b97\u5dee\u5546 | Calculating divided difference</p> \\[ f[x_0,x_1,\\cdots,x_n] = \\sum_{k=0}^{n}\\frac{f(x_k)}{\\omega'_{n+1}(x_k)} \\] <p>where </p> \\[ \\omega_{n+1}(x) = \\prod_{i=0}^{n}(x-x_i),\\quad \\omega'_{n+1}(x_j) = \\prod_{i=0 \\atop i \\neq j}^{n}(x_j-x_i) \\] Proof <p>We know from Lagrange polynomial of degree \\(n\\)</p> \\[ P_n(x) = \\sum_{i=0}^{n}f(x_i)\\frac{\\omega(x)}{(x-x_i)\\omega'(x_i)} \\] <p>where </p> \\[ \\omega(x) = \\prod_{j=0}^n(x-x_j), \\quad \\omega'(x_i) = \\prod_{j=0\\atop j\\neq i}^n(x_i-x_j) \\] <p>So the divided difference equals to the coefficient of the highest item of \\(P_n(x)\\), and we are done.</p> <p>Now solve Question 1 in the above part of the article.</p> Answer <p> \\(x_i\\) \\(f(x_i)\\) \\(f[x_i,x_j]\\) \\(f[x_i,x_j,x_k]\\) 1  2  4 8  1  5 -7  2 3 </p> <p>So the interpolating polynomial is </p> \\[ P_2(x) = 8-7(x-1)+ 3(x-1)(x-2) \\] <p>Some properties are the followings.</p> <p>\u5dee\u5546\u4e0e\u5fae\u5206\u5747\u503c\u7684\u5173\u7cfb | Relationship of DD &amp; Mean Value Differentials</p> <p>Suppose that \\(f \\in C^n[a, b]\\) and \\(x_0, x_1, \\cdots, x_n\\) are distinct numbers in \\([a, b]\\). Then a number \\(\\xi\\) exists in \\((a, b)\\) with</p> \\[ f[x_0,x_1\\cdots,x_n] = \\frac{f^{(n)}(\\xi)}{n!} \\] <p>Specifically, we denote \\(a=\\min\\{x_i:i=0,1\\cdots,n\\}\\) and \\(b=\\max\\{x_i:i=0,1\\cdots,n\\}\\)</p> <p>It is like </p> \\[ f[x_0,x_1] = \\frac{f(x_1)-f(x_0)}{x_1-x_0} = f'(\\xi) \\] <p>but add \\(n!\\) to the denominator.</p> <p>The collary can be quite simple.</p> <p>Collary of the above</p> <p>If \\(f(x)\\) is a polynomial of degree \\(k\\), \\(k&lt;n\\), so the \\(n\\)th divided difference is \\(0\\).</p>"},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#error-analysis-of-interpolation","title":"\u63d2\u503c\u8bef\u5dee\u5206\u6790 | Error Analysis of Interpolation","text":"<p>The following theorem gives the error bound.</p> <p>\u62c9\u683c\u6717\u65e5\u57fa\u51fd\u6570\u7684\u4f59\u9879 | The remainder of Lagrange interpolating polynomial</p> <p>Suppose \\(x_0, x_1, \\cdots , x_n\\) are distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n+1}[a, b]\\). Then, for each \\(x \\in [a, b]\\), a number \\(\\xi(x)\\) (generally unknown) between \\(x_0, x_1, \\cdots , x_n\\), and hence in \\((a, b)\\), exists with</p> \\[ f(x) = P(x) + \\frac{f^{n+1}(\\xi(x))}{(n+1)!}\\prod_{i=0}^{n}(x - x_i) \\] <p>where \\(P(x)\\) is the Lagrange interpolating polynomial.</p> <p>Prove it.</p> HintsProof <ul> <li>Using a function </li> </ul> \\[ g(t) = f(t) - P(t) - [f(x)-P(x)]\\prod_{i=0}^{n}\\frac{(t-x_i)}{x-x_i} \\] <p>which is \\(C^{n+1}[a, b]\\). Here we see \\(x\\neq x_k\\) is a constant for variable \\(t\\). Note that \\(f(x_k)=0(k=0,1,\\cdots n)\\) and \\(f(x)=0\\) for \\(x \\in [a, b]\\)(Readers can prove them by substituting in). We can see that \\(n+1+1=n+2\\) zero points here(\\(x,x_0,\\cdots, x_n\\)). </p> <ul> <li>Using generalized Rolle's Theorem: There exists \\(\\xi(x) \\in [a, b]\\) such that \\(g^{n+1}(\\xi(x))=0\\)</li> </ul> <p>And make \\(n+1\\) derivatives to \\(g(t)\\)(with respect to \\(t\\)), we have</p> \\[ g^{(n+1)}(t)=f^{(n+1)}(t)-P^{(n+1)}(t)-\\frac{(n+1)![f(x)-P(x)]}{\\prod\\limits_{i=0}^n (x-x_i)} \\] <p>substitute in \\(\\xi\\) we have</p> \\[ 0=g^{(n+1)}(\\xi)=f^{(n+1)}(\\xi)- \\frac{(n+1)![f(\\xi)-P(\\xi)]}{\\prod\\limits_{i=0}^n (x-x_i)} \\] <p>cause \\(P(x)\\) is \\(n\\)th polynomial, so \\(P^{(n+1)}(\\xi)=0\\). So solve for \\(f(x)\\) we have</p> \\[ f(x)=P(x)+\\frac{f^{n+1}(\\xi(x))}{(n+1)!}\\prod_{i=0}^{n}(x - x_i) \\] <p>and we can get the result. </p> <p>Or by using the relationship between divided difference and mean value differentials.</p> <p>\u725b\u987f\u63d2\u503c\u516c\u5f0f\u7684\u4f59\u9879 | The remainder of Newton's interpolating Formula</p> <p>Suppose \\(x_0, x_1, \\cdots , x_n\\) are distinct numbers in the interval \\([a, b]\\) and \\(f \\in C^{n+1}[a, b]\\). Then, for each \\(x \\in [a, b]\\), we have</p> \\[ \\begin{align*} f(x)=&amp;f[x_0]+f[x_0,x_1](x-x_1)+\\cdots\\\\ &amp;+f[x_0,x_1,\\cdots, x_n]\\prod_{i=0}^{n-1}(x-x_i)\\\\ &amp;+f[x_0,x_1,\\cdots,x_n,x]\\prod_{i=0}^n(x-x_i) \\end{align*} \\] Proof <p>Similar to proof of Lagrange interpolation polynomial. We assume \\(z\\neq x_0,x_1,\\cdots,x_n\\), then consider \\(n+1\\)th polynomial \\(Q(x)\\) that interpolates these \\(n+2\\) points \\(\\{z,x_0,x_1,\\cdots,x_n\\}\\), so we have</p> \\[ Q(x)=f[x_0]+\\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(x-x_j)\\right)+f[x_0,x_1,\\cdots,x_n,z]\\prod_{i=0}^n(x-x_i) \\] <p>which satisfies \\(f(x_i)=Q(x_i)(i=0,1,\\cdots,n)\\) and \\(f(z)=Q(z)\\), the latter one gives</p> \\[ f(z)= Q(z) = f[x_0]+\\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(z-x_j)\\right)+f[x_0,x_1,\\cdots,x_n,z]\\prod_{i=0}^n(z-x_i) \\] <p>substitute \\(z\\) with \\(x\\) and we are done.</p> <p>There is still one step left. For \\(z=x_i\\), divided difference \\(f[x_0,x_1,\\cdots,x_n,z]\\) is not defined. So for \\(x_i\\), we consider a sequence \\(\\{y_k^{i}\\}_{k=1}^\\infty\\), where \\(y_k^i\\neq x_i\\) and \\(\\lim_{k\\rightarrow \\infty}y_k^i=x_i\\). First we have the above theorem holds on \\(y_k^i\\), i.e.</p> \\[ \\begin{align*} f(y_k^i) &amp;= f[x_0]+\\sum_{i=1}^{n}\\left(f[x_0,x_1,\\cdots,x_i]\\prod_{j=0}^{i-1}(y_k^i-x_j)\\right)+f[x_0,x_1,\\cdots,x_n,y_k^i]\\prod_{i=0}^n(y_k^i-x_i)\\\\ &amp;\\overset{\\Delta}{=}Q(y^i_k) + R(y^i_k) \\end{align*} \\] <p>we know that \\(f(x)\\) and \\(Q(x)\\)(\\(n\\)th polynomial) are continuous on \\([a,b]\\), so \\(\\lim_{k\\rightarrow \\infty}f(y_k^i)=f(x_i)\\) and \\(\\lim_{k\\rightarrow \\infty}Q(y_k^i)=Q(x_i)=f(x_i)\\), so</p> \\[ \\lim_{k\\rightarrow \\infty}R(y_k^i)=0 \\] <p>we could define \\(R(x_i)=0\\) and the above equation still holds.</p> <p>The above error item is useful in Numerical Integration.</p> <p>Compared to the remainder of Taylor's extension</p> \\[ R_n(x) = \\frac{f^{n+1}(\\xi(x))}{(n+1)!}(x-x_0)^{n+1} \\] <p>Because \\(\\xi(x)\\) is usually unknown, so we often use a number \\(x' \\in  [a, b]\\) such that </p> \\[ |f^{n+1}(\\xi(x))|\\leq |f^{n+1}(x')| \\]"},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#interpolation-of-equidistant-points","title":"\u7b49\u8ddd\u63d2\u503c | Interpolation of Equidistant Points","text":"<p>In actual calculation in computer, we use difference to replace divided difference when we interpolate points with equal distance.</p> <p>Definition of Difference on Equidistant Points</p> <p>Difference of first order is defined by</p> \\[ \\Delta f(x_i)=f(x_{i+1})-f(x_i) \\] <p>Difference of second order is defined by</p> \\[ \\begin{align*} \\Delta^2 f(x_i)&amp;=\\Delta f(x_{i+1})-\\Delta f(x_i)\\\\ &amp;=f(x_{i+2})-2f(x_{i+1})+f(x_i) \\end{align*} \\] <p>By recursion, we define Difference of \\(n\\)th order</p> \\[ \\Delta^n f(x_i)=\\Delta^{n-1}f(x_{i+1})-\\Delta^{n-1}f(x_i) \\] <p>By induction, it is easy to see that</p> \\[ \\begin{align*} \\Delta^{n}f(x_i) = &amp;f(x_{i+n})-nf(x_{i+n-1})+C^1_{n}f(x_{i+n-2})\\\\ +&amp;\\cdots+(-1)^{n-1}nf(x_{i+1})+(-1)^nf(x_i)\\\\ =&amp;\\sum_{k=0}^n(-1)^k C_n^kf(x_{i+k}). \\end{align*} \\] <p>We usually use the recursive method for computing.</p> <p>In situation where points are equally distant, we have a relationship between divided difference and difference.</p> <p>relationship between divided difference and difference</p> <p>Under equidistant points, we have </p> \\[ f[x_0,x_1,\\cdots,x_n]=\\frac{\\Delta^{n}f(x_0)}{h^n n!} \\] <p>where \\(h\\) is the distance between two adjacent points.</p> Proof <p>By induction.</p>"},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#hermite-hermite-polynomials","title":"Hermite\u591a\u9879\u5f0f | Hermite Polynomials","text":"<p>We want to consider the smoothness of interpolating polynomials, so we need to consider its derivatives, especially the first order derivative. </p> <p>Definition of Osculating Polynomial</p> <p>The osculating polynomial approximating \\(f\\) is the polynomial \\(P(x)\\) of least degree such that</p> \\[ \\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k},\\quad, \\forall i = 0,1,\\cdots, n, \\forall k = 0,1,\\cdots, m_i \\] <p>Where \\(n\\) is the total number of sampling points and \\(m_i\\) is the degree of smoothness at point \\(x_i\\).</p> <p>If \\(m_i=1\\) for all \\(i=0,1,\\cdots, n\\), then the above polynomial is called Hermite Polynomial.</p> <p>Composition of Hermite Polynomial</p> <p>If \\(f \\in C^1[a, b]\\) and \\(x_0, \\cdots , x_n \\in [a, b]\\) are distinct, the unique polynomial of least degree agreeing with \\(f\\) and \\(f'\\) at \\(x_0,\\cdots , x_n\\) is the Hermite polynomial of degree at most \\(2n + 1\\) given by</p> \\[ H_{2n+1}(x) = \\sum_{j=0}^{n}f(x_j)H_{n,j}(x) + \\sum_{j=0}^{n}f'(x_j)\\hat{H}_{n,j}(x) \\] <p>where, for \\(L_{n, j}(x)\\) denoting the \\(j\\)th Lagrange coefficient polynomial of degree \\(n\\), we have</p> \\[ H_{n,j}(x) = [1-2(x-x_j)L'_{n,j}(x_j)]L^2_{n,j}(x),\\quad \\hat{H}_{n,j}(x) = (x-x_j)L^2_{n,j}(x) \\] <p>The composition is usually a little tedious.</p>"},{"location":"Math/Numerical_Analysis/NA/IntPo_Poly/#cubic-spline-interpolation","title":"\u6837\u6761\u63d2\u503c | Cubic Spline Interpolation","text":"<p>We change \\(\\frac{d^kP(x_i)}{dx^k} = \\frac{d^kf(x_i)}{dx^k}\\) into equations between adjacent curves. Because we often do not know the derivatives of the point.</p> <p>Define \\(s(x)\\) piece-wisely with \\(s_i(x)\\in [x_i,x_{i+1}]\\)</p> \\[ s_i(x)=a_i+b_ix+c_ix^2+d_ix^3, \\quad i=0,1,\\cdots,n-1 \\] <p>which has 4 parameters to be determined. So the overall number of parameters to be determined is \\(4n\\). The condition they have to satisfy</p> \\[ s_i(x_i)=f(x_i), s_i(x_{i+1})=f(x_{i+1}), i=0,1,\\cdots,n-1 \\quad \\text{[$2n$ equations]} \\] \\[ s'_i(x_{i})=s'_{i-1}(x_{i}),i=1,2,\\cdots, n-1\\quad \\text{[$n-1$ equations]} \\] \\[ s''_i(x_{i})=s''_{i-1}(x_{i}),i=1,2,\\cdots, n-1\\quad \\text{[$n-1$ equations]} \\] <p>So the above necessary condition gives \\(4n-2\\) equations while we have \\(4n\\) parameters to be determined. So we need to add another two equations for computing. There are many ways while the followings are usually seen.</p> <p>Three possible dealings</p> <p>(i) provide the derivatives of \\(2\\) endpoints, i.e. \\(s'(a)=f'(a)\\), \\(s'(b)=f'(b)\\), which is called D1 cubic spline or Clamped Cubic Spline.</p> <p>(ii) provide the second derivatives of \\(2\\) endpoints, i.e. \\(s''(a)=f''(a)\\), \\(s''(b)=f''(b)\\), which is called D1 cubic spline. Specially, if \\(s''(a)=s''(b)=0\\), it is called Natural Spline.</p> <p>(iii) take \\(s(a)=f(a)\\) or \\(s(b)=f(b)\\) out and provide extra \\(3\\) equations \\(s(a)=s(b)\\), \\(s'(a)=s'(b)\\), \\(s''(a)=s''(b)\\), which is called periodical cubic spline.</p> <p>The solution for solving parameters by given condition.</p> <p>Calculation of parameters</p> <p>Consider \\(s(x)\\) on \\([x_i,x_{i+1}]\\), if we denote \\(s''(x_i)=M_i\\), then the overall equation becomes</p> \\[ \\mu_i M_{i-1}+2M_i+\\lambda_i M_{i+1} = 6f[x_{i-1},x_i,x_{i+1}], \\quad i=1,2,\\cdots,n-1 \\] <p>where </p> \\[ \\mu_i=\\frac{x_i-x_{i-1}}{x_{i+1}-x_{i-1}}, \\lambda_i=\\frac{x_{i+1}-x_i}{x_{i+1}-x_{i-1}} \\] <p>for \\(i=0\\) and \\(i=n\\) two situation, we deduce different equations for different condition.</p> <p>(i) D1 cubic spline. we have</p> \\[ \\begin{cases} 2M_0+M_1=6f[x_0,x_0,x_1]\\\\ M_{n-2}+2M_{n-1}=6f[x_{n-2},x_{n-1}] \\end{cases} \\] <p> Q3. Interpolate the following points with natural cubic spline. \\(x_i\\) 0 1 2 \\(f(x_i)\\) 1 2 6 Answer <p>Define \\(s_0(x)=a_0+b_0x+c_0x^2+d_0x^3\\) and \\(s_1(x)=a_1+b_1(x-1)+c_1(x-1)^2+d_1(x-1)^3\\), then </p> \\[ \\begin{cases} b_0+2c_0+3d_0=b_1 \\quad &amp;\\text{derivative}\\\\ 2c_0+6d_0=2c_1\\quad &amp;\\text{second derivative}\\\\ 2c_0=0 \\quad &amp;s''(a)=0\\\\ 2c_1+6d_1=0 \\quad &amp; s''(b)=0 \\end{cases} \\] <p>with condition that satisfies passing data points</p> \\[ \\begin{cases} a_0=1\\\\  a_0+b_0+c_0+d_0=2\\\\ a_1=2\\\\ a_1+b_1+c_1+d_1=6 \\end{cases} \\] <p>So represent all the parameters with \\(d_1\\) or \\(d_0\\), we get</p> \\[ \\begin{cases} a_0=1\\\\ b_0=\\frac{1}{4}\\\\ c_0=0\\\\ d_0=\\frac{3}{4} \\end{cases} ,\\quad  \\begin{cases} a_1=2\\\\ b_1=\\frac{5}{2}\\\\ c_1=\\frac{9}{4}\\\\ d_1=-\\frac{3}{4}\\\\ \\end{cases} \\]"},{"location":"Math/Numerical_Analysis/NA/NI/","title":"Numerical Integration","text":"<p>Generally speaking, we encounter an integration problem </p> \\[ I(f) =\\int_a^b\\rho(x)f(x)dx \\] <p>which cannot be solved by indefinite integral, so we have to define a numerical formula to approximate the integration. Typical quadrature formula (\u6c42\u79ef\u516c\u5f0f) can be written as</p> \\[ \\begin{equation} I_n(f)=\\sum_{k=1}^nA_kf(x_k)\\label{I-genaral} \\end{equation} \\] <p>where the subscript \\(n\\) denotes sampling on \\(n\\) points, \\(x_k(k=1,2,\\cdots,n)\\) are nodes on \\([a,b]\\) and \\(A_k\\) are coefficients, whose value are only related with \\([a,b]\\), \\(\\rho(x)\\) and the sampling points, rather than the formula of \\(f\\) itself.</p> <p>Naively, we get error</p> \\[ E_n(f)= I(f)-I_n(f) \\] <p>which is obviously hard to get because we do not know the exact integration value. So we transfer to a concept called  Algebraic Precision.</p>"},{"location":"Math/Numerical_Analysis/NA/NI/#algebraic-precision","title":"\u4ee3\u6570\u7cbe\u5ea6 | Algebraic Precision","text":"<p>The degree of accuracy, or precision, of a quadrature formula is the largest positive integer \\(m\\) such that the formula is exact for \\(x^k\\) , for each \\(k = 0, 1,\\cdots, m\\). That is, we call a formula has \\(m\\) algebraic precision if there exists \\(m\\in \\mathbb{N}^+\\), s.t.</p> \\[ E_n(x^k)=0, k=0,1,\\cdots,m,\\quad E_n(x^{m+1})\\neq 0. \\] <p>It is easy to see that for all polynomial \\(p(x)\\) of degree no more than \\(m\\), we have \\(E_n(p)=0\\).</p> <p>If we find a simpler function \\(p(x)\\) that approximates \\(f(x)\\) on \\([a,b]\\), and \\(I(p)\\) is easy to get, then we can use \\(I(p)\\) to approximate \\(I(f)\\). That is, if </p> \\[ \\|f-p\\|=\\max_{x\\in [a,b]}|f(x)-p(x)| &lt;\\varepsilon \\] <p>then </p> \\[ |I(f)-I(p)|=\\left|\\int_a^b\\rho(x)[f(x)-p(x)]dx\\right| &lt;\\varepsilon (b-a) \\] <p>which is also small enough.</p>"},{"location":"Math/Numerical_Analysis/NA/NI/#newton-cotes-newton-cotes-formulas","title":"Newton-Cotes \u516c\u5f0f | Newton-Cotes Formulas","text":"<p>The simplest function is polynomials. So we try to find a polynomial \\(p(x)\\) to approximate \\(f(x)\\) and then use \\(I(p)\\) to approximate \\(I(f)\\). The following formulas are really natural if readers have been familiar with Interpolating Polynomial.</p> <p>Given \\(n+1\\) points \\(x_0&lt;x_1&lt;\\cdots&lt;x_n\\), we have a Lagrange Polynomial</p> \\[ p_n(x)=\\sum_{i=0}^n\\prod_{j=0\\atop j\\neq i}^n\\frac{(x-x_j)}{x_i-x_j}f(x_i) \\] <p>use the above polynomial as an approximation for integration. That is, </p> \\[ \\begin{equation} \\begin{cases} \\displaystyle I_{n+1}(p)=I(p_n)=\\sum\\limits_{i=0}^nA^n_if(x_i)\\\\ \\displaystyle A_i^n=\\int_a^b\\prod\\limits_{j=0\\atop j\\neq i}^n \\frac{(x-x_j)}{(x_i-x_j)}dx, i=0,1,\\cdots, n. \\end{cases}\\label{quadrature} \\end{equation} \\]"},{"location":"Math/Numerical_Analysis/NA/NI/#deduction","title":"\u63a8\u5bfc | Deduction","text":"<p>In equation \\(\\ref{quadrature}\\), if we let \\(\\rho(x)\\equiv1\\) and choose equidistant points:</p> \\[ x_k=a+kh, \\quad h=\\frac{b-a}{n},\\quad  k=0,1,\\cdots,n \\] <p>then the corresponding quadrature formula is called Newton-Cotes Formula. In fact, if we let \\(x=a+th\\), \\(t\\in [0,n]\\), then we have</p> \\[ \\begin{align*} A_i^n&amp;=\\int_0^n\\prod_{j=0\\atop j\\neq i}^n \\frac{(th-jh)}{(ih-jh)}d(a+th)\\\\ &amp;=h\\int_0^n\\prod_{j=0\\atop j\\neq i}^n\\frac{(t-j)}{(i-j)}dt \\end{align*} \\] <p>common rule of quadrature formula</p> <p>(i) Trapezoidal Rule</p> <p>If we let \\(n=1\\) and get Trapezoidal Rule of quadrature</p> \\[ I_2(f)=I(p_1)=\\frac{b-a}{2}[f(a)+f(b)] \\] <p>\\(f\\) is approximated by a linear expression.</p> <p>(ii) Simpson\u2019s Rule(Commonly used)</p> <p>If we let \\(n=2\\) and get Simpson\u2019s Rule of quadrature</p> \\[ I_3(f)=I(p_2)=\\frac{b-a}{6}\\left[f(a)+4f\\left(\\frac{a+b}{2}\\right)+f(b)\\right] \\] <p>\\(f\\) is approximated by a parabola expression.</p> <p>(iii) Cotes' Rule</p> <p>If we let \\(n=4\\) and get Cotes' rule of quadrature</p> \\[ I_5(f)=I(p_4)=\\frac{b-a}{90}\\left[7f(x_0)+32f(x_1)+12f(x_2)+32f(x_3)+7f(x_4)\\right] \\] <p>where \\(x_i=a+(b-a)/4\\cdot i\\), \\((i=0,1,2,3,4)\\).</p>"},{"location":"Math/Numerical_Analysis/NA/NI/#error-analysis","title":"\u8bef\u5dee\u5206\u6790 | Error analysis","text":"<p>Assume \\(x_i(i=0,1\\cdots,n)\\) are equidistant(\\(h\\) apart), then we introduce</p> \\[ \\omega_n(x)=\\prod_{i=0}^n(x-x_i) \\] <p>which have some properties.</p> <p>Properties of \\(\\omega_n(x)\\)</p> <p>(i) \\(\\omega_n((a+b)/2+\\xi)=(-1)^{n+1}\\omega_n((a+b)/2-\\xi)\\)</p> <p>(ii) For \\(\\xi\\neq x_i(i=0,1,\\cdots,n)\\), the following statements hold.</p> \\[ \\begin{align*} &amp;|\\omega_n(\\xi+h)|&lt;|\\omega_n(\\xi)|,\\quad \\forall a&lt;\\xi+h\\leq(a+b)/2\\\\ &amp;|\\omega_n(\\xi)|&lt;|\\omega_n(\\xi+h)|,\\quad \\forall (a+b)/2\\leq\\xi&lt;b \\end{align*} \\] <p>Look at the graph, in which \\(n=5,6\\).</p> <p><p> </p></p> <p>Then we define an integral of \\(\\omega_n(x)\\)</p> \\[ \\Omega_n(x)=\\int_a^x \\omega_n(s)ds, n=1,2,\\cdots \\] <p>which has a great property.</p> <p>Properties of \\(\\Omega_n(x)\\)</p> <p>When \\(n\\) is en even number, then</p> <p>(i) \\(\\Omega_n(a)=\\Omega_n(b)=0\\)</p> <p>(ii) \\(\\Omega_n(x)&gt;0, \\quad\\forall x\\in(a,b)\\)</p> <p>Then we can prove the error equation for Newton-Cotes Formula.</p> <p>Theorem of Error Analysis for Newton-Cotes Formula</p> <p>(i) If \\(n\\) (number of intervals) is even, and \\(f\\in C^{n+2}[a,b]\\), then we have</p> \\[ E_{n+1}(f)=\\frac{k_n}{(n+2)!}f^{(n+2)}(\\eta),\\quad a&lt;\\eta&lt;b \\] <p>where </p> \\[ k_n=\\int_a^bx\\omega_n(x)dx&lt;0 \\] <p>(ii) If \\(n\\) is odd, and \\(f\\in C^{n+1}[a,b]\\), then the error expression is</p> \\[ E_{n+1}(f)=\\frac{k_n}{(n+1)!}f^{(n+1)}(\\eta),\\quad a&lt;\\eta&lt;b \\] <p>where</p> \\[ k_n=\\int_a^b\\omega_n(x)dx&lt;0 \\] Proof <p>Using remainder of interpolation polynomial in Newton's divided difference formula.</p> <p></p> <p>Corollary of Error Analysis</p> <p>(i) For Tranpezoidal Formula, the rounding error is</p> \\[ E_2(f)=-\\frac{(b-a)^3}{12}f''(\\eta),\\quad a&lt;\\eta&lt;b \\] <p>(ii) For Simpson Formula, the rounding error is </p> \\[ E_2(f)=-\\frac{(b-a)^5}{2880}f^{(4)}(\\eta),\\quad a&lt;\\eta&lt;b \\]"},{"location":"Math/Numerical_Analysis/NA/NI/#numerical-stability","title":"\u6570\u503c\u7a33\u5b9a\u6027 | Numerical Stability","text":"<p>Actually, it is usually hard to get an accurate value of \\(f(x_k)\\), so we introduce error here, which may influence afterwards computation. Now we quantify this effect. </p> <p>Assume we use \\(\\tilde{f}(x_k)\\) to replace \\(f(x_k)\\), denote \\(\\varepsilon_k=f(x_k)-\\tilde{f}(x_k)\\), then the integral error</p> \\[ \\begin{align*} |I_n(f)-I_n(\\tilde{f})|&amp;=\\left|\\sum_{k=0}^n A_k f(x_k)-\\sum_{k=0}A_k \\tilde{f}(x_k)\\right|\\\\ &amp;\\leq \\sum_{k=0}^n |A_k||\\varepsilon_k|\\\\ &amp;&lt;\\sum_{k=0}^n |A_k|\\max_{0\\leq k\\leq n}{|\\varepsilon_k|} \\end{align*}\\] <p>So if \\(\\sum_{k=0}^n |A_k|\\) is bounded, then the error can be bounded. However, we could prove that this item would go to infinity as \\(n\\rightarrow \\infty\\), so higher order of Newton-Cotes formula is impractical.</p>"},{"location":"Math/Numerical_Analysis/NA/NI/#composite-numerical-integration","title":"\u590d\u5316\u79ef\u5206 | Composite Numerical Integration","text":"<p>We could use low order Newton-Cotes formula with Narrowing down \\(h\\). That is, use Simpson's formula multiple times on little interval of length \\(h\\). </p>"},{"location":"Math/Numerical_Analysis/NA/NI/#romberg-romberg-integration","title":"Romberg \u79ef\u5206 | Romberg Integration","text":"<p>Here we have another way to improve precision of integration, which is Richardson's Extrapolation. This is a general method.</p> <ul> <li>Richardson's Extrapolation</li> </ul> \\[ \\begin{equation} T_0(h) - I=\\alpha_1 h + \\alpha_2 h^2 +\\cdots \\label{richardson} \\end{equation} \\] <p>Let</p> \\[ \\begin{equation} T_0(h/2)-I=\\alpha_1 h/2 + \\alpha_2 (h/2)^2 + \\cdots \\label{richardson-2} \\end{equation} \\] <p>multiply \\(2\\) to both sides of equation \\(\\ref{richardson-2}\\) and Subtract equation \\(\\ref{richardson}\\), get</p> \\[ \\frac{2T_0(h/2)-T_0(h)}{2-1}-I=-\\frac{1}{2}\\alpha_2 h^2 +\\cdots \\] <p>So </p> \\[ \\begin{align*} T_1(h) &amp;= \\frac{2T_0(h/2)-T_0(h)}{2-1} = I + \\beta_1 h^2+\\beta_2 h^3+\\cdots \\\\ T_2(h) &amp;= \\frac{2^2T_1(h/2)-T_1(h)}{2^2-1} = I + \\gamma_1 h^3 +\\cdots \\\\ &amp;\\vdots\\\\ T_n(h) &amp;= \\frac{2^nT_{n-1}(h)-T_{n-1}(h)}{2^n-1} = I + \\xi_1 h^{n+1} +\\cdots  \\end{align*} \\]"},{"location":"Math/Numerical_Analysis/NA/NI/#adaptive-quadrature-methods","title":"\u81ea\u9002\u5e94\u6c42\u79ef\u65b9\u6cd5 | Adaptive Quadrature Methods","text":""},{"location":"Math/Numerical_Analysis/NA/NI/#non-equidistant-quadrature-formula","title":"\u975e\u7b49\u8ddd\u6c42\u79ef\u516c\u5f0f | Non-equidistant Quadrature formula","text":"<p>Could we free some limits of the above quadrature, and get some more accurate formula? To be more specific, could we let the coefficient of the quadrature to vary, and do not limit the node to be integrated, and then obtain a better result? </p> <p>Let us analyze that, if a quadrature has \\(m\\) algebraic precision, then its coefficients and nodes of formula \\(\\ref{I-genaral}\\) must follow the system</p> \\[ \\begin{cases} A_1+A_2+\\cdots+A_n&amp;=\\int_{a}^b\\rho(x)dx\\\\ A_1x_1+A_2x_2+\\cdots+A_nx_n&amp;=\\int_{a}^b\\rho(x)xdx\\\\ \\cdots&amp;\\\\ A_1x_1^m+A_2x_2^m+\\cdots+A_nx_n^m&amp;=\\int_{a}^b\\rho(x)x^mdx\\\\ \\end{cases} \\]"},{"location":"Math/Numerical_Analysis/NA/NI/#uniform-coefficient-formula","title":"\u4e00\u81f4\u7cfb\u6570\u516c\u5f0f | Uniform Coefficient Formula","text":"<p>Given coefficients \\(A_1,A_2,\\cdots,A_n\\), the above non-linear system of variables \\(x_1,x_2,\\cdots,x_n\\) might have solution, with at least \\(m\\) algebraic precision.</p> <p>Assume \\(\\rho(x)\\equiv 1\\), \\(A_1=A_2=\\cdots=A_n\\), then it must follow</p> \\[ I_n(f)=A_n\\sum_{i=1}^nf(x_i) \\] <p>and</p> \\[ A_n\\sum_{i=1}^nx_i^k=\\frac{1}{k+1}(b^{k+1}-a^{k+1}),\\quad k=1,2\\cdots,n \\] <p>Common expression for uniform coefficient formula</p> <p>(i) \\(n=1\\), then \\(A_1=b-a\\), \\(x_1=(a+b)/2\\), so </p> \\[ I_1(f)=(b-a)f\\left(\\frac{a+b}{2}\\right) \\] <p>(ii) \\(n=2\\), then \\(A_2=(b-a)/2\\), so</p> \\[ I_2=\\frac{b-a}{2}\\left[f\\left(\\frac{b+a}{2}-\\frac{b-2}{2\\sqrt{3}}\\right) +f\\left(\\frac{b+a}{2}+\\frac{b-2}{2\\sqrt{3}}\\right)  \\right] \\] <p>Notice that here nodes are central symmetry about \\(\\frac{(a+b)}{2}\\).</p>"},{"location":"Math/Numerical_Analysis/NA/NI/#gaussian-gaussian-quadrature","title":"Gaussian \u6c42\u79ef\u516c\u5f0f | Gaussian Quadrature","text":"<p>Given \\(n\\) unknown nodes \\(x_1&lt;x_2&lt;\\cdots&lt;x_n\\), and denote</p> \\[ \\omega_n(x)=\\prod_{i=1}^n(x-x_i) \\] <p>If we let </p> \\[ A_i=\\int_a^b\\rho(x)\\frac{\\omega_n(x)}{(x-x_i)\\omega_n'(x)}dx, \\quad i=1,2,\\cdots,n \\] <p>then the above quadrature must have at least \\(n-1\\) algebraic precision(readers can check this because it is Lagrange interpolation polynomial, but non-equidistant nodes). That is, when using \\(n\\) nodes to interpolate and then integrate, we must get a quadrature formula with \\(n-1\\) algebraic precision. Now we analyze where the upper bound is for this quadrature formula if we choose a proper position of there \\(n\\) nodes.</p> <p>Interpolation a function has a similar error formula like in Newton-Cotes Formula</p> \\[ E_n(f)=\\int_a^b\\rho(x)f[x_1,x_2,\\cdots,x_n,x]\\omega_n(x)dx \\] <p>Notice that the degree of \\(f[x_1,x_2,\\cdots,x_n,x]\\) declines \\(n\\)(Because \\(x\\) is \\(n\\) times divided), and assume \\(f(x)\\) is a polynomial of degree larger than \\(n-1\\). If we want to let the quadrature formula have \\(m(m&gt;n)\\) algebraic precision, then if and only if for all polynomial \\(q(x)\\) of degree no more then \\(m-n\\), we have</p> \\[ \\int_{a}^b\\rho(x)q(x)\\omega_n(x)dx=0 \\] <p>Apparently, \\(m-n\\) needs to be less than \\(n\\), for \\((\\omega_n,\\omega_n)&gt;0\\)(inner product), which means \\(m\\) could be at most \\(2n-1\\). If \\(\\omega_n(x)\\) is a \\(n\\)th orthogonal polynomial, it can satisfy the above condition. So if we choose the roots of this polynomial as nodes to be integrated, then the corresponding quadrature formula has \\(2n-1\\) algebraic precision.</p> <p>Theoretically speaking, Gaussian quadrature formula always exists, but in practice it is not easy to solve. Here we use method of undetermined coefficients.</p> <p>Using different orthogonal polynomials, we can directly deduce some Gaussian quadrature.</p> <p>Examples of Gaussian Quadrature</p> <p>(i) \\(\\rho(x)\\equiv 1\\) integrated on region \\([-1,1]\\), the corresponding polynomial is Legendre Polynomial </p> \\[ P_n(x)=\\frac{1}{2^n n!}\\frac{d^n{(x^2-1)}^n}{dx^n} \\] <p>So the coefficients are </p> \\[ A_i=\\int_{-1}^1 \\frac{P_n(x)dx}{(x-x_i)P'_n(x_i)}=\\frac{2}{(1-x_i^2)P'_n(x_i)^2} \\] <p>with error</p> \\[ E_n(f)=\\frac{2^{2n+1}(n!)^4}{[(2n)!]^3(2n+1)}f^{(2n)}(\\eta),\\quad \\eta\\in(-1,1) \\] <p>(ii) \\(\\rho(x)=\\sqrt{1-x^2}\\), integrated on region \\([-1,1]\\), the corresponding polynomial is Second class Chebyshev polynomial</p> \\[ U_n(x)=\\frac{\\sin[(n+1)\\arccos x]}{\\sqrt{1-x^2}} \\] <p>using its roots as integration nodes we get</p> \\[ \\int_{-1}^1\\sqrt{1-x^2}f(x)ds\\approx \\sum_{k=1}^n\\frac{\\pi}{n+1}\\sin^2 \\frac{k\\pi}{n+1}f\\left(\\cos \\frac{kx}{n+1}\\right) \\] <p>with error</p> \\[ E_n(f)=\\frac{\\pi}{2^{2n+1}(2n)!}f^{(2n)}(\\eta),\\quad \\eta\\in(-1,1) \\]"},{"location":"Math/Numerical_Analysis/NA/Solve_Equ/","title":"Solutions of Equations in One Variables","text":""},{"location":"Math/Numerical_Analysis/NA/Solve_Equ/#basic-ideas-for-solving-equation","title":"\u57fa\u672c\u60f3\u6cd5 | Basic ideas for Solving Equation","text":"<p>To find the solution of an equation, we hope to have an iteration method which takes good advantage of Computer resources like</p> \\[ \\begin{equation}  \\pmb{x}^{k} = f(\\pmb{x}^{k-1}) \\label{eq: iterative eq}  \\end{equation} \\] <p>for \\(k = 1, 2, \\cdots n\\). We use \\(\\pmb{x}\\) instead of \\(x\\) because the above iteration method also applies to solving linear system.</p> <p>Hopefully, if the above equation converges, that is, for \\(k \\rightarrow \\infty\\), \\(\\pmb{x}^{k-1}\\rightarrow \\pmb{x}^*\\), \\(\\pmb{x}^{k}\\rightarrow \\pmb{x}^*\\), and the equation becomes</p> \\[ \\pmb{x}^* = f(\\pmb{x}^*) \\] <p>where \\(\\pmb{x^*}\\) is the sulution of the equation to be solved.</p> <p>If the above thought gets right, then we can consider the converging speed of the iterative process, which makes great sense in practical applications. That is, in a given definition of distence,</p> \\[ \\frac{\\|\\pmb{x}^{k+1} - \\pmb{x}^*\\|}{\\|\\pmb{x}^{k}-\\pmb{x}^*\\|^\\alpha}  \\] <p>to be small as much as possible for each \\(k\\).</p>"},{"location":"Math/Numerical_Analysis/NA/Solve_Equ/#the-bisection-method","title":"\u4e8c\u5206\u6cd5 | the Bisection Method","text":"<p>This method is quite intuitive. By choosing two end points \\(a, b\\), we get another point (Mid-point here)</p> \\[ p = a+\\frac{b-a}{2} \\] <p>Then update \\(a, b\\) by evaluating whether \\(f(p)&gt;0\\) or not to narrow down the interval.</p> <p>What is interesting is the stopping procedure. Readers can see the following question if interested.</p> <p>When we calculate the new point \\(p\\) based on \\(a, b\\), we need to judge whether \\(p\\) is an appropriate answer. Apart from \\(f(p)=0\\), which condition do you think is the best?</p> <ol> <li>\\((b-a)/{|\\min{(a, b)}|}&lt;\\epsilon\\)</li> <li>\\(|p-p_{prev}|=(b-a)/2 &lt; \\epsilon\\)</li> <li>\\(f(p)&lt;\\epsilon\\)</li> </ol> Choose an answerAnwser <ul> <li>1</li> <li>2</li> <li>3</li> </ul> <p>Choose 1, which is close to relative error, currently the best.</p> <p>2: consider \\(\\{p_n\\}=\\sum\\limits_{i=1}^{n}\\frac{1}{k}\\).</p> <p>3: easy to see.</p> <p>The converging speed can discribed as the following:</p> \\[ |x_n- x^*| &lt; \\frac{(b-a)}{2^{n}} \\]"},{"location":"Math/Numerical_Analysis/NA/Solve_Equ/#Fixed-Point-Iteration","title":"\u4e0d\u52a8\u70b9\u6cd5 | Fixed-Point Iteration","text":"<p>As we said previously in Basic ideas for solving equation, we hope to find an iterative relation such that the converging point \\(x^*\\) is exactly what we want, which in this case, means that </p> \\[ f(x^*) = 0 \\] <p>So intuitively, we ask: Whether can we derive a relation from </p> \\[ \\begin{equation} f(x) = 0 \\label{zero-equation} \\end{equation} \\] <p>to </p> \\[ x = g(x) \\] <p>for iterative method?</p> <p>The answer is, of course, YES!</p> <p>One of the easist way to transform is adding \\(x\\) to both sides of equation \\(\\ref{zero-equation}\\), but in most cases this does not work. Because we rely on \\(f(x)\\) ifself for the convergence! </p> <p>Thus, it is necessary to find the condition for \\(x = g(x)\\) to converge. The following theorem <p></p> gives a Sufficient condition.</p> <p>\u4e0d\u52a8\u70b9\u5b58\u5728\u5b9a\u7406 | Fixed-Point Theorom</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b]\\), for all \\(x\\) in \\([a, b]\\). Suppose, in addition, that \\(g'\\) exists on \\((a, b)\\) and that a constant \\(0 &lt; k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>Then for any initial number \\(p_0 \\in [a, b]\\), the sequence \\(\\{p_n\\}_{n=0}^{\\infty}\\) defined by </p> \\[ p_n = g( p_{n\u22121}) \\quad n \\geq 1 \\] <p>converges to the unique fixed point \\(p \\in [a, b]\\).</p> <p>Proving it is easily.</p> HintsProof <ul> <li>using the differential mean value theorem.</li> </ul> <p>\\(\\forall n \\geq 1, \\exists \\zeta_n \\in (p_{n-1}, p) \\subset (a, b)\\), we have</p> \\[ |p_n-p| = |g(p_{n-1}) - g(p)| = g'(\\zeta_n)|p_{n-1}-p|\\leq k|p_{n-1}-p| \\] <p>by induction, we have</p> \\[ |p_n-p|\\leq k^{n}|p_0-p| \\] <p>Let \\(n \\rightarrow \\infty\\), \\(|p_n-p| \\rightarrow 0\\), that is, \\(p_n\\) converges to \\(p\\). </p> <p>What we use in the proof will benefit us in identifying the speed of converging process.</p>"},{"location":"Math/Numerical_Analysis/NA/Solve_Equ/#newtons-method","title":"\u725b\u987f\u6cd5 | Newton's Method","text":"<p>This method is also a fixed-point method. There are two perspectives to get the inspirations.</p> HintsVersion 1Version 2 <ul> <li>version1: shrink the derivative of the iterative function \\(g(x)\\).</li> <li>version2: using Taylor's expansion.</li> </ul> <p>We can know that given a random function \\(f(x)\\), it may not be convergent to some point \\(x^*\\) for \\(x^{k} = f(x^{k-1}) + x^{k-1}\\) in a given interval. So the queation is, can we formulate a function \\(g(x)\\) such that \\(x^{k} = g(x^{k-1})\\) is convergent?</p> <p>The answer is, again, YES!</p> <p>The following content tells us we can formulate \\(g(x)= x - f(x)/(f'(x))\\) such that \\(g'(x) &lt; 1\\) in a given interval.</p> <p>Readers can easily see that </p> \\[ \\begin{align*} g'(x) &amp;= 1 - \\frac{f'^2(x)-f''(x)f(x)}{f'^2(x)}\\\\ &amp;=\\frac{f''(x)f(x)}{f'^2(x)} \\end{align*} \\] <p>if we add some constrictions, it can be easy to make \\(g'(x)&lt;1\\).</p> <p>Here we make use of the Taylor's expansion(or the derivatives) of the goal function. </p> <p>Suppose that \\(f \\in C^2[a, b]\\), Let \\(x_0 \\in [a, b]\\) be an approximation to \\(x^*\\) such that \\(f(x^*) \\neq 0\\) and \\(|x_0-x^*|\\) is \"small\". Consider the first Taylor polynomial for \\(f(x)\\) expanded at \\(x_0\\):</p> \\[  f(x) = f(x_0) + (x-x_0)f'(x_0) + \\frac{(x-x_0)^2}{2}f''(\\zeta). \\] <p>If we let \\(x = x^*\\), and according to \\(f(x^*)=0\\), we get </p> \\[ 0 = f(x_0) + (x^*-x_0)f'(x_0) + \\frac{(x^*-x_0)^2}{2}f''(\\zeta) \\] <p>neglecting the square item, we get </p> \\[ 0 \\approx f(x_0) + (x^*-x_0)f'(x_0) \\] <p>to represent \\(x^*\\), we get</p> \\[ x^* = x_0 - \\frac{f(x_0)}{f'(x_0)} \\] <p>Then we can define the iterative relation as</p> \\[  x_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}  \\] <p>The following statement guarrantees the convergence of the above iterative method.</p> <p>\u725b\u987f\u6cd5\u6536\u655b\u6761\u4ef6 | conditions for convergence of Newton's method</p> <p>Let \\(f \\in C^2[a, b]\\). If \\(p \\in (a, b)\\) is such that \\(f (p) = 0\\) and \\(f'( p) \\neq 0\\), then there exists a \\(\\delta &gt; 0\\) such that Newton\u2019s method generates a sequence \\(\\{p_n\\}_{n=1}^{\\infty}\\) converging to \\(p\\) for any initial approximation \\(p_0 \\in [p \u2212 \\delta, p + \\delta]\\).</p> <p>Prove it.</p> HintsProof <ul> <li>make use of the condition \\(f(p) = 0\\) and \\(f'(p)\\neq 0\\)</li> </ul> <p>for \\(x \\in (a, b)\\), we aim to find a narrower interval \\((x^*-\\delta, x^*+\\delta)\\) to have \\(g(x)\\) map into itself. That is, </p> \\[ g(x)\\leq k, \\forall k\\in (0,1) \\] <p>Firstly, \\(f'(p)\\neq 0\\) implies that \\(\\exists \\delta_1 &gt;0\\) such that \\(f'(x)\\neq 0, \\forall x \\in [x^* - \\delta, x^*+\\delta]\\subset [a, b]\\).</p> <p>THus, we have</p> \\[ g'(x) = \\frac{f(x)f''(x)}{(f'^2(x))} \\] <p>capable of dividing non-zero numbers.</p> <p>Since \\(f\\in C^2[a,b]\\), we have \\(g' \\in C^1[x^*-\\delta_1, x^*+\\delta_1]\\) for the exact solution \\(x^*\\), we have \\(f(x^*)=0\\), so </p> \\[ g'(x^*) = 0 \\] <p>which implies that \\(\\exists 0&lt;\\delta &lt; \\delta_1\\), such that </p> \\[ g'(x)\\leq k, \\forall k \\in [x^*-delta, x^*+\\delta] \\] <p>By differential Mean Value Theorem, for \\(x \\in [x^*-delta, x^*+\\delta], \\exists \\zeta \\in [x, x^*]\\) such that </p> \\[ |g(x)-g(x^*)|=g'(\\zeta)|x - x^*|\\leq k|x-x^*|&lt;|x-x^*| \\] <p>which means that \\(g\\) maps into itself. By Fixed-Point Theorom, the sequence defined by Newton's method converges. </p> <ul> <li>Secant Method It may not be easy to find derivarive of function \\(f\\), so we can use difference instead. That is, we have to store two adjacent points for calculating differnce</li> </ul> \\[ f'(x^{k}) \\approx \\frac{f(x^{k}) - f(x^{k-1})}{x^{k}-x^{k-1}} \\] <p>generate \\(p_{k+1}\\) using the above approximation and iterate.</p>"},{"location":"Math/Numerical_Analysis/NA/Solve_Equ/#order-of-convergence","title":"\u6536\u655b\u9636\u6570 | Order of Convergence","text":"<p>So how to identify the speed of convergence? The following definition gives a glimpse.</p> <p>Suppose \\(\\{p_n\\}_{n=1}^{\\infty}\\) is a sequence that converges to \\(p\\), with \\(p_n \\neq p (\\forall n)\\). If positive constants \\(\\lambda\\) and \\(\\alpha\\) exist with</p> \\[ \\lim_{n\\rightarrow \\infty}{\\frac{|p_{n+1}-p|}{|p_n-p|^{\\alpha}}}=\\lambda \\] <p>then \\(\\{p_n\\}_{n=0}^{\\infty}\\) converges to \\(p\\) of order \\(\\alpha\\), with asymptotic error constant \\(\\lambda\\).</p> <ul> <li>(i) If \\(\\alpha=1 (\\lambda&lt;1)\\), the sequence is linearly convergent.</li> <li>(ii) If \\(\\alpha=2\\), the sequence is quadratically convergent.</li> </ul> <p>The following theorem gives a sufficient condition for linear convergence.</p> <p>\u7ebf\u6027\u6536\u655b\u7684\u5145\u5206\u6761\u4ef6 | sufficient condition of linear convergence</p> <p>Let \\(g \\in C[a, b]\\) be such that \\(g(x) \\in [a, b], \\forall x \\in [a, b]\\). Suppose, in addition, that \\(g\\) is continuous on \\((a, b)\\) and a positive constant \\(k &lt; 1\\) exists with</p> \\[ |g'(x)|\\leq k \\quad \\forall x \\in (a, b) \\] <p>If \\(g'(p) \\neq 0\\), then for any number \\(p_0=p\\) in \\([a, b]\\), the sequence </p> \\[ p_n=g(p_{n-1}) \\quad \\forall n \\geq 1 \\] <p>converges only linearly to the unique fixed point \\(p\\) in \\([a, b]\\).</p> <p>Prove it.</p> Hints <p>Prove that linear convergence represents \\(\\exists alpha=1, lambda\\), such that \\(\\lim\\limits_{n\\leftarrow \\infty}\\frac{|p_{n+1}-p^*|}{|p_{n}-p^*|} = \\lambda\\).</p> <ul> <li>multiple roots We see that the speed of convergence is limited by multiple roots.</li> </ul> <p>Here we have modified Newton's Method:</p> \\[ g(x)= x - \\frac{f(x)f'(x)}{f'^2(x)-f(x)f''(x)} \\]"},{"location":"Math/Numerical_Analysis/NA/Solve_Equ/#accelerating-convergence","title":"\u52a0\u901f\u6536\u655b | Accelerating convergence","text":"<ul> <li>Aitken's \\(\\Delta^2\\) Method</li> </ul> <p>Suppose \\(\\{p_n\\}_{n=0}^{\\infty}\\) is a linearly convergent sequence with limit \\(p\\). To motivate the construction of a sequence \\(\\{\\hat{p}_n\\}_{n=1}^{\\infty}\\) that converges more rapidly to \\(p\\) than does \\(\\{p_n\\}_{n=0}^{\\infty}\\), let us first assume that the signs of \\(p_n-p\\), \\(p_{n+1}-p\\) and \\(p_{n+2}-p\\) agree and that \\(n\\) is sufficiently large that </p> \\[ \\frac{p_{n+1}-p}{p_n-p} \\approx \\frac{p_{n+2}-p}{p_{n+1}-p} \\] <p>Then solving for \\(p\\) gives</p> \\[ p \\approx \\frac{p_{n+2}p_n-p_{n+1}^2}{p_{n+2}-2p_{n+1}+p_n} \\] <p>And to get \\(p_n\\) out gives</p> \\[ \\begin{align*} p &amp;\\approx p_n - \\frac{(p_{n+1}-p_n)^2}{p_{n+2} - 2p_{n+1} + p_n}\\\\ \\Rightarrow \\hat{p}_n &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta p_{n+1}-\\Delta p_{n}} \\quad \\text{(denote $\\Delta p_n = p_{n+1} - p_n$)}\\\\ &amp;= p_n - \\frac{(\\Delta p_n)^2}{\\Delta^2 P_{n}}  \\end{align*}  \\] <ul> <li>Steffensen's Method</li> </ul> <p>The following thought is based on that the generated sequence \\(\\hat{p}\\) is a better approximation to true \\(p^*\\). We make use of the constructed sequence \\(\\{\\hat{p}_n\\}\\) to update the original sequence \\(\\{p_n\\}\\). That is, after generating a new \\(\\hat{p}\\), we can update \\(p_0 \\leftarrow p\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/","title":"Ordinary Differential Equation","text":"<p>Reference</p> <ul> <li>\u5e38\u5fae\u5206\u65b9\u7a0b, \u67f3\u5f6c</li> <li>\u5e38\u5fae\u5206\u65b9\u7a0b, \u65b9\u9053\u5143</li> </ul>"},{"location":"Math/Ordinary_Differential_Equation/#preliminary","title":"\u9884\u5907\u77e5\u8bc6 | Preliminary","text":""},{"location":"Math/Ordinary_Differential_Equation/#elementary-integration-method","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":""},{"location":"Math/Ordinary_Differential_Equation/#system-of-linear-differential-equations-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":""},{"location":"Math/Ordinary_Differential_Equation/#general-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u4e00\u822c\u7406\u8bba | General Theory of ODE","text":""},{"location":"Math/Ordinary_Differential_Equation/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"Math/Ordinary_Differential_Equation/#contraction-mapping-method","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Contraction Mapping Method","text":""},{"location":"Math/Ordinary_Differential_Equation/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"Math/Ordinary_Differential_Equation/#continuous-dependence-of-solution-on-initial-value","title":"\u89e3\u5bf9\u521d\u503c\u7684\u8fde\u7eed\u4f9d\u8d56\u6027 | Continuous Dependence of Solution on Initial Value","text":""},{"location":"Math/Ordinary_Differential_Equation/#stability-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u7a33\u5b9a\u6027\u7406\u8bba | Stability Theory of ODE","text":""},{"location":"Math/Ordinary_Differential_Equation/#qualitative-theory-of-ode","title":"\u5fae\u5206\u65b9\u7a0b\u7684\u5b9a\u6027\u7406\u8bba | Qualitative Theory of ODE","text":""},{"location":"Math/Ordinary_Differential_Equation/#first-order-pratial-differential-equation","title":"\u4e00\u9636\u504f\u5fae\u5206\u65b9\u7a0b | First Order Pratial Differential Equation","text":""},{"location":"Math/Ordinary_Differential_Equation/EIM/","title":"\u521d\u7b49\u79ef\u5206\u6cd5 | Elementary Integration Method","text":"<p>This chapter gives primary method of solving special differential functions, which plays a great role in future study.</p> <p>Outline</p> <ul> <li> <p>\u4e00\u9636\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b, \u9f50\u6b21\u4e0e\u975e\u9f50\u6b21</p> </li> <li> <p>\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b</p> </li> <li> <p>\u9f50\u6b21\u65b9\u7a0b, \u5316\u7b80\u9f50\u6b21\u7684\u65b9\u6cd5</p> </li> <li> <p>\u5168\u5fae\u5206\u65b9\u7a0b\u3001\u79ef\u5206\u56e0\u5b50\u3001\u5206\u7ec4\u6c42\u79ef\u5206\u56e0\u5b50</p> </li> <li> <p>Bernouli \u65b9\u7a0b\u3001Riccati \u65b9\u7a0b</p> </li> <li> <p>\u9690\u5f0f\u5fae\u5206\u65b9\u7a0b, \u53ef\u89e3\u51fax, \u53ef\u89e3\u51fay, \u53cc\u66f2\u51fd\u6570\u53c2\u6570\u6cd5</p> </li> </ul>"},{"location":"Math/Ordinary_Differential_Equation/EIM/#exact-equation","title":"\u6070\u5f53\u65b9\u7a0b | Exact Equation","text":"<p>We focus on the symmetrical form</p> \\[ \\begin{equation} M(x,y)dx + N(x,y)dy = 0 \\label{eq-exact} \\end{equation} \\] <p>This can bring us great convenience for digging into one-order ODE because it can gives us both the relation \\(y=f(x)\\) or \\(x=g(y)\\).</p> <p>\u5168\u5fae\u5206\u65b9\u7a0b\u3001\u6070\u5f53\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Exact Equation</p> <p>If there exists a \\(\\mathit{\\varphi}(x, y) \\in C^{1}(D)\\) such that </p> \\[ d\\mathit{\\varphi}(x, y) = M(x,y)dx + N(x,y)dy \\] <p>then equation \\(\\ref{eq-exact}\\) is called Exact Equation.</p> <p>There are some questions to answer:</p> <ul> <li>How to judge an equation to be exact Equation?</li> <li>If so, how to find original function \\(\\varphi(x, y)\\)?</li> <li>If not, how to transform it into one exact Equation?</li> </ul> <p>In this pattern, we answer the first two equation and leave the third one after learning LFODE.</p> <p>\u65b9\u7a0b\u662f\u6070\u5f53\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for exact Equation</p> <p>Assume \\(D\\) is a simply connected region, and \\(M(x, y)\\), \\(N(x, y) \\in C(D)\\) with \\(\\frac{\\partial M}{\\partial y}\\) and \\(\\frac{\\partial N}{\\partial x} \\in C^{1}(D)\\). Then equation \\(\\ref{eq-exact}\\) is exact Equation if and only if</p> \\[ \\frac{\\partial M}{\\partial y} = \\frac{\\partial N}{\\partial x} \\] <p>Prove it.</p> Hints <p>\\(\\Rightarrow\\) is easy, by using second-order mixed partial derivatives of \\(\\mathit{\\varphi}\\).</p> <p>\\(\\Leftarrow\\). Using Green Formula/Theorem. </p> \\[ \\begin{align*} &amp;\\frac{\\partial P}{\\partial y} = \\frac{\\partial Q}{\\partial x} \\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = 0 \\quad \\forall\\text{closed loop } \\gamma\\\\ \\Leftrightarrow \\ &amp;\\int_{\\gamma}P(x, y)dx+Q(x,y)dy = C \\quad \\forall\\text{curve } \\gamma \\text{ connecting } (x_0, y_0), (x, y) \\\\ \\Leftrightarrow \\ &amp;\\exists \\mathit{\\varphi}(x,y) \\in C^{1}(D) \\text{ s.t } d\\mathit{\\varphi}(x, y) = P(x, y)dx+Q(x,y)dy \\end{align*} \\]"},{"location":"Math/Ordinary_Differential_Equation/EIM/#integral-factor","title":"\u79ef\u5206\u56e0\u5b50 | Integral Factor","text":"<p>This part we hope to find \\(\\mu(x, y)\\) so when we multiply it to both sides of equation \\(\\ref{eq-exact}\\) and we get</p> \\[ \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy = 0  \\] <p>such that there exists \\(\\mathit{\\varphi}(x, y)\\) which satisfies</p> \\[ d\\mathit{\\varphi}(x,y) = \\mu(x,y)M(x,y)dx + \\mu(x,y)N(x,y)dy  \\] <p>Naively, if \\(\\mathit{\\varphi}(x,y)\\in C^2\\), then </p> \\[ \\frac{\\partial (\\mu M)}{\\partial y} = \\frac{\\partial^2 \\mathit{\\varphi}}{\\partial x\\partial y}=\\frac{\\partial (\\mu N)}{\\partial x}  \\] <p>Theoretically speaking, we have to solve a PDE</p> \\[ \\begin{equation} M(x,y)\\frac{\\partial \\mu}{\\partial y} - N(x,y)\\frac{\\partial \\mu}{\\partial x} = \\left(\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}\\right)\\mu(x,y) \\label{eq-pde} \\end{equation} \\] <p>However, actually, it is very hard to solve the above PDE. So we focus on some special case like \\(\\mu(x,y)=\\mu(x)\\), \\(\\mu(y)\\), \\(\\mu(x+y)\\), \\(\\mu(xy)\\).</p> <p>Now we have the following theorem to judge whether we can get the above form of integral factors.</p> <p>\u65b9\u7a0b\u6709\u7279\u6b8a\u7c7b\u578b\u7684\u79ef\u5206\u56e0\u5b50\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition of special integral factor of ODE</p> <p>(i) Equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(x)\\) depending only on \\(x\\), if and only if</p> \\[ \\frac{\\frac{\\partial M}{\\partial y}-\\frac{\\partial N}{\\partial x}}{M} \\overset{\\Delta}{=} G(x) \\] <p>only depends only on \\(x\\). Then </p> \\[ \\mu(x) = e^{\\int_{x_0}^{x}G(t)dt} \\] <p>(ii) Similarly, Equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(y)\\) depending only on \\(y\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{N} \\overset{\\Delta}{=} G(y) \\] <p>only depends only on \\(y\\). Then </p> \\[ \\mu(y) = e^{\\int_{y_0}^{y}G(t)dt} \\] <p>(iii) More generally, equation \\(\\ref{eq-pde}\\) has solution \\(\\mu(\\varphi(x,y))\\), if and only if</p> \\[ \\frac{\\frac{\\partial N}{\\partial x}-\\frac{\\partial M}{\\partial y}}{N\\frac{\\partial \\varphi}{\\partial x}-M\\frac{\\partial \\varphi}{\\partial y}} \\overset{\\Delta}{=} f(\\varphi(x,y)) \\]"},{"location":"Math/Ordinary_Differential_Equation/EIM/#variable-separation-equation","title":"\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b | Variable Separation Equation","text":"<p>This chapter we discuss how to solve equation when it is not Exact Equation. The basic idea is, through transformation, we can convert an equation into an exact Equation.</p> <p>\u53d8\u91cf\u5206\u79bb\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Variable Separation Equation</p> <p>If there exists \\(M_1(x), M_2(y), N_1(x), N_2(y) \\in C^1(D)\\) such that </p> \\[ M(x, y) =M_1(x)M_2(y), N(x, y) = N_1(x), N_2(y) \\] <p>then we call equation \\(\\ref{eq-exact}\\) Variable Separation Equation.</p> <p>For this type of equation, we can multiply both sides </p> \\[ \\begin{equation} \\frac{1}{M_2(y)N_1(x)} \\label{eq-sep-factor} \\end{equation} \\] <p>equation \\(\\ref{eq-exact}\\) becomes</p> \\[ \\frac{M_1(x)}{N_1(x)}dx + \\frac{M_2(y)}{N_2(y)}dy = 0 \\] <p>This is an exact equation, and \\(\\ref{eq-sep-factor}\\) is called an Integral Factor of the equation.</p> <p>we can get its integral</p> \\[ \\int_{x_0}^{x}\\frac{M_1(t)}{N_1(t)}dt + \\int_{y_0}^{y}\\frac{M_2(s)}{N_2(s)}ds = c \\] <p>which is easily seen a solution of the original equation.</p> <p>And don't forget that if there exists \\(a_i (i= 1,2,\\cdots, m)\\) such that \\(N_1(a_i) = 0\\), or exists \\(b_j (j=1,2,\\cdots, n)\\) such that \\(M_2(b_j) = 0\\), then of course \\(x = a_i, y = b_j\\) are also solutions of the original solution.</p> <ul> <li>\u9f50\u6b21\u65b9\u7a0b | Homogeneous Equation</li> </ul> <p>The following equation can also be transferred into Variable Separation Equation.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u7684\u5b9a\u4e49 | Definition of Homogeneous Equation</p> <p>We call \\(f(x, y)\\) Homogeneous Function of degree \\(n\\) if</p> \\[ f(tx, ty) = t^n f(x, y) \\] <p>and call equation \\(\\ref{eq-exact}\\) Homogeneous equation if \\(M(x, y), N(x, y)\\) are Homogeneous Function.</p> <p>When we let \\(y = u x\\), then \\(dy = xdu + udx\\), substitute in the equation and get</p> \\[ \\begin{align*} M(x, ux)dx+N(x,ux)(xdu+udx)&amp;=0 \\\\ \\Leftrightarrow [M(x, ux)+N(x,ux)u]dx+N(x,ux)xdu&amp;=0  \\end{align*} \\] <p>extract \\(x\\) out by definition of Homogeneous Equation:</p> \\[ x^n[M(1, u)+N(1,u)u]dx+x^{n+1}N(1,u)du=0  \\] <p>If \\(x^{n+1}[M(1, u)+N(1,u)u]\\neq 0\\), then we divide both sides by this and get</p> \\[ \\frac{1}{x}dx + \\frac{N(1,u)}{M(1,u)+uN(1,u)}du=0 \\] <p>which is also Variable Separation Equation.</p> <p>Q1. Solve ODE</p> \\[ xydx+(2x^2+3y^2\u221220)dy=0 \\] Answer \\[ \\begin{align*} xydx+(2x^2+3y^2\u221220)dy&amp;=0\\\\ xdx+\\frac{2x^2+3y^2\u221220}{y}dy&amp;=0\\quad y\\neq 0\\\\ 2xdx+\\frac{2y(2x^2+3y^2\u221220)}{y^2}dy&amp;=0\\\\ \\end{align*} \\] <p>let \\(u=x^2\\), \\(v=y^2\\), then the above equals to </p> \\[ du+\\frac{2u+3v-20}{v}dv=0 \\] <p>so we can change it into homogeneous equation. Let \\(\\xi=u-10\\), \\(\\eta=v\\), then </p> \\[ d\\xi+\\frac{2\\xi+3\\eta}{\\eta}d\\eta=0 \\] <p>let \\(t=\\frac{\\xi}{\\eta}\\), then \\(d\\xi=td\\eta+\\eta dt\\), so</p> \\[ \\begin{align*} td\\eta+\\eta dt+\\frac{2t+3}{1}d\\eta&amp;=0\\\\ \\frac{dt}{3t+3}&amp;=-\\frac{d\\eta}{\\eta}\\\\ \\frac{1}{3}\\ln{(t+1)}&amp;=-\\ln{\\eta}+C\\\\ |t+1|&amp;=C\\frac{1}{|\\eta|^3}\\\\ |\\xi+\\eta||\\eta|^2&amp;=C\\\\ |x^2+y^2-20||y|^4&amp;=C \\end{align*} \\]"},{"location":"Math/Ordinary_Differential_Equation/EIM/#linear-first-order-differential-equation","title":"\u4e00\u9636\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b | Linear First-Order Differential Equation","text":"<p>Now we focus on a really important expression of ODE: Linear First-Order Differential Equation(LFODE):</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = q(x) \\label{eq-LFODE} \\end{equation} \\] <ul> <li>Homogeneous LFODE(H-LFODE)</li> </ul> <p>We let \\(q(x)\\equiv 0\\) in \\(\\ref{eq-LFODE}\\), we get</p> \\[ \\begin{equation} \\frac{dy}{dx} + p(x)y = 0 \\label{eq-H-LFODE} \\end{equation} \\] <p>rewrite it into symmetrical form:</p> \\[ p(x)ydx + dy = 0  \\] <p>which is Variable Separation Equation.</p> <p>So when \\(y\\neq 0\\), multiply both sides \\(1/y\\) and integrate</p> \\[ \\ln{|y|} + \\int_{x_0}^{x}p(t)dt = C \\] <p>get \\(y\\) out of form \\(x\\):</p> \\[ \\begin{align} y &amp;= \\pm e^{C}\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\nonumber \\\\ &amp;= C_1\\cdot e^{-\\int_{x_0}^{x}p(t)dt}  \\end{align} \\] <p>where \\(C_1=\\pm e^{C} \\neq 0\\), but we can include trivial solution \\(y \\equiv 0\\) by letting \\(C_1 = 0\\).</p> <ul> <li>Non-Homogeneous linear First-Order Differential Equation</li> </ul> <p>we have two ways to get the answer.</p> Version 1Version 2 <p>Making use of Integral Factors.</p> <p>To begin with, we convert equation \\(\\ref{eq-LFODE}\\) into symmetrical form:</p> \\[ \\begin{equation} (p(x)y-q(x))dx+dy=0 \\label{eq-SYM-LFODE} \\end{equation}  \\] <p>Multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides of equation \\(\\ref{eq-SYM-LFODE}\\):</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y \\right) - e^{\\int_{x_0}^{x}p(t)dt} q(x)dx = 0 \\] <p>That is,</p> \\[ d\\left(\\ e^{\\int_{x_0}^{x}p(t)dt} y- \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt} q(x)ds \\right)  = 0 \\] <p>integrate and get</p> \\[ e^{\\int_{x_0}^{x}p(t)dt} y - \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds + C = 0 \\] <p>extract \\(y\\) out and get:</p> \\[ y = e^{-\\int_{x_0}^{x}p(t)dt} \\left( C + \\int_{x_0}^{s}e^{\\int_{x_0}^{x}p(t)dt} q(s)ds \\right) \\] <p>Through Variation of Constants.</p> <p>We make a brave treatment: assume one special solution to equation \\(\\ref{eq-LFODE}\\) is </p> \\[ y = u\\cdot e^{-\\int_{x_0}^{x}p(t)dt} \\] <p>where \\(u\\) is a new variable.</p> <p>Subsititute in equation \\(\\ref{eq-LFODE}\\) and get</p> \\[ \\left(p(x) u e^{-\\int_{x_0}^{x}p(t)dt} -q(x) \\right) dx - u p(x) e^{-\\int_{x_0}^{x}p(t)dt} dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>That is</p> \\[ -q(x) dx + e^{-\\int_{x_0}^{x}p(t)dt} du = 0 \\] <p>multiply \\(e^{\\int_{x_0}^{x}p(t)dt}\\) to both sides and  integrate </p> \\[ u =  \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\] <p>So the special solution is</p> \\[ y =  e^{-\\int_{x_0}^{x}p(t)dt}\\cdot \\int_{x_0}^{x}e^{\\int_{x_0}^{s}p(t)dt}q(s)ds \\]"},{"location":"Math/Ordinary_Differential_Equation/EIM/#first-order-implicit-differential-equation","title":"\u4e00\u9636\u9690\u5f0f\u5fae\u5206\u65b9\u7a0b | First-order Implicit Differential Equation","text":"<p>Now we focus on equation</p> \\[ \\begin{equation} F(x,y, y') = 0 \\label{eq-para} \\end{equation} \\] <p>where \\(y'\\) cannot be explicitly solved out.</p> <ul> <li>\u53c2\u6570\u6cd5 | parametric method</li> </ul> <p>If we let \\(p = y'\\), then equation</p> \\[ \\begin{equation} F(x,y,p) = 0 \\label{eq-para-p} \\end{equation} \\] <p>represents a curved surface in 3-D space.</p> <p>And we can juggle equation \\(\\ref{eq-para-p}\\) and \\(dy=pdx\\) to get a curve in the space.</p>"},{"location":"Math/Ordinary_Differential_Equation/EIM/#singular-solution","title":"\u5947\u89e3 | Singular Solution","text":"<p>Definition of Singular Solution</p> <p>Assume \\(y=\\phi(x)\\) defined on region \\(J\\) is a special solution of ODE</p> \\[ F(x,y,y')=0 \\] <p>its integral curve is </p> \\[ \\varGamma=\\{(x,y): y=\\phi(x),x\\in J\\} \\] <p>If \\(\\forall M\\in \\varGamma\\), there exists \\(\\delta&gt;0\\),  such that a solution \\(\\psi(x)\\) different from \\(\\phi(x)\\) which is tangent to \\(\\phi(x)\\) at \\(M\\) in \\(O_\\delta(M)\\). Then we call \\(y=\\phi(x)\\) is a singular solution of the above ODE.</p> <p>In every point of \\(y-\\phi(x)\\), there exists another solution. That is, uniqueness is broken.</p> <p>Here we gives the necessary condition for the existence of singular solution.</p> <p>necessary condition for the existence of singular solution</p> <p>Assume \\(F(x,y,p)\\in C(G)\\), where \\(G\\subset \\mathbb{R}^3\\), and has continuous partial derivatives with respect to \\(y\\) and \\(p\\), i.e. \\(F_y'(x,y,p)\\) and \\(F_p'(x,y,p)\\). If \\(y=\\phi(x)(x\\in J)\\) is a singular solution of the ODE, and </p> \\[ (x,\\phi(x),\\phi'(x))\\in G,\\quad x\\in J \\] <p>then \\(y=\\phi(x)\\) satisfies</p> \\[ F(x,y,p)=0,\\quad F'_p(x,y,p)=0. \\] Proof <p>The first equation is easy because of singular solution is also a solution.</p> <p>We have to prove the second equation by contradiction.</p> <p>Then there exists \\((x_0,y_0,p_0)\\in G\\), such that we can use Implicite function theorem.</p> <p>Using the two equations are called p-test equation, we can get a formula of singular solution, but we still have to test whether it is the solution of ODE and whether it is singular solution. Here we give another theorem.</p> <p>Theorem for singular solution</p> <p>Assume \\(F(x,y,p)\\in C^2(G)\\), and \\(y=\\phi(x)\\) obtained from p-test equation is a solution of ODE. If </p> \\[ \\begin{align} &amp;F_y(x,\\phi(x),\\phi'(x))\\neq 0 \\label{F-y}\\\\ &amp;F_{pp}(x,\\phi(x),\\phi'(x))\\neq 0\\label{F-pp}\\\\ \\nonumber &amp;F_p(x,\\phi(x),\\phi'(x))=0 \\end{align} \\] <p>then \\(y=\\phi(x)\\) is the singular solution of ODE.</p> <p>Note: if equation \\(\\ref{F-y}\\) and \\(\\ref{F-pp}\\) does not satisfies, we still could not ensure whether \\(y=\\phi(x)\\) is a singular solution.</p>"},{"location":"Math/Ordinary_Differential_Equation/FstPDE/","title":"First Order Pratial Differential Equation","text":"<p>We will prove that we can use First Integral Method of ODEs to solve Quasi-linear First order PDE (\u4e00\u9636\u62df\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b).</p>"},{"location":"Math/Ordinary_Differential_Equation/FstPDE/#first-integral-method","title":"First Integral Method","text":"<p>Definition of First Intergal</p> <p>Consider ODEs</p> \\[ \\begin{equation} \\frac{d y_j}{dx}=f_{j}(x,y_1,\\cdots,y_n),\\quad j=1,2\\cdots, n\\label{ODEs} \\end{equation} \\] <p>where \\(f_j(x,y_1,\\cdots,y_n) (j=1,2,\\cdots,n)\\in C(D)\\), with \\(D\\subset \\mathbb{R}^{n+1}\\), and are continuously differentiable with respect to \\(y_1,y_2,\\cdots, y_n\\). </p> <p>Assume \\(V=V(x,y_1,\\cdots,y_n)\\in C^1(G)\\), where \\(G\\subset D\\). If \\(V\\) is not a constant function, but constant when following an arbitrary integral curve of the above ODEs \\(\\ref{ODEs}\\), i.e.</p> \\[ V(x,y_1(x),\\cdots,y_n(x))\\equiv c_0,\\quad x\\in J \\] <p>then we call \\(V(x,y_1,\\cdots,y_2)=c\\) is a First Integral of ODEs \\(\\ref{ODEs}\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/FstPDE/#property-of-first-integral","title":"Property of First Integral","text":"<p>Property of First Integral (From curve to Points)</p> <p>Assume non-constant function \\(\\varPhi(x,y_1,\\cdots,y_n)\\in C^1(G)\\), then </p> \\[ \\varPhi(x,y_1,\\cdots,y_n) = c \\] <p>is a first integral of ODEs \\(\\ref{ODEs}\\), iff</p> \\[ \\begin{equation} \\frac{\\partial \\varPhi}{\\partial x} + \\frac{\\partial \\varPhi}{\\partial y_1}f_1+\\cdots + \\frac{\\partial \\varPhi}{\\partial y_n}f_n =0, \\quad \\forall (x,y_1,\\cdots, y_n)\\in G\\label{Property} \\end{equation} \\] Proof <p>\"\\(\\Rightarrow\\)\". Assume \\(\\varPhi(x,y_1,\\cdots,y_n)\\) is a first integral of ODEs \\(\\ref{ODEs}\\), then \\(\\forall (x_0,y_{10},\\cdots,y_{n0})\\in G\\), which an integral curve passes is denoted as</p> \\[ y_1(x),y_2(x),\\cdots, y_n(x),\\forall x\\in J \\] <p>we have </p> \\[ \\varPhi(x,y_1(x),\\cdots,y_n(x))=c \\] <p>take partial derivatives from both sides and let \\(x=x_0\\), we have</p> \\[ \\frac{\\partial \\varPhi}{\\partial x} + \\frac{\\partial \\varPhi}{\\partial y_1}f_1+\\cdots + \\frac{\\partial \\varPhi}{\\partial y_n}f_n =0 \\] <p>which holds at point \\((x_0,y_{10},\\cdots,y_{n0})\\). But due to \\((x_0,y_{10},\\cdots,y_{n0})\\) is arbitrary, so the above equation holds forall points in \\(G\\).</p> <p>\"\\(\\Leftarrow\\)\". Assume equation \\(\\ref{Property}\\) holds, then it holds for arbitrary integral curve, i.e. denote \\(\\Gamma\\in G\\) </p> \\[ \\Gamma: y_1=\\phi_1(x),y_2=\\phi_2(x),\\cdots,y_n=\\phi_n(x),\\quad x\\in J \\] <p>and equation \\(\\ref{Property}\\) holds for \\(\\Gamma\\). That is, </p> \\[ \\frac{\\partial \\varPhi}{\\partial x} + \\frac{\\partial \\varPhi}{\\partial y_1}f_1+\\cdots + \\frac{\\partial \\varPhi}{\\partial y_n}f_n =0, \\quad \\forall (x,\\phi_1,\\cdots, \\phi_n)\\in G  \\] <p>which means</p> \\[ \\frac{d}{dx}\\varPhi(x,\\phi_1(x),\\cdots, \\phi_n(x))=0 \\] <p>So integrate we have</p> \\[ \\varPhi(x,\\phi_1(x),\\cdots,\\phi_n(x))=c_0, \\quad x\\in J. \\] <p>If we have a first integral for an ODEs, we can reduce the order for that ODEs. It is obvious if we can solve one variable \\(y_i\\) out and substitute it into ODEs \\(\\ref{ODEs}\\), then total number of variables are reduced to \\(n-1\\).</p> <p>Naively, if we have enough number of first integrals, we can solve total ODEs out. But we have to be careful, because some first integral are of the same type, or to be more specific, if a first integral could be expressed by another one, than we call them relevant.</p> <p>Independence of First Integrals</p> <p>We call \\(n\\) first Integrals in region \\(G\\)</p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j,\\quad j=1,2\\cdots,n. \\] <p>are independent, if </p> \\[ \\text{det}\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)}\\neq 0,\\quad \\forall(x,y_1,\\cdots,y_n)\\in G. \\] <p>Then we have the following theorem.</p> <p>Solution by First Integral Method</p> <p>If</p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j,\\quad j=1,2\\cdots,n \\] <p>are \\(n\\) independent first integrals in region \\(G\\) for ODEs \\(\\ref{ODEs}\\). Then the solution to ODEs \\(\\ref{ODEs}\\) is a function </p> \\[ y_j=\\phi_j(x,c_1,c_2,\\cdots,c_n),\\quad j=1,2,\\cdots,n \\] <p>specified by these \\(n\\) first integrals. And it expresses all the solution of ODEs \\(\\ref{ODEs}\\) on region \\(G\\).</p> Proof <p>(i) Cause \\(V_1,V_2,\\cdots,V_n\\) are independent, so </p> \\[ \\text{det}\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)}\\neq 0,\\quad \\forall(x,y_1,\\cdots,y_n)\\in G. \\] <p>By theorem of implicit function, from </p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j,\\quad j=1,2\\cdots,n \\] <p>solve </p> \\[ y_j=\\phi_j(x,c_1,\\cdots,c_n),\\quad j=1,2\\cdots,n \\] <p>(ii) Then we have to prove \\(\\phi_j (j=1,2,\\cdots,n)\\) are solution to original ODEs \\(\\ref{ODEs}\\).</p> <p>Notice that</p> \\[ \\begin{equation} V_j(x,\\phi_1(x,c_1,\\cdots,c_n),\\cdots,\\phi_n(x,c_1,\\cdots,c_n))\\equiv c_j,\\quad j=1,2\\cdots,n\\label{eq3} \\end{equation} \\] <p>take derivative with respect to \\(x\\) on both sides, we have</p> \\[ \\begin{equation} \\frac{\\partial V_j}{\\partial x}+\\frac{\\partial V_j}{\\partial y_1}\\phi_1'+\\cdots+\\frac{\\partial V_j}{\\partial y_n}\\phi_n'=0,\\quad j=1,2\\cdots,n\\label{eq1} \\end{equation} \\] <p>by Property of First Integral, we also have</p> \\[ \\begin{equation} \\frac{\\partial V_j}{\\partial x} + \\frac{\\partial V_j}{\\partial y_1}f_1+\\cdots + \\frac{\\partial V_j}{\\partial y_n}f_n =0, \\quad \\forall (x,y_1,\\cdots, y_n)\\in G \\label{eq2} \\end{equation} \\] <p>So subtract equation \\(\\ref{eq1}\\) to equation \\(\\ref{eq2}\\) and get</p> \\[ \\frac{\\partial V_j}{\\partial y_1}(\\phi_1'-f_1)+\\cdots+\\frac{\\partial V_j}{\\partial y_n}(\\phi_n'-f_n)=0,\\quad j=1,2\\cdots,n \\] <p>If we define </p> \\[ A=\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)} \\] \\[ \\pmb{y}=[(\\phi_1'-f_1),(\\phi_2'-f_2),\\cdots,(\\phi_n'-f_n)]^T \\] <p>then we have</p> \\[ A\\pmb{y}=\\pmb{0} \\] <p>Because \\(\\text{det}A\\neq 0\\), so it follows </p> \\[ \\pmb{y}=\\pmb{0} \\] <p>which means</p> \\[ \\phi_j=f_j,\\quad j=1,2,\\cdots,n. \\] <p>The above equation is exactly original ODEs. So \\(\\phi_j (j=1,2,\\cdots,n)\\) are solution to ODEs \\(\\ref{ODEs}\\).</p> <p>(iii) Next, we have to prove one part of the last statement: \\(\\phi_j (j=1,2,\\cdots,n)\\) are general solution of ODEs \\(\\ref{ODEs}\\). Initially, these \\(n\\) solutions are independent, i.e.</p> \\[ \\text{det}\\frac{\\partial (\\phi_1,\\phi_2,\\cdots,\\phi_n)}{\\partial(c_1,c_2,\\cdots,c_n)}\\neq 0 \\] <p>Actually, from equation \\(\\ref{eq3}\\), we take partial derivatives with respect to \\(c_k\\), we have</p> \\[ \\frac{\\partial V_j}{\\partial y_1}\\frac{\\partial \\phi_1}{\\partial c_k}+\\cdots+\\frac{\\partial V_j}{\\partial y_n}\\frac{\\partial \\phi_n}{\\partial c_k}=\\delta_{jk}=\\begin{cases}0,\\quad j\\neq k\\\\ 1,\\quad j=k.\\end{cases},\\quad j,k=1,2,\\cdots,n \\] <p>which means one matrices times another equals to \\(E\\), i.e.</p> \\[ \\text{det}\\frac{\\partial (\\phi_1,\\phi_2,\\cdots,\\phi_n)}{\\partial(c_1,c_2,\\cdots,c_n)}=\\left[\\text{det}\\frac{\\partial (V_1,V_2,\\cdots,V_n)}{\\partial(y_1,y_2,\\cdots,y_n)}\\right]^{-1}\\neq 0 \\] <p>(iv) Lastly these solutions could express all other solutions. That is, \\(\\forall y_1=z_1(x),y_2=z_2(x),\\cdots,y_n=z_n(x)\\) are solution to ODEs \\(\\ref{ODEs}\\), could we express them with \\(\\phi_1,\\phi_2,\\cdots,\\phi_n\\)?</p> <p>If we denote \\(y_1^0=z_1(x_0),\\cdots,y_n^0=z_n(x_0)\\), and from this point we could determine</p> \\[ c_1^0=V_1(x_0,y_1^0,\\cdots,y_n^0), \\cdots,c_n^0=V_n(x_0,y_1^0,\\cdots,y_n^0) \\] <p>Solve </p> \\[ V_j(x,y_1,\\cdots,y_n)=c_j^0,\\quad j=1,2,\\cdots,n \\] <p>out with \\(n\\) answers</p> \\[ \\phi^0_j(x,c_1^0,\\cdots,c_n^0),\\quad j=1,2\\cdots,n \\] <p>which follows the whole above proof property, i.e. it is solution to ODEs \\(\\ref{ODEs}\\) and satisfies initial condition </p> \\[ y_j^0=\\phi^0_j(x_0,c_1^0,\\cdots,c_n^0),\\quad j=1,2\\cdots,n \\] <p>Due to Uniqueness of Solution, we have </p> \\[ z_j(x)\\equiv \\phi_j(x),\\quad j=1,2\\cdots,n \\] <p>which means \\(z_j\\) could be expressed by \\(\\phi_j\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/FstPDE/#existence-of-first-integral","title":"Existence of First Integral","text":"<p>Practically speaking, it is really hard to find a first integral of ODEs \\(\\ref{ODEs}\\). Here we prove that in a sufficiently wide condition, first integral exists (locally).</p> <p>Existence of First Integral</p> <p>Assume \\(P_0(x_0,y_1^0,y_2^0,\\cdots, y_n^0)\\in G\\), then there exists a neighborhood of \\(P_0\\), denoted as \\(G_1\\subset G\\), such that ODEs \\(\\ref{ODEs}\\) has \\(n\\) independent first integrals in \\(G_1\\).</p> Proof <p>Choose a small neighborhood \\(G_1=O_\\delta(P_0)\\subset G\\), with a point \\((x_0,c_1,\\cdots,c_n)\\), consider with initial condition </p> \\[ y_1(x_0)=c_1,\\cdots,y_n(x_0)=c_n. \\] <p>ODEs has unique solution </p> \\[ y_1=\\phi_1(x,c_1,\\cdots,c_n),\\cdots,y_n=\\phi_n(x,c_1,\\cdots,c_n). \\] <p>where \\(\\phi_j(x,c_1,\\cdots,c_n) (j=1,2,\\cdots,n)\\) are continuously differentiable with respect to \\((x,c_1,\\cdots,c_n)\\). Using Continuous Differentiable dependence of solution on intial condition, we have</p> \\[ \\text{det}\\frac{\\partial (\\phi_1,\\cdots,\\phi_n)}{\\partial(c_1,\\cdots,c_n)}\\Bigg|_{x=x_0}=1 \\] <p>So there exists function group \\(V_1=V_1(x,y_1,\\cdots,y_n),\\cdots,V_n(x,y_1,\\cdots,y_n)\\) such that</p> \\[ V_1(x,y_1,\\cdots,y_n)=c_1,\\cdots,V_n(x,y_1,\\cdots,y_n)=c_n \\] <p>and</p> \\[ \\text{det}\\frac{\\partial(V_1,\\cdots,V_n)}{\\partial(y_1,\\cdots,y_n)}\\Bigg|_{x=x_0}=1 \\] <p>So there exists a neighborhood \\(G_1\\subset G\\) in which </p> \\[ \\text{det}\\frac{\\partial(V_1,\\cdots,V_n)}{\\partial(y_1,\\cdots,y_n)}\\neq 0 \\] <p>meaning \\(V_1,\\cdots,V_n\\) are \\(n\\) independent first integrals.</p> <p>The following theorem is similar to linear space.</p> <p>Maximum number of independent integrals</p> <p>ODEs \\(\\ref{ODEs}\\) has at most \\(n\\) independent first integrals.</p> Proof <p>By using there independence to deduce contradiction.</p>"},{"location":"Math/Ordinary_Differential_Equation/FstPDE/#expression-of-first-integral","title":"Expression of First Integral","text":"<p>The following theorem states the expression of integrals, which is like integral factors.</p> <p>expression of First Integral</p> <p>Assume \\(V_1=V_1(x,y_1,\\cdots,y_n)\\), \\(\\cdots\\), \\(V_n=V_n(x,y_1,\\cdots,y_n)\\) are \\(n\\) independent first integrals of ODEs \\(\\ref{ODEs}\\). Then for an arbitrary first integral \\(V=V(x,y_1,\\cdots,y_n)\\subset G\\), there exists \\(C^1\\) function \\(h\\) such that </p> \\[ V(x,y_1,\\cdots,y_n)=h(V_1,\\cdots,V_n). \\] Proof <p>Cause \\(V_1(x,y_1,\\cdots,y_n),\\cdots, V_n(x,y_1,\\cdots,y_n)\\) are independent first integrals, we have</p> \\[ J=\\text{det}\\frac{\\partial (V_1,\\cdots,V_n)}{\\partial (y_1,\\cdots,y_n)}\\neq 0 \\] <p>By theorem of implicite function, from ODEs \\(\\ref{ODEs}\\), we could solve </p> \\[ y_j=y_j(x,V_1,\\cdots,V_n),\\quad j=1,2\\cdots,n. \\] <p>For an arbitrary first integral \\(V(x,V_1,\\cdots,V_n)\\), let</p> \\[ h(x,V_1,\\cdots,V_n)=V(x,y_1(x,V_1,\\cdots,V_n),\\cdots,y_n(x,V_1,\\cdots, V_n)) \\] <p>Now we prove that \\(h\\) is irrelevant with \\(x\\), i.e.</p> \\[ \\frac{\\partial h}{\\partial x}=0. \\] <p>Actually, </p> \\[ \\begin{align} \\frac{\\partial h}{\\partial x}=\\frac{\\partial V}{\\partial x}+\\sum_{j=1}^n\\frac{\\partial V}{\\partial y_j}\\frac{\\partial y_j}{\\partial x}. \\label{h-x} \\end{align} \\] <p>By definition of first integral, we have</p> \\[ \\frac{\\partial V_j}{\\partial x}+\\sum_{k=1}^n \\frac{\\partial V_j}{\\partial y_k}\\frac{\\partial y_k}{\\partial x}=0, \\quad j=1,2,\\cdots,n \\] <p>that is, </p> \\[ \\left[\\begin{array}{cccc} \\displaystyle \\frac{\\partial V_1}{\\partial y_1} &amp; \\displaystyle \\frac{\\partial V_1}{\\partial y_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial V_1}{\\partial y_n} \\\\ \\displaystyle \\frac{\\partial V_2}{\\partial y_1} &amp; \\displaystyle \\frac{\\partial V_2}{\\partial y_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial V_2}{\\partial y_n}\\\\ \\displaystyle \\vdots&amp; \\vdots&amp; \\ddots &amp;\\vdots\\\\ \\displaystyle \\frac{\\partial V_n}{\\partial y_1} &amp; \\displaystyle \\frac{\\partial V_n}{\\partial y_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial V_n}{\\partial y_n} \\end{array}\\right] \\cdot \\left[\\begin{array}{c} \\displaystyle \\frac{\\partial y_1}{\\partial x}\\\\ \\displaystyle \\frac{\\partial y_2}{\\partial x}\\\\ \\displaystyle \\vdots\\\\  \\displaystyle \\frac{\\partial y_n}{\\partial x} \\end{array}\\right]=\\left[\\begin{array}{c} \\displaystyle \\frac{\\partial V_1}{\\partial x}\\\\ \\displaystyle \\frac{\\partial V_2}{\\partial x}\\\\  \\displaystyle \\vdots \\\\  \\displaystyle \\frac{\\partial y_n}{\\partial x}\\end{array}\\right] \\] <p>By Cramer principle, we have</p> \\[ \\begin{align} \\frac{\\partial y_j}{\\partial x}=\\frac{1}{J}\\text{det}\\frac{\\partial(V_1,\\cdots,V_n)}{\\partial (y_1,\\cdots,y_{j-1},x,y_{j+1},\\cdots,y_n)}\\label{Cramer} \\end{align} \\] <p>Substitute equation \\(\\ref{Cramer}\\) back into equation \\(\\ref{h-x}\\) and we have</p> \\[ \\begin{align*} \\frac{\\partial h}{\\partial x}&amp;=\\frac{\\partial V}{\\partial x}+\\sum_{j=1}^n\\frac{\\partial V}{\\partial y_j}\\frac{1}{J}\\text{det}\\frac{\\partial(V_1,\\cdots,V_n)}{\\partial (y_1,\\cdots,y_{k-1},x,y_{k+1},\\cdots,y_n)}\\\\ &amp;=\\frac{1}{J}\\left[ J \\frac{\\partial V}{\\partial x}+\\sum_{j=1}^n\\frac{\\partial V}{\\partial y_j}\\text{det}\\frac{\\partial(V_1,\\cdots,V_n)}{\\partial (y_1,\\cdots,y_{k-1},x,y_{k+1},\\cdots,y_n)}\\right]\\\\ &amp;=\\frac{1}{J}\\text{det}\\frac{\\partial (V,V_1,\\cdots,V_n)}{\\partial (x,y_1,\\cdots,y_n)} \\end{align*} \\] <p>While for the right side matrix, its corresponding linear system could be</p> \\[ \\left[\\begin{array}{ccccc} \\displaystyle \\frac{\\partial V}{\\partial x} &amp; \\displaystyle \\frac{\\partial V}{\\partial y_1} &amp; \\displaystyle \\frac{\\partial V}{\\partial y_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial V}{\\partial y_n} \\\\ \\displaystyle \\frac{\\partial V_1}{\\partial x} &amp; \\displaystyle \\frac{\\partial V_1}{\\partial y_1} &amp; \\displaystyle \\frac{\\partial V_1}{\\partial y_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial V_1}{\\partial y_n} \\\\ \\displaystyle \\frac{\\partial V_2}{\\partial x} &amp; \\displaystyle \\frac{\\partial V_2}{\\partial y_1} &amp; \\displaystyle \\frac{\\partial V_2}{\\partial y_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial V_2}{\\partial y_n}\\\\ \\displaystyle \\vdots&amp; \\vdots&amp; \\vdots &amp; \\ddots &amp;\\vdots\\\\ \\displaystyle \\frac{\\partial V_n}{\\partial x} &amp; \\displaystyle \\frac{\\partial V_n}{\\partial y_1} &amp; \\displaystyle \\frac{\\partial V_n}{\\partial y_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial V_n}{\\partial y_n} \\end{array}\\right]_{(n+1)\\times(n+1)} \\cdot \\left[\\begin{array}{c} 1 \\\\ \\displaystyle \\frac{\\partial y_1}{\\partial x}\\\\ \\displaystyle \\frac{\\partial y_2}{\\partial x}\\\\ \\displaystyle \\vdots\\\\  \\displaystyle \\frac{\\partial y_n}{\\partial x} \\end{array}\\right]_{(n+1)\\times 1}=\\pmb{0} \\] <p>which has non-zero solution (by property of first integral for \\(V,V_1,\\cdots,V_n\\)), meaning its determinant equals zero.</p> <p>And we are done.</p> <p>Note: the above conclusion holds locally. we could not guarantee the existence of first integrals in a large region.</p>"},{"location":"Math/Ordinary_Differential_Equation/FstPDE/#homogeneous-linear-first-order-pde","title":"Homogeneous linear first order PDE","text":"<p>The form of homogeneoud first order PDE can be </p> \\[ \\begin{align} \\sum_{j=1}^n A_j(x_1,x_2,\\cdots,x_n)\\frac{\\partial u}{\\partial x_j}=0 \\label{PDE} \\end{align} \\] <p>where \\(u=u(x_1,x_2,\\cdots,x_n)\\) is unknown function, coefficient function </p> \\[ A_j(x_1,x_2,\\cdots,x_n)\\in C^1,\\quad j=1,2,\\cdots,n,\\quad \\forall(x_1,x_2,\\cdots, n)\\in D \\] <p>and </p> \\[ \\sum_{j=1}^n|A_j(x_1,x_2,\\cdots,x_n)|&gt;0. \\] <p>Transformation of first order PDE</p> <p>If \\(A_1(x_1,x_2,\\cdots,x_n)\\neq 0\\), then equation \\(\\ref{PDE}\\) could be written as</p> \\[ \\frac{\\partial u}{\\partial x_1}+\\frac{\\partial u}{\\partial x_2}\\frac{A_2}{A_1}+\\cdots+\\frac{\\partial u}{\\partial x_n}\\frac{A_n}{A_1}=0 \\] <p>Compared to equation \\(\\ref{Property}\\) we could deduce that \\(u\\) is a first integral of ODEs</p> \\[ \\frac{d x_j}{dx_1}=\\frac{A_j}{A_1},\\quad j=2,3,\\cdots,n. \\] <p>We could transfer a PDE problem into a ODEs problem.</p> <p>So here comes the following theorem.</p> <p>General solution to First Order PDE</p> <p>We define characteristic equation of PDE \\(\\ref{PDE}\\)</p> \\[ \\begin{equation} \\frac{dx_1}{A_1(x_1,x_2,\\cdots,x_n)}=\\frac{dx_2}{A_2(x_1,x_2,\\cdots,x_n)}=\\cdots=\\frac{dx_n}{A_n(x_1,x_2,\\cdots,x_n)}.\\label{Characteristic} \\end{equation} \\] <p>which has \\(n-1\\) independent first integrals</p> \\[ \\phi_j(x_1,x_2,\\cdots,x_n)=c_j,\\quad j=1,2,\\cdots, n. \\] <p>Then the general solution to PDE \\(\\ref{PDE}\\) is</p> \\[ u=\\varPsi(\\phi_1(x_1,x_2,\\cdots,x_n),\\phi_2(x_1,x_2,\\cdots,x_n),\\cdots,\\phi_n(x_1,x_2,\\cdots,x_n)) \\] <p>where \\(\\varPsi\\) is an arbitary continuously differentiable function.</p> Proof <p>Assume \\(\\phi(x_1,x_2,\\cdots,x_n)\\) is a first integral of characteristic equation \\(\\ref{Characteristic}\\). Cause \\(A_1,A_2,\\cdots,A_n\\) do not equal zero, assume there exists a neighborhood, in which \\(A_1(x_1,x_2,\\cdots,x_n)\\neq 0\\). So characteristic equation \\(\\ref{Characteristic}\\) is equivalent to the following ODEs</p> \\[ \\frac{dx_j}{dx_i}=\\frac{A_j(x_1,x_2,\\cdots,x_n)}{A_1(x_1,x_2,\\cdots,x_n)}, \\quad j=2,3,\\cdots,n \\] <p>Thus \\(\\phi(x_1,x_2,\\cdots,x_n)\\) is also a first integral of this ODEs. So</p> \\[ \\frac{\\partial\\phi}{\\partial x_1}+\\sum_{j=2}^n\\frac{A_j(x_1,x_2,\\cdots,x_n)}{A_1(x_1,x_2,\\cdots,x_n)}\\frac{\\partial \\phi}{\\partial x_j}=0 \\] <p>that is,</p> \\[ \\sum_{j=1}^n A_j(x_1,x_2,\\cdots,x_n)\\frac{\\partial \\phi}{\\partial x_j}=0 \\] <p>Due to \\(\\phi_1,\\phi_2,\\cdots,\\phi_{n-1}\\) are \\(n-1\\) independent first integrals, so for an arbitrary \\(C^1\\) function \\(\\varPsi\\),</p> \\[ \\varPsi(\\phi_1,\\phi_2,\\cdots,\\phi_n) \\] <p>is also a first integral of characteristic equation \\(\\ref{Characteristic}\\). By the theory of expression with first integrals, for an arbitrary first integral \\(\\psi(x_1,x_2,\\cdots,x_n)\\), there exists a \\(C^1\\) function \\(\\varPsi_0\\) such that</p> \\[ \\psi(x_1,x_2,\\cdots,x_n)=\\varPsi_0(\\phi_0(x_1,x_2,\\cdots,x_n),\\phi_2(x_1,x_2,\\cdots,x_n),\\cdots,\\phi_{n-1}(x_1,x_2,\\cdots,x_n)). \\]"},{"location":"Math/Ordinary_Differential_Equation/FstPDE/#quasi-linear-first-order-pde","title":"Quasi-linear first order PDE","text":"<p>We call the following PDE Quasi-linear first order PDE</p> \\[ \\begin{align} \\sum_{j=1}^n A_j(x_1,\\cdots,x_n,u)\\frac{\\partial u}{\\partial x_j}=B(x_1,\\cdots,x_n,u) \\label{ql-PDE} \\end{align} \\] <p>where \\(A_1,\\cdots,A_n,B\\in C^1(G)\\). This PDE is characterized that it is linear with respect to the partial direvative of \\(u\\).</p> <p>Transformation of Quasi-linear first order PDE</p> <p>Assume \\(u=\\phi(x_1,\\cdots,x_n)\\) is solution to PDE \\(\\ref{ql-PDE}\\), if we let </p> \\[ \\varPhi(x_1,\\cdots,x_n)=\\phi(x_1,\\cdots,x_n)-u \\] <p>which follows</p> \\[ \\varPhi(x_1,\\cdots,x_n,u)\\equiv 0 \\Leftrightarrow u=\\phi(x_1,\\cdots,x_n) \\] <p>and</p> \\[ \\frac{\\partial \\varPhi}{\\partial x_j}=\\frac{\\partial \\phi}{\\partial x_j} (j=1,2,\\cdots,n),\\quad \\frac{\\partial \\varPhi}{\\partial u}=-1 \\] <p>So \\(\\varPhi\\) satisfies a homogeneous first order PDE</p> \\[ \\begin{align} \\sum_{j=1}^nA_j(x_1,\\cdots,x_n,u)\\frac{\\partial \\varPhi}{\\partial x_j}+B(x_1,\\cdots,x_n,u)\\frac{\\partial \\varPhi}{\\partial u}=0. \\label{alter-PDE} \\end{align} \\] <p>Here we transform a Quasi-linear PDE into a homogeneous PDE. Then we only have to show that the solution to PDE \\(\\ref{alter-PDE}\\) is also a solution to PDE \\(\\ref{ql-PDE}\\).</p> <p>Theorem for Quasi-linear first order PDE</p> <p>Assume general solution to PDE \\(\\ref{alter-PDE}\\) is </p> \\[ \\varPhi=\\varPhi(\\psi_1(x_1,\\cdots,x_n,u),\\cdots,\\psi_n(x_1,\\cdots,x_n,u)) \\] <p>where </p> \\[ \\psi_j(x_1,\\cdots,x_n,u)=c_j,\\quad j=1,2\\cdots,n \\] <p>are \\(n\\) independent first integrals of the corresponding characteristic equations</p> \\[ \\frac{dx_1}{A_1(x_1,\\cdots,x_n,u)}=\\cdots=\\frac{dx_n}{A_1(x_1,\\cdots,x_n,u)}=\\frac{du}{B(x_1,\\cdots,x_n,u)} \\] <p>with \\(\\varPhi\\) is a continuously differentiable function. Then the general solution to PDE \\(\\ref{ql-PDE}\\) is </p> \\[ \\begin{align} \\varPhi(\\psi_1(x_1,\\cdots,x_n,u),\\cdots,\\psi_n(x_1,\\cdots,x_n,u))=0 \\label{solu-ql} \\end{align} \\] <p>where \\(\\frac{\\partial}{\\partial u}\\varPhi(x_1,\\cdots,x_n,u)\\neq 0\\).</p> Proof <p>Similar to the previous proof, we have to show that all solution to PDE \\(\\ref{ql-PDE}\\) could be expressed by formula \\(\\ref{solu-ql}\\). That is, for an arbitrary solution \\(u=f(x_1,\\cdots,x_n)\\) of PDE \\(\\ref{ql-PDE}\\), there exists \\(\\varPhi_0\\) such that</p> \\[ \\varPhi_0(\\psi_1(x_1,\\cdots,x_n,f(x_1,\\cdots,x_n)),\\cdots,\\psi_n(x_1,\\cdots,x_n,f(x_1,\\cdots,x_n)))\\equiv 0 \\] <p>which is equivalent to that</p> \\[ \\psi_j(x_1,\\cdots,x_n,f(x_1,\\cdots,x_n))\\overset{\\Delta}{=}\\phi_j(x_1,\\cdots,x_n),\\quad j=1,2,\\cdots,n \\] <p>are relevant. Cause</p> \\[ \\frac{\\partial \\phi_j}{\\partial x_k}=\\frac{\\partial \\psi_j}{\\partial x_k}+\\frac{\\partial \\psi_j}{\\partial u}\\frac{\\partial f}{\\partial x_k},\\quad j,k=1,2,\\cdots,n \\] <p>by using \\(f(x_1,\\cdots,x_n)\\) is solution to PDE \\(\\ref{ql-PDE}\\), \\(\\psi_j(x_1,\\cdots,x_n,f(x_1,\\cdots,x_n))(j=1,2,\\cdots,n)\\) is solution to PDE \\(\\ref{alter-PDE}\\), we have</p> \\[ \\begin{align*} \\sum_{k=1}^nA_k(x_1,\\cdots,x_n,f(x_1,\\cdots,x_n))\\frac{\\partial \\phi_j}{\\partial x_k}&amp;=\\sum_{k=1}^n\\left[A_k\\frac{\\partial \\psi_j}{\\partial x_k}+A_k\\frac{\\partial \\psi_j}{\\partial u}\\frac{\\partial f}{\\partial x_k}\\right]\\\\ &amp;=\\sum_{k=1}^n \\left[A_k\\frac{\\partial \\psi_j}{\\partial x_k}+\\frac{\\partial \\psi_j}{\\partial u}A_k\\frac{\\partial f}{\\partial x_k}\\right]\\\\ &amp;=\\sum_{k=1}^n \\frac{\\partial \\psi_j}{\\partial x_k}A_k+\\frac{\\partial \\psi_j}{\\partial u}B_k\\\\ &amp;=0,\\quad j=1,2,\\cdots,n \\end{align*} \\] <p>which means the following matrix multiplication</p> \\[ \\left[\\begin{array}{cccc} \\displaystyle \\frac{\\partial \\phi_1}{\\partial x_1} &amp; \\displaystyle \\frac{\\partial \\phi_1}{\\partial x_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial \\phi_1}{\\partial x_n} \\\\ \\displaystyle \\frac{\\partial \\phi_2}{\\partial x_1} &amp; \\displaystyle \\frac{\\partial \\phi_2}{\\partial x_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial \\phi_2}{\\partial x_n}\\\\ \\displaystyle \\vdots&amp; \\vdots&amp; \\ddots &amp;\\vdots\\\\ \\displaystyle \\frac{\\partial \\phi_n}{\\partial x_1} &amp; \\displaystyle \\frac{\\partial \\phi_n}{\\partial x_2} &amp;\\cdots &amp;  \\displaystyle \\frac{\\partial \\phi_n}{\\partial x_n} \\end{array}\\right] \\cdot \\left[\\begin{array}{c} \\displaystyle A_1\\\\ \\displaystyle A_2\\\\ \\displaystyle \\vdots\\\\  \\displaystyle A_n \\end{array}\\right]=\\pmb{0} \\] <p>\\(A_1,A_2,\\cdots,A_n\\) do not equal zeto at the same time, so </p> \\[ \\text{det}\\frac{\\partial (\\phi_1,\\cdots,\\phi_n)}{\\partial(x_1,\\cdots,x_n)}=0 \\] <p>which means \\(\\phi_1,\\cdots,\\phi_n\\) are relevent.</p> <p>Solve the general solution to PDE</p> \\[ xz\\frac{\\partial z}{\\partial x}+yz\\frac{\\partial z}{\\partial y} = -xy \\] <p>and its solution plane which passes curve \\(y=x^2\\), \\(z=x^3\\).</p> Answer <p>The corresponding characteristic equation</p> \\[ \\frac{dx}{xz}=\\frac{dy}{yz}=\\frac{dz}{-xy} \\] <p>which has two independent first integrals</p> \\[ \\frac{x}{y}=c_1,\\quad z^2+xy=c_2 \\] <p>so the general solution to PDE is</p> \\[ V\\left(\\frac{x}{y},z^2+xy\\right)=0 \\] <p>where \\(V(\\xi,eta)\\) is an arbitrary continuously differentiable function.</p> <p>From above we could solve</p> \\[ f\\left(\\frac{x}{y}\\right)=z^2+xy \\] <p>substitute initial condition, we have</p> \\[ f\\left(\\frac{1}{x}\\right)=x^6+x^3 \\Rightarrow f(x)=1/x^6+1/x^3 \\] <p>so the solution with initial condition is </p> \\[ (z^2+xy)=\\left(\\frac{y}{x}\\right)^6+\\left(\\frac{y}{x}\\right)^3 \\]"},{"location":"Math/Ordinary_Differential_Equation/LODEs/","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4 | System of Linear Differential Equations (LODEs)","text":"<p>This chapter we focus on </p> \\[ \\begin{equation} \\frac{d \\symbfit{X}(t)}{dt} = \\symbfit{A}(t)\\symbfit{X}(t)+\\symbfit{B}(t) \\label{eq-LODEs} \\end{equation} \\] <p>with initial condition</p> \\[ \\begin{equation} \\symbfit{X}(t_0)=\\symbfit{X}_0 \\label{eq-initial-LODEs} \\end{equation} \\] <p>where \\(t_0\\in I=(a,b)\\), \\(\\symbfit{X}_0 = (x_1^0,x_n^0,\\cdots, x_n^0)^T\\) is a given constant vector.</p> <p>Outline</p> <ul> <li> <p>\u5e38\u7cfb\u6570\u7ebf\u6027\u65b9\u7a0b\u7ec4\u6c42\u89e3: \u5355\u7279\u5f81\u6839 (\u5b9e\u3001\u590d), \u91cd\u7279\u5f81\u6839 (\u5b9e\u3001\u590d), \u6c42\u901a\u89e3, \u7279\u89e3, \u5e26\u521d\u503c\u7684\u89e3</p> </li> <li> <p>\u9ad8\u9636\u5fae\u5206\u65b9\u7a0b: \u7279\u5f81\u6839\u6cd5</p> </li> <li> <p>\u6c42\u7279\u89e3: \u5f85\u5b9a\u7cfb\u6570\u6cd5, \u5e38\u6570\u53d8\u6613\u6cd5</p> </li> <li> <p>Euler \u65b9\u7a0b, \u4ee3\u5165 \\(y=x^r\\) \u5f97\u5230 \\(r\\) \u7684\u591a\u9879\u5f0f\uff0c\u9636\u6570\u524d\u9762\u5373\u4e3a\u7cfb\u6570\u3002</p> </li> <li> <p>\u5e42\u7ea7\u6570\u89e3\u6cd5</p> </li> </ul>"},{"location":"Math/Ordinary_Differential_Equation/LODEs/#existence-and-uniqueness-of-lodes","title":"\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027 | Existence and Uniqueness of LODEs","text":"<p>This is quite similar to Picard Theorem in chapter Existence and Uniqueness Theorem, but it is still useful to give a special form of Picard Sequence for LODEs, which is also an approximation to solving it.</p> <p>\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7ec4\u89e3\u7684\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Theorem of Existence and Uniqueness of LODEs</p> <p>LODEs \\(\\ref{eq-LODEs}\\) with initial condition \\(\\ref{eq-initial-LODEs}\\) has only one solution on interval \\(I\\).</p> <p>Prove it.</p> Hints <p>We have to measure the distance in matrix. Now we need to give a definition of norm of vectors and matrixes(to see more details in Norm of vectors and matrixes) in Numerical Analysis.</p> \\[ \\Vert\\mathbfit{X} \\Vert = \\sum_{i=1}^{n}|x_i|, \\quad \\Vert\\mathbfit{A} \\Vert = \\sum_{i=1}^{n}\\sum_{j=1}^{n}|{a_{ij}}| \\] <p>It is easy to see that ...</p> <ul> <li>convert LODEs into its equivalent integral equations.</li> </ul> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <ul> <li>formulate Picard Sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align} \\mathbfit{X}_0(t) &amp;= \\mathbfit{X}_0 \\nonumber\\\\ \\mathbfit{X}_1(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_0(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ \\mathbfit{X}_2(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_1(s)+\\mathbfit{B}(s)\\right] ds \\nonumber\\\\ &amp;\\vdots \\nonumber\\\\ \\mathbfit{X}_n(t) &amp;= \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}_{n-1}(s)+\\mathbfit{B}(s)\\right] ds \\label{eq-LODEs-integral}\\\\ \\end{align} \\] <p>Consider similarly and we can say the above sequence is well-defined.</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>denote</p> \\[ C = \\sup_{s\\in J}\\Vert\\mathbfit{A}(s)\\Vert, \\quad D = C\\Vert\\mathbfit{X}(s)\\Vert + \\sup_{s\\in J}\\Vert\\mathbfit{B}(s)\\Vert \\] <p>we can get </p> \\[ \\begin{align*} \\Vert \\mathbfit{X}_1(t) - \\mathbfit{X}_0(t) \\Vert &amp;\\leq D |t-t_0|\\\\ \\Vert\\mathbfit{X}_2(t) - \\mathbfit{X}_1(t) \\Vert &amp;\\leq \\int_{t_0}^{t} \\Vert \\mathbfit{A}(s) \\Vert \\Vert \\mathbfit{X}_{1}(s)- \\mathbfit{X}_{0}(s) \\Vert ds \\\\ &amp;\\leq \\int_{t_0}^{t} C D |s-t_0| ds = \\frac{D}{C} \\frac{(C|t-t_0|)^2}{2}\\\\ &amp;\\vdots\\\\ \\Vert\\mathbfit{X}_n(t) - \\mathbfit{X}_{n-1}(t) \\Vert &amp;\\leq \\frac{D}{C} \\frac{(C|t-t_0|)^{n}}{(n)!} \\end{align*} \\] <p>which shows the Picard Sequence converges.</p> <ul> <li>Prove the convergent function is solution of LODEs \\(\\ref{eq-LODEs}\\).</li> </ul> <p>If we denote \\(\\mathbfit{X}(t) = \\lim_{n\\rightarrow \\infty}\\mathbfit{X}_n(t)\\) and let \\(n\\rightarrow \\infty\\) on both sides of integral equation \\(\\ref{eq-LODEs-integral}\\), we get </p> \\[ \\mathbfit{X}(t) = \\mathbfit{X}_0 + \\int_{t_0}^{t}\\left[ \\mathbfit{A}(s)\\mathbfit{X}(s)+\\mathbfit{B}(s)\\right] ds \\] <p>which is a solution.</p> <ul> <li>prove uniqueness.</li> </ul> <p>Similar to proof in Picard Theorem.</p>"},{"location":"Math/Ordinary_Differential_Equation/LODEs/#boundary-problem-of-second-order-lode","title":"\u4e8c\u9636\u65b9\u7a0b\u8fb9\u503c\u95ee\u9898 | Boundary Problem of Second-Order LODE","text":"<p>This pattern we focus on LODE</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = f(x) \\label{eq: BP-SecondOrder} \\end{equation} \\] <p>with Boundary Condition \\(y(a)=\\alpha, y(b)=\\beta\\), where \\(p(x), q(x) \\in C^1[a, b]\\)</p> <ul> <li>H-LODE</li> </ul> <p>\u5171\u8f6d\u70b9 | Conjugate Point</p> <p>If homogeneous LODE(H-LODE)</p> \\[ \\begin{equation} y''+p(x)'+q(x)y = 0 \\label{eq: BP-SO-H} \\end{equation} \\] <p>with boundary condition \\(y(a)=0, y(b)=0\\), has non-zero solution, then \\(\\{a, b\\}\\) is called the Conjugate Point of the H-LODE.</p> <p>We usually take use of the following method to check if the boundary point will induce indefinite solutions or no solutions.</p> <p>\u5171\u8f6d\u70b9\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for Conjugate Point</p> <p>\\(\\{a, b\\}\\) is the Conjugate Point of H-LODE, if and only if \\(\\forall y_1, y_2\\) of the solution of H-LODE, which are linear irrelevant, satisfies</p> \\[ \\left| \\begin{array}{cc} y_1(a)&amp; y_2(a)\\\\ y_1(b)&amp; y_2(b) \\end{array} \\right| =0 \\] Hints <p>substitute the boundary condition and we get two linear equation system for parameters \\(c_1, c_2\\). And the above is the determinant of the system.</p> <p>\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u8981\u6761\u4ef6 | Necessary and Sufficient Condition for existing only one solution for H-LODE</p> <p>H-LODE \\(\\ref{eq: BP-SO-H}\\) with boundary point \\(y(a)=\\alpha, y(b)=\\beta\\) has only one solution, if and only if \\(\\{a, b\\}\\) is not the conjugate point of the H-LODE.</p> Hints <p>Focus on the determinant of the linear irelevantly solutions of \\(y_1, y_2\\).</p> <ul> <li>Non-H-LODE</li> </ul> <p>We partition the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) into three parts.</p> <p>\u7ebf\u6027\u975e\u9f50\u6b21\u65b9\u7a0b\u5b58\u5728\u552f\u4e00\u89e3\u7684\u5145\u5206\u6761\u4ef6 | Sufficient Condition for existing only one solution for Non-H-LODE</p> <p>If \\(\\{a, b\\}\\) is not the conjugate point of H-LODE \\(\\ref{eq: BP-SO-H}\\), then Non-H-LODE \\(\\ref{eq: BP-SecondOrder}\\) has only one solution.</p> <p>There are two ways to prove it.</p> Version 1Version 2 <p>Assume \\(y_1(x)\\) is a solution of H-LODE </p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 0, \\quad y(b) = 1 \\label{eq: BP-1} \\end{equation} \\] <p>and \\(y_2(x)\\) is a solution of H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = 0, \\quad y(a) = 1, \\quad y(b) = 0 \\label{eq: BP-2} \\end{equation} \\] <p>and \\(y_3(x)\\) is a solution of Non-H-LODE</p> \\[ \\begin{equation} y'' + p(x)y' +q(x)y = f(x), \\quad y(a) = 0, \\quad y(b) = 0 \\label{eq: BP-3} \\end{equation} \\] <p>and the solution of LODE \\(\\ref{eq: BP-SecondOrder}\\) can be represented as</p> \\[ y = \\alpha y_1 + \\beta y_2 + y_3 \\] <p>where \\(y_1, y_2\\) are linearly irrelevant because of  existence and uniqueness theorem.</p> <p>We can get \\(y_3\\) through Variation of Constant. That is, let \\(y_3 = u_1(x) y_1(x)+u_2(x)y_2(x)\\), then </p> \\[ u_1 = \\int \\frac{-y_2 f}{W}dt, \\quad u_2 = \\int \\frac{y_1 f}{W}dt \\] <p>Substitute the boundary condition \\(\\ref{eq: BP-1}, \\ref{eq: BP-2}, \\ref{eq: BP-3}\\) we get </p> \\[ u_2(a)=0, \\quad u_1(b)=0 \\] <p>Based on this, we can transform the \\(u_1(x), u_2(x)\\) to definite integral whose upper limit of integral is variable</p> \\[ u_1 = \\int_{x}^{b} \\frac{y_2 f}{W}dt, \\quad u_2 = \\int_{x}^{a} \\frac{y_1 f}{W}dt \\] <p>So we can write particular solution </p> \\[ y_3 = y_1(x) \\int_{x}^{b} \\frac{y_2(t) f(t)}{W(t)}dt  + y_2(x)\\int_{x}^{a} \\frac{y_1(x) f(x)}{W(x)}dt \\] <p>If we define Green Function as</p> \\[ G(x,t)= \\begin{cases} \\displaystyle \\frac{y_2(x)y_1(t)}{W(t)}, \\quad a\\leq t\\leq x  \\\\ \\displaystyle \\frac{y_1(x)y_2(t)}{W(t)}, \\quad x\\leq t\\leq b  \\end{cases} \\] <p>then </p> \\[ y_3(x) = \\int_{a}^{b}G(x, t)f(t)dt \\] <p>According to the textbook.</p>"},{"location":"Math/Ordinary_Differential_Equation/LODEs/#s-l-sturm-liouville-boundary-problem","title":"S-L \u8fb9\u503c\u95ee\u9898 | Sturm-Liouville Boundary Problem","text":"<p>Solve for PDE</p> \\[ \\begin{cases} \\displaystyle u_{tt} = a^2 u_{xx} , \\quad 0\\leq x\\leq L, t\\geq 0\\\\ \\displaystyle u|_{x=0} = u|_{t=0} = 0 \\end{cases} \\] <p>Assume we can seperate the variables \\(x, t\\). That is, let \\(u = X(x)T(t)\\) and substitute, we get </p> \\[ \\begin{align*} X(x)T''(t)&amp;=a^2X''(x)T(t)\\\\ \\Rightarrow \\frac{T''(t)}{a^2T(t)} = \\frac{X''(x)}{X(x)} &amp;\\overset{\\Delta}{=}constant =-\\lambda \\end{align*} \\] <p>Thus, we have to find \\(T(t), X(x)\\) such that </p> \\[ T''(t)+a^2\\lambda T(t)= 0 \\] \\[ \\begin{cases} X''(x) + \\lambda X(x)= 0 \\\\ X(0)=X(L)=0 \\end{cases} \\]"},{"location":"Math/Ordinary_Differential_Equation/Lyapunov/","title":"Stability Theory of ODE","text":"<p>This chapter we focus on </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x})\\label{eq1} \\end{equation} \\] <p>We consider continuous dependence of the solutions on initial condition on a larger interval, i.e. \\((\\beta,\\infty)\\), for we have show the dependency holds on finite intervals. However, this is not always as expected. So we have to introduce some basic ideas.</p>"},{"location":"Math/Ordinary_Differential_Equation/Lyapunov/#lyapunov-lyapunov-stability","title":"Lyapunov \u7a33\u5b9a\u6027 | Lyapunov Stability","text":"<p>We assume that \\(\\pmb{x}=\\pmb{\\varphi}(t)\\) is a special solution of the ODE \\(\\ref{eq1}\\). </p> <p></p> <p>Lyapunov Stability</p> <p>(i) Lyapunov Stable</p> <p>If \\(\\forall \\varepsilon&gt;0\\), \\(\\forall t_0&gt;\\beta\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\delta\\), ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) has a solution \\(\\pmb{x}(t;t_0,\\pmb{x}_0)\\) which exists on \\([t_0,+\\infty)\\) and satisfies</p> \\[ \\|\\pmb{x}(t;t_0,\\pmb{x}_0)-\\varphi(t)\\|&lt;\\varepsilon, \\quad \\forall t\\in [t_0,+\\infty) \\] <p>then we call the special solution \\(\\pmb{x}=\\pmb{\\varphi}(t)\\) is Lyapunov stable, or stable for short.</p> <p>(ii) Lyapunov Unstable</p> <p>If \\(\\exists \\varepsilon_0&gt;0\\), \\(\\exists t_0&gt;\\beta\\), \\(\\forall \\delta&gt;0\\), \\(\\exists \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\delta\\), ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) has a solution \\(\\pmb{x}(t;t_0,\\pmb{x}_0)\\) which exists on \\([t_0, \\alpha)\\)(\\(\\alpha&lt;+\\infty\\)) or there exists \\(t_1&gt;t_0\\), and it satisfies </p> \\[ \\|\\pmb{x}(t_1;t_0,\\pmb{x}_0)-\\varphi(t)\\|\\geq\\varepsilon_0 \\] <p>then the special solution is Lyapunov Unstable.</p> <p>(iii) Attractive</p> <p>If \\(\\exists t_0&gt;\\beta\\), \\(\\exists \\zeta&gt;0\\), \\(\\forall \\pmb{x}_0\\), s.t. \\(\\|\\pmb{x}_0-\\pmb{\\varphi}(t_0)\\|&lt;\\zeta\\), solution of ODE \\(\\ref{eq1}\\) with initial value \\(\\pmb{x}(t_0)=\\pmb{x}_0\\) satisfies</p> \\[ \\lim_{t\\rightarrow +\\infty}\\|\\pmb{x}(t;t_0,\\pmb{x}_0)-\\pmb{\\varphi}(t)\\|=0 \\] <p>then the special solution is Attractive.</p> <p>(iv) Asymptotic stable</p> <p>If a special solution is Lyapunov stable and attractive, then we call it Asymptotic stable.</p> <p>To simplify the analysis, we choose to make a translation. That is, if we let </p> \\[ \\pmb{y}=\\pmb{x}-\\pmb{\\varphi}(t), \\pmb{F}(t,\\pmb{y})=\\pmb{f}(t,\\pmb{y}+\\pmb{\\varphi}(t))-\\pmb{f}(t,\\pmb{\\varphi}(t)) \\] <p>then we only need to discuss </p> \\[ \\begin{align*} \\frac{d\\pmb{y}}{dt}&amp;=\\frac{d\\pmb{x}}{dt}-\\frac{d\\pmb{\\varphi}(t)}{dt}\\\\ &amp;=\\pmb{f}(t,\\pmb{x})-\\pmb{f}(t,\\pmb{\\varphi}(t))\\\\ &amp;=\\pmb{f}(t,\\pmb{y}+\\pmb{\\varphi}(t))-\\pmb{f}(t,\\pmb{\\varphi}(t))\\\\ &amp;=\\pmb{F}(t,\\pmb{y}) \\end{align*} \\] <p>with special solution \\(\\pmb{y}=0\\), i.e. \\(\\pmb{f}(t,\\pmb{0})=\\pmb{0}\\). We call solution \\(\\pmb{y}\\equiv 0\\) zero solution.</p>"},{"location":"Math/Ordinary_Differential_Equation/Lyapunov/#linear-approximate-method","title":"\u7ebf\u6027\u8fd1\u4f3c\u5224\u522b\u6cd5 | Linear Approximate Method","text":"<p>If we represent ODE \\(\\ref{eq1}\\) as </p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{A}(t)\\pmb{x}+\\pmb{R}(t,\\pmb{x})\\label{eq3} \\end{equation} \\] <p>where </p> \\[ \\pmb{A}(t)=\\frac{\\partial \\pmb{f}}{\\partial \\pmb{x}}\\Bigg|_{\\pmb{x}=\\pmb{0}} \\] <p>and \\(\\pmb{R}(t,\\pmb{x})\\) is the summation of higher order items, i.e.</p> \\[ \\lim_{\\|\\pmb{x}\\|\\rightarrow 0}\\frac{\\|\\pmb{R}(t,\\pmb{x})\\|}{\\|\\pmb{x}\\|}=0. \\]"},{"location":"Math/Ordinary_Differential_Equation/Lyapunov/#lyapunov-stability-of-lodes","title":"Lyapunov Stability of LODEs","text":"<p>Firstly, we discuss the Lyapunov stability of LODEs</p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{A}(t)\\pmb{x}. \\label{eq2} \\end{equation} \\] <p>Theorem of Lyapunov Stability of LODEs</p> <p>Assume \\(\\pmb{\\varPhi}(t)\\) is a basic solution matrix of LODEs \\(\\ref{eq2}\\), then its zero solution is </p> <p>(i) stable iff \\(\\forall t_0&gt;\\beta\\), \\(\\exists K(t_0)&gt;0\\), s.t.</p> \\[ \\|\\pmb{\\varPhi}(t)\\| \\leq K,\\quad, t\\geq t_0 \\] <p>(ii) asymptotic stable iff </p> \\[ \\lim_{t\\rightarrow +\\infty}\\|\\pmb{\\varPhi}(t)\\|=0 \\] <p>For time-varying system, it is hard to solve a basic solution matrix. But for time-invariant system, we can not only solve the basic solution matrix, but also deduce a better result.</p> <p>Theorem for Lyapunov Stability of time-invariant System</p> <p>Consider const-coeffient system</p> \\[ \\frac{d\\pmb{x}}{dt}=\\pmb{A}\\pmb{x} \\] <p>its zero solution is </p> <p>(i) asymptotic stable iff all the eigenbalues of \\(\\pmb{A}\\) are negative.</p> <p>(ii) stable iff all the eigenvalues of \\(\\pmb{A}\\) are not positive, and for eigenvalue whose real part is zero, its corresponding Jordan block is one roder (or can be diagonalized).</p> <p>(iii) unstable iff there exists one positive eigenvalue or for eigenvalue whose real part is zero, its corresponding Jordan block is one roder(or cannot be diagonalized).</p>"},{"location":"Math/Ordinary_Differential_Equation/Lyapunov/#lyapunov-stability-of-non-linear-odes","title":"Lyapunov Stability of Non-linear ODEs","text":"<p>For non-linear system, we can have some results that is similar to the above. To simplify the problem, we let the linear part \\(\\pmb{A}(t)\\equiv \\pmb{A}\\) in ODE \\(\\ref{eq3}\\) be a constant matrix. Then, we can have the following Theorem</p> <p>Theorem for non-linear system using Linear Approximatie Method</p> <p>(i) If all the eigenvalues of \\(\\pmb{A}\\) are negative, then the zero solution of ODE \\(\\ref{eq3}\\) is asymptotic stable.</p> <p>(ii) If there exists an eigenvalue of \\(\\pmb{A}\\) which is positive, then the zero solution of ODE \\(\\ref{eq3}\\) is unstable.</p> HintsProof <p>Use Picard theorem and extension theorem to get a solution on \\([0,t*)\\), then show that solution can tend to \\(0\\) by using Gronwall inequation.</p>"},{"location":"Math/Ordinary_Differential_Equation/Lyapunov/#second-method-of-lyapunov","title":"\u7b2c\u4e8c\u6cd5 | Second Method of Lyapunov","text":"<p>The first method does not carry on because it makes use of power series. But his second method did gain ground, which employs an energy function which characterizes the solution. So this method is also called Direct Method.</p> <p>Here, we only consider autonomous ODEs, that is,</p> \\[ \\begin{equation} \\frac{d\\pmb{x}}{dt}=\\pmb{f}(\\pmb{x})\\label{eq4} \\end{equation} \\] <p>with the right side of ODE not containing variable \\(t\\).</p> <p>The basic idea can be shown in the following diagram.</p> \\[ \\begin{align*} \\|\\pmb{x}_0\\|&amp;\\ll 1\\\\ &amp;\\Downarrow \\quad \\text{(by continuity of $V$)} \\\\ V(\\pmb{x}_0)&amp;\\ll 1\\\\  &amp;\\Downarrow \\quad \\text{(by $\\frac{d V}{dt}&lt; 0$)}\\\\ V[\\pmb{x}(t)]&amp;\\leq V[\\pmb{x}(0)]\\ll 1\\\\ &amp;\\Downarrow \\quad \\text{(by monotony)}\\\\ \\|\\pmb{x}(t)\\|&amp;\\leq\\|\\pmb{x}(0)\\|\\ll 1 \\end{align*} \\] <p>Definition of Definite Sign Function</p> <p>Assume \\(h&gt;0\\), function \\(V(\\pmb{x})\\in C^1(\\|\\pmb{x}\\|\\leq h)\\), if \\(V(\\pmb{0})=0\\) and </p> <p>(i) \\(V(\\pmb{x})&gt;0\\)(or \\(&lt;0\\)) on \\(0&lt;\\|\\pmb{x}\\|\\leq h\\), then we call \\(V(\\pmb{x})\\) is a definite positive(or negative) function.</p> <p>(ii) \\(V(\\pmb{x})\\geq0\\)(or \\(\\leq0\\)) on \\(0&lt;\\|\\pmb{x}\\|\\leq h\\), then we call \\(V(\\pmb{x})\\) is a constant positive(or negative) function.</p> <p>Denifite positive function has a property.</p> <p>Property of Definite Positive Function</p> <p>\\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}\\), s.t. \\(V(\\pmb{x})&lt;\\delta\\), then \\(\\|\\pmb{x}\\|&lt;\\varepsilon\\). </p> HintsProof <p>Pay attention to the positive definite function.</p> <p>\\(\\forall \\varepsilon&gt;0\\), let </p> \\[ \\delta=\\min_{\\varepsilon\\leq\\|\\pmb{x}\\|\\leq h}V(\\pmb{x}) \\] <p>then </p> \\[ \\|\\pmb{x}\\|\\geq \\delta \\Rightarrow V(\\pmb{x})\\geq \\min_{\\varepsilon\\leq\\|\\pmb{x}\\|\\leq h}V(\\pmb{x}) = \\delta. \\] <p>So</p> \\[ V(\\pmb{x})&lt;\\delta\\Rightarrow \\|\\pmb{x}\\|&lt;\\varepsilon \\] <p></p> <p>Theorem of Stability test</p> <p>In ODE \\(\\ref{eq4}\\), assume \\(h&gt;0\\). If there exists a definite positive function \\(V(\\pmb{x})\\) on \\(\\|\\pmb{x}\\|\\leq h\\), whose total derivative </p> \\[ \\frac{dV}{dt}=\\nabla^{\\pmb{x}}V(\\pmb{x})\\cdot \\frac{d\\pmb{x}}{dt}=\\sum_{i=1}^n\\frac{\\partial V}{\\partial x_i}\\frac{d x_i}{dt}=\\sum_{i=1}^n\\frac{\\partial V}{\\partial x_i}f_i \\] <p>(i) is a constant negative function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov stable.</p> <p>(ii) is a definite negative function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov asymptotic stable.</p> <p>(iii) is a definite positive function, then the zero solution to ODE \\(\\ref{eq4}\\) is Lyapunov unstable.</p> <p>For (i) in Theorem of Stability Test, if we find another condition, then the system can also be asymptotic stable, and this is exactly the following theorem.</p> <p>LaSalle's invariance principle</p> <p>If ODE \\(\\ref{eq4}\\) satisfies </p> <p>For (iii) in Theorem of Stability Test, we can have a weaker theorem.</p> <p>Theorem for Lyapunov Unstable</p> <p>For ODE \\(\\ref{eq4}\\), if there exists a open region \\(\\mathcal{N}\\subset B_h(\\pmb{0})\\), satisfies</p> <p>(i) \\(V(\\pmb{x})\\in C^1(\\mathcal{N})\\).</p> <p>(ii) \\(\\pmb{0}\\in \\partial \\mathcal{N}\\), \\(\\exists \\delta&gt;0\\), \\(\\forall \\pmb{x}\\in \\partial \\mathcal{N}\\cap B_\\delta(\\pmb{0})\\), such that</p> \\[ V(\\pmb{x})=0 \\] <p>(iii) \\(\\forall \\pmb{x}\\in \\mathcal{N}\\cap B_\\delta(\\pmb{0})\\), \\(V(\\pmb{x})\\) and \\(\\frac{dV}{dt}\\) are both definite positive function.</p> <p>For a specific problem, we can choose first quadrant of the plane </p> \\[ \\mathcal{N}=\\{(x,y): x&gt;0,y&gt;0\\} \\] <p>with a typical \\(V(\\pmb{x})=xy\\), then test if \\(\\frac{dV}{dt}\\) is definite positive on \\(\\mathcal{N}\\).</p> <p>Example1. Determine the Lyapunov stability of the following system.</p> \\[ \\begin{cases} x'=x^5+\\sin{x^5}+2y+6y^5\\\\ y'=-e^x+8x^3+e^{y^3} \\end{cases} \\] Answer <p>use Hamilton system approximation.</p> <p>Example2. Determine the Lyapunov stability of the following system.</p> \\[ \\begin{cases} x'=-3x^3y^4\\\\ y'=x^4y^3 \\end{cases} \\] Answer <p>The above one cannot use Hamilton system approximation for it only has one item. But we can divide one by two, and get</p> \\[ \\frac{dy}{dx}=-\\frac{x}{3y} \\] <p>which is a homogeneous equation. So the solution is \\(x^2+3y^2=C\\). We can choose Lyapunov function \\(V=0.5x^2+1.5y^3\\), then \\(dV/dt\\equiv 0\\), which is Lyapunov stable but not asymptotic stable.</p>"},{"location":"Math/Ordinary_Differential_Equation/Preliminary/","title":"Preliminary","text":""},{"location":"Math/Ordinary_Differential_Equation/Preliminary/#jordan-matrix","title":"Jordan Matrix","text":"<p>Jordan Normal Form Theorem</p> <p>\\(A\\in \\mathbb{C}^{n\\times n}\\), \\(P_A(\\lambda)=\\prod_{i=1}^s(\\lambda-\\lambda_i)^{n_i}\\), then it is similar to Jordan normal form \\(diag(J_1,J_2,\\cdots,J_s)\\), \\(J_i\\in \\mathbb{n_i\\times n_i}\\), where \\(n_i\\) is algebraic multiplicity of \\(A\\), </p> \\[ J_i=diag(J_{i_1}, J_{i_2},\\cdots, J_{i_{m_i}}), \\quad m_i \\text{ is the geometric multiplicity of } A \\] \\[ J_{ij}=J_{l_{ij}}(\\lambda_i)=\\left[\\begin{array}{cccc} \\lambda_i &amp; 1\\\\ &amp;\\lambda_i &amp;\\ddots\\\\ &amp;&amp;\\ddots&amp;1\\\\ &amp;&amp;&amp;\\lambda_i \\end{array}     \\right]\\in \\mathbb{C}^{l_{ij}\\times l_{ij}} \\] <p>And the Jordan normal form is unique if we do not consider the sequence of small blocks.</p> <p>Minimal polynomial</p> <p>Matrix \\(A\\) has a minimal polynomial </p> \\[ d_{A}(\\lambda)=d_{J}(\\lambda)=\\prod_{i=1}^s(\\lambda-\\lambda_i)^{k_i} \\] <p>where</p> \\[ k_i=\\max_{1\\leq j\\leq m_i}\\{l_{ij}\\} \\] <p>\\(m_i\\) determines how much blocks compose the whole Jordan normal form. For a Jordan normal form \\(J_i\\), with \\(n_i\\) algebraic multiplicity of \\(\\lambda_i\\), has \\(m_i\\) number of Jordan small blocks. We can easily see that the prime diagonal has \\(n_i-m_i\\) number of \\(1\\), which means rank\\((J-\\lambda_i I)=n_i-m_i\\). Then to confirm the composition of these small blocks, we have to write \\(n_i-m_i\\) as a summation of \\(m_i\\) number non-negative integers, each of which corresponds to a Jordan matrix.</p> <p>Not complicatedly speaking, we have to calculate the power of \\(J_i-\\lambda_i\\) to get the result.</p> <p>Denote \\(\\delta_p^i\\) as the number of \\(p\\)th Jordan small blocks corresponding to eigenvalue \\(\\lambda_i\\), where \\(p=1,2\\cdots, n_i\\), so</p> \\[ \\begin{align*} \\sum_{p=1}^{n_i}\\delta_p^i=m_i, \\quad \\text{total number of blocks}\\\\ \\sum_{p=1}^{n_i}p\\delta_p^i=n_i\\quad \\text{total rank of Jordan} \\end{align*} \\] <p>Introduce </p> \\[ r_k^i=r(A-\\lambda_i I)^k \\] <p>so</p> \\[ \\begin{align*} r_1^i&amp;=r(A-\\lambda_i I)\\\\ &amp;=r(J-\\lambda_i I)\\\\ &amp;=n-n_i+r(J_i-\\lambda_i I)\\\\ &amp;=n-n_i + \\sum_{p=1}^{n_i}(p-1)\\delta_p^i \\quad \\text{1 order block becomes 0 when substraction} \\end{align*} \\] <p>So after \\(k\\) times power</p> \\[ r_k^i=n-n_i+\\sum_{p=k}^{n_i}(p-k)\\delta_p^i \\quad \\text{Why?} \\] <p>and</p> \\[ \\begin{align*} r_{k-1}^i&amp;=n-n_i+\\sum_{p=k-1}^{n_i}(p-k+1)\\delta_p^i\\\\ &amp;=n-n_i+\\sum_{p=k}^{n_i}(p-k+1)\\delta_p^i \\end{align*} \\] <p>If we define</p> \\[ d_k^i:=r_{k-1}^i-r^i_k=\\sum_{p=k}^{n_i}\\delta_p^i \\] <p>and </p> \\[ d_{k+1}^i=\\sum_{p=k+1}^{n_i}\\delta_p^i \\] <p>Substract the aboce two equaiton, we get</p> \\[ \\delta_p^i=d_k^i-d_{k+1}^i \\] <p>So if a Jordan small block has \\(\\delta_p^i\\) number, then the rank decline from power \\(p-1\\) to power \\(p\\) of \\((A-\\lambda_i I)\\) would display.</p>"},{"location":"Math/Ordinary_Differential_Equation/Qualitative_Theory/","title":"Qualitative Theory of ODE","text":""},{"location":"Math/Ordinary_Differential_Equation/Qualitative_Theory/#singular-point-analysis","title":"Singular Point Analysis","text":""},{"location":"Math/Ordinary_Differential_Equation/Qualitative_Theory/#linear-system-of-two-dimension","title":"Linear System of Two dimension","text":"<p>Consider </p> \\[ \\begin{equation} \\begin{cases} x'=ax+by\\\\ y'=cx+dy \\end{cases}, \\quad \\hat{\\pmb{x}}=\\pmb{A}x \\label{eq1} \\end{equation} \\] <p>with det\\((\\pmb{A})\\neq 0\\).</p> <p>General method for drawing sketch in a plane</p> <p>For system of two dimension in equation \\(\\ref{eq1}\\), follow steps below.</p> <p>(i) check \\(\\lambda_1\\), \\(\\lambda_2\\).</p> <p>To be more specific, directly check det\\((\\pmb{A})=\\lambda_1\\lambda_2\\) and tr\\((\\pmb{A})=\\lambda_1+\\lambda_2\\)</p> <p>See the pictrues below to pinpoint the position for type of Singular Point.</p> <p><p> </p></p> <p>If the position is above the parabola of the above plane, then go to part (iii) directly.</p> <p>(ii) determine the straight line.</p> <p>Solve for \\(k\\) of the straight line.</p> \\[ \\frac{y'}{x'}\\Bigg|_{y=kx}=k \\] <p>please do not forget \\(k=\\infty\\) would not be included from above.</p> <p>For deprecated node, we could only solve for one \\(k\\).</p> <p>(iii) determine the type of direction.</p> <p>Genaral method is to check for </p> \\[ xy'-yx'|_{(x_0,y_0)}=r^2\\theta'=\\begin{cases}&gt;0, \\quad \\text{anticlockwise}\\\\ &lt;0,\\quad \\text{clockwise}\\end{cases} \\] <p>But we can choose special point for direction.</p> Proof for part (ii)Proof for part (iii)Special cases for (iii) <p>On linear curve, we have \\(y(t)=kx(t)\\), so \\(y'(t)=kx'(t)\\), and </p> \\[ \\frac{y'}{x'}\\Bigg|_{y=kx}=k \\] <p>Solve for \\(k\\).</p> <p>we have polar expression</p> \\[ x=r\\cos \\theta, \\quad y=r\\sin \\theta \\] <p>then</p> \\[ \\begin{align} x'&amp;=r'\\cos \\theta -r\\sin \\theta\\cdot \\theta',\\quad \\label{xprime}\\\\y'&amp;=r'\\sin \\theta + r\\cos \\theta \\cdot\\theta'\\label{yprime} \\end{align} \\] <p>\\((\\ref{xprime})\\times \\sin\\theta\\) - \\((\\ref{yprime})\\times \\cos\\theta\\) and get:</p> \\[ x'\\sin\\theta-y'\\cos\\theta=-r\\cdot\\theta' \\] <p>multiply both sides by \\(r\\) and get</p> \\[ x'y-y'x=-r^2\\cdot \\theta' \\] <p>So </p> \\[ xy'-yx'=r^2\\cdot \\theta' \\] <p>both sides have the same sign.</p> <p>For \\(0&lt;\\lambda_1&lt;\\lambda_2\\) or \\(\\lambda_1&lt;\\lambda_2&lt;0\\), we can substitute \\((1,0)\\) into \\(y'\\) to see the direction.</p> <p>For \\(\\lambda_1&lt;0&lt;\\lambda_2\\), we can inspect a point \\((0,1)\\) to see the direction.</p> <p>For \\(\\lambda_1=\\lambda_2\\), first we check if there is indefinite solution of \\(k\\). If so, then ... we can inspect a point on linear curve.</p> <p>For \\(\\lambda=\\alpha\\pm i\\beta\\), we can also substitute \\((1,0)\\) into \\(y'\\) to see the direction.</p> <p>Example. Draw the craft.</p> \\[ \\begin{cases} x'=4x+\\sqrt{3}y\\\\ y'=2x+5y \\end{cases} \\] Answer <ul> <li> <p>check the type of \\(\\lambda_1\\), \\(\\lambda_2\\).</p> </li> <li> <p>determine the linear curve.</p> </li> <li> <p>determine the type of direction.</p> </li> </ul>"},{"location":"Math/Ordinary_Differential_Equation/Qualitative_Theory/#non-linear-system-of-two-dimension","title":"Non-Linear System of Two dimension","text":"<p>This part we focus on </p> \\[ \\begin{equation} \\begin{cases} x'=P(x,y)=ax+by+\\varPhi(x,y)\\\\ y=Q(x,y)=cx+dy+\\varPsi(x,y) \\end{cases}\\label{eq2} \\end{equation} \\] <p>where \\(\\varPhi(x,y)\\), \\(\\varPsi(x,y)\\) satisfy</p> \\[ \\varPhi(x,y)=o(r),\\quad \\varPsi(x,y)=o(r), \\quad r=\\sqrt{x^2+y^2}\\rightarrow 0  \\] <p>with its linear appriximation system</p> \\[ \\begin{equation} \\begin{cases} x'=ax+by\\\\ y=cx+dy \\end{cases}\\label{eq3} \\end{equation} \\] <p>We can also use its linear spproximation system to test. To be more specific, we have the following theorem.</p> <p>Theorem for Non-Linear System of Two dimension</p> <p>(i) If singular point is \\(N_-\\)(\\(N_+\\)) of the linear approsimation system \\(\\ref{eq3}\\), then it is also the \\(N_-\\)(\\(N_+\\)) of the original non-linear system \\(\\ref{eq2}\\).</p> <p>(ii) If singular point is \\(F_-\\)(\\(F_+\\)) of the linear approsimation system \\(\\ref{eq3}\\), then it is also the \\(F_-\\)(\\(F_+\\)) of the original non-linear system \\(\\ref{eq2}\\).</p> <p>(iii) If singular point is \\(S\\) of the linear approsimation system \\(\\ref{eq3}\\), then it is also the \\(S\\) of the original non-linear system \\(\\ref{eq2}\\).</p> <p>(iv) If singular point is \\(S_-\\)(\\(S_+\\)) or \\(D_-\\)(\\(D_+\\)) of the linear approsimation system \\(\\ref{eq3}\\), and there exists \\(\\delta&gt;0\\), such that \\(\\varPhi(x,y)\\), \\(\\varPsi(x,y)\\) satisfy</p> \\[ \\varPhi(x,y)=o(r^{1+\\delta}), \\quad\\varPsi(x,y)=o(r^{1+\\delta}), \\quad r=\\sqrt{x^2+y^2}\\rightarrow 0 \\] <p>then it is also the \\(S_-\\)(\\(S_+\\)) or \\(D_-\\)(\\(D_+\\)) of the original non-linear system \\(\\ref{eq2}\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/","title":"General Theory of ODE","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/#existence-and-uniqueness-theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/#proof-using-contraction-mapping","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/#comparison-theorem","title":"\u6bd4\u8f83\u5b9a\u7406 | Comparison Theorem","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/#continuous-dependence-of-solution-on-initial-value","title":"\u89e3\u5bf9\u521d\u503c\u7684\u8fde\u7eed\u4f9d\u8d56\u6027 | Continuous Dependence of Solution on Initial Value","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/#method-of-power-series","title":"\u5e42\u7ea7\u6570\u89e3\u6cd5 | Method of Power Series","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/","title":"Comparison Theorem","text":"<p>Based on the theorem on Extension of solution, we can only deduce that solution of an ODE can extend to its boundary Once the right function \\(f(x,y)\\) is continous on an open set \\(G\\).</p> <p>But it does not tell us the existing interval of independent variable \\(x\\). In particular, for two dimension problem, \\(y'=f(x,y)\\in C(\\mathbb{R}^2)\\), it could either be </p> \\[ x\\rightarrow \\beta &lt;\\infty, y\\rightarrow \\infty \\] <p>or</p> \\[ x\\rightarrow \\infty, y\\rightarrow \\beta &lt;\\infty \\] <p>So to better know the existing interval of \\(x\\), we have to give a more detailed tool for analysis. </p> <p>Note: The theorem presented below are only appliable to two dimension problem of ODE, cause in high dimension problem, we cannot compare the magnitude of \\(\\pmb{y}\\) but under some metric.</p> <p>To unify the form, in this part we talk about ODE</p> \\[ \\begin{equation} \\frac{dy}{dx}=f(x,y),\\quad y(x_0)=y_0 \\label{eq1} \\end{equation} \\]"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#first-comparison-theorem","title":"First Comparison Theorem","text":"<p>First Comparison Theorem</p> <p>Assume \\(f(x,y), F(x,y)\\in C(G)\\), where \\(G\\) is an open set, and </p> \\[ f(x,y) &lt;F(x,y),\\quad \\forall(x,y)\\in G \\] <p>And we assume \\(y=\\phi(x)\\) and \\(y=\\varPhi(x)\\) on \\((a,b)\\) are solution to ODE</p> \\[ \\begin{cases} y'=f(x,y)\\\\ y(x_0)=y_0 \\end{cases}, \\quad \\begin{cases} y'=F(x,y)\\\\ y(x_0)=y_0 \\end{cases} \\] <p>respectively, where \\((x_0,y_0)\\in G\\). Then </p> \\[ \\phi(x)&lt;\\varPhi(x), \\quad x_0&lt;x&lt;b \\] HintsProof <p>Formulate a function </p> \\[ \\psi(x)=\\varPhi(x)-\\phi(x) \\] <p>then prove by contradiction.</p> <p>Let</p> \\[ \\psi(x)=\\varPhi(x)-\\phi(x) \\] <p>then by initial condition</p> \\[ \\psi(x_0)=0,\\quad \\psi'(x_0)=F(x_0,y_0)-f(x_0,y_0)&gt;0 \\] <p>So we can see that there exists \\(\\delta&gt;0\\), such that</p> \\[ \\psi(x)=\\varPhi(x)-\\phi(x)&gt;0,\\quad x_0&lt;x\\leq x_0+\\delta \\] <p>If proposition </p> \\[ \\varPhi(x)&gt;\\phi(x),\\quad \\forall x\\in[x_0,b) \\] <p>does not hold, then \\(\\exists x_0+\\delta&lt;x_1&lt;b\\) such that</p> \\[ \\varPhi(x_1)&lt;\\phi(x_1) \\] <p>Let</p> \\[ \\alpha=\\min\\{x\\in (x_0+\\delta,b): \\psi(x)=0\\} \\] <p><p> </p></p> <p>so we can deduce that </p> \\[ \\psi(\\alpha)=0,\\quad \\psi(x)&gt;0,\\quad \\forall x\\in (x_0,\\alpha) \\] <p>easy to see that \\(\\psi(\\alpha)\\leq 0\\). But</p> \\[ \\psi'(\\alpha)= F(\\alpha,\\varPhi(\\alpha))-f(\\alpha,\\phi(\\alpha))&gt;0 \\] <p>which contradicts!</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#upper-and-lower-solution","title":"Upper and Lower Solution","text":"<p>To simplify the notation, we have to introduce upper and lower solution on the right side.</p> <p>Definition of upper and lower solution</p> <p>(i) If function \\(v(x)\\) on \\([x_0,b)\\) satisfies</p> \\[ \\frac{d v(x)}{dx}&lt;f(x,v(x)), \\quad v(x_0)\\leq y_0 \\] <p>then it is called the lower solution of ODE \\(\\ref{eq1}\\) on the right side.</p> <p>(ii) If function \\(w(x)\\) on \\([x_0,b)\\) satisfies</p> \\[ \\frac{d w(x)}{dx}&gt;f(x,v(x)), \\quad w(x_0)\\leq y_0 \\] <p>then it is called the upper solution of ODE \\(\\ref{eq1}\\) on the right side.</p> <p>From image point of view, we can say that the tangent of points on the upper solution on the right side must be bigger than the vector field on the same points. They are shown in the following graph.</p> <p><p> </p></p> <p>Theorem for upper and lower solution</p> <p>Assume \\(v(x)\\) and \\(w(x)\\) are defined in the above definition, then </p> \\[ v(x)&lt;\\phi(x)&lt;w(x),\\quad \\forall x\\in [x_0,b) \\] HintsProof <ul> <li>Define an assistant function</li> </ul> \\[ g(x)=\\frac{dw(x)}{dx}-f(x,w(x)),\\quad F(x,y)=f(x,y)+g(x) \\] <p>which is larger than zero on the line \\(y=w(x)\\), then can deduce the result just by First Comparison Theorem</p> <p>\\(w(x)\\) satisfies ODE</p> \\[ \\begin{align*} \\frac{dy}{dx}&amp;=F(x,y)\\\\ &amp;=f(x,y)+g(x)\\\\ &amp;=f(x,y)+\\frac{dw(x)}{dx}-f(x,w(x))\\\\ &amp;&gt;f(x,y) \\end{align*} \\] <p>So by First Comparison Theorem we have \\(w(x)&gt;\\phi(x)\\) on interval \\([x_0,b)\\).</p> <p>It is similar to prove \\(v(x)&lt;\\phi(x)\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#maximum-and-minimum-solution","title":"Maximum and Minimum solution","text":"<p>Maximum and Minimum solution</p> <p>If there are two solution \\(\\varPsi(x)\\), \\(\\varPhi(x)\\) to the ODE \\(\\ref{eq1}\\), such that for all solution \\(y(x)\\) to the same ODE, </p> \\[ \\varPsi(x)\\leq y(x)\\leq \\varPhi(x),\\quad \\forall x\\in[x_0,b) \\] <p>then we call \\(\\varPsi(x)\\), \\(\\varPhi(x)\\) the minimum and maximum solution of ODE on interval \\([x_0,b)\\) respectively.</p> <p>Apparently, the minimum and maximum solution are unique. Here we prove their existence.</p> <p></p> <p>Theorem of existence for minimum and maximum solution</p> <p>Assume \\(f(x,y)\\in C(R)\\) where</p> \\[ R=\\{(x,y):x_0\\leq x\\leq x_0+a, |y-y_0|\\leq b\\} \\] <p>Then initial problem \\(\\ref{eq1}\\) has maximum and minimum solution on \\([x_0,x_0+h)\\), where</p> \\[ h&lt;\\alpha=\\min\\{a,b/M\\}, \\quad  M=\\max_{(x,y)\\in R}{|f(x,y)|} \\] HintsProof <p>Formulate assistant initial problems</p> \\[ \\frac{dy}{dx}=f(x,y)+\\varepsilon_n, \\quad y(x_0)=y_0 \\] <p>where \\(\\varepsilon_n&gt;0\\), \\((n=1,2,\\cdots)\\) which is monotonically decreasing to \\(0\\), e.g. \\(\\varepsilon_n=1/n\\). Then show its limit of solution sequence is the maxmimum solution.</p> <p>It is similar to use \\(-\\varepsilon_n\\) to prove the existence of minimum solution.</p> <p>Formulate assistant initial problems</p> \\[ \\begin{align} \\frac{dy}{dx}=f(x,y)+\\varepsilon_n, \\quad y(x_0)=y_0 \\label{eq2} \\end{align} \\] <p>where \\(\\varepsilon_n&gt;0\\), \\((n=1,2,\\cdots)\\) which is monotonically decreasing to \\(0\\), e.g. \\(\\varepsilon_n=1/n\\). </p> <p>By Peano Theorem, ODE \\(\\ref{eq2}\\) has solution on interval \\(|x-x_0|\\leq \\alpha_n\\), where</p> \\[ \\alpha_n=\\min\\{a,b/M_n\\},\\quad M_n=\\max_{(x,y)\\in D}\\{|f(x,y)+\\varepsilon_n|\\} \\] <p>Notice that</p> \\[ \\lim_{n\\rightarrow \\infty}\\alpha_n = \\alpha = \\min\\{a,b/M\\}  \\] <p>so we choose \\(h&lt;\\alpha\\) such that all ODE \\(\\ref{eq2}\\) can have a solution \\(y=\\phi_n(x)\\) on interval \\(|x-x_0|\\leq h\\).</p> <ul> <li>Show that \\(\\{\\phi_n(x)\\}\\) has a subsequence that converges to the solution of the original ODE \\(\\ref{eq1}\\).</li> </ul> <p>This is similar as we prove in Peano Theorem.</p> <ul> <li>Show that the limit function \\(y=\\varPhi(x)\\) is the maximum solution of ODE \\(\\ref{eq1}\\). </li> </ul> <p>By First Comparison Theorem we have \\(\\phi_n(x)&gt;y(x)\\), for all solution \\(y(x)\\) of the original ODE \\(\\ref{eq1}\\). Then replace \\(n\\) with \\(n_j\\) and let \\(n\\rightarrow \\infty\\) we have</p> \\[ \\varPhi(x)\\geq y(x),\\quad \\forall x\\in [x_0,h] \\] <p>which shows that \\(\\varPhi(x)\\) is the maximum solution of ODE \\(\\ref{eq1}\\).</p> <p>Corollary</p> <p>Initial problem \\(\\ref{eq1}\\) has unique solution iff its maximum solution equals to minimum solution for all independent variable \\(x\\) on a given interval.</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#second-comparison-theorem","title":"Second Comparison Theorem","text":"<p>In First Comparison Theorem, we need to find \\(F(x,y)\\) which is strictly larger than \\(f(x,y)\\). What if we happen to find \\(F(x,y)\\) also equals \\(f(x,y)\\)?</p> <p>Apparently, we need to strengthen our condition to get a corresponding answer. This gives us the Second Comparison Theorem as below.</p> <p>Second Comparison Theorem</p> <p>Assume \\(f(x,y), F(x,y)\\in C(G)\\), where \\(G\\) is an open set, and </p> \\[ f(x,y) \\leq F(x,y),\\quad \\forall(x,y)\\in G \\] <p>And we assume \\(y=\\phi(x)\\) and \\(y=\\varPhi(x)\\) on \\((a,b)\\) are solution to ODE</p> \\[ \\begin{cases} y'=f(x,y)\\\\ y(x_0)=y_0 \\end{cases}, \\quad \\begin{cases} y'=F(x,y)\\\\ y(x_0)=y_0 \\end{cases} \\] <p>respectively, where \\((x_0,y_0)\\in G\\). And \\(y=\\phi(x)\\) is the minimum solution on the right side. Then </p> \\[ \\phi(x)\\leq \\varPhi(x), \\quad x_0&lt;x&lt;b \\] HintsProof <p>Also by formulating assistant initial problems.</p> <p>We still consider assistant problems</p> \\[ \\frac{dy}{dx}=f(x,y)-\\varepsilon_n,\\quad y(x_0)=y_0 \\] <p>So by the proof of Theorem for Maximum and Minimum Solution, we can have function subsequence </p> \\[ \\phi_{n_j}\\rightrightarrows \\phi \\] <p>which satisfies</p> \\[ \\phi(x)\\leq \\varPhi(x) \\]"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Comparison_Theorem/#result-for-estimation","title":"Result for estimation","text":"<p>Application for Comparison Theorem</p> <p>Assume \\(f(x,y)\\in C(G)\\) where</p> \\[ G=\\{(x,y): x_0&lt;x&lt;b,-\\infty&lt;y&lt;+\\infty\\} \\] <p>We have \\((x_0,y_0)\\in G\\), denote \\([x_0,\\alpha)\\) is the maximum existence interval on the right side for solution to ODE \\(\\ref{eq1}\\).</p> <p>(i) If ODE \\(\\ref{eq1}\\) has upper and lower solution \\(w(x)\\), \\(v(x)\\) with their public existing interval \\([x_0,\\beta)\\), then \\(\\beta\\leq \\alpha\\).</p> <p>(ii) If ODE \\(\\ref{eq1}\\) has upper solution \\(w(x)\\) (or lower solution \\(v(x)\\)) with its maximum existing interval \\([x_0,\\beta)\\), and satisfies</p> \\[ \\lim_{x\\rightarrow \\beta^-}w(x)=-\\infty (\\text{or } \\lim_{x\\rightarrow \\beta^-}v(x)=+\\infty ) \\] <p>then \\(\\alpha\\leq \\beta\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/","title":"Continuous Dependence of Solution on Initial Value","text":"<p>Here we focus on </p> \\[ \\begin{equation} \\dot{\\mathbfit{y}} = \\mathbfit{f}(x, \\mathbfit{y},\\pmb{\\lambda}),\\quad  \\mathbfit{y}(x_0) = \\mathbfit{y}_0 \\label{eq1} \\end{equation} \\] <p>where \\(\\mathbfit{x}\\in \\mathbb{R}^n\\) and \\(\\mathbfit{f}: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\in C(D)\\) is a vector function(or vector field).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/#continuous-dependency","title":"Continuous Dependency","text":"<p>Continuous Dependency means that the solution of ODE with initial values would not differ very much from real solution when the errors of parameters are small enough.</p> <p>Now we consider making a transformation. Let </p> \\[ t=x-x_0,\\quad \\pmb{u}=\\pmb{y}-\\pmb{y}_0 \\] <p>then</p> \\[ \\begin{align*} \\frac{d \\pmb{u}(t)}{dt} &amp;= \\frac{d\\pmb{y}}{d x} \\\\ &amp;=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\\\ &amp;=\\pmb{f}(t+x_0,\\pmb{u}+\\pmb{y_0},\\pmb{\\lambda}) \\end{align*} \\] <p>and initial value becomes</p> \\[ \\pmb{u}(0)=\\pmb{0} \\] <p>So we only consider dependence on parameters \\(\\pmb{\\lambda}\\) of ODE</p> \\[ \\begin{equation} \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda}),\\quad \\pmb{y}(0)=\\pmb{0}  \\label{eq2} \\end{equation} \\] <p>Theorem for ODE with parameters</p> <p>Consider a region</p> \\[ G=\\{(x,\\pmb{y},\\pmb{\\lambda}):|x|\\leq a,|\\pmb{y}|\\leq b, |\\pmb{\\lambda}|\\leq c\\} \\] <p>and \\(\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\in C(G)\\) and satisfies Lypschitz condition with respect to \\(\\pmb{y}\\), i.e. \\(\\forall (x,\\pmb{y}_1,\\pmb{\\lambda})\\), \\((x,\\pmb{y}_2,\\pmb{\\lambda})\\) we have</p> \\[ |\\pmb{f}(x,\\pmb{y}_1,\\pmb{\\lambda})-\\pmb{f}(x,\\pmb{y}_2,\\pmb{\\lambda})|\\leq L|\\pmb{y}_1-\\pmb{y}_2| \\] <p>where \\(L\\) is Lypschitz constant. If we let </p> \\[ M=\\max_{(x,\\pmb{y},\\pmb{\\lambda})\\in G}\\{\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\}, \\quad \\alpha = \\min\\{a,b/M\\} \\] <p>then the solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\lambda})\\) of ODE \\(\\ref{eq2}\\) is continuous on region</p> \\[ D=\\{|x|\\leq \\alpha, |\\pmb{\\lambda}|\\leq c\\} \\] Proof <p>Similar to Picard Theorem</p> <p>Corollary</p> <p>Assume region </p> \\[ R=\\{|x-x_0|\\leq a, |\\pmb{y}-\\pmb{y}_0|\\leq b\\} \\] <p>If \\(\\pmb{f}(x,\\pmb{y})\\in C(R)\\) and satisfies Lypschitz condition with respect to \\(\\pmb{y}\\). Then initial problem</p> \\[ \\frac{d \\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}),\\quad \\pmb{y}(x_0)=\\pmb{\\eta} \\] <p>has a solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\eta})\\) is continuous on region</p> \\[ Q=\\left\\{(x,\\pmb{y}): |x-x_0|\\leq \\frac{\\alpha}{2}, |\\pmb{y}-\\pmb{y}_0|\\leq \\frac{b}{2}\\right\\} \\] <p>where </p> \\[ M=\\max_{(x,\\pmb{y},\\pmb{\\lambda})\\in G}\\{\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\}, \\quad \\alpha = \\min\\{a,b/M\\}. \\] <p>Actually, we do not need Lypschitz condition but the uniqueness of solution to ODE \\(\\ref{eq1}\\). Here comes the following theorem.</p> <p>Theorem for dependence on initial value</p> <p>Consider ODE</p> \\[ \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda}) \\] <p>where \\(\\pmb{f}\\) is bounded and continuous on region \\(G\\subset \\mathbb{R}\\times \\mathbb{R}^n \\times \\mathbb{R}^m\\). Assume for all initial points \\((x_0,\\pmb{y}_0)\\), the solution \\(\\pmb{y}=\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\) to the ODE exists uniquely on interval \\(I_0\\), where \\(I_0\\subset \\mathbb{R}\\) is finite. Then \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), \\(\\forall (\\xi, \\pmb{\\eta}, \\pmb{\\lambda}')\\in G\\) s.t.</p> \\[ |(\\xi, \\pmb{\\eta}, \\pmb{\\lambda}')- (x_0, \\pmb{y}_0, \\pmb{\\lambda})|&lt;\\delta \\] <p>we have </p> \\[ |\\pmb{\\phi}(x;\\xi,\\pmb{\\eta},\\pmb{\\lambda}')-\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})|&lt;\\varepsilon,\\quad \\forall x\\in I_0 \\] HintsProof <p>By contradiction.</p> <p>We only prove for ODE \\(\\ref{eq1}\\).</p> <ul> <li>Transfer the problem into another form</li> </ul> <p>If we introduce \\(\\pmb{z}=\\pmb{\\lambda}\\), then </p> \\[ \\frac{d\\pmb{z}}{dx}=\\pmb{0} \\] <p>cause \\(\\pmb{\\lambda}\\) is a comstant. Then ODE \\(\\ref{eq1}\\) becomes</p> \\[ \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{z}), \\quad \\frac{d\\pmb{z}}{dx}=\\pmb{0} \\] <p>with initial value</p> \\[ \\pmb{y}(x_0)=\\pmb{y}_0, \\quad \\pmb{z}(x_0)=\\pmb{0} \\] <p>We transfer the dependence on parameters into the dependence on initial values. In the following part, we only consider dependence on initial value.</p> <ul> <li>By contradiction</li> </ul> <p>Assume the above theorem does not hold, then \\(\\exists \\varepsilon_0&gt;0\\), \\(\\forall \\delta_n=1/n\\), \\(\\exists(\\xi_n,\\pmb{\\eta}_n, \\pmb{\\lambda}'_n)\\in G\\) with </p> \\[ |(\\xi_n,\\pmb{\\eta}_n, \\pmb{\\lambda}'_n)-(x_0,\\pmb{y}_0,\\pmb{\\lambda})|&lt;\\delta_i \\] <p>which satisfies that, there exsits \\(x_n\\in I_0\\)</p> \\[ \\begin{equation} |\\pmb{\\phi}(x_n;\\xi_n,\\pmb{\\eta}_n,\\pmb{\\lambda}'_n)-\\pmb{\\phi}(x_n;x_0,\\pmb{y}_0,\\pmb{\\lambda})|\\geq\\varepsilon_0\\label{ieq1} \\end{equation}\\] <p>Because \\(I_0\\) is bouned closed interval, so \\(\\{x_n\\}\\) has a convergent subsquence \\(\\{x_{n_j}\\}\\) which converges to \\(\\overline{x}\\in I_0\\).</p> <p>Notice that </p> \\[ \\pmb{\\phi}(x;\\xi_n,\\pmb{\\eta}_n)=\\pmb{\\eta}_n+\\int_{{\\xi}_n}^{x}\\pmb{f}(s,\\pmb{\\phi}(s;\\xi_n,\\pmb{\\eta}_n))ds \\] <p>it is easy to show that sequence \\(\\{\\pmb{\\phi}(x;\\xi_n,\\pmb{\\eta}_n)\\}\\) is uniforly bounded and equicontinuous. Actually, if we assume \\(M=\\max_{(x,\\pmb{y},\\pmb{\\lambda})\\in G}|\\pmb{f}|\\) and denote \\(\\pmb{\\psi}_n(x)=\\pmb{\\phi}(x;\\xi_n,\\pmb{\\eta}_n)\\), then </p> \\[ |\\pmb{\\psi}_n(x)-\\pmb{\\eta}|\\leq M|I_0|,\\quad \\forall n \\] \\[ |\\pmb{\\psi}_n(x_1)-\\pmb{\\psi}_n(x_2)|\\leq M|x_1-x_2|,\\quad \\forall n \\] <p>Then by Ascoli-Arzel\u00e0 Theorem, we have a uniforly convergent subsequence \\(\\{\\pmb{\\phi}(x;\\xi_{n_j},\\pmb{\\eta}_{n_j})\\}\\) which converges to \\(\\pmb{\\psi}(x)\\) and they all satisfy</p> \\[ \\pmb{\\phi}(x;\\xi_{n_j},\\pmb{\\eta}_{n_j})=\\pmb{\\eta}_{n_j}+\\int_{\\xi_{n_j}}^{x}\\pmb{f}(s,\\pmb{\\phi}(s;\\xi_{n_j},\\pmb{\\eta}_{n_j}))ds \\] <p>let \\(j\\rightarrow \\infty\\) and we get </p> \\[ \\pmb{\\psi}(x)=\\pmb{y}_0+\\int_{x_0}^x\\pmb{f}(x_0,\\pmb{\\psi}(x))ds \\] <p>which means \\(\\pmb{\\psi}(x)\\equiv \\pmb{\\phi}(x;x_0,\\pmb{y}_0)\\). </p> <ul> <li>Use the subsequence of \\(\\{x_{n_j}\\}\\)</li> </ul> <p>from inequation \\(\\ref{ieq1}\\), we have</p> \\[ |\\pmb{\\psi}_{n_j}(x_{n_j})-\\pmb{\\psi}(x_{n_j})|\\geq \\varepsilon_0 \\] <p>let \\(j\\rightarrow \\infty\\), we deduce</p> \\[ |\\pmb{\\psi}(\\overline{x})-\\pmb{\\psi}(\\overline{x})|\\geq \\varepsilon_0 \\] <p>which contradicts the uniqueness!</p> <p>Here interval \\(I_0\\) is finite is necessary because in the proof we make use of the finite length of the interval. If a solution can satisfy the property on an infinite interval like \\((\\beta,\\infty)\\), then we call it Lyapunov stable.</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/#continuous-differentiability","title":"Continuous Differentiability","text":"<p>Similar to the above part, if \\(\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\in C^1(G)\\) in ODE \\(\\ref{eq1}\\), then we can say the solution is continuous differential on a region. To be more specific, we have the following theorem.</p> <p>Theorem for Continuous Differentiability</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda})\\in C(G)\\) where</p> \\[ G=\\{(x,\\pmb{y},\\pmb{\\lambda}):|x|\\leq a,|\\pmb{y}|\\leq b, |\\pmb{\\lambda}|\\leq c\\} \\] <p>and has continuous partial derivatives with respect to \\(\\pmb{y}\\) and \\(\\pmb{\\lambda}\\). Then the solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\lambda})\\) to ODE \\(\\ref{eq1}\\) is continuous differential on region \\(D\\), where </p> \\[ D=\\{|x|\\leq \\alpha, |\\pmb{\\lambda}|\\leq c\\} \\] Proof <p>Show that the partial derivatives can be the limit of divided difference.</p> <p>Corollary 1</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})\\in C(R)\\) where</p> \\[ R=\\{(x,y): |x-x_0|\\leq a, |\\pmb{y}-\\pmb{y}_0|\\leq b\\} \\] <p>and has partial derivative with respect to \\(\\pmb{y}\\), i.e. \\(\\pmb{f}'_{\\pmb{y}}(x,\\pmb{y})\\). Then for all \\(\\pmb{\\eta}\\), s.t. \\(|\\pmb{\\eta}-\\pmb{y}_0|&lt;\\frac{b}{2}\\), initial problem </p> \\[ \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{\\eta} \\] <p>has solution \\(\\pmb{y}=\\pmb{\\phi}(x,\\pmb{\\eta})\\) is conntinuous on region </p> \\[ D=\\left\\{(x,y): |x-x_0|\\leq \\frac{h}{2}, |\\pmb{\\eta}-\\pmb{y}_0|\\leq \\frac{b}{2}\\right\\} \\] <p>Corollary 2</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})\\in C(G)\\) were</p> \\[ G=\\{(x,y): |x-x_0|\\leq a, |\\pmb{y}-\\pmb{y}_0|\\leq b,|\\pmb{\\lambda}|\\leq c\\} \\] <p>and has partial derivative with respect to \\(\\pmb{y}\\) and \\(\\pmb{\\lambda}\\). Then initial problem</p> \\[ \\begin{align} \\frac{d\\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y},\\pmb{\\lambda}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0\\label{eq3} \\end{align} \\] <p>has unique solution \\(\\pmb{y}=\\pmb{\\phi}(x;x_0;\\pmb{y}_0,\\pmb{\\lambda})\\) which is differentiable for \\((x_0,\\pmb{y}_0,\\pmb{\\lambda})\\) on region</p> \\[ R=\\{|x-x_0|\\leq, |\\pmb{\\lambda}|\\leq c\\} \\]"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Continuous_Dependence/#solve-for-derivatives","title":"Solve for Derivatives","text":"<p>Assume \\(\\pmb{y}=\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\) is the solution to ODE \\(\\ref{eq3}\\), then it satisfies the integral equation</p> \\[ \\begin{equation} \\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})=\\pmb{y}_0+\\int_{x_0}^x \\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda})ds \\label{eq4} \\end{equation} \\] <p>Take partial derivative on both sides with respect to \\(x_0\\), \\(\\pmb{y}_0\\), \\(\\pmb{\\lambda}\\), we get</p> \\[ \\begin{align*} \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}&amp;= -\\pmb{f}(x_0,\\pmb{y}_0,\\pmb{\\lambda}) + \\int_{x_0}^x \\frac{\\partial }{\\partial \\pmb{y}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}&amp;=\\pmb{E}_n+ \\int_{x_0}^x \\frac{\\partial }{\\partial \\pmb{y}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}&amp;=\\int_{x_0}^x \\left(\\frac{\\partial }{\\partial \\pmb{y}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}+\\frac{\\partial }{\\partial \\pmb{\\lambda}}\\pmb{f}(s,\\pmb{\\phi}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\pmb{\\lambda}) \\right)ds \\end{align*} \\] <p>Or to be more concise:</p> \\[ \\begin{align*} \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}&amp;= -\\pmb{f}(x_0,\\pmb{y}_0,\\pmb{\\lambda}) + \\int_{x_0}^x \\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}}\\frac{\\partial \\pmb{\\phi}}{\\partial x_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}&amp;=\\pmb{E}_n+ \\int_{x_0}^x \\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}} \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}ds\\\\ \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}&amp;=\\int_{x_0}^x \\left(\\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}} \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}}+\\frac{\\partial }{\\partial \\pmb{\\lambda}}\\pmb{f} \\right)ds \\end{align*} \\] <p></p> <p>ODE for Derivatives</p> <p>If we let</p> \\[ \\begin{align*} \\pmb{u}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{\\phi}}{\\partial x_0}\\\\ \\pmb{V}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{y}_0}\\\\ \\pmb{W}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{\\phi}}{\\partial \\pmb{\\lambda}_0}\\\\ \\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{f}}{\\partial \\pmb{y}}(x,\\pmb{\\phi},\\pmb{\\lambda})\\\\ \\pmb{B}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}) &amp;= \\frac{\\partial \\pmb{f}}{\\partial \\pmb{\\lambda}}(x,\\pmb{\\phi},\\pmb{\\lambda})\\\\ \\end{align*} \\] <p>then \\(\\pmb{u}\\), \\(\\pmb{V}\\) and \\(\\pmb{W}\\) satisfy ODE with initial value respectively</p> \\[ \\begin{align*} \\frac{d\\pmb{u}}{dx}&amp;=\\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\pmb{u},\\quad \\pmb{u}(x_0)=-\\pmb{f}(x_0,\\pmb{y}_0,\\pmb{\\lambda})\\\\ \\frac{d\\pmb{V}}{dx}&amp;=\\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\pmb{V},\\quad\\pmb{V}(x_0)=\\pmb{E}_0\\\\ \\frac{d\\pmb{W}}{dx}&amp;=\\pmb{A}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda})\\pmb{W}+\\pmb{B}(x;x_0,\\pmb{y}_0,\\pmb{\\lambda}),\\quad \\pmb{W}(x_0)=0 \\end{align*} \\] <p>If \\(\\pmb{y}\\) and \\(\\pmb{\\lambda}\\) is one dimension, then the above ODE becomes much simpler. And we can use method from Elementary Integration Method.</p> <p>Q1. Assume \\(\\phi(x;x_0,y_0,\\mu)\\) is the solution to ODE</p> \\[ y'=y+\\mu(x^2+y^2),\\quad y|_{x=x_0}=y_0 \\] <p>Solve \\(\\frac{\\partial y}{\\partial \\mu}\\) when \\(x_0=0\\), \\(y_0=1\\), \\(\\mu=0\\).</p> Answer <p>Write the integral form</p> \\[ \\phi(x;x_0,y_0,\\mu)=y_0+\\int_{x_0}^x(\\phi(s;x_0,y_0,\\mu)+\\mu(s+\\phi(s;x_0,y_0,\\mu)^2))ds  \\] <p>we only show the dependence of variables:</p> \\[ \\phi(\\mu)=y_0+\\int_{x_0}^x(\\phi(\\mu)+\\mu(s+\\phi(\\mu)^2))ds  \\] <p>take derivative of \\(\\mu\\), we get</p> \\[ \\frac{\\partial \\phi}{\\partial \\mu}=\\int_{x_0}^x\\left(\\frac{\\partial\\phi}{\\partial \\mu}+s+\\phi(\\mu)^2+2\\phi\\mu\\frac{\\partial \\phi}{\\partial \\mu}\\right)ds \\] <p>So function \\(\\frac{\\partial\\phi}{\\partial \\mu}\\) satisfies ODE</p> \\[ \\begin{equation} u'=x+\\phi^2+(1-2\\mu\\phi)u, \\quad u(0)=0 \\label{mu} \\end{equation}\\] <p>For initial value, we can solve the solution to ODE </p> \\[ \\phi=e^x \\] <p>substitute initial value and solution into equation \\(\\ref{mu}\\) and we get</p> \\[ u'=x+e^{2x}+u,\\quad u(0)=0 \\] <p>so we get </p> \\[ \\begin{align*} u &amp;= e^x\\left[\\int_{0}^x(s+e^{2s})e^{-s}ds\\right]\\\\ &amp;=e^{2x}-x-1 \\end{align*} \\]"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/","title":"\u538b\u7f29\u6620\u5c04\u6cd5 | Proof using Contraction Mapping","text":"<p>Reference</p> <ul> <li> <p>Ordinary Differential Equation, Vladimir Igorevich Arnold</p> </li> <li> <p>\u5e38\u5fae\u5206\u65b9\u7a0b, \u041b.\u0421.\u5e9e\u7279\u91cc\u4e9a\u91d1</p> </li> </ul> <p>This part we want to prove Picard Theorem from another perspective. And this method is also universal on multi-dimensional Cauchy Problem. So to simplify the notation, we change the problem equivalently to </p> \\[ \\begin{equation} \\dot{\\mathbfit{x}} = \\mathbfit{f}(t, \\mathbfit{x}),\\quad  \\mathbfit{x}(t_0) = \\mathbfit{x}_0 \\label{eq-vector-cauchy} \\end{equation} \\] <p>where \\(\\mathbfit{x}\\in \\mathbb{R}^n\\) and \\(\\mathbfit{f}: \\mathbb{R}^{n+1} \\mapsto \\mathbb{R}^n \\in C(D)\\) is a vector function(or vector field). To simplify the problem, we assume \\(\\pmb{f} \\in C^r(D)\\), where \\(r\\geq 1\\).</p> <p>Assume we give a Euvlid Structure in region \\(D\\in \\mathbb{R}^{n+1}\\). For all \\((t_0,\\pmb{x}_0)\\in D\\), we consider a cylinder(\u67f1\u4f53) with sufficiently small parameters \\(a\\) and \\(b\\)</p> \\[ \\Gamma= \\{(t, \\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p>which is still a subset of \\(D\\).</p> <p>Then the Picard Theorem becomes:</p> <p>Picard Theorem</p> <p>Assume \\(\\pmb{f}\\) of problem \\(\\ref{eq-vector-cauchy}\\) is continous and differentiable(or Lipschitz condition) on region \\(\\Gamma\\), then for all given \\(\\pmb{x}\\) which is sufficiently close to \\(\\pmb{x}_0\\), there exsits a neighberhood of \\(t_0\\), such that there exists a unique solution \\(\\pmb{\\varphi}(t)\\).</p> Hints <ul> <li>Using the fixed point theorem (existence and uniqueness).</li> </ul>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#group-basis","title":"\u7fa4\u8bba\u57fa\u7840 | Group Basis","text":"<p>Firstly, let us introduce some basic ideas about groups. </p> <p>\u7fa4\u7684\u5b9a\u4e49 | Definition of Group</p> <p>Firstly, we have to define Law of Composition.</p> <p>Assume \\(S\\) is a set. A Law of Composition is a map </p> \\[ S\\times S \\mapsto S. \\] <p>\\(S\\times S\\) deontes the product set, whose elements are pairs \\(a\\), \\(b\\) of elements of \\(S\\).</p> <p>A group is a set \\(G\\) together with a law of composition(Here we use sign \\(\\cdot\\) (multiplicative notation), and sign \\(+\\) (additive notation) also can be applied) that has the following properties:</p> <ul> <li>The law of composition is associative.</li> </ul> \\[ (ab)c=a(bc), \\quad \\forall a,b,c\\in G \\] <ul> <li>\\(G\\) contains an identity element \\(1\\) (\\(0\\) in sign \\(+\\)) such that </li> </ul> \\[ 1a=a \\text{ and } a1=a,\\quad \\forall a\\in G \\] <ul> <li>Every element \\(a\\) of \\(G\\) has an inverse, i.e. an element \\(b\\) such that </li> </ul> \\[ ab=1 \\text{ and } ba=1 \\] <p>which is denoted by \\(a^{-1}\\) (\\(-a\\) in additive notation).</p> <p>\u7fa4\u7684\u6027\u8d28 | Properties of Group</p> <ul> <li>Cancellation Law</li> </ul> <p>Let \\(a\\), \\(b\\), \\(c\\) be elements of a group \\(G\\) whose law of composition is written multiplicatively. If </p> \\[ ab=ac \\text{ or } ba = ca \\Rightarrow b=c. \\] <p>If </p> \\[ ab=a \\text{ or } ba =a \\Rightarrow b=1. \\] Proof <p>Multiply both sides of the above equation on the left by \\(a^{-1}\\).</p> <p>Example.</p> <p>\\(n\\times n\\) general liear group is the group of all invertible \\(n\\times n\\) matrices, denoted by</p> \\[ GL_{n} =\\{n\\times n \\text{ invertible matrices } A\\}. \\] <p>where the law of composition is matrix multiplication.</p> <p>\u540c\u6001 | Homomorphism</p> <p>Let \\(G\\) and \\(G'\\) be groups written with multiplicative notation. A Homomorphism \\(\\varphi:G\\mapsto G'\\) is a map from \\(G\\) to \\(G'\\) such that </p> \\[ \\varphi(ab) = \\varphi(a)\\varphi(b), \\quad \\forall a,b\\in G \\] <p>If we let \\(\\varphi\\) to be a bijection, then we call it Isomorphism(\u540c\u6784).</p> <p>Example.</p> <p>the determinent function det: \\(GL_n(\\mathbb{R})\\mapsto \\mathbb{R}^\\times\\)</p> <p>\u540c\u6001\u7684\u6027\u8d28 | Properties of Homomorphism</p> <p>Let \\(\\varphi: G\\mapsto G'\\) be a group homomorphism.</p> <p>(i) If \\(a_1,a_2,\\cdots, a_k\\) are elements of \\(G\\), then </p> \\[ \\varphi(a_1 a_2\\cdots a_k)=\\varphi(a_1)\\varphi(a_2)\\cdots\\varphi(a_n) \\] <p>(ii) \\(\\varphi\\) maps the identity to identity, i.e. </p> \\[ \\varphi(1_G)=1_{G'} \\] <p>(iii) \\(\\varphi\\) maps inverses to inverses, i.e.</p> \\[ \\varphi(a^{-1}) = \\varphi(a)^{-1} \\] Proof <p>(i) by induction(Strong induction).</p> <p>(ii) using \\(1\\cdot 1=1\\) and \\(\\varphi(1)\\varphi(1)=\\varphi(1\\cdot 1)=\\varphi(1)\\), cancel both sides to obtain and get \\(\\varphi(1)=1_{G'}\\).</p> <p>(iii) similarly, \\(a^{-1}a=1\\) and \\(\\varphi(a^{-1})\\varphi(a)=\\varphi(a^{-1}a)=\\varphi(1_G)=1_{G'}\\), and we are done.</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#phase-space","title":"\u76f8\u7a7a\u95f4 | Phase Space","text":"<p>Then we have to introduce some comception about phase.</p> <p>\u5355\u53c2\u6570\u53d8\u6362\u7fa4 | One-Parameter Group of Transformation</p> <p>\\(M\\) is a set. A family of mapping \\(\\{g^t\\}_{t\\in \\mathbb{R}}\\) which maps set \\(M\\) into itself is called One-Parameter Group of Transformation of set \\(M\\), if </p> \\[ g^{t+s} = g^tg^s, \\quad \\forall t,s\\in \\mathbb{R} \\] <p>and \\(g^0\\) is identity mapping(\u6052\u7b49\u6620\u5c04).</p> <p>\u76f8\u6d41\u3001\u76f8\u7a7a\u95f4\u3001\u8fd0\u52a8\u3001\u76f8\u66f2\u7ebf\u7684\u5b9a\u4e49 | Definition of Phase Flow</p> <p>A couple composed of set \\(M\\) and its one-parameter group of transformation \\(\\{g^t\\}\\), denoted as \\((M, \\{g^t\\})\\), is called Phase Flow. And here \\(M\\) is called the phase space of the phase flow. The element of \\(M\\) is called Phase Point.</p> <p>Consider a mapping </p> \\[ \\begin{equation} \\varphi: \\mathbb{R}\\mapsto M, \\quad \\varphi(t) = g^tx,\\quad x\\in M \\label{map-motion} \\end{equation} \\] <p>which maps a real straight line into phase space. Then it is called a motion of phase point \\(x\\) under the action of phase flow. </p> <p>The image of \\(\\mathbb{R}\\) under mapping \\(\\varphi\\) is called the phase curve of phase flow \\((M,\\{g^t\\})\\).</p> <ul> <li>Fixed Point</li> </ul> <p>If the phase curve of a phase point \\(x\\in M\\) is itself, i.e.</p> \\[ g^tx=x,\\quad \\forall t\\in \\mathbb{R} \\] <p>then \\(x\\) is called the fixed point of the phase flow \\((M, \\{g^t\\})\\).</p> <p>In fact one-parameter group of transformation is exchangeable(\\(g^tg^s=g^{t+s}=g^{s+t}=g^sg^t\\)). And it is also a bijection. This is easy to prove. </p> <p>Firstly we prove it is surjection. \\(\\forall x\\in M\\), \\(\\exists g^{-t}x \\in M\\), such that \\(g^t(g^{-t}x)=x\\). Then we prove it is a injective mapping. \\(g^tx=g^ty\\), then \\(x=g^0x=g^{-t}g^tx=g^{-t}g^ty=g^0y=y\\).</p> <p>With the above property we can easily see the following theorem.</p> <p>\u76f8\u7a7a\u95f4\u4e2d\u7684\u70b9\u4ec5\u6709\u4e00\u6761\u76f8\u66f2\u7ebf</p> <p>For all \\(x\\in M\\), there only exists one phase curve.</p> Proof <p>Because \\(g^t\\) is a bijection.</p> <p>Now we introduce two important conceptions.</p> <p>\u6620\u5c04\u7684\u56fe\u5f62 | Graph of a mapping</p> <p>The graph of a mapping \\(f : A\\mapsto B\\) is a subset of the direct product(\u76f4\u79ef) \\(A\\times B\\):</p> \\[ \\{(a,f(a))| a\\in A\\} \\] <p>\u6269\u5f20\u76f8\u7a7a\u95f4\u3001\u79ef\u5206\u66f2\u7ebf | Expanded Phase Space, Integral Curve</p> <p>The Expanded Phase Space of phase flow \\((M, \\{g^t\\})\\) is the direct product \\(\\mathbb{R} \\times M\\).</p> <p>The Integral Curve of phase flow \\((M, \\{g^t\\})\\) is the graph of the motion \\(\\ref{map-motion}\\).</p> <p>Now we have to make use of DIfferential in Euclid Space.</p> <p>\u53ef\u5fae\u51fd\u6570\u3001\u53ef\u5fae\u6620\u5c04\u3001\u5fae\u5206\u540c\u80da | Differentiable Function, Differentiable Mapping, Diffeomorphism</p> <p>Assume \\(U \\subset\\mathbb{R}^n, V \\subset\\mathbb{R}^m\\). Then</p> <p>A Differentiable Function is a function \\(f: U\\mapsto \\mathbb{R}\\) which is \\(r\\) times differentiable.</p> <p>A Differentiable Mapping is a mapping \\(f:U\\mapsto V\\) defined by </p> \\[ y_i = f_i(x_1,x_2,\\cdots,x_n), \\quad i=1,2\\cdots, m \\] <p>where \\(f_i: U\\mapsto \\mathbb{R}\\) is a Differentiable function. If \\(y_i: V\\mapsto \\mathbb{R}\\) is a coordinate of \\(\\pmb{y}\\in \\mathbb{R}^m\\), then \\(y_i\\circ f: U\\mapsto \\mathbb{R}\\) is a Differentiable function in \\(U\\).</p> <p>A Diffeomorphism is a bijection \\(f:U\\mapsto V\\), such that \\(f\\) and \\(f^{-1}\\) are both Differentiable mappings.</p> <p>\u4e0e\u5750\u6807\u8f74\u6709\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>The Phase Speed \\(\\pmb{v}(x)\\) of phase flow \\(g^t\\) at point \\(x\\in M\\) is </p> \\[ \\pmb{v}(x) = \\frac{d}{dt}\\Bigg|_{t=0}g^tx \\] <p>And at time \\(\\tau\\), we have phase speed </p> \\[ \\pmb{v}(g^\\tau x) = \\frac{d}{dt}\\Bigg|_{t=\\tau}g^tx. \\] <p>Now we let \\(M\\) to be a region in Euclid Space \\(\\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots,x_n\\). And if \\(x_i:M\\mapsto \\mathbb{R}\\) is the coordinate of \\(\\pmb{x}\\in M\\), then the vector \\(\\pmb{v}(x)\\) is defined by \\(v_i: M\\mapsto \\mathbb{R}, i=1,2\\cdots, n\\):</p> \\[ v_i(\\pmb{x}) = \\frac{d}{dt}\\Bigg|_{t=0}x_i(g^t\\pmb{x}) \\] <p>So we define a Vector Field \\(\\pmb{v}\\) on \\(M\\) when neglecting \\(t\\).</p> <p>\u4e0e\u5750\u6807\u8f74\u65e0\u5173\u7684\u76f8\u901f\u5ea6\u3001\u5411\u91cf\u573a | Phase Speed, Vector Field</p> <p>Firstly, we have to define that a coordinate \\(\\{y_i\\}_{i=1}^{n}:U\\mapsto \\mathbb{R}\\) is admissive, if mapping</p> \\[ y:U\\mapsto \\mathbb{R}^n,\\quad y(\\pmb{x}) = y_1(\\pmb{x})\\pmb{e}_1 + y_2(\\pmb{x})\\pmb{e}_2 + \\cdots +y_n(\\pmb{x})\\pmb{e}_n \\] <p>is a diffeomorphism.</p> <p>So we have a proposition:</p> <p>Two curves \\(\\varphi_1\\), \\(\\varphi_2\\) that pass \\(\\pmb{x}\\in U\\) are tangent with each other if and only if two curves \\(y\\circ \\varphi_1\\), \\(y\\circ\\varphi_2\\) that pass \\(y(\\pmb{x})\\in V\\) are tangent with each other.</p> <p><p> </p></p> <p>So the velocity vector of curve \\(\\varphi:I\\mapsto U\\) that pass \\(\\pmb{x}\\in U\\) is </p> \\[ \\pmb{v}=\\dot{\\varphi}(0), \\quad \\pmb{v}=\\frac{d\\varphi}{dt}\\Bigg|_{t=0}. \\] <p>\u5207\u5411\u91cf\u3001\u5207\u7a7a\u95f4 | Tangent Vector, Tangent Space</p> <p>Assume \\(U\\in \\mathbb{R}^n\\) with coordinates \\(x_1,x_2,\\cdots, n\\) and map \\(\\varphi:I\\mapsto U\\) maps an interval on \\(\\mathbb{R}\\) to \\(U\\) such that \\(\\varphi(0)=\\pmb{x}\\in U\\). and also its velocity is determined by </p> \\[ v_i = \\frac{d}{dt}\\Bigg|_{t=0}(x_i\\circ \\varphi), i=1,2,\\cdots, n. \\] <p>Two curves \\(\\varphi_1, \\varphi_2:I\\mapsto U\\) pass the same point \\(\\pmb{x}\\) are tangent with each other if \\(t\\rightarrow 0\\), \\(\\rho(\\varphi_1, \\varphi_2)\\rightarrow 0\\).</p> <p><p> </p></p> <p>A set composed by all tangent vectors of the curves passing \\(\\pmb{x}\\) is a linear space with dimension \\(n\\), which is called Tangent Space, denoted by \\(TU_x\\).</p> <p>Now we want to give a space without coordinates.</p> <p>\u6620\u5c04\u7684\u5bfc\u6570 | Derivative of Mapping</p> <p>Assume that \\(f:U\\mapsto V\\) is a differentiable mapping from \\(\\pmb{x}=(x_1,x_2,\\cdots,x_n)\\) to \\(\\pmb{y}=(y_1,y_2,\\cdots,y_m)\\). Let \\(\\pmb{y}=f(\\pmb{x})\\in V\\).</p> <p>The Derivative of mapping \\(f\\) at \\(\\pmb{x}\\) is a mapping from tangent space at \\(\\pmb{x}\\in U\\) to tangent space at \\(\\pmb{y}\\in V\\)</p> \\[ f_*|_{\\pmb{x}}: TU_{\\pmb{x}} \\mapsto TV|_{f(\\pmb{x})} \\] <p>This mapping maps velocity vector of curve \\(\\varphi\\) at \\(\\pmb{x}\\) into velocity vector of \\(f\\circ \\varphi\\) at \\(f(\\pmb{x})\\), i.e.</p> \\[ f_*|_{\\pmb{x}}\\left(\\frac{d\\varphi}{dt}\\Bigg|_{t=0} \\right) = \\frac{d}{dt}\\Bigg|_{t=0}(f\\circ \\varphi). \\] <p>which defines a linear mapping from \\(TU_{\\pmb{x}}\\) to \\(TV_{f(\\pmb{x})}\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#contraction-mapping-in-metric-space","title":"\u5ea6\u91cf\u7a7a\u95f4\u4e0b\u7684\u538b\u7f29\u6620\u5c04 | Contraction Mapping in Metric Space","text":"<p>Then, let us recall the definition of metric space. Now we can rewrite the Lipschitz Condition with terms of metric space.</p> <p>Lipschitz \u6761\u4ef6 | Lipschitz Condition</p> <p>Assume \\(A: M_1 \\mapsto M_2\\) is a mapping from a metric space \\(M_1\\) (with metric \\(\\rho_1\\)) to another matric space \\(M_2\\) (with metric \\(\\rho_1\\)). If there exists a constant \\(L&gt;0\\), s.t.</p> \\[ \\rho_2(Ax,Ay)\\leq L\\rho_1(x,y),\\quad \\forall x,y \\in M_1 \\] <p>then we say mapping \\(A\\) satisfies Lipschitz Condition.</p> <p>In Proof 1 we will show how to use Lipschitz condition to shrink the region of \\((t, \\pmb{x})\\)(Expanded Phase Space) such that the contraction mapping maps the metric space into itself.</p> <p>Usually, a mapping that maps a space \\(M\\) into itself would be of great use for us, so here comes the following definition.</p> <p>\u538b\u7f29\u6620\u5c04\u7684\u5b9a\u4e49 | Definition of Contraction mapping</p> <p>Assume \\(A: M \\mapsto M\\) is a mapping from complete metric space with metric \\(\\rho\\) to itself. If there exists a constant \\(k\\), \\(0&lt;k&lt;1\\), s.t. </p> \\[ \\rho(Ax,Ay)\\leq k\\rho(x, y),\\quad \\forall x, y\\in M. \\] <p>then \\(A\\) is called a Contraction mapping.</p> <p>If \\(Ax =x \\in M\\), we call \\(x\\) is a fixed point of mapping \\(A\\). Then the following theorem is quite useful in our future proof. </p> <p>Banach\u4e0d\u52a8\u70b9\u5b9a\u7406 | Banach Fixed-Point Theorem</p> <p>(This also known as contraction mapping theorem.)</p> <p>Assume \\(A: M\\mapsto M\\) is a contraction mapping, then \\(A\\) has a unique fixed-point.</p> HintsProof <p>We prove by showing that \\(A^nx\\) is a Cauchy Sequence and its limit \\(X\\) falls in \\(M\\). Then use the property of limits to show it is fixed-point of \\(A\\). Then prove the uniqueness by using property of metric \\(\\rho\\).</p> <p>Let \\(d = \\rho(x,Ax)\\), then </p> \\[ \\rho(A^nx,A^{n+1}x)\\leq k^n\\rho(x,Ax) = k^nd \\] <p>Sequence \\(A^nx\\), \\(n=1,2,\\cdots\\) is convergent. So its limit exists and let \\(X \\overset{\\Delta}{=} \\lim\\limits_{n\\rightarrow \\infty}A^nx\\).</p> <p>So </p> \\[ AX = A \\lim\\limits_{n\\rightarrow \\infty}A^nx = \\lim\\limits_{n\\rightarrow \\infty}A^{n+1}x = X \\] <p>which means \\(X\\) is a fixed-point of \\(A\\). Then we prove the uniqueness of fixed point. Assume there are two fixed points of \\(A\\), denoted as \\(X\\), \\(Y\\). See that</p> \\[ \\rho(X, Y)  =\\rho(AX, AY) \\leq k\\rho(X,Y) \\] <p>Because \\(0&lt;k&lt;1\\), so \\(\\rho(X,Y)=0\\) \\(\\Rightarrow\\) \\(X=Y\\).</p> <p>Readers can compare this theorem to Fixed-Point Theorem in Numerical Analysis. </p> <p>Note that we have an abstract set \\(M\\) with abstract metric \\(\\rho\\). In the next two parts, we will let \\(M\\) to be a set of mappings and \\(\\rho\\) to be defined based on natural norm of vectors. </p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#Proof-1","title":"\u8bc1\u660e1 | Proof 1","text":"<p>Now we consider Picard mapping \\(A\\) is a mapping from a mapping to another mapping</p> \\[ A: (\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\mapsto (A\\pmb{\\varphi}: t\\mapsto \\pmb{x}) \\] <p>defined by</p> \\[ (A\\pmb{\\varphi})(t) = \\pmb{x}_0 + \\int_{t_0}^{t}\\pmb{f}(\\tau, \\pmb{\\varphi}(\\tau))d\\tau \\] <p>Geometrically speaking, tangent line of each point on \\(A\\pmb{\\varphi}\\), i.e. \\((A\\pmb{\\varphi})'(t)\\), equals to the vector field determined by \\(\\pmb{\\varphi}(t)\\), i.e. \\(f(t, \\pmb{\\varphi}(t))\\). This can help us find a smaller region for \\(A\\pmb{\\varphi}\\) to be well-defined.</p> <p>And note that \\(\\pmb{\\varphi}\\) is a solution of Cauchy Problem \\(\\ref{eq-vector-cauchy}\\) if and only if \\(\\pmb{\\varphi} = A \\pmb{\\varphi}\\). So we have to prove \\(A\\) is a contraction mapping in a complete metric space.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\) | Some Constants \\(a\\), \\(b\\), \\(M\\)</li> </ul> <p>Let </p> \\[ M=\\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}\\|, \\quad L= \\max_{\\pmb{x}\\in \\Gamma}\\|\\pmb{f}_*\\| \\text{(or by Lipschitz condition)} \\] <p>which can be obtained because \\(\\Gamma\\) is a compact set. Firstly, we consider a Cylinder</p> \\[ \\Gamma = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq b\\} \\] <p><p> </p></p> <p>Now consider a Cone(\u9525\u4f53) with opening \\(M\\) and sufficient small height \\(a\\) at point \\((t_0,\\pmb{x}_0)\\)</p> \\[ K_0 = \\{(t,\\pmb{x}): |t-t_0|\\leq a, \\|\\pmb{x}-\\pmb{x}_0\\|\\leq M|t-t_0|\\} \\] <p>Which means our mapping \\(\\pmb{\\varphi}\\) after contraction mapping \\(A\\) is still well-defined. See details in the following proof.</p> Proof \\[ \\begin{align*} \\|A\\pmb{\\varphi}(t) - \\pmb{x}_0\\| &amp;=\\left\\|\\int_{t_0}^t\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))d\\tau\\right\\| \\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau,\\pmb{\\varphi}(\\tau))\\|d\\tau\\right|\\\\ &amp;\\leq M |t-t_0| \\end{align*} \\] <p>So as long as \\(a&lt;b/M\\), \\(\\|A\\pmb{\\varphi}(t)-\\pmb{x}_0\\|\\leq b\\).</p> <ul> <li>Prove \\(\\pmb{\\varphi}\\) compose a complete metric space.</li> </ul> <p>We define a Space \\(M\\) composed of mappings \\(\\pmb{\\varphi}: \\mathbb{R}\\mapsto \\mathbb{R}^n\\) defined by \\(\\pmb{\\varphi}(t)=\\pmb{x}\\).</p> <p>Define a metric \\(\\rho\\) to be</p> \\[ \\rho(\\pmb{\\varphi}_1, \\pmb{\\varphi}_2) = \\|\\pmb{\\varphi}_1-\\pmb{\\varphi}_2\\| = \\max_{|t-t_0|\\leq a}|\\pmb{\\varphi}_1(t)-\\pmb{\\varphi}_2(t)| \\] <p>Then \\(\\{M, \\rho\\}\\) is a complete metric space.</p> <ul> <li>Prove \\(A\\) is a contraction mapping.</li> </ul> Proof <p>That is, \\(\\forall \\pmb{\\varphi}_1, \\pmb{\\varphi}_2 \\in M\\)</p> \\[ \\begin{align*} \\|A\\pmb{\\varphi}_1- A\\pmb{\\varphi}_2\\| &amp;= \\left\\|\\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))]d\\tau\\right\\|\\\\ &amp;\\leq \\left|\\int_{t_0}^t\\|\\pmb{f}(\\tau, \\pmb{\\varphi}_1(\\tau))-\\pmb{f}(\\tau, \\pmb{\\varphi}_2(\\tau))\\|d\\tau \\right|\\\\ &amp;\\leq L \\int_{t_0}^t \\|\\pmb{\\varphi}_1(\\tau) - \\pmb{\\varphi}_2(\\tau)\\|d\\tau \\\\ &amp;\\leq L \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\| \\int_{t_0}^t d\\tau = L |t-t_0| \\|\\pmb{\\varphi}_1 - \\pmb{\\varphi}_2\\|  \\end{align*} \\] <p>So as long as \\(a&lt;1/L\\), \\(A\\) is a contraction mapping.</p> <p>By Banach Fixed Point Theorem, there exists only one soluion of ODE.</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#2-proof-2","title":"\u8bc1\u660e2 | Proof 2","text":"<p>In this part, we consider a different mapping.</p> <ul> <li>\u5b9a\u4e49\u57df\\((t, \\pmb{x})\\)</li> </ul> <p>In order to let all the possible cone to be in the cylinder, we have to shrink the region to make the region is well defined. That is, there exsits sufficiently small parameters \\(b'&lt;b\\) such that \\(\\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\). So now we get a smaller region (a smaller cylinder)</p> \\[ \\Gamma' = \\{(t, \\pmb{x}): |t-t_0|&lt;a', \\|\\pmb{x}-\\pmb{x}_0\\|&lt;b'\\} \\] <p><p> </p></p> <p>We also have to prove that \\(A\\) maps \\(M\\) into itself. That is, \\(\\|\\varphi(t)-\\pmb{x}_0\\|\\leq b'\\). Here we have to make \\(a'\\) to satisfy some condition.</p> <p>Consider all the possible continuous mapping \\(\\pmb{h}\\) which maps the above cylinder into \\(\\mathbb{R}^n\\), which is a solution of original ODE problem. Assume \\(M\\) is a set composed of these mappings with additional condition</p> \\[ \\|\\pmb{h}(t, \\pmb{x})\\| \\leq C|t-t_0| \\] <p>Specially, we let \\(\\pmb{h}(t_0,\\pmb{x})=0\\). We introduce a metric </p> \\[ \\rho(\\pmb{h}_1,\\pmb{h}_2)=\\|\\pmb{h}_1-\\pmb{h}_2\\|=\\max_{(t,\\pmb{x})\\in \\Gamma' }\\|\\pmb{h}_1(t,\\pmb{x})-\\pmb{h}_2(t,\\pmb{x})\\| \\] <p> </p> <p>Note: Space \\(M\\) is dependent on \\(a'\\), \\(b'\\) and \\(C\\).</p> <p>Now we really have to consider a mapping </p> \\[ A: M\\mapsto M \\] <p>defined by</p> \\[ (A\\pmb{h})(t, \\pmb{x}) = \\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau, \\pmb{x}))d\\tau \\] <p>\\(A\\) \u662f\u538b\u7f29\u6620\u5c04 | \\(A\\) is a contraction mapping</p> <p>\\(A\\) is a contraction mapping from \\(M\\) to \\(M\\), if \\(a'\\) is sufficiently small.</p> <p>Prove it. The following proof are set to find how small \\(a'\\) shoulc be.</p> HintsProof <p>Firstly, prove \\(A\\) maps \\(M\\) to itself. Then Prove if is a constraction mapping.</p> <ul> <li>\\(A\\) maps \\(M\\) to itself.</li> </ul> \\[ \\begin{align*} \\|(A\\pmb{h}(t, \\pmb{x}))\\| &amp;\\leq \\|\\int_{t_0}^t\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}(\\tau,\\pmb{x}))d\\tau\\| \\\\ &amp;\\leq |\\int_{t_0}^tCd\\tau| \\leq C|t-t_0| \\end{align*} \\] <p>so \\(AM\\subset M\\).</p> <ul> <li>\\(A\\) is a constraction mapping.</li> </ul> <p>Estimate \\(A\\pmb{h}_1- A\\pmb{h}_2\\):</p> \\[ (A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x}) = \\int_{t_0}^t[\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_1(\\tau,\\pmb{x}))-\\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_2(\\tau,\\pmb{x}))]d\\tau \\] <p>Denote \\(\\pmb{f}_i(\\tau) = \\pmb{f}(\\tau, \\pmb{x}+\\pmb{h}_i(\\tau,\\pmb{x}))\\), \\(i=1,2\\), so</p> \\[ \\begin{align*} \\|\\pmb{f}_1(\\tau)- \\pmb{f}_2(\\tau)\\| &amp;\\leq L\\|\\pmb{h}_1(\\tau)-\\pmb{h}_2(\\tau)\\| \\quad\\text{(using Lipschitz condition)}\\\\  &amp;\\leq L\\|\\pmb{h}_1-\\pmb{h}_2\\| = L\\rho(\\pmb{h}_1, \\pmb{h}_2) \\quad \\text{(by definition of metric)} \\end{align*} \\] <p>And </p> \\[ \\|(A\\pmb{h}_1-A\\pmb{h}_2)(t,\\pmb{x})\\| \\leq L a'\\rho(\\pmb{h}_1, \\pmb{h}_2) \\] <p>So if \\(La'&lt;1\\), then \\(A\\) is a contraction mapping.</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Contraction_Mapping_Method/#lipschitz-appendix-from-continuity-differentiability-to-lipschitz-condition","title":"\u9644\u5f55\uff1a\u8fde\u7eed\u53ef\u5fae\u5230Lipschitz\u8fde\u7eed | Appendix: from Continuity &amp; Differentiability to Lipschitz Condition","text":"<p>we will choose natural metric </p> \\[ \\rho(\\pmb{x}, \\pmb{y}) = \\|\\pmb{x}-\\pmb{y}\\| = \\sqrt{(\\pmb{x}-\\pmb{y},\\pmb{x}-\\pmb{y})} \\] <p>so space \\(\\mathbb{R}^n\\) with the above metric is a complete metric space.</p> <p>So in this case, we have a parallel theorem for smoothness and Lipschitz condition in \\(\\mathbb{R}\\).</p> <p>\u8fde\u7eed\u53ef\u5fae\u6620\u5c04\u6ee1\u8db3Lipschitz\u6761\u4ef6 | Continuously Differentiable mapping satisfies Lipschitz condition</p> <p>Assume \\(V\\subset U \\subset\\mathbb{R}^m\\) is a convex and contract set, and continuously differentiable mapping \\(\\pmb{f}\\) which maps \\(U\\) to \\(\\mathbb{R}^n\\) satisfies Lipschitz condition, and the constant </p> \\[ L = \\sup_{\\pmb{x}\\in V} \\|\\pmb{f}_{*\\pmb{x}}\\|  \\] <p>where \\(\\pmb{f}_*|_{\\pmb{x}}=\\pmb{f}_{*\\pmb{x}}:T\\mathbb{R}^m_{\\pmb{x}}\\mapsto T\\mathbb{R}^n_{\\pmb{x}}\\)</p> <p><p> </p></p> <p>Prove it.</p> Hints <p>Let \\(\\pmb{z}(t)=\\pmb{x}+t(\\pmb{y}-\\pmb{x})\\), \\(0\\leq t\\leq 1\\). Then</p> \\[ \\begin{align*} \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x})&amp;=\\int_{0}^1\\frac{d}{dt}\\pmb{f}(\\pmb{z}(\\tau))d\\tau\\\\ &amp;=\\int_0^1\\pmb{f}_{*\\pmb{z}(\\tau)}[\\pmb{\\dot{z}}(\\tau)]d\\tau \\\\ &amp;=\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})d\\tau \\end{align*}\\\\ \\] <p>where \\(\\pmb{y}-\\pmb{x}\\) is a constant, so</p> \\[ \\begin{align*} \\left\\| \\pmb{f}(\\pmb{y})-\\pmb{f}(\\pmb{x}) \\right\\| &amp;=\\left\\|\\int_0^1 \\pmb{f}_{*\\pmb{z}(\\tau)} (\\pmb{y}-\\pmb{x})(\\tau)d\\tau\\right\\|\\\\ &amp;\\leq \\int_0^1 \\|\\pmb{f}_{*\\pmb{x}}\\| \\|\\pmb{y}-\\pmb{x}\\|d\\tau\\\\ &amp;\\leq L\\|\\pmb{y}-\\pmb{x}\\| \\end{align*} \\] <p>and we are done.</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/","title":"Existence & Uniqueness Theorem","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Existence-and-Uniqueness-Theorem","title":"\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Existence and Uniqueness Theorem","text":"<p>This chapter we focus on ODE with initial value problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>The above problem is also called Cauchy Problem, for Cauchy firstly prove that the solution to the problem \\(\\ref{eq-cauchy}\\) exists uniquely when \\(f(x, y)\\) has continuous partial derivative to \\(y\\), that is, \\(\\frac{\\partial f}{\\partial y} \\in C(G)\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#preliminary-knowledge","title":"\u9884\u5907\u77e5\u8bc6 | Preliminary knowledge","text":"<p>The following inequation is really useful in estimating solution of ODE by offering the upper bound.</p> <p>Gronwall \u4e0d\u7b49\u5f0f | Gronwall Inequation</p> <p>Assume \\(\\alpha(x), u(x) \\in C[a, b]\\) are non-negative functions and \\(C, K\\) are non-negative constants. If </p> \\[ u(x) \\leq C + \\int_{a}^{x} \\left[ \\alpha(s) u(s)+K\\right] ds \\] <p>then </p> \\[ \\begin{align*} u(x) &amp;\\leq \\left[ C + \\int_{a}^{x}Ke^{-\\int_{a}^{s}\\alpha(t)dt}ds\\right]e^{\\int_{a}^{x}\\alpha(s)ds}\\\\ &amp;\\leq \\left[ C + K(x-a)\\right]e^{\\int_{a}^{x}\\alpha(s)ds} \\end{align*} \\] <p>Prove it.</p> Hints <p>Convert it into Ordinary Differential Inequation. Then solve it like what you do in solving ODE.</p> <p>\u4e00\u81f4\u6709\u754c\u7684\u5b9a\u4e49 | Definition of Uniform Bound</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Uniformly Bounded, if \\(\\exists M&gt;0\\), s.t.</p> \\[ |f_\\lambda(x)|\\leq M, \\quad \\forall \\lambda\\in \\Lambda, \\forall x \\in I \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are all bounded by some constant \\(M\\).</p> <p>\u7b49\u5ea6\u8fde\u7eed\u7684\u5b9a\u4e49 | Definition of Equicontinuous</p> <p>Assume \\(\\Lambda\\) is an infinite set, set \\(I\\)(or interval \\([x, b]\\)) \\(\\subset \\mathbb{R}\\). A family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I\\) is Equicontinuous, if \\(\\forall \\varepsilon&gt;0, \\exists \\delta&gt;0\\), s.t. \\(\\forall x_1, x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\)</p> \\[ |f_\\lambda(x_1)-f_\\lambda(x_2)|&lt; \\varepsilon \\] <p>The above definition means that functions in \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are continuous equally. Usually \\(\\delta&gt;0\\) depends on \\(f_\\lambda\\), but here we mean the degree of continuity of these functions is similar.</p> <p>Example. Function sequence \\(\\{f_n(x)\\}\\) satisfying</p> \\[ f_n(x) = (-1)^{n} + x^n \\] <p>is uniformly bounded and equicontinuous on region \\(\\{x: |x|\\leq 1/2\\}\\), is uniformly bounded but not equicontinuous on region \\(\\{x: |x|\\leq 1\\}\\), and is neither uniformly bounded nor equicontinuous on region \\(\\{x: |x|\\leq 2\\}\\).</p> <p>The condition of the following theorem can be narrowed down to denumerable sets \\(\\Lambda\\) and interval \\(R \\subset \\mathbb{R}\\).</p> <p></p> <p>\u5f15\u74061: \u4e00\u81f4\u6709\u754c\u7684\u51fd\u6570\u5217\u6709\u6536\u655b\u51fd\u6570\u5b50\u5217(\u70b9\u6001) | Lemma 1: Uniformly Bounded Function Sequence has convergent Function Subsequence</p> <p>Assume set \\(\\Lambda\\) is denumerable. If a family of function \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) defined on \\(I \\subset \\mathbb{R}\\) is uniformly bounded, then \\(\\forall A = \\{x_m\\}_{m=1}^{\\infty} \\subset I\\), \\(\\exists \\{f_{\\lambda_k}\\}_{k=1}^\\infty\\), a function subsequence of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\), such that </p> \\[ \\{f_{\\lambda_k}(x)\\}, \\forall x \\in A \\] <p>is convergent.</p> <p>Prove it.</p> HintsProof <p>Using diagonal methods. </p> <p>Note that \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\) is obviously bounded. So by Weierstrass Balzano Theorem, we get a convergent subsequence </p> \\[ f_{11}(x_1), f_{12}(x_1), \\cdots, f_{1n}(x_1) \\cdots \\] <p>which converges to \\(y_1\\).</p> <p>Now consider substitute \\(x_1\\) by \\(x_2\\), which becomes</p> \\[ f_{11}(x_2), f_{12}(x_2), \\cdots, f_{1n}(x_2) \\cdots \\] <p>According to the condition \"Uniformly Bounded\", the above sequence is also bounded, so we can find another convergent subsequence from it</p> \\[ f_{21}(x_2), f_{22}(x_2), \\cdots, f_{2n}(x_2) \\cdots \\] <p>which converges to \\(y_2\\).</p> <p>So we can say function subsequence of \\(\\{f_\\lambda(x_1)\\}_{\\lambda \\in \\Lambda}\\)</p> \\[ f_{21}(x), f_{22}(x), \\cdots, f_{2n}(x) \\cdots \\] <p>is convergent on \\(x\\in \\{x_1,x_2\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{2n}(x_2) = y_2\\).</p> <p>Continue the above procedure, we can get another function subsequnce</p> \\[ f_{31}(x), f_{32}(x), \\cdots, f_{3n}(x) \\cdots \\] <p>which is convergent on \\(x \\in \\{x_1,x_2,x_3\\}\\). That is, \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_1) = y_1\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_2) = y_2\\), \\(\\lim\\limits_{n\\rightarrow \\infty}f_{3n}(x_3) = y_3\\).</p> <p>Continue, it is not hard to imagine we get a function subsequence \\(\\{f_{nn}(x)\\}\\) which converges to \\(y(x)\\) on \\(x \\in \\{x_1,x_2,\\cdots, x_n\\}\\), with \\(y(x)\\) defined as </p> \\[ y(x_m) = y_m,\\quad  m=1,2,\\cdots \\] <p>So to express the subsequence more specificly, we can define \\(\\tilde{f}_n(x) = f_{nn}(x)\\), and get a subsequence \\(\\{\\tilde{f}_n(x)\\}_{n=1}^{\\infty}\\) such that </p> \\[ \\lim_{n\\rightarrow \\infty}\\tilde{f}_n(x) = y(x), \\forall x\\in I. \\] <p>And we are done.</p> <p>The above theorem offers that we can find a function sequence convergent on denumerable set \\(A\\), which is pointwise convergence.</p> <p>Now if we let the above function subsequence \\(\\{f_{\\lambda_k}(x)\\}\\) to be Equicontinuous, then it can be uniformly convergent on the whole interval \\(I\\). This is exactly the following theorem(in which \\(I\\) is a general region, not just an interval). Note that the above denumerable set can be said as \"dense set\".</p> <p></p> <p>\u5f15\u74062: \u70b9\u6001\u6536\u655b\u3001\u7b49\u5ea6\u8fde\u7eed\u7684\u51fd\u6570\u5217\u5728\u7d27\u96c6\u4e0a\u4e00\u81f4\u6536\u655b | Lemma 2: Equicontinuous Function Sequence is Uniformly Convergent given Pointwise Convergence on Denumerable Sets</p> <p>Assume \\(\\{f_n\\}_{n=1}^\\infty\\) defined on compact set \\(I \\subset \\mathbb{R}\\) is Equicontinuous, and there exists a dense subset \\(R \\subset I\\), such that \\(\\{f_n\\}_{n=1}^\\infty\\) is convergent on \\(R\\), then \\(\\{f_n\\}_{n=1}^\\infty\\) is uniformly convergent on \\(I\\). And denote</p> \\[ f_n(x) \\rightrightarrows f(x), \\quad n\\rightarrow \\infty \\] <p>where the convergent function \\(f(x)\\) is continuous on \\(I\\). </p> <p>Prove it.</p> HintsProofIf \\(I=[a,b]\\) <p>Using Equicontinuity and compact characteristic of set \\(I\\).</p> <p>We hope to prove that \\(\\forall \\varepsilon, \\exists N&gt;0\\), s.t. \\(\\forall n&gt;m&gt;N\\), \\(\\forall x \\in I\\), we have </p> \\[ |f_n(x)-f_m(x)| &lt; \\varepsilon \\] <ul> <li>use the Equicontinuity. </li> </ul> <p>That is, \\(\\forall \\varepsilon\\), \\(\\exists \\delta&gt;0\\), \\(\\forall x_1,x_2\\in I\\) and \\(|x_1-x_2|&lt;\\delta\\), \\(\\forall n\\), we have</p> \\[ |f(x_1)-f(x_2)| &lt; \\frac{\\varepsilon}{9} \\] <ul> <li>use the characteristic of compact set.</li> </ul> <p>Note that there exists open coverings of finite number for compact set \\(I\\). That is, we have \\(x_j\\in I\\), \\(j=1,2,\\cdots, k_0\\), and \\(\\{O_{\\delta'}(x_j)\\}_{j=1}^{k_0}\\) s.t. </p> \\[ I \\subset \\bigcup_{j=1}^{k_0}O_{\\delta'}(x_j) \\] <p>To let it help us prove, we can let \\(\\delta'&lt;\\delta\\) and we can still have a valid corresponding \\(k_0\\) which is finite.</p> <p>Because \\(R\\) is a dense set on \\(I\\), so \\(\\forall O(x_j)\\), \\(\\exists y_j\\in R\\) such that \\(y_j \\in O(x_j)\\).</p> <ul> <li>use the hypothesis of pointwise convergence.</li> </ul> <p>Because \\(\\{f_n\\}_{n=1}^{\\infty}\\) is convergent on dense set \\(R\\), we can have \\(\\forall \\varepsilon&gt;0\\)(choose the same one as the above one), \\(\\exists N&gt;0\\), \\(\\forall n&gt;m&gt;N\\), \\(\\forall y_j \\in R\\), we have</p> \\[ |f_n(y_j)-f_m(y_j)|&lt;\\frac{\\varepsilon}{9} \\] <p>That is, as long as \\(n&gt;m&gt;N\\), we have</p> \\[ \\begin{align} |f_n(x_j)-f_m(x_j)| &amp;\\leq |f_n(x_j)-f_n(y_j)| + |f_n(y_j)-f_m(y_j)| + |f_m(y_j)-f_m(x_j)| \\nonumber\\\\ &amp;&lt; |f_n(x_j)-f_n(y_j)| + \\frac{\\varepsilon}{9} + |f_m(y_j)-f_m(x_j)|\\quad \\text{by pointwise-convergence}\\nonumber\\\\ &amp;&lt;\\frac{\\varepsilon}{3} \\quad \\text{the 1st and 3rd hold for equicontinuity} \\label{bound-open-covering} \\end{align} \\] <p>Note that \\(\\forall x \\in I\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\), such that \\(x\\in O_{\\delta'}(x_j)\\), so</p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{3} + |f_m(x_j)-f_m(x)|\\quad \\text{by the above condition }\\ref{bound-open-covering} \\\\ &amp;&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>In this case, we do not have to find a open covering of \\([a,b]\\) but just partition the closed interval \\([a,b]\\) into \\(k_0\\) distinct closed intervals whose length are less then \\(\\delta\\). Denote these intervals as \\(I_j\\), \\(j=1,2\\cdots, k_0\\).</p> <p>Then \\(\\forall x \\in [a, b]\\), \\(\\exists j\\in {1,2,\\cdots, k_0}\\) such that \\(x\\in I_j\\). Because \\(R\\) is dense on \\([a,b]\\), then we are able to find an element \\(x_j \\in I_j\\), which means </p> \\[ |x-x_j|&lt; \\text{length of }I_q &lt;\\delta \\] <p>So </p> \\[ \\begin{align*} |f_n(x)-f_m(x)| &amp;\\leq |f_n(x)-f_n(x_j)| + |f_n(x_j)-f_m(x_j)| + |f_m(x_j)-f_m(x)|\\\\ &amp;&lt; |f_n(x)-f_n(x_j)| + \\frac{\\varepsilon}{9} + |f_m(x_j)-f_m(x)|\\quad \\text{by pointwise-convergence} \\\\ &amp;&lt;\\frac{\\varepsilon}{3}&lt;\\varepsilon \\quad \\text{the 1st and 3rd hold for equicontinuity} \\end{align*} \\] <p>By utilizing the above two theorems, we can prove the following important theorem. </p> <p>Ascoli-Arzel\u00e0 \u5b9a\u7406 | Ascoli-Arzel\u00e0 Theorem</p> <p>Assume \\(\\Lambda\\) is denumerable. If a family of sequence \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) is uniformly bounded and equicontinuous on closed interval \\([a, b]\\), then there exists a function subsequence \\(\\{f_{\\lambda_k}\\}_{k=1}^{\\infty}\\) of \\(\\{f_\\lambda\\}_{\\lambda\\in\\Lambda}\\) which is uniformly convergent on \\([a, b]\\).</p> <p>Prove it.</p> HintsProof <p>Making use of the above two lemmas.</p> <p>Firstly, we have to find a dense set \\(R\\). Naively, we can choose rational numbers on \\([a,b]\\). That is,</p> \\[ R \\overset{\\Delta}{=}\\mathbb{Q}\\cap [a,b]. \\] <p>Then by lemma 1, using its uniformly bounded,  we can find a function subsequence of \\(\\{f_\\lambda(x)\\}_{\\lambda\\in\\Lambda}\\), denoted as \\(\\{f_{n}\\}_{n=1}^{\\infty}\\), which is convergent on \\(R\\). Finally, by lemma 2, using its equicontinous, the subsequence is uniformly convergent on \\([a,b]\\).</p> <p>And by property of uniformly convergent function sequence, we can see that the convergent funtion is continuous on \\(I\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Picard-Theorem","title":"Picard\u5b58\u5728\u552f\u4e00\u6027\u5b9a\u7406 | Picard Theorem of Existence and Uniqueness","text":"<p>Picard uses the following condition to prove his theorem.</p> <p>Lipschitz\u6761\u4ef6\u7684\u5b9a\u4e49 | Definition of Lipschitz Condition</p> <p>Function \\(f(x, y)\\) defined at region \\(G\\), satisfies Lipschitz condition with respect to \\(y\\), if \\(\\exists L\\) s.t. \\(\\forall (x, y_1), (x, y_2) \\in G\\)</p> \\[ |f(x, y_1)-f(x,y_2)|\\leq L|y_1-y_2| \\] <p>Also, Picard focuses on a typical rectangular region</p> \\[ \\begin{equation} R = \\{(x,y): |x-x_0| \\leq a, |y-y_0|\\leq b\\} \\label{region} \\end{equation} \\] <p>to give his iterative method.</p> <p>Picard\u5b9a\u7406 | Picard Theorem</p> <p>Assume \\(f(x, y) \\in C(G)\\) satisfies Lipschitz condition with respect to \\(y\\), then Cauchy Problem has unique solution on interval \\([x-\\alpha, x+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsClassical Proof <p>There typically 4 steps. </p> <p>Firstly, Convert the differential problem into an equivalent integral problem. Then, formulate the so-called Picard sequence and prove it convergent. Furthermore, we have to prove that Picard sequence converges to the solution of integral equation. Finally, we prove the uniqueness by Gronwall Inequation.</p> <ul> <li>Convert the differential problem into an integral problem. That is, solving equation \\(\\ref{eq-cauchy}\\) equals to solving integral equation</li> </ul> \\[ \\begin{align} y(x) &amp;= y(0) + \\int_{x_0}^{x} f(s, y(s))ds \\nonumber\\\\ &amp;=y_0+ \\int_{x_0}^{x} f(s, y(s))ds \\label{eq-integral} \\end{align} \\] <p>readers can prove its equivalence(by proving solution of one side is also solution of the other).</p> <ul> <li>Formulate Picard sequence.</li> </ul> <p>Define:</p> \\[ \\begin{align*} y_0(x) &amp;= y_0\\\\ y_1(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_0(s))ds\\\\ y_2(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_1(s))ds\\\\ &amp;\\vdots\\\\ y_n(x) &amp;= y_0 + \\int_{x_0}^{x} f(s, y_{n-1}(s))ds\\\\ \\end{align*} \\] <p>We can say the above function sequence \\(\\{y_n(x)\\}_{n=1}^\\infty\\) is well-defined because of the following condition it satisfies:</p> \\[ |y_n(x)-y(x)| \\leq b \\quad \\&amp; \\quad y_n(x)\\in C(R) \\] <p>The above conition enables \\(f(x, y_{n-1}(x))\\) still falls on \\(R\\) and can be integrated(readers can prove that above two condition by induction).</p> <ul> <li>Prove Picard Sequence convergent.</li> </ul> <p>This is the most magic part. The following deduction may be the inspiration:</p> \\[ \\begin{align*} |y_1(x)-y_0(x)| &amp;= \\left| \\int_{x_0}^{x} f(s, y_0)ds \\right| \\leq M\\left| x - x_0\\right| \\\\ |y_2(x)-y_1(x)| &amp;= \\left| \\int_{x_0}^{x}\\left[ f(s, y_1(s)) - f(s, y_0(s))\\right]ds \\right| \\\\  &amp;\\leq \\int_{x_0}^{x} \\left| f(s, y_1(s)) - f(s, y_0(s)) \\right| ds  \\\\  &amp;\\leq L \\int_{x_0}^{x} \\left|y_1(s) - y_0(s)\\right| ds\\quad \\text{(Using Lipschitz Condition)}\\\\ &amp;\\leq LM \\int_{x_0}^{x}\\left|s - x_0\\right|ds =\\frac{LM}{2}|x-x_0|^2 \\quad \\text{(Using the first item)} \\end{align*} \\] <p>So we can allege that</p> \\[ |y_n(x)-y_{n-1}(x)| \\leq \\frac{ML^{n-1}}{n!}|x-x_0|^n \\] <p>and prove it by induction(to be proved by readers).</p> <p>With the above condition, we can use Weierstrass test to prove Picard sequence converges. To be specific, we can see that the Picard Sequence is controlled by a Series of constant terms, which satisfies Cauchy Convergence Theorem. That is, \\(\\forall \\varepsilon&gt;0, \\exists N&gt;0, \\forall n&gt;m&gt;N\\), s.t.</p> \\[ \\begin{align*} |y_n(x)-y_{m}(x)| &amp;\\leq \\sum_{k=m+1}^{n}\\frac{ML^{k-1}}{k!}|x-x_0|^k \\\\ &amp;= \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{L^{k}}{k!}|x-x_0|^k \\\\ &amp;\\leq \\frac{M}{L}\\sum_{k=m+1}^{n}\\frac{(L\\alpha)^{k}}{k!} \\end{align*} \\] <p>And we notice that Series of constant terms</p> \\[ \\sum_{n=1}^\\infty \\frac{(L\\alpha)^{n}}{n!} \\] <p>converges(to be specific, converges to \\(e^{L\\alpha}\\)), so Picard Sequence also converges.</p> <ul> <li>Prove Picard Sequence converges to solution of equation \\(\\ref{eq-cauchy}\\).</li> </ul> <p>This part is quite easy, to be done by readers.</p> <ul> <li>Prove uniqueness.</li> </ul> <p>Follow the traditional logic: contradiction.</p> <p>Assume there are \\(\\varphi_1(x), \\varphi_2(x)\\) two distinct solutions to equation \\(\\ref{eq-cauchy}\\), then subtract one from the other:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;=\\left| \\int_{x_0}^{x} \\left[ f(s, \\varphi_1(s)) - f(s, \\varphi_2(s))\\right]ds \\right| \\nonumber\\\\ &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\quad \\text{(Using Lipschitz Condition)} \\label{eq-same} \\end{align} \\] <p>And by using Gronwall inequation, we can get </p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq 0 \\] <p>So \\(\\varphi_1(x) = \\varphi_2(x)\\), that is, there exists only one solution for equation \\(\\ref{eq-cauchy}\\).</p> <p>In another way, if we don't want to use Gronwall Inequation, we can still say that the two solutions are the same. Assume that \\(\\varphi_1(x), \\varphi_2(x)\\) have a common region \\(J = [x_0-d,x_0+d]\\), where \\(0&lt;d\\leq \\alpha\\). Then \\(|\\varphi_1(x)-\\varphi_2(x)|\\) is continuous and bounded on region \\(J\\) and denote</p> \\[ K = \\max_{x\\in J}\\{|\\varphi_1(x)-\\varphi_2(x)|\\} \\] <p>So the right side of inequation \\(\\ref{eq-same}\\) can be bounded:</p> \\[ \\begin{align} |\\varphi_1(x) - \\varphi_2(x)| &amp;\\leq L \\int_{x_0}^{x} \\left|\\varphi_1(s) - \\varphi_2(s)\\right|ds  \\nonumber\\\\ &amp;\\leq LK |x-x_0| \\label{eq-same1} \\end{align} \\] <p>Substitute inequation \\(\\ref{eq-same1}\\) again into the right side of inequation \\(\\ref{eq-same}\\) and get</p> \\[ \\begin{align*} |\\varphi_1(x) - \\varphi_2(x)| \\leq KL^2 \\frac{|x-x_0|^2}{2} \\end{align*} \\] <p>repeat the above method iteratively and we can prove the following by induction:</p> \\[ |\\varphi_1(x) - \\varphi_2(x)| \\leq K \\frac{(L|x-x_0|)^n}{n!} \\] <p>Let \\(n \\rightarrow \\infty\\), we get \\(|\\varphi_1(x) - \\varphi_2(x)| \\rightarrow 0\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#osgood-osgood-condition","title":"Osgood \u6761\u4ef6 | Osgood Condition","text":"<p>We consider a condition which is slightly weaker than Lipschitz condition but still can guarrantee the convergence of Picard Sequence and its uniqueness.</p> <p>Osgood \u6761\u4ef6 | Osgood Condition</p> <p>Assume \\(D\\) is a region on \\(\\mathbb{R}^2\\), and function \\(f(x,y)\\in C(D)\\). If \\(\\forall (x,y_1), (x,y_2)\\in D\\), s.t.</p> \\[ |f(x,y_1)-f(x,y_2)|\\leq F(|y_1-y_2|) \\] <p>where \\(F(r)&gt;0 \\in C(\\mathbb{R})\\) satisfies </p> \\[ \\int_{0}^{\\varepsilon}\\frac{1}{F(r)}dr = +\\infty, \\quad \\forall \\varepsilon &gt;0 \\] <p>then we say that \\(f(x,y)\\) satisfies Osgood condition with respect to \\(y\\).</p> <p>Obviously, if \\(f(x,y)\\) satisfies Lipschitz condition, it satisfies Osgood condition. In fact, we can choose \\(F(r) = Lr\\).</p> <p>Osgood \u5b9a\u7406 | Osgood Theorem</p> <p>If  \\(f(x,y) \\in C(D)\\) satisfies Osgood condition with respect to \\(y\\), then \\(\\forall (x_0,y_0)\\in D\\), the solution to Cauchy problem \\(\\ref{eq-cauchy}\\) exists uniquely.</p> <p>Prove it.</p> HintsProof <p>The existence is guarranteed by Peano Theorem in the following part. You only need to prove the uniqueness, which is proved by contradiction, which is similar to prove the uniqueness of \\(f(x, y)\\) that decrease monotonically with respect to \\(y\\).</p> <p>Assume we have two distinct solution \\(y_1(x)\\), \\(y_2(x)\\), then there exists \\(x_2\\) such that \\(y_1(x_2)\\neq y_2(x_2)\\). Let \\(y_1(x) &gt; y_2(x)\\). </p> <p>Then by feature of guarantee code(\u4fdd\u53f7\u6027), there must exist a region such that \\(y_1(x) &gt; y_2(x)\\), so let \\(x_1 = \\max\\{x\\in [x_0,x_2] : y_1(x) = y_2(x)\\}\\).</p> <p><p> </p></p> <p>So we have </p> \\[ y_1(x) &gt; y_2(x), \\quad \\forall x\\in (x_1,x_2] \\] <p>define \\(r(x) = y_1(x) - y_2(x) \\in (0, m]\\), where m is determined by \\(m = \\max\\limits_{x\\in (x_1,x_2]}\\{y_1(x) - y_2(x)\\}\\)</p> <p>So by condition of the proposition, we have</p> \\[ |r'(x)| = |y_1'(x) - y_2'(x)| = |f(x,y_1(x))- f(x,y_2(x))| \\leq F(|y_1(x) - y_2(x)|) \\] <p>because \\(y_1(x) &gt; y_2(x)\\), so we take \"\\(\\vert \\cdot \\vert\\)\" out and get</p> \\[ r'(x) = y_1'(x) - y_2'(x) = f(x,y_1(x))- f(x,y_2(x)) \\leq F(y_1(x) - y_2(x)) \\] <p>divide both sides \\(F(y_1(x) - y_2(x))\\) and integrate on \\((x_1, x_2]\\), which is an improper integral on the left</p> \\[ \\int_{x_1}^{x_2}\\frac{r'(x)}{F(r(x))}dx\\leq \\int_{x_1}^{x_2}dx = x_2-x_1  \\] <p>so the left item can be \\(+\\infty\\) by condition while the right item is less than \\(+\\infty\\), i.e.</p> \\[ +\\infty=\\int_{0}^{r(x_2)}\\frac{1}{F(r)}dr=\\int_{r(x_1)}^{r(x_2)}\\frac{1}{F(r)}dr\\leq x_2-x_1 &lt;+\\infty \\] <p>which contradicts!</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#Peano-Theorem","title":"Peano\u5b9a\u7406 | Peano Theorem","text":"<p>When \\(f(x,y)\\) does not satisfy Lipschitz Condition with respect to \\(y\\), we cannot guarantee the existence and uniqueness of the solution to Cauchy Problem \\(\\ref{eq-cauchy}\\). However, when \\(f(x,y)\\) is continuous, Peano proved that Cauchy Problem \\(\\ref{eq-cauchy}\\) has solution. </p> <p>Peano\u5b9a\u7406 | Peano Theorem</p> <p>Assume \\(f(x, y)\\) is continous in \\(R(\\ref{region})\\), then Cauchy Problem \\(\\ref{eq-cauchy}\\) has at least one solution in interval \\([x_0-\\alpha, x_0+\\alpha]\\), where</p> \\[ \\alpha = \\min \\left\\{a, \\frac{b}{M} \\right\\}, \\quad M = \\max_{(x, y) \\in R}\\left\\{\\left|f(x, y)\\right|\\right\\} \\] <p>Prove it.</p> HintsProof <p>There are several methosd to prove Peano Theorem, like Euler's Arc method, Tonelli(\u6258\u5185\u5229) Sequence method, fixed-point method in functional analysis, while Euler's Arc method is thought to be the dawm to calculating ODE numerically, so we will prove it this way. </p> <p>The idea is to construct approximation solution which converges to the real solution. In fact, Picard Sequence is also an approximation solution.</p> <ul> <li>Cauchy Problem \\(\\ref{eq-cauchy}\\) can be converted equivalently to integral equation \\(\\ref{eq-integral}\\).</li> </ul> <p>We only discuss the existence of solution of right side(\\([x_0, x_0+\\alpha]\\)). We can make similar treatment to the left side.</p> <ul> <li>Formulate Euler Polygons/Polygonal Arc(\u6b27\u62c9\u6298\u7ebf).</li> </ul> <p>The idea is simple: go ahead step by step as small as possible, employing the slope of this sampling points.</p> <p>Firstly, we partition the region \\([x_0,x_0+\\alpha]\\) into \\(n\\) parts(usually of equal length), that is</p> \\[ x_0 &lt; x_1 &lt; x_2 &lt; \\cdots &lt; x_n = x_0 + \\alpha \\] <p>and then define curves</p> \\[ \\begin{align*} l_1: y &amp;= y_0 + f(x_0, y_0)(x-x_0), \\quad x_0\\leq x\\leq x_1 \\\\ l_2: y &amp;= y_1 + f(x_1, y_1)(x- x_1), \\quad x_1 \\leq x\\leq x_2\\\\ &amp;\\vdots \\\\ l_n: y &amp;= y_{n-1} + f(x_{n-1}, y_{n-1})(x - x_{n-1}), \\quad x_{n-1} \\leq x \\leq x_n \\end{align*} \\] <p><p> </p> we can formulate a polygonal arc on \\([x_0, x_0+\\alpha]\\):</p> \\[ E_n = \\bigcup_{s=1}^{n}l_s \\] <p>and its function expresstion is </p> \\[ \\begin{equation} y_n(x) = y_0 + \\sum_{i=0}^{j-1}f(x_i, y_i)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j) \\label{eq-euler} \\end{equation} \\] <p>where \\(\\forall x \\in (x_0, x_0 +\\alpha], \\exists j\\) s.t. </p> \\[ x_j &lt; x\\leq x_{j+1} \\] <p>Here closed interval \\([x_0, x_0+\\alpha]\\) is of great importance, for it can guarantee the condition for Ascoli-Arzel\u00e0 Theorem to be true.</p> <ul> <li>\u4e00\u81f4\u6709\u754c | Uniform Bound</li> </ul> <p>By definition, it is easy to prove that </p> \\[ |y_n(x)-y_0| \\leq b, \\quad \\forall n, \\forall x \\in [x_0,x_0+\\alpha] \\] <ul> <li>\u7b49\u5ea6\u8fde\u7eed | Equicontinuous</li> </ul> <p>By definition, it is easy to see that \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta = \\varepsilon/M\\), s.t. \\(\\forall x_1, x_2 \\in [x_0,x_0+\\alpha]\\) and \\(|x_1-x_2|&lt;\\delta\\), we have</p> \\[ |y_n(x_1)-y_n(x_2)|\\leq \\max_{(x,y)\\in R}|f(x,y)| |x_1-x_2| &lt; M \\cdot \\frac{\\varepsilon}{M} = \\varepsilon \\] <ul> <li>Using Ascoli-Arzel\u00e0 Theorem</li> </ul> <p>So the function sequence \\(\\{y_n(x)\\}\\) has a uniformly convergent subsequence \\(\\{y_{n_j}(x)\\}\\).</p> <ul> <li>Prove Euler's Arc converges to the solution of ODE.</li> </ul> <p>This means that we have to consider the error of \\(y_{n_j}(x)\\) and \\(y(x)\\). Firstly, we rewrite the formula of Euler's Arc \\(\\ref{eq-euler}\\).</p> <p>Note that</p> \\[ \\begin{align*} f(x_i,y_i)(x_{i+1}-x_i)&amp;=\\int_{x_i}^{x_{i+1}}f(x_i,y_i)dx\\\\ &amp;=\\int_{x_i}^{x_{i+1}}f(x,y_n(x)) + f(x_i,y_i) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d_n(i) \\overset{\\Delta}{=} \\int_{x_i}^{x_{i+1}}[f(x_i,y_i) - f(x,y_n(x))]dx \\] <p>then </p> \\[ \\begin{equation} f(x_i,y_i)(x_{i+1}-x_i) = \\int_{x_i}^{x_{i+1}}f(x,y_n(x))dx + d_n(i) \\label{eq-front} \\end{equation} \\] <p>Similarly, we have \\(x_j&lt;x\\leq x_{j+1}\\)</p> \\[ \\begin{align*} f(x_j,y_j)(x-x_j)&amp;=\\int_{x_j}^{x_{j+1}}f(x_j,y_j)dx\\\\ &amp;=\\int_{x_j}^{x_{j+1}}f(x,y_n(x)) + f(x_j,y_j) - f(x,y_n(x))dx\\\\ \\end{align*} \\] <p>Define</p> \\[ d^*_n(x) \\overset{\\Delta}{=} \\int_{x_j}^{x}[f(x_j,y_j) - f(x,y_n(x))]dx  \\] <p>then </p> \\[ \\begin{equation} f(x_j,y_j)(x-x_j) = \\int_{x_j}^{x_{j+1}}f(x,y_n(x))dx +d_n^*(x) \\label{eq-back} \\end{equation} \\] <p>So with the above two transformation \\(\\ref{eq-front}\\), \\(\\ref{eq-back}\\), the formula of Euler's Arc \\(\\ref{eq-euler}\\)(summation of linear expresstions) becomes an integral-like expression:</p> \\[ \\begin{align*} y_n(x) &amp;= y_0 + \\sum_{i=0}^{j-1}f(x_k, y_k)(x_{i+1} - x_{i}) + f(x_j, y_j)(x - x_j)\\\\ &amp; = y_0 + \\int_{x_0}^{x}f(x,y_n(x))dx + \\delta_n(x) \\end{align*} \\] <p>where </p> \\[ \\begin{equation} \\delta_n(x) = \\sum_{i=0}^{j-1}d_n(i) + d^*_n(x) \\label{eq-residue} \\end{equation} \\] <p>we know that for each \\(y_n\\), we can have variables \\(x, y\\) bounded by</p> \\[ |x-x_j|\\leq \\frac{\\alpha}{n}, \\quad |y-y_j|\\leq M \\frac{\\alpha}{n} \\] <p>So \\(\\forall \\varepsilon &gt;0\\), \\(\\exists N(\\varepsilon) = \\frac{\\alpha}{\\varepsilon}\\), s.t. \\(n&gt;N\\), </p> \\[ |x-x_j|\\leq \\frac{\\alpha}{N} = \\varepsilon, \\quad |y-y_j|\\leq M \\frac{\\alpha}{N} = M\\varepsilon \\] <p>which means the region of each integral interval can be as small as possible so long as \\(n\\rightarrow \\infty\\).</p> <p>So in this case, for each item in residue \\(\\ref{eq-residue}\\), we can bound it into small value corresponding to \\(\\varepsilon\\) by making use of the continuity of \\(f(x,y)\\), which means \\(\\forall\\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), s.t. \\(\\forall (x,y) \\in O[(x_j,y_j),\\delta]\\), </p> \\[ |f(x,y)-f(x_j,y_j)| &lt;\\varepsilon \\] <p>The above one is easy to accomplish because we can let \\(n\\) be large enough, so all \\(x \\in [x_j,x_{j+1}]\\) with its \\(y_n(x)\\) can fall in \\(O[(x_j,y_j),\\delta]\\).</p> <p>So the whole residue \\(\\ref{eq-residue}\\) can be bounded:</p> \\[ \\delta_n(x) \\leq \\varepsilon \\frac{\\alpha}{n} + j \\varepsilon \\frac{\\alpha}{n} &lt; \\alpha \\varepsilon \\] <p>So we can say \\(y_{n_j}(x)\\) defined as</p> \\[ y_{n_j}(x) = y_0 + \\int_{x_0}^{x}f(s, y_{n_j}(s))ds + \\delta_{n_j}(x), \\quad x \\in [x_0,x_0+\\alpha] \\] <p>where \\(\\delta_{n_j}(x)\\) satisfies</p> \\[ \\delta_{n_j}(x)\\rightrightarrows 0 \\] <p>Which also means, if we denote </p> \\[ \\varphi(x) \\overset{\\Delta}{=} \\lim_{n_j\\rightarrow \\infty}y_{n_j}(x) \\] <p>and let \\(n_j \\rightarrow \\infty\\), we get</p> \\[ \\varphi(x) = y_0 + \\int_{x_0}^{x}f(s, \\varphi(s))ds \\] <p>which means function subsequence \\(\\{y_{n_j}(x)\\}\\) converges to the solution of integral form \\(\\ref{eq-integral}\\) of ODE \\(\\ref{eq-cauchy}\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Existence_Uniqueness_Theorem/#comments-on-previous","title":"Comments on Previous","text":"<p>Here we list the difference between Picard Theorem and Peano Theorem.</p> Theorem Picard Theorem Peano Theorem Condition \\(f\\in C(R)\\), Lipschitz Condition with respect to \\(y\\) only \\(f\\in C(R)\\) Sequence the whole Picard Sequence \\(\\{y_n(x)\\}\\) converges subsequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges Solution exists uniquely exists only, might have a lot of solutions <ul> <li>Note 1: Uniqueness can help bound Euler's Arc. See the following theorem.</li> </ul> <p>\u6709\u552f\u4e00\u89e3\u7684\u67ef\u897f\u95ee\u9898\uff0c\u5176\u6b27\u62c9\u6298\u7ebf\u5168\u5e8f\u5217\u6536\u655b | Euler's Arc converges given uniqueness</p> <p>If Cauchy problem \\(\\ref{eq-cauchy}\\) has unique solution, then the whole sequence of Euler's Arc \\(\\{\\varphi_n(x)\\}\\) converges.</p> <p>Prove it.</p> HintsProof <p>Use contradiction.</p> <p>Assume that the conclusion does not hold, then by definition of negative proposition, we have</p> \\[ \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall N_0&gt;0, \\exists n_0,m_0&gt;N_0, |\\varphi_{n_0}(\\overline{x})-\\varphi_{m_0}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which is the negative proposition of the Cauchy Convergence form</p> \\[ \\forall \\varepsilon&gt;o, \\forall x\\in I, \\exists N&gt;0, \\forall n&gt;m&gt;N, |\\varphi_{n}(x)-\\varphi_{m}(x)|&lt;\\varepsilon. \\] <p>In order to help us prove, we can take two sequence from the above negative proposition. That is, let \\(N_0=j\\), \\(j=1,2,\\cdots\\), we can take the corresponding \\(n_j\\), \\(m_j\\) out to compose a subsequence of Euler's arc \\(\\{\\varphi_n(x)\\}\\), i.e.</p> \\[ \\begin{equation} \\exists \\varepsilon_0&gt;0, \\exists \\overline{x} \\in I, \\forall j=1,2,\\cdots, \\exists n_j,m_j&gt;j, |\\varphi_{n_j}(\\overline{x})-\\varphi_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\label{con-non-convergent} \\end{equation} \\] <p>See that \\(\\{\\varphi_{n_j}(x)\\}\\) is still uniformly bounded and equicontinous on \\(I\\), so there exists its subsequence \\(\\{\\tilde{\\varphi}_{n_j}(x)\\}\\) such that</p> \\[ \\{\\tilde{\\varphi}_{n_j}(x)\\} \\rightrightarrows \\varphi_1(x), \\quad \\forall x\\in I \\] <p>and also \\(\\varphi_1(x)\\) is still the solution of Cauchy problem \\(\\ref{eq-cauchy}\\). Similarly, we can have  </p> \\[ \\{\\tilde{\\varphi}_{m_j}(x)\\} \\rightrightarrows \\varphi_2(x), \\quad \\forall x\\in I \\] <p>where \\(\\{\\tilde{\\varphi}_{m_j}(x)\\}\\) is a subsequence of \\(\\{\\varphi_{m_j}(x)\\}\\). According to the condition of the theorem, we have </p> \\[ \\varphi_1(x) = \\varphi_2(x), \\quad \\forall x \\in I \\] <p>while in \\(\\ref{con-non-convergent}\\) we also take the subsequence of \\(\\{\\varphi_{n_j}(x)\\}\\) and \\(\\{\\varphi_{m_j}(x)\\}\\)</p> \\[ |\\tilde{\\varphi}_{n_j}(\\overline{x})-\\tilde{\\varphi}_{m_j}(\\overline{x})|\\geq\\varepsilon_0 \\] <p>which means \\(\\varphi_1(x)\\) and \\(\\varphi_2(x)\\) must have distance on some \\(\\overline{x}\\), which contradicts with \\(\\varphi_1(x) = \\varphi_2(x)\\) on all \\(x\\in I\\)!</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/","title":"Extension of Solution","text":""},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Extension_of_Solution/#extension-of-solution","title":"\u89e3\u7684\u5ef6\u62d3 | Extension of Solution","text":"<p>For Cauchy problem</p> \\[ \\begin{equation} \\frac{dy}{dx} = f(x, y), \\quad y(x_0) = y_0. \\label{eq-cauchy} \\end{equation} \\] <p>it is clear that we are satisfied with the result of intervals in the previous chapter, especially when we have to shrink the interval of solution using Contraction Mapping Method.</p> <p>So a naive idea is, can we use the Peano/Picard theorem repeatedly to extend the interval of parameter \\(t\\)? If so, in what cases can we extend it and to where can we extend?</p> <p>Here comes the following theorem.</p> <p>\u89e3\u7684\u5ef6\u62d3\u5b9a\u7406 | Theorem of extension of solution</p> <p>Assume \\(f(x,y)\\in C(G)\\), where \\(G\\) is an open set(region). Then the solution of Cauchy problem \\(\\ref{eq-cauchy}\\) can extend to its boundary. </p> HintsProof <p>The proof is equivalent to prove that, for each closed set \\(G_1\\subset G\\) and \\((x_0,y_0)\\in G_1\\), the solution \\(\\Gamma\\) can extend to \\(G\\) \\ \\(G_1\\).</p> <p>We only consider positive extension, i.e. \\(x\\geq x_0\\).</p> <p>Consider closed region \\(G_1\\subset G\\) that has point \\((x_0,y_0)\\) with the solution denoted by \\(\\varphi(x)\\) that satisfies initial condition \\(y(x_0)=y_0\\). </p> <ul> <li>Use the property of Open Set.</li> </ul> <p>Because \\(G\\) is an open set, the distance between \\(G\\) and \\(G_1\\) can not be zero, that is, \\(\\exists \\delta_0&gt;0\\)., s.t.</p> \\[ \\{(x,y): |x-a|&lt;\\delta_0, |y-b|&lt;\\delta_0, (a,b)\\in \\partial G_1\\} \\subset G \\] <p>Here \\(\\delta_0\\) is a constaint condition that guarantees the extended interval of solution will not exceed the boundary of \\(G_1\\).</p> <ul> <li>Using \\(\\delta_0\\) to generate extended intervals with length \\(\\delta'\\)</li> </ul> <p>For any closed region \\(G_1\\subset G\\), Denote </p> \\[ M=\\max_{(x,y)\\in G_1}|f(x_y)|+1 &lt; \\infty, \\quad R_{\\delta_0}(x',y')=\\{(x,y): |x-x'|\\leq \\delta_0, |y-y'|\\leq \\delta_0\\} \\] <p>where \\((x',y')\\in G_1\\).</p> <p>For Cauchy problem with initial condition \\(y(x_0)=y_0\\) in region \\(R_{\\delta_0} (x_0,y_0)\\), according to Peano Theorem, we can have a solution \\(\\varphi_0(x)\\) on interval \\([x_0-\\delta',x_0+\\delta']\\), where \\(\\delta'=\\min\\{\\delta_0, \\delta_0/M\\}\\). If there exists point on the curve \\((x,\\varphi_0(x)) \\in G\\)\\\\(G_1\\), then we prove it.</p> <p><p> </p></p> <p>If not, then the furthest point on the right \\((x_0+\\delta', \\varphi_0(x_0+\\delta'))\\) must be in \\(G_1\\). So consider cauchy problem with initial condition \\(y(x_0+\\delta') = \\varphi_0(x_0+\\delta')\\) in region \\(R_{\\delta_0} (x_0+\\delta',\\varphi_0(x_0+\\delta'))\\), according to Peano Theorem, we can get another solution \\(\\varphi_1(x)\\) on interval \\(x_0,x_0+2\\delta'\\).</p> <p>Repeat the above procedure, we can say the interval can be extended to \\([x_0, x_0+n\\delta']\\). If we denote the distance between \\((x_0,y_0)\\) and \\(\\partial G_1\\) as \\(D_1\\), distance between \\((x_0,y_0)\\) and \\(\\partial G\\) as \\(D\\), then choose \\(n\\) such that \\(n\\delta'&gt;D_1\\) and in the same time \\(n\\delta'&lt;D\\).</p> <p>And we are done.    </p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Power_Series/","title":"Method of Power Series","text":"<p>Lots of ODE cannot be sovled using elementary integration method, so we have to give up solution of finite form and try to find solution of infinite form, like series.</p> <p>To have a more global understanding, we focus on</p> \\[ \\begin{equation} \\frac{d \\pmb{y}}{dx}=\\pmb{f}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy} \\end{equation} \\] <p>where \\(\\pmb{f}(x,\\pmb{y})\\) is analytic on region \\(R\\subset \\mathbb{R}\\times \\mathbb{R}^n\\), i.e.</p> \\[ \\pmb{f}(x,\\pmb{y})=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>where \\(\\pmb{a}_{i{j_1}{j_2}\\cdots{j_n}}\\in \\mathbb{R}^n\\).</p> <p>To simplify the notation, we denote \\(\\pmb{j}=(j_1,j_2,\\cdots,j_n)\\), and </p> \\[ (\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}=(y_1-y_{10})^{j_1}\\cdots(y_n-y_{n0})^{j_n} \\] <p>denote </p> \\[ \\sum_{i=0,\\pmb{j}=0}^\\infty=\\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\] <p>Since \\(\\pmb{f}(x,\\pmb{y})\\) is analytic with respect to \\(\\pmb{y}\\), so by Picard Theorem, there exists only one solution. Now the question is, to prove that the solution is analytic, i.e. \\(\\exists \\delta&gt;0\\), s.t. \\(x\\in O_\\delta(x_0)\\)</p> \\[ \\pmb{y}(x)=\\sum_{k=0}^{\\infty}\\pmb{c}_k(x-x_0)^k \\] <p>where \\(\\pmb{c}_k=(c_{n1},c_{n2},\\cdots,c_{nk}) \\in \\mathbb{R}^n\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Power_Series/#excellent-series","title":"Excellent Series","text":"<p>To prove that the solution is analytic, we have to use a method of proof, that is, using Excellent Series.</p> <p>Definition of Excellent Series</p> <p>Assume there are two power series</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series1} \\end{equation} \\] <p>and</p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}}. \\label{series2} \\end{equation} \\] <p>If \\(A_{i\\pmb{j}}&gt;0\\) and they satisfies</p> \\[ |a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}, \\quad \\forall i,\\pmb{j} \\] <p>Then we call series \\(\\ref{series2}\\) is an excellent series of series \\(\\ref{series1}\\). If series \\(\\ref{series2}\\) converges on closed region</p> \\[ \\{(x,y): |x-x_0|\\leq \\alpha,|\\pmb{y}-\\pmb{y}_0|\\leq\\beta\\} \\] <p>then we call its summing function \\(\\pmb{f}(x,\\pmb{y})\\) is an Excellent function of series \\(\\ref{series1}\\).(note that in this case, series \\(\\ref{series1}\\) also converges)</p> <p></p> <p>\u5f15\u74061: \u89e3\u6790\u51fd\u6570\u6709\u5b9a\u4e49\u57df\u6536\u7f29\u7684\u4f18\u51fd\u6570 | Lemma 1: A analytic function has an excellent function within smaller region</p> <p>If \\(f(x,\\pmb{y})\\) is analytic on region</p> \\[ R: \\{(x,y): |x-x_0|&lt;\\alpha, |\\pmb{y}-\\pmb{y}_0|&lt;\\beta\\} \\] <p>then \\(\\exists M&gt;0\\), s.t.</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>is an Excellent function of \\(f(x,\\pmb{y})\\) on a smaller region </p> \\[ R_0:\\{(x,y): |x-x_0|&lt;a,|\\pmb{y}-\\pmb{y}_0|&lt;b\\}. \\] HintsProof <p>Use Abel Second Theorem. Note that we can not guarrantee the convergence on boundaries, so we choose open intervals.</p> <p>We can represent \\(f(x,\\pmb{y})\\) in terms of power series</p> \\[ f(x,\\pmb{y})=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\] <p>in region \\(R\\). Then by Abel Second Theorem, \\(\\exists a\\in(0,\\alpha), b\\in(0,\\beta)\\) s.t.</p> \\[ \\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}a^i b^{j_1+j_2+\\cdots+j_n} \\] <p>is convergent, so each item of the above series can be bounded by a number \\(M&gt;0\\):</p> \\[ |a_{i\\pmb{j}}|a^i b^{j_1+j_2+\\cdots+j_n}\\leq M \\Rightarrow |a_{i\\pmb{j}}|\\leq \\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Now, the following thing is a little tricky. Define </p> \\[ A_{i\\pmb{j}}=\\frac{M}{a^i b^{j_1+j_2+\\cdots+j_n}} \\] <p>Consider a power of series </p> \\[ \\begin{equation} \\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{series-exce} \\end{equation} \\] <p>which is convergent because it can add up to:</p> \\[ F(x,\\pmb{y})=\\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)} \\] <p>with its range of definition \\(R_0\\). By definition, it is an excellent function of \\(f(x,\\pmb{y})\\).</p> <p>With the definition of Excellent function, we have to use it to formulate an excellent series of the original series. This is the following theorem.</p> <p></p> <p>\u5f15\u74062: \u7528\u4e0a\u8ff0\u4f18\u51fd\u6570\u5efa\u7acb\u7684\u5fae\u5206\u65b9\u7a0b\u6709\u89e3\u6790\u89e3 | Lemma2: ODE combined with The above Excellent Function has a solution that can be represented by Power Series</p> <p>Cauchy problem </p> \\[ \\frac{d \\pmb{y}}{dx}=\\pmb{F}(x,\\pmb{y}), \\quad \\pmb{y}(x_0)=\\pmb{y}_0 \\label{eq-cauchy-prime} \\] <p>has a analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\) on region \\(O_\\rho(x_0)\\), where \\(F_i(x,\\pmb{y}) = F(x,\\pmb{y})\\) that is same all over \\(i\\) is given from the above lemma and </p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\}. \\] HintsProof <p>use elementary integration method.</p> <p>We let \\(u=y_i\\), \\(i=1,2,\\cdots,n\\) and we only need to solve the equation </p> \\[ \\frac{d u}{dx}=F(x,u), \\quad u(x_0)=u_0 \\] <p>where </p> \\[ F(x,u) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{u-u_{0}}{b}\\right)^n} \\] <p>(Here readers can see that \\(u-u_0=y_i-y_{i0}\\).)</p> <p>The above ODE is a variable separation equation. So change the form and integrate on \\([x_0,x]\\)</p> \\[ \\frac{-b}{n+1}\\left(1-\\frac{u-u_0}{b}\\right)^{n+1} +\\frac{b}{n+1}= -aM\\ln\\left(1-\\frac{x-x_0}{a}\\right) \\] <p>get \\(u\\) out:</p> \\[ u = u_0 + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}} \\] <p>That is,</p> \\[ y_i(x) = y_{i0} + b- b\\left[\\frac{aM(n+1)}{b} \\ln\\left(1-\\frac{x-x_0}{a}\\right) + 1\\right]^{\\frac{1}{n+1}}, \\quad \\forall i=1,2\\cdots,n \\] <p>We want to use this form to get a power series. See that \\(\\ln{\\left(1-\\frac{x-x_0}{a}\\right)}\\) can be represented by power series of \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). And also \\((1+s)^{\\frac{1}{n+1}}\\) can be represented by power series of \\(s\\) when \\(|s|&lt;1\\). So by combine the above two, we know \\(y_i(x)\\) can be represented by \\((x-x_0)\\) once \\(|x-x_0|&lt;a\\). To be more specific, We have to let the radius of converence to satisfy</p> \\[ \\begin{cases} \\displaystyle 1-\\frac{\\rho}{a} \\geq 0 \\\\ \\displaystyle \\frac{aM(n+1)}{b}\\ln\\left(1-\\frac{\\rho}{a}\\right)\\leq 1  \\end{cases} \\Rightarrow \\begin{cases} \\rho\\leq a \\\\ \\rho\\leq a\\{1-e^{b/[(n+1)aM]}\\} \\end{cases} \\] <p>choose</p> \\[ \\rho=a\\{1-e^{b/[(n+1)aM]}\\} \\] <p>So solution of the above ODE</p> \\[ \\pmb{y}(x)=(y_1(x),y_2(x),\\cdots, y_n(x)) \\] <p>can be represented by power series of \\((x-x_0)\\) when \\(|x-x_0|&lt;\\rho\\).</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Power_Series/#proof","title":"\u8bc1\u660e | Proof","text":"<p>Cauchy \u5b9a\u7406 | Cauchy Theorem</p> <p>Assume \\(\\pmb{f}(x,\\pmb{y})=[f_1(x,\\pmb{y}), f_2(x,\\pmb{y}),\\cdots, f_n(x,\\pmb{y})]\\) is an analytic function on region \\(R\\). So problem \\(\\ref{eq-cauchy}\\) has a unique analytic solution \\(\\pmb{y} = \\pmb{y}(x)\\) on \\(O_\\rho(x)\\), where \\(\\rho\\) is given in Lemma 2.</p> HintsProof <ul> <li> <p>Represent solution with power series. Show that it is unique.</p> </li> <li> <p>Use an excellent series to prove the above power series convergent.</p> </li> </ul> <ul> <li>Represent \\(f_k(x,\\pmb{y})\\) with power series</li> </ul> \\[ \\begin{equation} f_k(x,\\pmb{y})=\\sum_{i=0,\\pmb{j}=0}^\\infty a_{i\\pmb{j}}^k (x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\label{eq-f} \\end{equation} \\] <p>And represent solution with power series</p> \\[ \\begin{equation} y_k(x) = y_{k0} + \\sum_{i=1}^\\infty c_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\label{eq-y} \\end{equation} \\] <p>substitute \\(\\ref{eq-f}\\) and \\(\\ref{eq-y}\\) into ODE</p> \\[ \\frac{d y_k}{dx} = f_k(x,\\pmb{y}),\\quad k=1,2\\cdots,n \\] <p>and get</p> \\[ \\begin{align*} \\sum_{i=0}^\\infty (i +1) c_{i+1}^k(x-x_0)^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty \\Bigg\\{ &amp; a^k_{i{j_1}{j_2}\\cdots{j_n}}(x-x_0)^i \\times \\\\  &amp;\\left[\\sum_{i=1}^\\infty c_i^1(x-x_0)^i\\right]^{j_1} \\times \\\\ &amp;\\left[\\sum_{i=1}^\\infty c_i^2(x-x_0)^i \\right]^{j_2} \\times \\\\ &amp;\\cdots \\\\ &amp; \\left[\\sum_{i=1}^\\infty c_i^n(x-x_0)^i \\right]^{j_n}\\Bigg\\},\\quad k=1,2\\cdots,n. \\end{align*} \\] <p>Denote \\(X = x-x_0\\), and we have</p> \\[ \\sum_{i=0}^\\infty (i +1) c_{i+1}^kX^i = \\sum_{i,j_1,j_2,\\cdots,j_n=0}^\\infty a^k_{i{j_1}{j_2}\\cdots{j_n}}X^i \\left(\\sum_{i=1}^\\infty c_i^1X^i\\right)^{j_1} \\left(\\sum_{i=1}^\\infty c_i^2X^i\\right)^{j_2} \\cdots \\left(\\sum_{i=1}^\\infty c_i^nX^i\\right)^{j_n} \\] <p>and get \\(c_i^k\\) out in terms of \\(a^k_{i\\pmb{j}}\\)</p> \\[ \\begin{align*} c_1^k &amp;= a^k_{00\\cdots 0}\\\\ c_2^k &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0}+a^k_{010\\cdots 0}c^1_1 + a^k_{0010\\cdots 0}c^2_1+\\cdots a^k_{00\\cdots01}c^n_1) \\\\ &amp;= \\frac{1}{2!} (a^k_{10\\cdots 0} + a^k_{010\\cdots 0}a^1_{00\\cdots 0} + a^k_{0010\\cdots 0}a^2_{00\\cdots 0}+\\cdots a^k_{00\\cdots01}a^n_{00\\cdots 0}) \\end{align*} \\] <p>Generally, we have </p> \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\". Theoretically, we can represent the solution by definite power series.</p> <p>We leave the proof of this part as an additional work in Appendix at the end of the doc.</p> <ul> <li>Prove the above series converges.</li> </ul> <p>Here we formulate another ODE and use Excellent function to bound the above power series.</p> <p>Since \\(f_k(x,\\pmb{y})\\) is analytic on region \\(R\\), by Lemma 1, there exists an excellent function of \\(f_k(x,\\pmb{y})\\) on a smaller region \\(R_0\\):</p> \\[ F_k(x,\\pmb{y}) = \\frac{M}{\\left(1-\\frac{x-x_0}{a}\\right)\\left(1-\\frac{y_1-y_{10}}{b}\\right)\\cdots\\left(1-\\frac{y_n-y_{n0}}{b}\\right)}, \\quad k=1,2\\cdots n \\] <p>If we represent both of them in terms of power series</p> \\[ \\begin{align*} f_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty a_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\\\ F_k(x,\\pmb{y})&amp;=\\sum_{i=0.\\pmb{j}=0}^\\infty A_{i\\pmb{j}}(x-x_0)^i(\\pmb{y}-\\pmb{y}_0)^{\\pmb{j}} \\end{align*} \\] <p>then we have a relation \\(|a_{i\\pmb{j}}|&lt;A_{i\\pmb{j}}\\), which matters in the following proof.</p> <p>Now consider an ODE</p> \\[ \\frac{d y_k}{dx} = F_k(x,\\pmb{y}),\\quad y_k(x_0)=y_k,\\quad  k=1,2,\\cdots,n,\\quad   \\] <p>by Lemma 2, the above ODE has an analytic solution \\(\\pmb{y}=\\pmb{y}(x)\\), represented by power series</p> \\[ y_k(x) = y_{k0} + \\sum_{i=1}^\\infty C_i^k(x-x_0)^i,\\quad k=1,2,\\cdots,n \\] <p>Similarly, we have </p> \\[ \\begin{align*} C^k_m&amp;=P_m^k(A^l_{00\\cdots 0}, A^l_{01\\cdots 0},\\cdots,A^l_{i{j_1}\\cdots{j_n}})\\\\ &amp;=P_m^k(|A^l_{00\\cdots 0}|, |A^l_{01\\cdots 0}|,\\cdots,|A^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq P_m^k(|a^l_{00\\cdots 0}|, |a^l_{01\\cdots 0}|,\\cdots,|a^l_{i{j_1}\\cdots{j_n}}|)\\\\ &amp;\\geq |c_m^k| \\end{align*} \\] <p>So power series \\(\\sum\\limits_{i=1}^\\infty C_i^k(x-x_0)^i\\) is en excellent series of \\(\\sum\\limits_{i=1}^\\infty c_i^k(x-x_0)^i\\). Since the former is convergent by Lemma 2, so the latter also converges.</p>"},{"location":"Math/Ordinary_Differential_Equation/General_Theory/Power_Series/#Appendix","title":"\u9644\u5f55 | Appendix: Relation between \\(c\\) and \\(a\\)","text":"<p> Theorem. Prove  \\[ c^k_m=P_m^k(a^l_{00\\cdots 0}, a^l_{01\\cdots 0},\\cdots,a^l_{i{j_1}\\cdots{j_n}}) \\] <p>where \\(i+j_1+j_2+\\cdots+j_n\\leq m-1\\), \\(1\\leq l\\leq n\\). Thus, \\(P_m^k\\) is a polynomial represented by \\(a^l_{00\\cdots 0}\\), \\(a^l_{01\\cdots 0}\\), \\(\\cdots\\), \\(a^l_{i{j_1}\\cdots{j_n}}\\) with positve operator \"+\".  </p> HintsProof <p>Use induction.</p>"},{"location":"Math/Real_Analysis/","title":"Real Analysis","text":"<p>Reference</p> <ul> <li> <p>\u5b9e\u53d8\u51fd\u6570, \u5468\u6027\u4f1f</p> </li> <li> <p>\u5b9e\u53d8\u51fd\u6570\u8bba, \u5468\u6c11\u5f3a</p> </li> <li> <p>\u5b9e\u53d8\u51fd\u6570\u4e0e\u6cdb\u51fd\u5206\u6790\u6982\u8981, \u90d1\u7ef4\u884c</p> </li> </ul>"},{"location":"Math/Real_Analysis/#preliminary-sets","title":"Preliminary: Sets","text":""},{"location":"Math/Real_Analysis/#lebesgue-measure","title":"Lebesgue Measure","text":""},{"location":"Math/Real_Analysis/#measurable-function","title":"Measurable Function","text":""},{"location":"Math/Real_Analysis/#lebesgue-integral","title":"Lebesgue Integral","text":""},{"location":"Math/Real_Analysis/#differential-integral","title":"Differential &amp; Integral","text":""},{"location":"Math/Real_Analysis/#lp-space","title":"\\(L^p\\) Space","text":""},{"location":"Math/Real_Analysis/Differential_Integral/","title":"Differential &amp; Integral","text":"<p>From we already known, differential operation and integration operation could be inverse in terms of Riemann integral.</p> <p>Riemann Integration</p> <p>(i) If \\(f\\in R[a,b]\\), and is continuous at \\(x_0\\), then </p> \\[ F(x)=\\int_a^x f(t)dt \\] <p>is differentiable at \\(x_0\\), and \\(F'(x_0)=f(x_0)\\).</p> <p>(ii) If \\(f\\in C^1[a,b]\\), and \\(f'\\in R[a,b]\\), then \\(\\forall x\\in [a,b]\\), we have</p> \\[ \\int_a^xf'(t)dt=f(x)-f(a). \\] <p>In this chapter, we aim to extend the above theorem in terms of Lebesgue Integration.</p>"},{"location":"Math/Real_Analysis/Differential_Integral/#vitalis-covering-theorem","title":"Vitali's Covering Theorem","text":"<p>Vitali Covering</p> <p>Assume \\(E\\subset \\mathbb{R}\\), \\(\\Lambda=\\{I_\\alpha\\}\\) is a family of intervals. If \\(\\forall x\\in E\\), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists I_\\alpha\\in \\Lambda\\), such that \\(x\\in I_\\alpha\\), \\(m(I_\\alpha)&lt;\\varepsilon\\), then we call \\(\\Lambda\\) is a covering in a sense of Vitali.</p> <p>The following theorem is the main result in a sense of Vitali.</p> <p>Vitali's Covering Theorem</p> <p>Assume \\(E\\subset \\mathbb{R}\\), and \\(m^*(E)&lt;+\\infty\\). If \\(\\Lambda\\) is a Vitali covering of \\(E\\), then \\(\\forall \\varepsilon&gt;0\\), there exist finite number of intervals \\(\\{I_k\\}_{1\\leq k\\leq n}\\in \\Lambda\\) which are mutually disjoint, such that</p> \\[ m\\left(E-\\bigcup_{k=1}^n I_k\\right)&lt;\\varepsilon. \\] Proof"},{"location":"Math/Real_Analysis/Differential_Integral/#dini-differential-quotient","title":"Dini Differential Quotient","text":"<p>Dini Differential Quotient</p> <p>Assume \\(f\\) is defined on a neighborhood of \\(x_0\\in \\mathbb{R}\\), define </p> <p>(i) upper right DQ: \\(D^+f(x_0)=\\overline{\\lim}\\limits_{h\\rightarrow 0^+}\\frac{f(x_0+h)-f(x_0)}{h}\\),</p> <p>(ii) lower right DQ: \\(D_+f(x_0)=\\underline{\\lim}\\limits_{h\\rightarrow 0^+}\\frac{f(x_0+h)-f(x_0)}{h}\\),</p> <p>(iii) upper left DQ: \\(D^-f(x_0)=\\overline{\\lim}\\limits_{h\\rightarrow 0^-}\\frac{f(x_0+h)-f(x_0)}{h}\\),</p> <p>(iv) lower left DQ: \\(D_-f(x_0)=\\underline{\\lim}\\limits_{h\\rightarrow 0^-}\\frac{f(x_0+h)-f(x_0)}{h}\\).</p> <p>There are some properties.</p> <p></p> <p>Properties of Dini Differential Quotient</p> <p>(i) \\(D^+f(x_0)\\geq D_+f(x_0)\\), \\(D^-f(x_0)\\geq D_-f(x_0)\\).</p> <p>(ii) \\(D^+(-f)=-D_+f\\), \\(D^-(-f)=-D^-f\\).</p> <p>Example. Calculate Dini Differential Quotient of </p> \\[ f(x)=\\begin{cases} \\sin \\frac{1}{x},\\quad &amp;x\\neq 0,\\\\ 0,\\quad &amp;x=0. \\end{cases} \\] <p>and \\(g(x)=xf(x)\\).</p> Answer <p>\\(D^+f=D^-f=+\\infty\\), \\(D_+f=D_-f=-\\infty\\).</p> <p>\\(D^+g=D^-g=+1\\), \\(D_+g=D_-g=-1\\).</p>"},{"location":"Math/Real_Analysis/Differential_Integral/#monotonic-real-function","title":"Monotonic Real Function","text":"<p>For monotonically increasing function, we could prove it has differential quotient. </p> <p>Lemma: Monotonically increasing function has Dini DQ</p> <p>Assume \\(f\\) is a monotonically increasing real function on \\([a,b]\\), then \\(f'(x)\\) exists \\(a.e.\\).</p> Proof <p>For monotonically increasing function \\(f\\), to show that its four Dini differential quotients are of the same, we only have to show that </p> \\[ D^+f\\leq D_-f,\\quad D^-f\\leq D_+f. a.e.x\\in [a,b]. \\] <p>because the above two inequation combined with (i) in Property of Dini differential quotient could deduce \"=\".</p> <p>In a nutshell, we have to show that </p> \\[ m(E_1)=m(E_2)=0, \\] <p>where </p> \\[ E_1=\\{x: D^+f&gt; D_-f\\}, E_2=\\{D^-f&gt; D_+f\\}. \\] <p>We prove this using Vitali's covering Theorem.</p> <p>Finally, we have the following theorem. </p> <p>Lebesgue Monotonic Differential Theorem</p> <p>Assume \\(f(x)\\) is a monotonically increasing real function on \\([a,b]\\), then \\(f'(x)\\) exists \\(a.e.\\), and \\(f'\\in L[a,b]\\), and </p> \\[ \\int_{[a,b]}f'(x)dx\\leq f(b)-f(a). \\] Proof <p>Let </p> \\[ f_n(x)=n\\left[f\\left(x+\\frac{1}{n}\\right)-f(x)\\right], \\quad x\\in [a,b]. \\] <p>and \\(f(x)=f(b), x&gt;b\\). So because \\(f\\) is monotonically increasing, \\(f_n(x)\\leq 0\\), and from Lemma which tells us \\(f\\) has differential quotient, we have</p> \\[ \\lim_{n\\rightarrow \\infty}f_n(x)=f'(x), \\quad a.e.x\\in [a,b]. \\] <p>By Fatou Lemma, we have</p> \\[ \\begin{align*} \\int_a^b f'(x)dx&amp;\\leq \\underline{\\lim}\\limits_{n\\rightarrow \\infty}\\int_a^b f_n(x)dx\\\\ &amp;=\\underline{\\lim}\\limits_{n\\rightarrow \\infty}\\int_a^b n\\left[f\\left(x+\\frac{1}{n}\\right)-f(x)\\right] dx\\\\ &amp;=\\underline{\\lim}\\limits_{n\\rightarrow \\infty}\\left[ n\\int_b^{b+\\frac{1}{n}}f(x)dx-n\\int_a^{a+\\frac{1}{n}}f(x) dx\\right]\\\\ &amp;=\\underline{\\lim}\\limits_{n\\rightarrow \\infty}\\left[ f(b)- n\\int_a^{a+\\frac{1}{n}}f(x) dx\\right]\\quad\\text{by }f(x)=f(b), x&gt;b\\\\ &amp;\\leq f(b)-f(a). \\end{align*} \\] <p>See the following typical examples to check the condition and conclusion of the above theorem.</p> <p>Example. Assume \\(E\\subset [a,b]\\) is a zero-measure set. Then there exists a monotonically increasing real continuous function \\(f\\) on \\([a,b]\\), such that for every \\(x\\in E\\), we have \\(f'(x)=\\infty\\).</p> Proof <p>For zero-measure set \\(E\\), there exists a monotonically decreasing sequence of sets \\(\\{G_n\\}_{n\\geq 1}\\), such that</p> \\[ E\\subset G_n,\\quad G_n&lt;\\frac{1}{2^n}. \\] <p>If we let </p> \\[ f_n(x)=m([a,x]\\cap G_n), \\quad f(x)=\\sum_{i=1}^\\infty f_n(x). \\] <p>Similar to proof in Example for measure of interval intersection, we could prove that \\(f_n\\) is non-negative monotonically increasing continuous function which is not generalized(convergent, so we could change order in series).</p> <p>Now for all \\(x\\in E\\), \\(n\\geq 1\\), so long as \\(h\\) is sufficiently small, such that \\([x,x+h]\\subset G_n\\). Because \\(G_k\\) is monotonically decreasing, so \\([x,x+h]\\subset G_k\\), for \\(k=1,2,\\cdots,n\\). So differential quotient</p> \\[ \\begin{align*} \\frac{f(x+h)-f(x)}{h}&amp;=\\frac{1}{h}\\left[\\sum_{k=1}^\\infty f_k(x+h)- \\sum_{k=1}^\\infty f_k(x)\\right]\\\\ &amp;=\\frac{1}{h}\\sum_{k=1}^\\infty \\left[ f_k(x+h)-f_k(x)\\right]\\\\ &amp;\\geq \\frac{1}{h}\\sum_{k=1}^n \\left[ f_k(x+h)-f_k(x)\\right]\\\\  &amp;=\\frac{1}{h}\\sum_{k=1}^n m([x,x+h]\\cap G_k)\\\\ &amp;=\\frac{1}{h}\\sum_{k=1}^n m([x,x+h])\\\\ &amp;=\\frac{nh}{h}=n \\end{align*} \\] <p>So \\(f'(x)=\\lim\\limits_{n\\rightarrow \\infty}n=\\infty\\).</p> <p>This example shows that in Lebesgue Monotonic Differential Theorem, we could not weaken condition \"\\(f\\) is differential \\(a.e.x\\in E\\)\" any more.</p> <p>Example. Prove: There exists a strictly monotonically increasing continuous function on \\([a,b]\\), such that </p> \\[ f'(x)=0,\\quad a.e.x\\in [a,b]. \\] Proof <p>This example shows that we could not change \"\\(\\leq\\)\" into \"\\(=\\)\" in conclusion of Lebesgue Monotonic Differential Theorem</p>"},{"location":"Math/Real_Analysis/Differential_Integral/#bounded-variation-function","title":"Bounded Variation Function","text":"<p>Assume \\(f(x)\\) is a real function (not genaralized) on \\([a,b]\\), we make a partition \\(\\Delta: a=x_0&lt;x_1&lt;\\cdots&lt;x_n=b\\) and sum \\(|f(x_i)-f(x_{i-1})|\\) up, denoted by</p> \\[ V_\\Delta(f)=\\sum_{i=1}^\\infty|f(x_i)-f(x_{i-1})|, \\] <p>which is called variation of \\(f(x)\\) on \\([a,b]\\) corresponding to partition \\(\\Delta\\). Apparently, a variation of a function is dependent on its partition \\(\\Delta\\), so we want to get rid of its influence, by defining its Total Variation as</p> \\[ V_a^b(f):=\\sup\\{V_\\Delta: \\forall \\Delta \\text{ of } [a,b]\\}. \\] <p>If \\(V_a^b&lt;\\infty\\), then we call \\(f(x)\\) is a bounded variation function. The whole set of bounded variation functions on \\([a,b]\\) is denoted by \\(BV([a,b])\\).</p> <p>Corollary: Monotonic real function must be bounded variation function</p> <p>Monotonic real function (not generalized) must be bounded variation.</p> Proof <p>\\(V_\\Delta(f)=|f(b)-f(a)|&lt;\\infty\\).</p> <p>While monotonic real function is bounded variation function, it is not so for continuous function, as shown in the following example.</p> <p> Example. Assume  \\[ f(x)=\\begin{cases} x\\cos\\frac{\\pi}{2x},\\quad &amp;x\\in(0,1],\\\\ 0,\\quad &amp;x=0. \\end{cases} \\] <p>Then Prove: \\(f(x)\\) is continuous on \\([0,1]\\), but is not bounded variation. </p> Proof <p>For \\(n\\geq 1\\), construct a partition</p> \\[ \\Delta_n:\\left\\{0,\\frac{1}{2n},\\frac{1}{2n-1},\\cdots,\\frac{1}{2},1 \\right\\} \\] <p>then</p> \\[ \\begin{align*} V_\\Delta (f)&amp;=\\frac{1}{2n}+\\frac{1}{2n}+\\frac{1}{2(n-1)}+\\frac{1}{2(n-1)}+\\cdots+\\frac{1}{2}+\\frac{1}{2}\\\\ &amp;=\\sum_{i=1}^n \\frac{1}{i}\\rightarrow \\infty (n\\rightarrow \\infty) \\end{align*} \\] <p>So it is not bounded variation.</p> <p>Properties for bounded variation function</p> <p>(i) Bounded variation function on \\([a,b]\\) must be bounded.</p> <p>(ii) Operations. If \\(f\\) and \\(g\\) are both bounded variation function, then \\(f\\pm g\\) and \\(fg\\) both are bounded variation functions. And if \\(|g(x)|\\geq \\lambda&gt;0\\), then \\(f/g\\) are also bounded variation function.</p> <p>(iii) Region additivity. If \\(f\\) is a function on \\([a,b]\\), then \\(\\forall c\\in [a,b]\\),</p> \\[ V_a^b(f)=V^c_a(f)+V^b_c(f). \\] Proof <p>(i) \\(\\forall a\\in [a,b]\\), we have</p> \\[ |f(x)-f(a)|&lt;V_a^b (f)&lt;\\infty \\] <p>so \\(|f(x)|\\leq |f(a)|+V_a^b (f)\\).</p> <p>(ii) We could use </p> \\[ \\begin{align*} V_\\Delta(f+g)&amp;=\\sum_{i}|(f+g)(x_i)-(f+g)(x_{i-1})|\\\\ &amp;\\leq \\sum_{i}|f(x_i)-f(x_{i-1})| + \\sum_{i}|g(x_i)-g(x_{i-1})|\\\\ &amp;=V_\\Delta(f) + V_\\Delta(g)\\leq V_a^b(f)+V^b_a(g) \\end{align*} \\] <p>to show that \\(f+g\\) is also bounded variation function.</p> <p>(iii) For \"\\(\\leq\\)\", we use \\(\\tilde{\\Delta}=\\Delta\\cup \\{c\\}\\).</p> <p>For \"\\(\\geq\\)\", we use \\(\\Delta=\\Delta_1\\cup \\Delta_2\\).</p> <p>From property (iii), we could know that \\(\\forall x\\in [a,b]\\), \\(f\\) is also bounded variation on \\([a,x]\\). So function \\(F(x)=V_a^x(f)\\) is called incomplete bounded variation function, which is apparently a non-negative function and monotonically increases.</p> <p>Continuity relation between \\(f\\) and \\(V_a^x(f)\\)</p> <p>If \\(f\\) is a bounded variation function on \\([a,b]\\), then \\(f(x)\\) and \\(V_a^x(f)\\) have same left and right continuous points.</p> Proof <ul> <li>Points from \\(V_a^x(f)\\) to \\(f\\).</li> </ul> <p>Assume \\(x_0\\in [a,b)\\) is a right continuous point of \\(V_a^x(f)\\), then</p> \\[ |f(x_0+h)-f(x_0)|\\leq V_{x_0}^{x_0+h}(f)=V_{a}^{x_0+h}(f)-V_{a}^{x_0}(f). \\] <p>So \\(x_0\\) is also the right continuous point of \\(f\\). Similar logic for left continuous points.</p> <ul> <li>Points from \\(f\\) to \\(V_a^x(f)\\).</li> </ul> <p>Assume \\(x_0\\in[a,b)\\) is a right continuous point of \\(f\\). Then \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that</p> \\[ \\begin{align} |f(x)-f(x_0)|&lt;\\frac{\\varepsilon}{2},\\quad x_0&lt;x&lt;x_0+\\delta. \\label{right-continuous-point} \\end{align} \\] <p>and because \\(\\lim_{x\\rightarrow x_0^+}V_a^x(f)&lt;\\infty\\)</p> \\[ \\begin{align} 0&lt;V_a^{x_2}(f)-V_a^{x_1}(f)&lt;\\frac{\\varepsilon}{2},\\quad x_0&lt;x_1&lt;x_2&lt;x_0+\\delta. \\label{F-continuous} \\end{align} \\] <p>\\(\\forall x\\in (x_0,x_0+\\delta)\\), for a parition \\(\\Delta: \\{y_k\\}_{0\\leq k\\leq n}\\)of \\([x_0,x]\\):</p> \\[ x_0=y_0&lt;y_1&lt;\\cdots &lt;y_n=x, \\] <p>by inequation \\(\\ref{right-continuous-point}\\), we have</p> \\[ |f(y_1)-f(y_0)|&lt;\\frac{\\varepsilon}{2}. \\] <p>by inequation \\(\\ref{F-continuous}\\), we have</p> \\[ \\sum_{k=2}^n |f(y_k)-f(y_{k-1})|\\leq V_{y_1}^x(f)=V_a^{x}(f)-V_a^{y_1}(f)&lt;\\frac{\\varepsilon}{2}. \\] <p>so combine the above two inequation, we have</p> \\[ \\sum_{k=1}^n|f(y_k)-f(y_{k-1})|&lt;\\varepsilon. \\] <p>By the arbitrariness of partition, we have</p> \\[ V_a^x(f)-V_a^{x_0}(f)=V_{x_0}^x(f)\\leq \\varepsilon, \\quad x_0&lt;x&lt;x_0+\\delta. \\] <p>So \\(V_a^x(f)\\) is right continuous at \\(x_0\\). Similar logic for left continuous point. We are done.</p> <p>From the above theorem, we could have the following corollary.</p> <p>Corollary: bounded variation function has at most denumerable discontinuity points</p> <p>If \\(f\\) is bounded variation on \\([a,b]\\), then the number of discontinuity points of \\(f\\) is at most denumerable.</p> Proof <p>\\(f\\) has the same discontinuity points with \\(V_a^x(f)\\), while the latter is a monotonically increasing function, which has at most denumerable discontinuity points as Example shows.</p> <p>Jordan Decomposition Theorem: description of bounded variation function</p> <p>\\(f\\) is a bounded variation function, iff \\(f\\) could be expressed by the subtraction of two monotonically increasing real functions.</p> Proof <ul> <li> <p>\"\\(\\Leftarrow\\)\". \\(g,h\\) are monotonically increasing, so \\(g, h\\) are both bounded variation functions. By property (ii), we could know that \\(f=g-h\\) is also bounded variation function.</p> </li> <li> <p>\"\\(\\Rightarrow\\)\". Assume \\(f\\) is bounded variation, then \\(\\forall a\\leq x_1\\leq x_2\\leq b\\),</p> </li> </ul> \\[ f(x_2)-f(x_1)\\leq V_{x_1}^{x_2}(f)=V_a^{x_2}(f)-V_a^{x_1}(f), \\] <p>which means \\(V_a^{x_1}(f)-f(x_1)\\leq V_a^{x_2}(f)-f(x_2)\\).</p> <p>so define \\(g=V_a^x(f)\\), \\(h=V_a^x(f)-f(x)\\), both are monotonically increasing. Then \\(f=g-h\\) and we are done.</p> <p></p> <p>Corollary: bounded variation function must be differential a.e.</p> <p>Bounded variation function \\(f\\) on \\([a,b]\\) must be differential a.e. and its derivative \\(f'\\) is L integrable.</p> Proof <p>\\(f=g-h\\), where \\(g,h\\) are monotonically increasing function. By Lebesgue Monotonic Differential Theorem, \\(g,h\\) is differential \\(a.e.\\) on \\([a,b]\\), and their derivatives \\(g',h'\\) are also L integrable. So by operations, \\(f\\) is also differential \\(a.e.\\) on \\([a.b]\\) and \\(f'=f'-h'\\) is also L integrable.</p>"},{"location":"Math/Real_Analysis/Differential_Integral/#indefinite-integration-its-differential","title":"Indefinite Integration &amp; its Differential","text":"<p>Definition of Indefinite Integration</p> <p>If \\(f\\) is L integrable on \\([a,b]\\), then </p> \\[ F(x)=\\int_a^x f(t)dt, \\quad x\\in [a,b] \\] <p>is called Indefinite Integration of \\(f\\).</p> <p>Using what we have known from above, we have the following property for Indefinite integration.</p> <p></p> <p>Property of Indefinite Integration</p> <p>If \\(f\\) is L integrable on \\([a,b]\\), then its indefinite integration \\(F(x)\\) is a continuous bounded variation function on \\([a,b]\\) and satisfies</p> \\[ V_a^b(f)=\\int_a^b|f(t)|dt. \\] Proof <p>\\(F\\) is continuous because of absolute continuity of L integral.</p> <p>To get to the result of this part, we should have a lemma proved.</p> <p>Lemma: indefinite integration equals zero</p> <p>Assume \\(f\\) is L integral on \\([a,b]\\), and its indefinite integration </p> \\[ F(x)\\equiv 0, \\] <p>then \\(f(x)=0,\\quad a.e.x\\in [a,b]\\).</p> Proof <p>For zero-constant function \\(F\\), we have its total variation function \\(V_a^b(F)=0\\). By Property of Indefinite Integration, we have</p> \\[ \\int_a^b|f(t)|dt=0. \\] <p>Then by Example in Non-negative measurable function, we have</p> \\[ |f(x)|=0,\\quad a.e.x\\in [a,b]. \\] <p>and we are done.</p> <p>Finally we have the following theorem.</p> <p>Theorem for Derivative of indefinite integration</p> <p>Assume \\(f\\) is L integrable on \\([a,b]\\), and \\(F(x)\\) is an indefinite integration of \\(f\\), then</p> \\[ F'(x)=f(x),\\quad a.e.x\\in [a,b]. \\] Proof <p>Since \\(F\\) is bounded variation and continuous on \\([a,b]\\), then by Corollary of Jordan Decomposition, we have \\(F\\) has differential \\(a.e.x\\in [a,b]\\) and its derivative \\(F'\\) is L integrable.</p> <ul> <li>\\(f\\) is bounded. So \\(|f(x)|\\leq M\\). </li> </ul> <p>Let </p> \\[ g_n(x)=n\\left[F\\left(x+\\frac{1}{n}\\right)-F(x)\\right], \\] <p>then</p> \\[ \\lim_{n\\rightarrow \\infty}g_n(x)=F'(x),\\quad a.e.x\\in [a,b]. \\] <p>We check \\(g_n\\) for its condition of LDCT. That is,</p> \\[ |g_n(x)|=\\left|n\\left[F\\left(x+\\frac{1}{n}\\right)-F(x)\\right]\\right|=\\left|n\\int_x^{x+\\frac{1}{n}}f(t)dt\\right|\\leq M. \\] <p>So by LDCT, we have </p> \\[ \\begin{align*} \\int_a^x F'(t)dt&amp;=\\lim_{n\\rightarrow \\infty}\\int_a^xg_n(t)dt\\\\ &amp;=\\lim_{n\\rightarrow \\infty}n\\int_a^x\\left[F\\left(t+\\frac{1}{n}\\right)-F(t)\\right]dt\\\\ &amp;=\\lim_{n\\rightarrow \\infty}\\left[n\\int_a^x F\\left(t+\\frac{1}{n}\\right)dt-n\\int_a^x F(t)dt\\right]\\\\ &amp;=F(x)-F(a)=\\int_a^xf(t)dt. \\end{align*} \\] <ul> <li>\\(f\\) is measurable. Let \\(f\\geq 0\\) might as well.</li> </ul>"},{"location":"Math/Real_Analysis/Differential_Integral/#absolute-continuous-function","title":"Absolute Continuous Function","text":"<p>Definition of Absolute continuous function</p> <p>Assume \\(f\\) is a real function on \\([a,b]\\). If \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that for finite number of open intervals \\(\\{(a_k,b_k)\\}_{1\\leq k\\leq n}\\) which are mutually disjoint, so long as \\(\\sum_{k=1}^n |b_k-a_k|&lt;\\delta\\), </p> \\[ \\sum_{k=1}^n|f(b_k)-f(a_k)|&lt;\\varepsilon, \\] <p>then we call \\(f\\) is an absolute continuous function.</p> <p>Corollary: Indefinite integration of L-integrable function must be absolute continuous</p> <p>Assume \\(f\\) is L integrable on \\([a,b]\\), then its indefinite integration \\(F\\) is absolute continuous.</p> Proof <p>Notice that</p> \\[ \\begin{align*} \\sum_{k=1}^n|F(b_k)-F(a_{k})|&amp;=\\sum_{k=1}^n\\left|\\int_{a_{k}}^{b_k} f(t)dt\\right|\\\\ &amp;\\leq \\sum_{k=1}^n\\int_{a_{k}}^{b_k} |f(t)|dt\\\\ &amp;=\\int_{\\bigcup\\limits_{k=1}^n (a_k,b_k)} |f(t)|dt \\end{align*} \\] <p>By absolute continuity of L integral, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that  \\(\\forall \\{a_k,b_k\\}\\) satisfying \\(\\sum_{k=1}^n(b_k-a_k)&lt;\\delta\\), we have</p> \\[ \\sum_{k=1}^n|F(b_k)-F(a_{k})|\\leq \\int_{\\bigcup\\limits_{k=1}^n (a_k,b_k)} |f(t)|dt&lt;\\varepsilon. \\] <p>For absolute continuous function, we have the following properties.</p> <p>Properties of Absolute continuous function</p> <p>(i) Absolute continuous function must be continuous and consistently continuous.</p> <p>(ii) Operations. If \\(f\\) and \\(g\\) are absolute continuous functions, then \\(f\\pm g\\), \\(fg\\) are absolute continuous. And if \\(g\\) has no zero points, \\(f/g\\) is also absolute continuous. </p> <p>(iii) Function composition. If \\(f\\) and \\(\\varphi\\) are absolute continuous on \\([a,b]\\) and \\([p,q]\\) respectively, and \\(a\\leq \\varphi(x)\\leq b\\), \\(\\varphi\\) is strictly monotonically increasing, then function</p> \\[ f(\\varphi(t)) \\] <p>is absolute continuous on \\([p,q]\\).</p> <p>(iv) If \\(f\\) is absolute continuous function on \\([a,b]\\), then it must be bounded variation function.</p> Proof <p>(i) Choose \\(n=1\\) in definition.</p> <p>(ii) Use some formula of triangle inequation.</p> <p>(iii) </p> <p>(iv) .</p>"},{"location":"Math/Real_Analysis/Differential_Integral/#mean-value-theorem-for-integration","title":"Mean Value Theorem for Integration","text":"<p>First mean value theorem for integration</p> <p>Assume \\(f\\) is continuous on \\([a,b]\\), \\(g\\) is non-negative and L integrable on \\([a,b]\\). Then \\(\\exists \\xi\\), such that </p> \\[ \\int_a^bf(x)g(x)dx=f(\\xi)\\int_a^b g(x)dx. \\] Proof <ul> <li> <p>\\(\\int_a^b g(x)dx=0\\). It holds apparently.</p> </li> <li> <p>\\(\\int_a^b g(x)dx\\neq 0\\).</p> </li> </ul> <p>Second mean value theorem for integration</p> <p>Assume \\(f\\) is L integrable, \\(g\\) is monotonic, then \\(\\exists \\xi\\), such that</p> \\[ \\int_a^bf(x)g(x)dx=g(a)\\int_a^\\xi f(x)dx+g(b)\\int_\\xi^bf(x)dx. \\] Proof <p>Let \\(g\\) is monotonically increasing.</p> <ul> <li>(i) \\(g\\in AC[a,b]\\). Use Integration by parts. </li> </ul> <p>Then \\(g'\\geq 0\\) and \\(g'\\in L[a,b]\\). Since \\(f\\in L[a,b]\\), we have \\(F(x)=\\int_a^xf(t)dt\\in AC[a,b]\\) and \\(F'(x)=f(x),\\quad a.e.x\\in [a,b]\\).</p> <p>So</p> \\[ \\begin{align*} \\int_a^bf(x)g(x)dx&amp;=\\int_a^bF'(x)g(x)dx\\\\ &amp;=F(x)g(x)|_a^b-\\int_a^bF(x)g'(x)dx\\\\ &amp;=F(b)g(b)-F(\\xi)\\int_a^bg'(x)dx\\\\ &amp;=F(b)g(b)-F(\\xi)(g(b)-g(a)) \\end{align*} \\] <ul> <li>(ii) \\(g\\) is just monotonically increasing.</li> </ul> <p>Use a partition \\(\\Delta: a=x_0&lt;x_1&lt;\\cdots&lt;x_n=b\\), where \\(x_k=a+k(b-a)/n, k=1,2,\\cdots,n\\).</p> <p>Let</p> \\[ g_n(x)=\\begin{cases} g(x),\\quad &amp;x=x_k,0\\leq k\\leq n,\\\\ g(x_{k-1})+\\frac{n(g(x_k)-g(x_{k-1}))}{(b-a)}(x-x_{k-1}),\\quad &amp;x\\in (x_{k-1},x+k). \\end{cases} \\] <p>Check the convergence of \\(g_n\\). If \\(x\\) is the continuity points, then \\(\\lim\\limits_{n\\rightarrow \\infty}g_n(x)=g(x)\\). If \\(x\\) is discontinuity point, it is at most denumerable, which means it is zero-measure set, meaning that </p> \\[ \\lim\\limits_{n\\rightarrow \\infty}g_n(x)=g(x),\\quad a.e.x\\in [a,b]. \\] <p>We know that \\(g_n\\in AC[a,b]\\), so by (i)</p> \\[ \\int_a^bf(x)g_n(x)dx=g_n(a)\\int_a^{\\xi_n} f(x)dx+g_n(b)\\int_{\\xi_n}^bf(x)dx. \\] <p>We want to use LDCT. Since \\(f(x)g_n(x)\\leq |f(x)|\\max\\{g(b),g(a)\\}\\in L[a,b]\\), </p> \\[ \\begin{align*} \\int_a^bf(x)g(x)dx&amp;=\\lim_{n\\rightarrow\\infty}\\int_a^bf(x)g_n(x)dx\\\\ &amp;=\\lim_{n\\rightarrow\\infty}\\left[g_n(a)\\int_a^{\\xi_n} f(x)dx+g_n(b)\\int_{\\xi_n}^bf(x)dx\\right]\\\\ &amp;=g(a)\\int_a^\\xi f(x)dx+g(b)\\int_\\xi^bf(x)dx. \\end{align*} \\]"},{"location":"Math/Real_Analysis/Lebesgue_Integral/","title":"Lebesgue Integral","text":"<p>This part, we want to use conclusions from previous parts to extend the concept of integral.</p> <p>The basic idea is, first define integral of simple function, then use convergence of simple function sequence to define integral of measurable function. Note that we first define them on non-negative functions. Then by making use of positive and negative part of function to extend to the whole functions.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#non-negative-simple-function-integral","title":"Non-negative simple function integral","text":"<p>Lebesgue Integral of non-negative Simple Function</p> <p>Assume \\(f\\) is a non-negative simple function defined on region measurable set \\(D\\), that is, there exists a partition \\(\\{E_i\\}_{1\\leq i\\leq S}\\), and non-negative real number set \\(\\{a_i\\}_{1\\leq i\\leq S}\\), such that</p> \\[ f(x)=\\sum_{i=1}^S a_i \\chi_{E_i} (x),\\quad x\\in D. \\] <p>Then its Lebesgue integral is defined by</p> \\[ \\int_{D}f(x)dx=\\sum_{i=1}^S a_i m(E_i). \\]"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#properties","title":"Properties","text":"<p>Invariance of Lebesgue Integral</p> <p>Assume \\(f\\) and \\(g\\) are non-negative simple functions on measurable set \\(D\\), and satisfy \\(f(x)=g(x), a.e.x \\in  D\\), then their Lebesgue integrals on \\(D\\) are of the same.</p> Proof <p>Making use of the Partition of simple functions. </p> <p>That is, let \\(\\{E_i\\}_{1\\leq i\\leq N}\\) and \\(\\{F_j\\}_{1\\leq j\\leq M}\\) to be the partition of \\(f\\) and \\(g\\) respectively, let non-negative number sets \\(\\{a_i\\}\\) and \\(\\{b_j\\}\\) to be the range of \\(f\\) and \\(g\\) respectively. Thus </p> \\[ f(x)=\\sum_{i=1}^N a_i \\chi_{E_i}(x), \\quad g(x)=\\sum_{j=1}^M b_j \\chi_{F_j}(x). \\] <p>Once \\(E_i\\cap F_j\\neq \\varnothing\\), we have \\(a_i=b_j\\).</p> <p>Properties for L integral of non-negative simple function</p> <p>Assume \\(f\\) and \\(g\\) are both simple functions on measurable set \\(D\\), then</p> <p>(i) Monotonicity. If \\(f(x)\\leq g(x),a.e.x\\in D\\), then \\(\\int_D fdx\\leq \\int_D gdx\\).</p> <p>(ii) \\(\\int_D fdx \\leq \\max f(x)\\cdot m(D)\\). If \\(m(D)=0\\), then \\(\\int_D fdx=0\\)</p> <p>(iii) Linearity. If \\(\\lambda\\) and \\(\\mu\\) are two non-negative real number, then </p> \\[ \\int_D (\\lambda f + \\mu g)dx=\\lambda \\int_Dfdx+\\mu \\int_D gdx. \\] <p>(iv) Interval additivity. If \\(A, B\\subset D\\) and \\(A\\cap B=\\varnothing\\), then</p> \\[ \\int_{A\\cup B}fdx=\\int_A fdx+\\int_B fdx. \\] Proof <p>(i) similar to proof in Invariance of Lebesgue Integral, just have \\(a_i\\leq b_j\\).</p> <p>(ii) easy to see.</p> <p>(iii) Choose partition \\(\\{E_i\\cap F_j\\}_{1\\leq i\\leq N, 1\\leq j\\leq M}\\).</p> <p>(iv) For partition \\(\\{E_i\\}_{1\\leq i\\leq N}\\), see that</p> \\[ E_i\\cap (A\\cup B)=(E_i\\cap A)\\cup (E_i\\cap B) \\] <p>and \\((E_i\\cap A)\\cap (E_i\\cap B)=\\varnothing\\).</p> <p>The following lemma is a brigde between non-negative simple function and measurable function.</p> <p></p> <p>Lemma: Bridge</p> <p>Assume \\(g\\) and \\(\\{f_n\\}_{n\\geq 1}\\) are non-negative simple functions on measurable set \\(E\\), and satisfy</p> <p>(i) \\(\\{f_n\\}_{n\\geq 1}\\) are monotonically increasing \\(a.e.x\\in E\\).</p> <p>(ii) \\(0\\leq g(x)\\leq \\lim_{n\\rightarrow \\infty}f_n(x) (a.e.x\\in E)\\).</p> <p>Then</p> \\[ \\int_{E}g(x)dx\\leq \\lim_{n\\rightarrow \\infty} \\int_E f_n(x)dx. \\] Proof <p>Notice that \\(\\lim_{n\\rightarrow \\infty}f_n(x)\\) would not be a simple function, so we could not use the monotonicity of Lebesgue integral for simple functions.</p> <p>(i) \\(g(x)\\equiv c&gt;0\\). </p> <p>\\(\\forall 0&lt;\\lambda&lt;1\\), define </p> \\[ E_k:=\\{f_n\\geq \\lambda c\\}, \\forall n\\in \\mathbb{N}^+. \\] <p>It is easy to validify that \\(E_1\\subset E_2\\subset \\cdots\\) and \\(E=\\bigcup_{n=1}^\\infty E_n\\). Thus</p> \\[ \\int_E f_1dx=\\int_{E_n} f_ndx+\\int_{E-E_n}f_ndx\\geq \\int_{E_n}f_ndx\\geq \\lambda c m(E_n) \\] <p>let \\(n\\rightarrow \\infty\\) on both sides, we have</p> \\[ \\lim_{n\\rightarrow \\infty}\\int_{E}f_ndx\\geq \\lambda c m(E)=\\lambda\\int_E gdx \\] <p>at last we let \\(\\lambda\\rightarrow 1\\) and get the result.</p> <p>(ii) Making use of the interval additivity.</p> <p></p> <p>Theorem for limit of monotonically increasing sequences</p> <p>Assume \\(\\{f_n\\}\\) and \\(\\{g_n\\}\\) are non-negative simple functions on measurable set \\(D\\). If \\(\\forall x\\in D\\), \\(\\{f_n(x)\\}\\) and \\(\\{g_n(x)\\}\\) monotonically increase and converge to the same limit, then </p> \\[ \\lim_{n\\rightarrow \\infty}\\int_D f_ndx = \\lim_{n\\rightarrow \\infty}\\int_D g_n dx. \\] Proof <p>The proof is to make use of the lemma above twice.</p> <p>\\(\\forall n \\geq 1\\), \\(\\forall a.e. x\\in D\\), we have</p> \\[ 0\\leq g_n(x)\\leq \\lim_{k\\rightarrow \\infty}g_k(x)\\leq \\lim_{k\\rightarrow \\infty} f_k(x). \\] <p>By Lemma, we have</p> \\[ \\int_D g_ndx\\leq \\lim_{k\\rightarrow \\infty}\\int_D f_kdx. \\] <p>Let \\(n\\rightarrow \\infty\\), we have \\(\\lim\\limits_{n\\rightarrow \\infty}\\int\\limits_D g_ndx\\leq \\lim\\limits_{k\\rightarrow \\infty}\\int\\limits_D f_kdx\\). The opposite direction is of the same logic.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#non-negative-measurable-function-integral","title":"Non-negative measurable function integral","text":"<p>Lebesgue integral of measurable function</p> <p>Assume \\(f\\) is a non-negative measurable function on measurable set \\(D\\). According to Approximation with Simple Function, there exists a non-negative function sequence \\(\\{f_n\\}\\) on \\(D\\), such that \\(\\forall x\\in D\\), \\(\\{f_n(x)\\}\\) monotonically increase and converges to \\(f(x)\\). Thus we define Lebesgue integral of \\(f\\) to be</p> \\[ \\int_D f dx =\\lim_{n\\rightarrow \\infty }\\int_D f_n dx. \\] <p>When \\(\\int_D fdx&lt;\\infty\\), we call \\(f\\) is L integrable on \\(D\\).</p> <p>Example. Use definition of Lebesgue integral to calculate</p> \\[ \\int_{[0,\\infty)} e^{-x}dx \\] <p></p> <p>Equivalent definition for Lebesgue integral of non-negative measurable function</p> <p>Assume \\(f(x)\\) is a non-negative measurable function on \\(E\\), then </p> \\[ \\int_E fdx=\\sup_{\\varphi(x)\\leq f(x)}\\left\\{\\int_E \\varphi(x) dx: \\varphi(x) \\text{ is a non-negative simple function}\\right\\}. \\] <p>Or to be more specific, if we define \\(\\varPhi(f,E)\\) is a set composed by non-negative simple function \\(\\varphi\\) that satisfy \\(\\varphi\\leq f\\) on \\(E\\), then</p> \\[ \\lim_{k\\rightarrow \\infty}\\int_E \\varphi_k(x)dx=\\sup_{\\varphi\\in \\varPhi(f,E)}\\int_E \\varphi(x)dx. \\] Proof <ul> <li> <p>\\(\\leq\\). Apparently, because \\(\\forall k\\in \\mathbb{N}^+\\), we have \\(\\varphi_k\\in \\varPhi(f,E)\\), so \"\\(\\leq\\)\" holds naturally.</p> </li> <li> <p>\\(\\geq\\). We know that \\(\\forall \\varphi\\in \\varPhi(f,E)\\), it satisfies \\(\\varphi(x)\\leq f(x)=\\lim_{k\\rightarrow \\infty}\\varphi_k(x),\\forall x\\in E\\), so by monotonicity</p> </li> </ul> \\[ \\int_E \\varphi(x)dx\\leq\\int_E\\lim_{k\\rightarrow \\infty}\\varphi_k(x)dx \\] <p>by Theorem for limit of monotonically increasing sequences, we have</p> \\[ \\int_E \\varphi(x)dx\\leq\\lim_{k\\rightarrow \\infty}\\int_E \\varphi_k dx \\] <p>apply superior operation on both sides and we have \"\\(\\geq\\)\" hold.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#properties_1","title":"Properties","text":"<p>Properties for L integral of non-negative measurable function</p> <p>Assume \\(f\\) is a non-negative measurable function on \\(E\\), then </p> <p>(i) \\(0\\leq \\int_E f(x)dx\\leq \\infty\\).</p> <p>(ii) Linearity. If \\(\\lambda\\) and \\(\\mu\\) are two non-negative real number, then </p> \\[ \\int_E (\\lambda f + \\mu g)dx=\\lambda \\int_E fdx+\\mu \\int_E gdx. \\] <p>(iii) if \\(A\\subset E\\) is also a measurable set, then </p> \\[ \\int_A f(x)dx=\\int_E f(x)\\chi_A(x)dx \\] <p>(iv) if \\(E=A\\cup B, A\\cap B=\\varnothing\\), then </p> \\[ \\int_E fdx=\\int_Afdx+\\int_B fdx. \\] <p>(v) Monotonicity. If \\(f(x)\\leq g(x), \\forall x\\in E\\), then </p> \\[ \\int_E f(x)dx\\leq \\int_E g(x)dx. \\] Proof <p>(iv) Using interval additivity of non-negative simple functions.</p> <p>(v) similar to monotonicity for L integral of simple functions. </p> <p></p> <p>Example. Assume \\(m(E)&gt;0\\), \\(f\\in L(E)\\), \\(f\\geq 0\\), prove: if </p> \\[ \\int_E fdx=0, \\] <p>then </p> \\[ f(x)=0,\\quad a.e.x\\in E. \\] Proof <p>Transfer problem into</p> \\[ m(\\{f(x)&gt;0\\})=0. \\] <p>\\(\\forall n\\geq 1\\), </p> \\[ \\begin{align*} \\int_E f(x)dx&amp;\\geq \\int_{\\{f\\geq 1/n\\}}f(x)dx\\\\ &amp;\\geq \\int_{\\{f\\geq 1/n\\}}\\frac{1}{n}dx\\\\ &amp;=\\frac{1}{n}m(\\{f\\geq 1/n\\}) \\end{align*} \\] <p>which means \\(m(\\{f\\geq 1/n\\})=0\\), so</p> \\[ \\{f&gt;0\\}=\\bigcup_{n=1}^\\infty\\{f\\geq \\frac{1}{n}\\} =0, \\] <p>which means \\(f(x)=0, a.e.x\\in E\\).</p> <p></p> <p>Chebyshev Inequation</p> <p>Assume \\(f\\) is a non-negative function on \\(E\\), then \\(\\forall \\alpha&gt;0\\), </p> \\[ m(\\{f\\geq \\alpha\\})\\leq \\frac{1}{\\alpha}\\int_E f(x)dx. \\] Proof <p>Let \\(A=\\{f\\geq \\alpha\\}\\), then</p> \\[ \\int_E fdx\\geq \\int_A fdx \\geq \\int_A \\alpha dx = \\alpha m(A). \\] <p>Example. \\(f_k(x)\\geq 0\\), \\(f_k\\in L(E)\\). If </p> \\[ \\lim_{k\\rightarrow \\infty}\\int_E f_k(x)dx=0, \\] <p>then </p> \\[ f_k\\overset{m}{\\rightarrow} 0. \\] Proof <p>\\(\\forall \\sigma&gt;0\\), by Chebyshev Inequation</p> \\[ \\sigma \\cdot m(\\{|f_k-0|\\geq \\sigma\\})\\leq \\int_{\\{|f_k-0|\\geq \\sigma\\}} f_kdx \\leq \\int_E f_kdx \\rightarrow 0(k\\rightarrow \\infty)  \\] <p>which means \\(m(\\{|f_k-0|\\geq \\sigma\\})\\rightarrow 0\\).</p> <p>Example. Assume \\(f\\) is a non-negative function on \\(\\mathbb{R}\\) with positive period \\(T\\), \\(f\\in L([0,T])\\), prove:</p> \\[ \\lim_{x\\rightarrow \\infty}\\frac{1}{x}\\int_0^x f(t)dt=\\frac{1}{T}\\int_0^T f(t)dt. \\] Proof <p>For all \\(k\\geq 1\\), we have</p> \\[ \\int_{kT}^{(k+1)T}f(t)dt=\\int_0^T f(t+kT)dt=\\int_0^Tf(t)dt. \\] <p>So when \\(kT\\leq x\\leq(k+1)T\\), or \\(\\frac{1}{T}\\geq \\frac{k}{x}\\geq \\frac{k}{(k+1)T}\\). Let \\(k\\rightarrow \\infty\\), we have the middle item</p> \\[ \\frac{k}{x}\\rightarrow \\frac{1}{T}. \\] <p>so</p> \\[ \\begin{align} \\nonumber\\int_0^x f(t)dt&amp;=\\int_0^{kT}f(t)dt+\\int_{kT}^x f(t)dt\\\\ \\nonumber\\int_0^xf(t) dt&amp;=k\\int_0^T f(t)dt+\\int_{kT}^x f(t)dt\\\\ \\Rightarrow \\frac{1}{x}\\int_0^xf(t)dt&amp;=\\frac{k}{x}\\int_0^Tf(t)dt+\\frac{1}{x}\\int_{kT}^xf(t)dt\\label{T-int} \\end{align} \\] <p>Since </p> \\[ \\left|\\int_{kT}^x f(t)dt\\right|\\leq \\int_{kT}^x |f(t)|dt\\leq \\int_{kT}^{(k+1)T} |f(t)|dt=\\int_0^T|f(t)|dt=C \\] <p>is bounded. So \\(\\frac{1}{x}\\int_{kT}^xf(t)dt\\rightarrow 0 (x\\rightarrow \\infty)\\). So we let \\(x\\rightarrow \\infty\\) in equation \\(\\ref{T-int}\\), we have the result.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#genaral-measurable-function-integral","title":"Genaral measurable function integral","text":"<p>Lebesgue integral of Genaral measurable function</p> <p>Assume \\(f\\) is a measurable fuunction on \\(D\\). \\(\\forall x\\in D\\), define positive and negative part of \\(f\\)</p> \\[ f^+(x):=\\max\\{0,f(x)\\},\\quad f^-(x):=\\max\\{0,-f(x)\\} \\] <p>then it is easy to see that \\(f^+\\) and \\(f^-\\) are non-negative measurable functions and</p> \\[ f(x)=f^+(x)-f^-(x),\\quad |f(x)|=f^+(x)+f^-(x). \\] <p>If either of \\(f^+\\) and \\(f^-\\) are \\(\\infty\\) at the same time, then define Lebesgue integral of \\(f\\) to be</p> \\[ \\int_D f(x)dx=\\int_D f^+(x)dx-\\int_D f^-(x)dx. \\] <p>when the above item is finite, we call \\(f\\) is Lebesgue integrable on \\(D\\), denoted as \\(f\\in L(D)\\).</p> <p>Equivalent condition for Lebesgue integrable</p> <p>\\(f\\in L(D)\\) iff \\(|f|\\in L(D)\\).</p> <p>When \\(f\\) is Lebesgue integrable, it apparently satisfies</p> \\[ \\left|\\int_D fdx\\right|\\leq \\int_D |f|dx \\] Proof <p>\\(f\\in L(D)\\) iff \\(f^+, f^-\\in L(D)\\) iff \\(|f|\\in L(D)\\). </p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#properties_2","title":"Properties","text":"<p>Properties for L integral of general measurable function</p> <p>(i) If \\(f\\in L(E)\\), then \\(|f(x)|&lt;+\\infty, a.e.x\\in E\\).</p> <p>(ii) If \\(f\\) is a generalized real function on zero-measure set \\(A\\), then \\(\\int_A fdx=0\\).</p> <p>(iii) If \\(f\\) and \\(g\\) are measurable functions on \\(E\\), and \\(f(x)=g(x),a.e.x\\in E\\), then \\(f\\) and \\(g\\) are both L integrable or not integrable. When integrable, their values of integral are of the same.</p> <p>(iv) Homotonicity &amp; Linearity. If \\(f, g\\in L(E)\\), \\(a,b\\in \\mathbb{R}\\), then \\(af+bg\\in L(E)\\) and</p> \\[ \\int_E (af(x)+bg(x))dx=a\\int_E f(x)dx+b\\int_E g(x)dx. \\] <p>(v) Interval additivity. If \\(f\\in L(E_1)\\), \\(f\\in L(E_2)\\), and \\(E_1\\cap E_2=\\varnothing\\), then \\(f\\in L(E_1\\cup E_2)\\) and</p> \\[ \\int_{E_1\\cup E_2}fdx=\\int_{E_1} fdx+\\int_{E_2} fdx \\] Proof <p>(i) from equivalent definition.</p> <p>(ii) easy to see.</p> <p>(iii) \\(f=g,a.e.x\\in E\\) \\(\\Rightarrow\\) \\(f^+=g^+, f^-=g^- a.e.x\\in E\\).</p> <p>(iv) pay attention that \\((af)^+\\) has 3 different partition versions, i.e. \\(a=0\\), \\(a&gt;0\\) and \\(a&lt;0\\). </p> <p>(v) Notice that \\(|f+g|\\leq |f|+|g|&lt;\\infty\\), so \\(f+g\\in L(E)\\).</p> <p></p> <p>Absolute Continuity of L integral</p> <p>Assume \\(f\\in L(D)\\). Then \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that \\(\\forall\\) measurable set \\(A\\subset D\\), so long as \\(m(A)&lt;\\delta\\), we have</p> \\[ \\left|\\int_A f(x)dx\\right|&lt;\\varepsilon. \\] Proof <p>Assume \\(f\\) is non-negative (discuss \\(f^+\\) and \\(f^-\\)). \\(\\forall \\varepsilon&gt;0\\), \\(\\exists\\) non-negative simple function \\(\\varphi\\leq |f|\\), such that</p> \\[ \\int_D |f|dx&lt;\\int_D \\varphi dx+\\frac{\\varepsilon}{2}. \\] <p>Because \\(\\varphi\\) is bounded on \\(D\\), so \\(\\varphi(x)\\leq M\\). Choose \\(\\delta=\\frac{\\varepsilon}{2M}\\), such that for all \\(A\\subset D\\), \\(m(A)&lt;\\delta\\), we have</p> \\[ \\begin{align*} \\left|\\int_A fdx\\right|&amp;\\leq \\int_A|f|dx\\\\ &amp;=\\int_A (|f|-\\varphi)dx + \\int_A \\varphi dx\\\\ &amp;&lt;\\frac{\\varepsilon}{2}+m(A)\\cdot M&lt;\\varepsilon. \\end{align*} \\]"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#levi-theorem","title":"Levi Theorem","text":"<p>Levi Theorem</p> <p>Assime \\(f\\) and \\(\\{f_n\\}_{n\\geq 1}\\) are non-negative measurable function on \\(D\\), satisfy </p> \\[ f_1(x)\\leq f_2(x)\\leq\\cdots,\\quad x\\in D \\] <p>and \\(\\lim\\limits_{n\\rightarrow \\infty}f_n(x)=f(x)\\), then </p> \\[ \\int_D fdx=\\lim_{n\\rightarrow \\infty}\\int_D f_n dx. \\] Proof version \u2160Proof version \u2161 <ul> <li>\\(\\int_D fdx\\geq\\lim\\limits_{n\\rightarrow \\infty}\\int_D f_n dx\\).</li> </ul> <p>Notice that limit function \\(f\\) is also measurable, so its L integral has definition. Then since \\(f_1\\leq f_2\\leq\\cdots\\leq f\\), by monotonicity of L integral, we have \\(\\int_E f_n(x)dx\\) monotonically increase, so its limit number \\(\\lim_{n\\rightarrow \\infty}\\int_E f_n(x)dx\\) has definition. By Monotonicity of L integral, </p> \\[ \\lim_{n\\rightarrow \\infty}\\int_E f_ndx\\leq \\int_E fdx. \\] <ul> <li>\\(\\int_D fdx\\leq\\lim\\limits_{n\\rightarrow \\infty}\\int_D f_n dx\\).</li> </ul> <p>Choose a non-negative simple function \\(\\varphi(x)\\), such that \\(\\varphi(x)\\leq f(x),x\\in E\\). Let \\(\\lambda\\in (0,1)\\), denoted</p> \\[ D_n=\\{x\\in D: f_n(x)\\geq \\lambda\\varphi(x)\\} \\] <p>which is monotonically increasing, and \\(\\lim_{n\\rightarrow \\infty}D_n=D\\). So</p> \\[ \\int_D f_ndx\\geq \\int_{D_n}f_ndx\\geq \\int_{D_n}\\lambda \\varphi(x)dx=\\lambda\\int_{D_n}\\varphi(x)dx. \\] <p>If we let \\(n\\rightarrow \\infty\\), the right side of inequation is a limit of integral region, i.e.</p> \\[ \\lim_{n\\rightarrow \\infty}\\int_D f_ndx\\geq \\lambda\\int_D \\varphi(x)dx. \\] <p>at last, let \\(\\lambda\\rightarrow 1\\), we have</p> \\[ \\lim_{n\\rightarrow \\infty}\\int_D f_ndx\\geq \\int_D \\varphi(x)dx. \\] <p>by another definition of L integral of non-negative measurable function, we have</p> \\[ \\lim_{n\\rightarrow \\infty}\\int_D f_ndx\\geq \\int_D f(x)dx. \\] <p>Use another equvalent non-negative simple function sequence \\(\\{\\psi_n\\}\\) of non-negative measurable function \\(f_n\\) to approximate \\(f\\).</p> <p>Firstly, we have to make another simple function sequence useing its definition. That is, for every \\(n\\geq 1\\), \\(f_n\\) is defined by sequence of non-negative simple function </p> \\[ \\{\\varphi_k^{(n)}\\}_{k\\geq 1}, \\] <p>which monotonically increase and converge to \\(f_n\\). Here, the method is a little tricky. For every \\(k\\geq 1\\), define</p> \\[ \\psi_k(x):=\\max\\{\\varphi_k^{(1)}(x),\\varphi_k^{(2)}(x),\\cdots,\\varphi_k^{(k)}(x),\\}, \\quad x\\in D. \\] <p>So \\(\\psi_k(x)\\) is also a non-negative simple function and </p> \\[ \\begin{align} \\nonumber 0\\leq \\psi_1(x)\\leq \\psi_2(x)\\leq\\cdots\\leq \\psi_k(x)&lt;\\cdots,\\quad x\\in D.\\\\ \\varphi_k^{(n)}(x)\\leq \\psi_k(x)\\leq f_k(x),\\quad  1\\leq n\\leq k,x\\in D.\\label{ineq1} \\end{align} \\] <p>Notice that inequation \\(\\ref{ineq1}\\) use \\(n\\leq k\\).</p> <p>So by monotonicity of non-negative simple function, we have</p> \\[ \\begin{align} \\int_D\\varphi_k^{(n)}(x)\\leq \\int_D\\psi_k(x)\\leq \\int_D f_k(x), \\quad 1\\leq n\\leq k.\\label{ineq2} \\end{align} \\] <p>In inequation \\(\\ref{ineq1}\\), let \\(k\\rightarrow \\infty\\) firstly and let \\(n\\rightarrow \\infty\\), and we have</p> \\[ \\begin{align} \\nonumber f_n(x)\\leq \\lim_{k\\rightarrow \\infty}\\psi_k(x)&amp;\\leq f(x). \\quad n\\geq 1,x\\in D.\\\\ \\Rightarrow \\lim_{k\\rightarrow \\infty}\\psi_k(x)&amp;=f(x),\\quad x\\in D.\\label{eq1} \\end{align} \\] <p>Similarly, in inequation \\(\\ref{ineq2}\\), let \\(k\\rightarrow \\infty\\) firstly and let \\(n\\rightarrow \\infty\\), and we have</p> \\[ \\begin{align} \\nonumber \\int_D f_n(x)dx=\\lim_{k\\rightarrow \\infty}\\int_D\\varphi_k^{(n)}(x)&amp;\\leq \\lim_{k\\rightarrow \\infty}\\int_D\\psi_k(x)\\leq \\lim_{k\\rightarrow \\infty}\\int_D f_k(x), \\quad n\\geq 1.\\\\ \\Rightarrow \\lim_{k\\rightarrow \\infty}\\int_D\\psi_k(x)&amp;=\\lim_{n\\rightarrow \\infty}\\int_D f_n(x)dx.\\label{eq2} \\end{align} \\] <p>Equation \\(\\ref{eq1}\\) means \\(f\\) could be approximated by \\(\\{\\psi_K(x)\\}\\), which is just a nen-negative simple function sequence. Equation \\(\\ref{eq2}\\) means </p> \\[ \\int_D f(x)dx\\overset{\\text{definition}}{=}\\lim_{k\\rightarrow \\infty}\\int_D\\psi_k(x)=\\lim_{n\\rightarrow \\infty}\\int_D f_n(x)dx. \\] <p>Example. Some examples which do not satisfy condition of Levi Theorem.</p> <p>(i) \\(f_k(x)=\\chi_{[k,k+1]}(x),\\quad E=\\mathbb{R}\\).</p> <p>\\(f_k(x)\\rightarrow f(x)=0\\), but</p> \\[ \\int_E f_kdx =1&gt;0=\\int_E fdx \\] <p>(ii) \\(f_k(x)=\\frac{1}{k}\\chi_{[0,k]}(x),\\quad E=\\mathbb{R}\\).</p> <p>\\(f_k(x)\\rightrightarrows f(x)=0\\), but</p> \\[ \\int_E f_kdx =1&gt;0=\\int_E fdx \\] <p>Example. Assume \\(f\\) is L integrable on \\(E\\), prove: </p> \\[ \\lim_{k\\rightarrow \\infty}k\\cdot m(\\{|f|&gt;k\\})=0. \\] Answer <p>Use \\(\\{|f|&gt;k\\}\\) to narrow down the integral region.</p> <p>Let </p> \\[ f_k(x)=\\begin{cases} f(x),\\quad &amp;|f|\\leq k,\\\\ 0,\\quad &amp;|f|&gt;k. \\end{cases} \\] <p>Then \\(|f_k|\\) \\(\\nearrow\\) \\(|f|\\). By Levi Theorem, </p> \\[ \\begin{align*} \\int_E |f|dx&amp;=\\lim_{k\\rightarrow \\infty} \\int_E |f_k| dx\\\\ \\Rightarrow \\lim_{k\\rightarrow \\infty}\\int_E(|f|-|f_k|)dx&amp;=0 \\end{align*} \\] <p>While </p> \\[ \\begin{align*} \\int_E(|f|-|f_k|)dx&amp;= \\int_{\\{|f|\\leq k\\}}+\\int_{\\{|f|&gt;k\\}}(|f|-|f_k|)dx\\\\ &amp;=\\int_{\\{f&gt;k\\}}|f|dx \\\\ &amp;&gt; k\\cdot m(\\{|f|&gt;k\\})\\rightarrow 0 (k\\rightarrow \\infty). \\end{align*} \\] <p>There are two natural corollaries derived from Levi Theorem.</p> <p></p> <p>Corollary 1: Series form of Levi Theorem</p> <p>Assume \\(\\{f_n\\}_{n\\geq 1}\\) is a non-negative measurable function sequence, then </p> \\[ \\int_E \\sum_{n=1}^\\infty f_n(x)dx=\\sum_{n=1}^\\infty \\int_E f_n(x)dx. \\] Proof <p>Let \\(u_k(x)=\\sum_{i=1}^k f_i(x)\\), which satisfies the condition of Levi Theorem.</p> <p>Corollary 2: Denumerable Additivity</p> <p>Assume \\(E\\in \\Omega\\), \\(E=\\bigcup_{n=1}^\\infty E_n\\), where \\(E_i\\cap E_j=\\varnothing\\). If \\(f\\in L(E)\\), \\(f\\geq 0\\), then \\(\\forall k\\in \\mathbb{N}^+\\), \\(f\\in L(E_k)\\), and </p> \\[ \\int_E f(x)dx=\\sum_{k=1}^\\infty \\int_{E_k} f(x)dx. \\] Proof <p>Let \\(f_k(x)=f(x)\\sum_{i=1}^k \\chi_{E_i}(x)\\) and use property (iii).</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#fatou-lemma","title":"Fatou Lemma","text":"<p>Fatou Lemma</p> <p>Assume \\(\\{f_n\\}_{n\\geq 1}\\) is a non-negative measurable function sequence, then </p> \\[ \\int_D \\underline{\\lim}\\limits_{n\\rightarrow \\infty} f_ndx\\leq \\underline{\\lim}\\limits_{n\\rightarrow \\infty} \\int_D f_n dx. \\] Proof <p>From the definition of limit inferior of function, for every \\(n\\geq 1\\), we could define </p> \\[ g_n(x)=\\inf_{k\\geq n}f_k(x),\\quad x\\in D. \\] <p>so \\(\\{g_n\\}\\) monotonically increase and converges to \\(\\underline{\\lim}\\limits_{n\\rightarrow \\infty}f_n(x)\\). By Levi Theorem, </p> \\[ \\int_D \\underline{\\lim}\\limits_{n\\rightarrow \\infty}f_n(x)dx=\\lim_{n\\rightarrow \\infty}\\int_D g_n(x)dx. \\] <p>Since \\(g_n(x)\\leq f_n(x)\\), so </p> \\[ \\begin{align*} \\int_D \\underline{\\lim}\\limits_{n\\rightarrow \\infty}f_n(x)dx&amp;=\\underline{\\lim}\\limits_{n\\rightarrow \\infty}\\int_D g_n(x)dx\\\\ &amp;\\leq \\underline{\\lim}\\limits_{n\\rightarrow \\infty}\\int_D f_n(x)dx \\end{align*} \\] <p>This is a quite weak theorem, thus could be used in many fields.</p> <p>Example. \\(f_k(x)\\geq 0\\), \\(f_k\\in L(E)\\), and \\(f_k\\rightarrow f, a.e.x\\in E\\). If </p> \\[ \\exists M&gt;0, \\int_E f_k(x)dx\\leq M, \\] <p>then </p> \\[ f\\in L(E). \\] Proof \\[ \\begin{align*} \\int_E fdx&amp;=\\int_{E_1}\\lim_{k\\rightarrow \\infty}f_kdx\\\\ &amp;\\leq \\lim_{k\\rightarrow \\infty}\\int_{E_1}f_kdx \\quad\\text{by Fatou Lemma}\\\\ &amp;\\leq \\lim_{k\\rightarrow \\infty}\\int_{E}f_kdx&lt;M \\end{align*} \\] <p>Note that limit and limit inferior are of the same here.</p> <p> Example. Assume non-negative measurable function sequence \\(\\{f_k\\}\\) satisfies \\(f_k\\overset{m}{\\rightarrow}f\\), then prove hunmy \\[ \\int_E fdx\\leq \\underline{\\lim}\\limits_{k\\rightarrow \\infty} f_kdx. \\] Proof <p>Get a subsequence \\(\\{f_{k_s}\\}\\) such that </p> \\[ \\int_E f_kdx=\\lim_{s\\rightarrow \\infty}\\int_{E} f_{k_s}dx \\] <p>Since \\(f_k\\overset{m}{\\rightarrow} f\\), there exists a subsequence \\(\\{f_{k_{s_i}}(x)\\}\\rightarrow f(x), a.e.x\\in E\\), then by Fatou Lemma(1<sup>st</sup> \"\\(\\leq\\)\"), we have</p> \\[ \\begin{align*} \\int_E fdx&amp;=\\int_E \\lim_{i\\rightarrow \\infty} f_{k_{s_i}}dx\\\\ &amp;\\leq \\lim_{i\\rightarrow \\infty} \\int_E f_{k_{s_i}}dx\\\\ &amp;\\leq \\lim_{s\\rightarrow \\infty} \\int_E f_{k_{s}}dx\\\\ &amp;=\\underline{\\lim}\\limits_{k\\rightarrow \\infty} f_kdx. \\end{align*} \\]"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#lebesgues-dominated-convergence-theorem","title":"Lebesgue's Dominated Convergence Theorem","text":"<p>Lebesgue's Dominated Convergence Theorem</p> <p>Assume \\(\\{f_n\\}_{n\\geq 1}\\) is a measurable function sequence \\(a.e.\\) on \\(E\\), \\(f_n\\rightarrow f, a.e.x\\in E\\). If there exists \\(g\\in L(E)\\), such that \\(\\forall n\\geq 1\\), \\(|f_n(x)|\\leq g(x), a.e.x\\in E\\), then \\(f\\) and \\(f_n\\) is L integrable on \\(E\\) and </p> \\[ \\lim_{n\\rightarrow \\infty}\\int_E f_ndx=\\int_E f dx \\] <p>where \\(g\\) is also called the controlling function.</p> Proof <p>Since \\(|f_n|\\leq g\\), so its limit value \\(|f|\\leq g\\), so \\(g\\in L(E)\\) could induce \\(f\\) and \\(f_n\\) are L integrable.</p> <p>Since \\(|f_k-f|\\leq |f_k|+|f|\\leq 2g\\), we could define </p> \\[ g_k(x)=2g-|f_k-f|\\geq 0, \\] <p>then by Fatou Lemma</p> \\[ \\begin{align*} \\int_E \\underline{\\lim}\\limits_{k\\rightarrow \\infty}g_k(x)dx&amp;\\leq \\underline{\\lim}\\limits_{k\\rightarrow \\infty}\\int_E g_k(x)dx\\\\ \\int_E \\underline{\\lim}\\limits_{k\\rightarrow \\infty} (2g-|f_k-f|)dx&amp;\\leq \\underline{\\lim}\\limits_{k\\rightarrow \\infty}\\int_E (2g-|f_k-f|)dx\\\\ \\int_E 2gdx &amp;\\leq \\underline{\\lim}\\limits_{k\\rightarrow \\infty}\\left[\\int_E 2gdx - \\int_E |f_k-f|dx\\right]\\\\ \\int_E 2gdx &amp;\\leq \\int_E 2g dx + \\underline{\\lim}\\limits_{k\\rightarrow \\infty} \\left(-\\int_E |f_k-f|dx\\right)\\\\ 0&amp;\\leq -\\overline{\\lim}\\limits_{k\\rightarrow \\infty}\\int_E |f_k-f|dx\\\\ \\overline{\\lim}\\limits_{k\\rightarrow \\infty}\\int_E |f_k-f|dx&amp;\\leq 0 \\end{align*} \\] <p>So by relationship of limit superior and inferior</p> \\[ 0\\leq \\underline{\\lim}\\limits_{k\\rightarrow \\infty}|f_k-f|dx\\leq \\overline{\\lim}\\limits_{k\\rightarrow \\infty}|f_k-f|dx\\leq 0 \\] <p>which gives </p> \\[ \\lim_{k\\rightarrow \\infty}|f_k-f|dx=0 \\] <p>Since</p> \\[ \\left|\\int_E f_kdx-\\int_E fdx\\right|=\\left|\\int_E (f_k-f)dx\\right|\\leq \\int_E |f_k-f|dx\\rightarrow 0(k\\rightarrow \\infty) \\] <p>and we are done.</p> <p>Another form of Dominated Convergence Theorem</p> <p>Assume \\(\\{f_n\\}_{n\\geq 1}\\) is a measurable function sequence \\(a.e.\\) on \\(E\\), \\(f_n\\overset{m}{\\rightarrow} f, a.e.x\\in E\\). If there exists \\(g\\in L(E)\\), such that \\(\\forall n\\geq 1\\), \\(|f_n(x)|\\leq g(x), a.e.x\\in E\\), then \\(f\\) and \\(f_n\\) is L integrable on \\(E\\) and </p> \\[ \\lim_{n\\rightarrow \\infty}\\int_E f_ndx=\\int_E f dx \\] Proof <p>Use contradiction and the above Dominated Convergence Theorem.</p> <p>Let \\(a_k=\\int_E f_kdx\\), \\(a=\\int_E fdx\\). Assume \\(a_k\\nrightarrow a\\), then \\(\\exists \\{k_j\\}\\), such that the convergence of its subsequence</p> \\[ \\lim_{j\\rightarrow \\infty}\\int_E f_{k_j}dx=A\\neq a. \\] <p>we want to find a point that contradicts. Since \\(f_k\\overset{m}{\\rightarrow}f\\), we have \\(f_{k_j}\\overset{m}{\\rightarrow}f\\). By Riesz Theorem, we have</p> \\[ f_{k_{j_i}}\\rightarrow f, a.e.x\\in E \\] <p>Combined with \\(|f_{k_{j_i}}|\\leq g\\), by Dominated Convergence Theorem, we have</p> \\[ \\lim_{i\\rightarrow \\infty}\\int_E f_{k_{j_i}}dx=\\int_E fdx=A \\] <p>while \\(a_{k_j}\\rightarrow A\\neq a\\), we have \\(a_{k_{j_i}}\\rightarrow A\\neq a\\), which contradicts!</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#applications-of-ldct","title":"Applications of LDCT","text":"<p>The following come some applications of LDCT.</p> <p>Corollary 1: Term-by-term Integration Theorem</p> <p>Assume \\(\\{f_n\\}\\) are measurable functions on \\(D\\), satisfies </p> \\[ \\sum\\limits_{n=1}^\\infty \\int\\limits_E |f_n|dx&lt;\\infty, \\] <p>then \\(f:=\\sum\\limits_{n=1}^\\infty f_n\\) converges absolutely \\(a.e.\\) on \\(D\\), and </p> \\[ \\int_E fdx = \\sum_{n=1}^\\infty \\int_E f_ndx \\] Proof <p>Consider define \\(F_k(x):=\\sum\\limits_{j=1}^\\infty f_j,k\\in \\mathbb{N}^+\\), then \\(g=\\sum\\limits_{k=1}^\\infty |f_k|\\).</p> <p>Apply LDCT to \\(F_k\\).</p> <p></p> <p>Corollary 2: theorem for limit of integral region</p> <p>Assume \\(f\\) is a measurable function on \\(E\\), \\(E_1\\subset E_2\\subset \\cdots\\) is a sequence of subsets of \\(E\\), which satisfy \\(E=\\bigcup_{k=1}^\\infty E_k\\). If \\(f\\in L(E)\\), then </p> \\[ \\int_E fdx=\\lim_{k\\rightarrow \\infty}\\int_{E_k}fdx. \\] Proof <p>Let \\(f_k(x):=f(x)\\chi_{E_k}(x)\\), so \\(\\lim_{k\\rightarrow \\infty}f_k=f(x)\\) and \\(|f_k|\\leq |f|\\in L(E)\\), by LDCT, we have</p> \\[ \\begin{align*} \\int_{E}fdx&amp;=\\lim_{k\\rightarrow \\infty}\\int_{E}f_kdx\\\\ &amp;=\\lim_{k\\rightarrow \\infty}\\int_{E}f(x)\\chi_{E_k}dx\\\\ &amp;=\\lim_{k\\rightarrow \\infty}\\int_{E_k}f(x)dx \\end{align*} \\] <p>Example. Calculate </p> \\[ \\lim_{n\\rightarrow \\infty}\\int_{(0,+\\infty)}\\frac{n\\sqrt{x}}{1+n^2x^2}\\sin^{24}(nx)dx \\] Answer <p>Let \\(f_n(x)=\\frac{n\\sqrt{x}}{1+n^2x^2}\\sin^{24}(nx)\\), \\(f(x)=0\\). Here we have to discuss a controlling function.</p> <ul> <li>\\(x\\in(0,1]\\). Notice</li> </ul> \\[ \\left|\\frac{n\\sqrt{x}}{1+n^2x^2}\\sin^{24}(nx)\\right|\\leq \\left|\\frac{n\\sqrt{x}}{1+n^2x^2}\\right|\\leq \\frac{n\\sqrt{x}}{2nx}=\\frac{1}{\\sqrt{x}}. \\] <p>whose L integral is finite on \\((0,1]\\), so Choose \\(g(x)=\\frac{1}{2\\sqrt{x}}\\).</p> <ul> <li>\\(x\\in (1,+\\infty)\\). Notice</li> </ul> \\[ \\left|\\frac{n\\sqrt{x}}{1+n^2x^2}\\right|\\leq \\frac{n\\sqrt{x}}{n^2x^2}\\leq \\frac{1}{x^{3/2}}. \\] <p>whose L integral is finite on \\((1,+\\infty)\\), so choose \\(g(x)=\\frac{1}{x^{3/2}}\\).</p> <p>Thus consider</p> \\[ g(x)=\\begin{cases} \\frac{1}{2\\sqrt{x}},\\quad x\\in(0,1]\\\\ \\frac{1}{x^{3/2}},\\quad x\\in (1,+\\infty). \\end{cases} \\] <p>So \\(f_n(x)\\) satisfy condition of LDCT, meaning</p> \\[ \\lim_{n\\rightarrow \\infty}\\int_{(0,+\\infty)}\\frac{n\\sqrt{x}}{1+n^2x^2}\\sin^{24}(nx)dx=\\int_{(0,+\\infty)}0dx=0. \\] <p> </p> <p>Corollary 3: Theorem of denumberable additivity</p> <p>Assume \\(a\\) is a measurable function on \\(E\\), \\(\\{E_k\\}_{k\\geq 1}\\) is a sequence of subsets of \\(E\\), which satisfies \\(E=\\bigcup_{k} E_k\\) and \\(E_i\\cap E_j=\\varnothing, \\forall i\\neq j\\). If \\(f\\in L(E)\\), then</p> \\[ \\int_E fdx=\\sum_{k=1}^\\infty \\int_{E_k}fdx. \\] Proof <p>Construct another sequence of sets \\(\\{\\tilde{E_k}\\}\\) as</p> \\[ \\tilde{E_k}:=\\bigcup_{j=1}^kE_j, \\] <p>which monotonically increase and \\(\\lim\\limits_{k\\rightarrow \\infty}\\tilde{E_k}=E\\), so by Corollary 2: theorem for limit of integral region, we have</p> \\[ \\begin{align*} \\int_E fdx&amp;=\\lim_{k\\rightarrow \\infty}\\int_{\\tilde{E_k}} fdx\\\\ &amp;=\\lim_{k\\rightarrow \\infty}\\int_{\\bigcup_{j=1}^kE_j} fdx\\\\ &amp;=\\lim_{k\\rightarrow \\infty}\\sum_{j=1}^k \\int_{E_j}fdx\\\\ &amp;=\\sum_{j=1}^\\infty \\int_{E_j}fdx. \\end{align*} \\] <p>Notice derivative is also a limit operation, so here comes the following theorem.</p> <p></p> <p>Corollary 4: Parameter derivatives of L integral</p> <p>Assume \\(f(x,y)\\) is a function on \\(E\\times I\\), where \\(E\\in \\Omega(\\mathbb{R}^n)\\), \\(I\\subset \\mathbb{R}\\) is an interval. If </p> <p>(i) \\(\\forall y\\in I\\), \\(f(x,y)\\in L(E)\\) with respect to \\(x\\), </p> <p>(ii) \\(\\forall x\\in E\\), \\(f(x,y)\\in C(I)\\) with respect to \\(y\\),</p> <p>(iii) \\(\\exists 0\\leq F(x)\\in L(E)\\), such that</p> \\[ \\left|\\frac{\\partial }{\\partial y}f(x,y)\\right|\\leq F(x),\\quad \\forall (x,y)\\in E\\times I, \\] <p>then </p> \\[ \\frac{\\partial}{\\partial y}\\int_E f(x,y)dx=\\int_E \\frac{\\partial}{\\partial y}f(x,y)dx,\\quad \\forall y\\in I. \\] Proof <p>\\(\\forall y\\in I\\), \\(\\exists h_k&gt;0\\), s.t. \\(y+h_k\\in I\\), then define</p> \\[ g_k(x,y):=\\frac{f(x,y+h_k)-f(x,y)}{h_k}. \\] <p>which is apprarently a measurable function, and its limit function is \\(g(x)=\\frac{\\partial }{\\partial y}f(x,y)\\). We have to find an appropriate controlling function \\(G(x)\\) to control \\(g_k\\). Notice that by mean value theorem</p> \\[ g_k(x,y)=\\left|\\frac{\\partial }{\\partial y}f(x,y+\\theta_k h_k)\\right|\\leq F(x),\\quad \\forall x\\in E, \\text{ where }\\theta_k\\in (0,1). \\] <p>So by LDCT, we have </p> \\[ \\begin{align*} \\lim_{k\\rightarrow \\infty}\\frac{1}{h_k}\\left[\\int_E f(x,y+h_k)dx-\\int_E f(x,y)dx\\right]&amp;=\\lim_{k\\rightarrow \\infty}\\int_{E} g_k(x,y)dx=\\int_{E}\\lim_{k\\rightarrow \\infty} g_k(x,y)dx\\\\ \\frac{\\partial}{\\partial y}\\int_E f(x,y)dx&amp;=\\int_E \\frac{\\partial}{\\partial y}f(x,y)dx \\end{align*} \\] <p>Assume \\(f\\in L[a,b]\\), prove</p> \\[ \\frac{d}{dx}\\int_{[a,b]}f(y)\\sin(xy)dy=\\int_{[a,b]}yf(y)\\cos(xy)dy. \\] Answer <p>Easy to see that \\(F(x)=f(y)\\sin(xy)\\in L([a,b])\\) with respect to \\(y\\) and \\(F(x)\\in C(I)\\) with respect to \\(x\\). Notice that</p> \\[ \\left|\\frac{\\partial}{\\partial x}F(x)\\right|=|yf(y)\\cos(xy)|\\leq |yf(y)| \\in L([a,b]\\times I). \\] <p>So by Theorem for Parameter derivatives of L integral, we are done.</p> <p>Corollary 5: Approximation with functions</p> <p>Assume \\(f\\in L([a,b])\\), prove: \\(\\forall \\varepsilon&gt;0\\), \\(\\exists\\)</p> <p>(i) bounded measurable function \\(g\\), such that \\(\\int_a^b|f-g|dx&lt;\\varepsilon\\).</p> <p>(ii) continuous function \\(h\\), such that \\(\\int_a^b|f-h|dx&lt;\\varepsilon\\).</p> <p>(iii) polynomial \\(P\\), such that \\(\\int_a^b|f-P|dx&lt;\\varepsilon\\).</p> <p>(iv) step function \\(S\\), such that \\(\\int_a^b|f-S|dx&lt;\\varepsilon\\).</p> Proof <p>(i) Let \\(f_k(x)=f\\cdot \\chi_{\\{|f|\\leq k\\}}(x)\\). Since</p> \\[ |f_k(x)-f(x)|\\leq |f(x)|\\in L([a,b]) \\] <p>and \\(\\lim_{k\\rightarrow\\infty}|f_k-f|=0,a.e.x\\in [a,b]\\). By LDCT we have </p> \\[ \\lim_{k\\rightarrow \\infty}\\int_a^b |f-f_k|dx=0. \\] <p>So there exists \\(K\\), when \\(k\\geq K\\), we have </p> \\[ \\int_a^b |f-f_k|dx&lt;\\varepsilon. \\] <p>Let \\(g=f_{k_0}\\) for some \\(k_0\\geq K\\).</p> <p>(ii) By (i), there exists bounded measurable function \\(g\\) such that \\(\\int_a^b|f-g|dx&lt;\\frac{\\varepsilon}{2}\\). Let \\(|g(x)|\\leq M\\). To get a continuous function, we could use Luzin theorem, there exists continuous function \\(h\\) such that \\(m(\\{h\\neq g\\})&lt;\\frac{\\varepsilon}{4M}\\) and \\(\\max|h(x)|\\leq M\\), then</p> \\[ \\begin{align*} \\int_a^b |f-h|dx&amp;\\leq \\int_a^b|f-g|dx+\\int_a^b|g-h|dx\\\\ &amp;\\leq \\frac{\\varepsilon}{2}+2M\\cdot m(\\{g\\neq h\\})\\\\ &amp;&lt;\\frac{\\varepsilon}{2}+2M\\cdot \\frac{\\varepsilon}{4M}=\\varepsilon. \\end{align*} \\]"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#riemann-integral-lebesgue-integral","title":"Riemann Integral &amp; Lebesgue Integral","text":"<p>Deduction</p> <p>Assume \\(f\\) is bounded on \\([a,b]\\), and we have a partition \\(\\{x_k\\}_{0\\leq k\\leq n}\\) on \\([a,b]\\)</p> \\[ \\Delta_k: a=x_0&lt;x_1&lt;\\cdots&lt;x_n=b. \\] <p>Define \\(M_j=\\sup_{x\\in [a,b]}\\{f(x)\\}\\), \\(n_j=\\inf_{x\\in [a,b]}\\{f(x)\\}\\), and define two function</p> \\[ \\begin{align*} M_{\\Delta_k} (x)=\\sum_{j=1}^n M_j \\chi_{[x_{j-1},x_j]}(x)\\\\ m_{\\Delta_k} (x)=\\sum_{j=1}^n m_j \\chi_{[x_{j-1},x_j]}(x) \\end{align*} \\] <p>which apparently is measurable function (simple function), and we have</p> \\[ \\begin{align*} m_{\\Delta_k}(x)\\leq f(x)\\leq M_{\\Delta_k} (x) \\end{align*} \\] <p>So we have an important relationship</p> \\[ \\begin{align*} \\int_{[a,b]}M_{\\Delta_k}(x)dx=\\sum_{j=1}^n M_j m([x_{j-1},x_j])=\\sum_{j=1}^n M_j \\Delta x_k = \\overline{S}_{\\Delta_k}(f)\\\\ \\int_{[a,b]}m_{\\Delta_k}(x)dx=\\sum_{j=1}^n m_j m([x_{j-1},x_j])=\\sum_{j=1}^n m_j \\Delta x_k = \\underline{S}_{\\Delta_k}(f) \\end{align*} \\] <p>Since \\(\\Delta_k\\) is not denumerable, so we choose to let \\(\\Delta_k\\) to be an equi-distant partition of \\([a,b]\\) with distance \\(N_k=2^k\\):</p> \\[ \\Delta_k: a=x_0^{(k)}&lt;x_1^{(k)}&lt;\\cdots&lt;x_{N_k}^{(k)}=b \\] <p>and \\(\\Delta_{k+1}\\) is a refinement of \\(\\Delta_{k}\\), \\(\\Vert\\Delta_k\\Vert\\rightarrow 0 (k\\rightarrow \\infty)\\).</p> <p>Since \\(\\int_{[a,b]}m_{\\Delta_k}(x)dx\\) does not increase, \\(\\int_{[a,b]}M_{\\Delta_k}(x)dx\\) does not decrease, we have their limit </p> \\[ \\begin{align*} \\lim_{k\\rightarrow\\infty}\\int_{[a,b]}M_{\\Delta_k}(x)dx=\\lim_{k\\rightarrow \\infty}\\overline{S}_{\\Delta_k}(f) = \\inf \\overline{S}_{\\Delta_k}(f)\\\\ \\lim_{k\\rightarrow\\infty}\\int_{[a,b]}m_{\\Delta_k}(x)dx=\\lim_{k\\rightarrow \\infty}\\underline{S}_{\\Delta_k}(f) = \\inf \\underline{S}_{\\Delta_k}(f) \\end{align*} \\] <p>So from what we have known in Riemann integral, </p> \\[ \\begin{equation} f\\in R[a,b]\\Leftrightarrow \\lim_{k\\rightarrow\\infty}\\int_{[a,b]}m_{\\Delta_k}(x)dx=\\lim_{k\\rightarrow\\infty}\\int_{[a,b]}M_{\\Delta_k}(x)dx \\label{R-L-1} \\end{equation} \\] <p>Deduction 2</p> <p>Thus we have to use integral limit theorem. That is, define Baire upper and lower function \\(M(x)=\\lim\\limits_{k\\rightarrow \\infty}M_{\\Delta_k}(x)\\), \\(m(x)=\\lim\\limits_{k\\rightarrow \\infty}m_{\\Delta_k}(x)\\), which is bounded, satisfying condition of LDCT, i.e.</p> \\[ \\begin{align*} \\lim_{k\\rightarrow \\infty}\\int_{[a,b]} M_{\\Delta_k}(x)dx=\\int_{[a,b]}m(x)dx\\\\ \\lim_{k\\rightarrow \\infty}\\int_{[a,b]} m_{\\Delta_k}(x)dx=\\int_{[a,b]}m(x)dx. \\end{align*} \\] <p>So equivalence \\(\\ref{R-L-1}\\) becomes</p> \\[ \\begin{align*} f\\in R[a,b]&amp;\\Leftrightarrow \\int_{[a,b]}m(x)dx=\\int_{[a,b]}M(x)dx\\\\ &amp;\\Leftrightarrow \\int_{[a,b]}[M(x)-m(x)]dx=0\\\\ &amp;\\Leftrightarrow M(x)=m(x), a.e.x\\in [a,b]\\quad \\text{by } \\end{align*} \\] <p>The last Equivalence is due to Example.</p> <p>Note that about Barie functions, we have a theorem </p> <p>Baire Theorem</p> <p>Assume \\(f\\) is bounded on \\([a,b]\\), \\(x_0\\in [a,b]\\), then \\(f\\) is continuous at \\(x_0\\), iff \\(M(x_0)=m(x_0)\\).</p> Proof <ul> <li>\"\\(\\Rightarrow\\)\".</li> </ul> <p>\\(f\\) is continuous at \\(x_0\\), then \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\delta&gt;0\\), such that </p> \\[ |f(x)-f(x_0)|&lt; \\varepsilon,\\quad \\forall x\\in O_\\delta(x_0) \\] <p>that is, \\(f(x_0)-\\varepsilon&lt;f(x)&lt;f(x_0)+\\varepsilon\\)\u3002 There exists sufficient large \\(k\\) such that </p> \\[ f(x_0)-\\varepsilon&lt;m_{\\Delta_k}(x)&lt;M_{\\Delta_k}(x)&lt;f(x_0)+\\varepsilon. \\] <ul> <li>\"\\(\\Leftarrow\\)\".</li> </ul> <p>Finally we have the following theorem.</p> <p>Riemann integral and Lebesgue integral</p> <p>If \\(f\\in R[a,b]\\), then \\(f\\in L[a,b]\\), and </p> \\[ (R)\\int_a^b fdx=(L)\\int_{[a,b]}fdx \\] Proof <p>Sufficient &amp; Necessary condition for Riemann integrable</p> <p>Assume \\(f\\) is bounded on \\([a,b]\\), then \\(f\\in R[a,b]\\), iff \\(f\\in C(a.e.x\\in [a,b])\\).</p> Proof <ul> <li>\"\\(\\Rightarrow\\)\".</li> </ul> <p> Example.  <p>(i) Riemann function</p> \\[ R(x)=\\begin{cases}\\frac{1}{q},\\quad x=\\frac{p}{q}\\\\ 0,\\quad x\\in \\mathbb{Q}^c \\end{cases}. \\] <p>is Riemann integrable.</p> <p>(ii) Characteristic function of Cantor set \\(\\chi_C(x)\\) is Riemann integrable.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#generalized-riemann-integral","title":"Generalized Riemann integral","text":"<p>Because genaralized Riemann integral is a limit in a sense of Cauchy, we could not say generalized Riemann integral could be extended into Lebesgue integral. </p> <p>Example. \\(f(x)=\\sin x/x\\), then their generalized integral</p> \\[ \\int_0^\\infty\\frac{\\sin x}{x}dx=\\frac{\\pi}{2}. \\] <p>but </p> \\[ \\int_0^\\infty \\left|\\frac{\\sin x}{x}\\right|=\\infty. \\] <p>so \\(f\\notin L([0,\\infty))\\).</p> <p>In another words, Lebesgue integral is an absolute convergent intergal. So we have the following relation regarding generalized integral.</p> <p>Generalized Riemann integral of non-negative function</p> <p>(i) Assume \\(f(x)\\geq 0\\), if</p> \\[ (R)\\int_0^\\infty fdx=\\lim_{A\\rightarrow \\infty}\\int_0^A fdx \\] <p>converges, then \\(f\\in L((0,\\infty))\\) and</p> \\[ (L)\\int_{(0,\\infty)}fdx=(R)\\int_0^\\infty fdx. \\] Proof <p>(i) First we prove that \\(f\\in L((0,\\infty))\\). Notice that</p> \\[ f\\in R([a,b])\\Rightarrow f\\in L([a,b]) \\] <p>let \\(b\\rightarrow \\infty\\), we get \\(f\\in L((0,\\infty))\\).</p> <p>Then we prove the equation. Notice that</p> \\[ (R)\\int_0^\\infty fdx=\\lim_{k\\rightarrow \\infty}(R)\\int_0^kfdx=\\lim_{k\\rightarrow \\infty}(L)\\int_{(0,k)}fdx=\\lim_{k\\rightarrow \\infty}\\int_{(0,\\infty)}f(x)\\chi_{(0,k)}(x)dx \\] <p>Define \\(f_k(x)=f(x)\\chi_{(0,k)}(x)\\), then \\(f_k \\nearrow\\), satisfy condition of Levi Theorem, so</p> \\[ (L)\\int_{(0,\\infty)}fdx=\\lim_{k\\rightarrow \\infty}(L)\\int_{(0,\\infty)}f_kdx=(R)\\int_0^\\infty fdx \\] <p>Corollary: Generalized Riemann integral</p> <p>Assume \\(f\\) is bounded on finite intervals and </p> \\[ (R)\\int_0^\\infty |f|dx=\\lim_{A\\rightarrow \\infty}\\int_0^A |f|dx \\] <p>converges, then \\(f\\in L((0,\\infty))\\) and</p> \\[ (L)\\int_{(0,\\infty)}fdx=(R)\\int_0^\\infty fdx. \\] <p>Note that we can prove in a similar way that</p> \\[ (L)\\int_{(-\\infty,+\\infty)}fdx=(R)\\int_{-\\infty}^{+\\infty} fdx. \\] Proof <p>This function might not be non-negative, so we should use LDCT. That is, Define \\(f_k(x)=f(x)\\chi_{(0,k)}(x)\\), which converges to \\(f(x)\\), and</p> \\[ |f_k(x)|\\leq |f(x)| \\] <p>while the latter one is L integrable, so by LDCT, we have \\(f\\in L((0,\\infty))\\) and</p> \\[ (L)\\int_{(0,\\infty)}f dx=\\lim_{k\\rightarrow \\infty}(L)\\int_{(0,k)}f_kdx=\\lim_{k\\rightarrow \\infty} (R)\\int_0^k f_kdx = \\lim_{k\\rightarrow \\infty} (R)\\int_0^k fdx = \\int_0^\\infty fdx \\] <p>Theoretically speaking, the Riemann integral of conditional convergent function could not be extended to Lebesgue integral.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#multiple-lebesgue-integral","title":"Multiple Lebesgue integral","text":"<p>This part we aim to find whether the following equation holds</p> \\[ \\begin{align} \\int_{\\mathbb{R}^p\\times \\mathbb{R}^q}f(x,y)dxdy=\\int_{\\mathbb{R}^q}\\left[\\int_{\\mathbb{R}^p}f(x,y)dx\\right]dy\\label{multiple-integral} \\end{align} \\] <p>where \\(x\\in \\mathbb{R}^p\\), \\(y\\in \\mathbb{R}^q\\).</p> <p>Firstly, we consider characteristic functions.</p> <p>Deduction 1</p> <p>Assume \\(f=\\chi_E\\), then equation \\(\\ref{multiple-integral}\\) becomes </p> \\[ \\begin{align} m(E)=\\int_{\\mathbb{R}^p\\times \\mathbb{R}^q}\\chi_E(x,y) dxdy=\\int_{\\mathbb{R}^q}\\left[\\int_{\\mathbb{R}^p}\\chi_E (x,y)dx\\right]dy.\\label{mid-equation-integral} \\end{align} \\] <p>Definition of Cross Section</p> <p>Assume \\(E\\in \\Omega(\\mathbb{R}^{p+q})\\), \\(\\forall y\\in \\mathbb{R}^q\\), then define</p> \\[ E^y:=\\{x\\in \\mathbb{R}^p| (x,y)\\in E\\} \\] <p>the cross section of \\(E\\) at \\(y\\).</p> <p>Deduction 2</p> <p>With the above definition, check that \\(\\chi_E(x,y)=1\\Leftrightarrow (x,y)\\in E\\), i.e. \\(x\\in \\{x\\in \\mathbb{R}^p: (x,y)\\in E\\}=E^y\\). So we could have an equivalent relation</p> \\[ \\chi_E(x,y)=\\chi_{E^y}(x). \\] <p>Thus equation \\(\\ref{mid-equation-integral}\\) becomes </p> \\[ m(E)=\\int_{\\mathbb{R}^q}\\left[\\int_{\\mathbb{R}^p}\\chi_{E^y} (x)dx\\right]dy=\\int_{\\mathbb{R}^q}m(E^y)dy. \\] <p>Before we begin proof, we have to give some properties of the cross section.</p> <p>Properties about cross section</p> <p>Assume \\(A,B,\\{E_k\\}\\) are subsets of \\(\\mathbb{R}^{p+q}\\), \\(\\forall y\\in \\mathbb{R}^q\\), then </p> <p>(i) If \\(A\\subset B\\), then \\(A^y\\subset B^y\\).</p> <p>(ii) \\((\\bigcup_k E_k)^y=\\bigcup_k E_k^y\\), \\((\\bigcap_k E_k)^y=\\bigcap_k E_k^y\\).</p> <p>(iii) \\((A-B)^y=A^y-B^y\\).</p> <p>(iv) If \\(E_k \\nearrow A\\), then \\((E_k)^y\\nearrow A^y\\); If \\(E_k \\searrow A\\), then \\((E_k)^y\\searrow A^y\\)</p> <p>Theorem for Cross Section</p> <p>Assume \\(E\\subset \\mathbb{R}^{p+q}\\) is measurable, then</p> <p>(i) \\(\\forall a.e.y\\in \\mathbb{R}^q\\), \\(E^y\\) is measurable with respect to \\(y\\).</p> <p>(ii) \\(m(E^y)\\) is a non-negative function on \\(\\mathbb{R}^q\\) and </p> \\[ \\int_{\\mathbb{E}^q}m(E^y)dy=m(E). \\] Proof <ul> <li>(i) \\(E=I\\times J\\), where \\(I\\subset \\mathbb{R}^p\\), \\(J\\subset \\mathbb{R}^q\\) are half-open cubes. </li> </ul> <p>So \\(\\forall a.e.y\\in \\mathbb{R}^q\\), we have</p> \\[ E^y=\\begin{cases} I,\\quad y\\in J\\\\ \\varnothing,\\quad y\\notin J \\end{cases} \\] <p>meaning \\(E^y\\) is measurable. Its measure (a function of \\(y\\))</p> \\[ m(E^y)=m(I)\\cdot \\chi_{J}(y) \\] <p>So by Measure of direct product (the 3<sup>rd</sup> \"=\"), its integral</p> \\[ \\begin{align*} \\int_{\\mathbb{R}^q}m(E^y)dy&amp;=\\int_{\\mathbb{R}^q}m(I)\\chi_J (y)dy\\\\ &amp;=m(I)m(J)\\\\ &amp;=m(I\\times J)=m(E). \\end{align*} \\] <ul> <li>(ii) \\(E\\) is a bounded open set on \\(\\mathbb{R}^{p+q}\\). So by configuration of open sets, we have</li> </ul> \\[ E=\\bigcup_{k=1}^\\infty I_k, \\quad I_k\\subset \\mathbb{R}^{p+q}, \\text{ &amp; } E_i\\cap E_j=\\varnothing. \\] <p>So </p> \\[ E^y=\\left(\\bigcup_{k=1}^\\infty I_k\\right)^y = \\bigcup_{k=1}^\\infty I_k^y \\] <p>which is measurable. Then by denumerable additivity</p> \\[ \\begin{align} m(E^y)=\\sum_{k=0}^\\infty m(I_k^y)\\label{Ey} \\end{align} \\] <p>is a non-negative measurable function. Thus by Series form of Levi Theorem (the 3<sup>rd</sup> \"=\") and equation \\(\\ref{Ey}\\) (the 4<sup>th</sup> \"=\")</p> \\[ \\begin{align*} m(E)&amp;=\\sum_{k=1}^\\infty m(I_k)\\\\ &amp;=\\sum_{k=1}^\\infty \\int_{\\mathbb{R}^q} m(I_k^y)dy\\quad\\text{by (i)}\\\\ &amp;=\\int_{\\mathbb{R}^q} \\sum_{k=1}^\\infty m(I_k^y)dy \\\\ &amp;=\\int_{\\mathbb{R}^q} m(E^y)dy  \\end{align*} \\] <ul> <li>(iii) \\(E\\) is a bounded \\(G_\\delta\\) set.</li> </ul> <p>That is, \\(E=\\bigcap_{k=1}^\\infty G_k\\), where \\(G_k\\) are mutually disjoint open sets. Here we let \\(G_k\\) monotonically decrease (increase makes no sense for intersection), i.e. \\(G_1\\supset G_2\\supset \\cdots\\), so the cross section</p> \\[ E^y=\\bigcap_{k=1}^\\infty G_k^y, \\quad G_1^y\\supset G_2^y\\supset\\cdots \\] <p>so by limit operation of measure, its measure (a function of \\(y\\))</p> \\[ m(E^y)=m(\\lim_{k\\rightarrow \\infty}G_k^y)=\\lim_{k\\rightarrow \\infty}m(G_k^y) \\] <p>meaning it could be expressed as a form of limit of measurable functions. So we could use LDCT, if we check that </p> \\[ |m(G_k^y)|\\leq |m[(I\\times J)^y]|=m(I)\\chi_J(x)\\in L(\\mathbb{R}^q). \\] <p>So by LDCT, </p> \\[ \\begin{align*} \\int_{\\mathbb{R}^q}m(E^y)dy&amp;=\\lim_{k\\rightarrow \\infty}\\int_{\\mathbb{R}^q}m(G_k^y)dy\\\\ &amp;=\\lim_{k\\rightarrow \\infty}m(G_k)\\quad \\text{by (ii)}\\\\ &amp;=m(E) \\end{align*} \\] <p>(iv) \\(E\\) is a zeo-measure set in \\(\\mathbb{R}^{p+q}\\).</p> <p>So by [Approximation me]</p> <p>(v) \\(E\\) is a bounded measurable set.</p> <p>(vi) \\(E\\) is a general measurable set.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#tonelli-theorem","title":"Tonelli Theorem","text":"<p>Then we consider \\(f\\) is a measurable function.</p> <p>Tonelli Theorem</p> <p>Assume \\(f(x,y)\\) is a non-negative measurable function with \\((x,y)\\in \\mathbb{R}^p\\times \\mathbb{R}^q\\), then </p> <p>(i) \\(\\forall a.e.y\\in \\mathbb{R}^q\\), \\(f(x,y)\\), as a function of \\(x\\in \\mathbb{R}^q\\) is non-negative measurable.</p> <p>(ii) \\(F(y):=\\int_{\\mathbb{R}^p}f(x,y)dx\\) is non-negative measurable on \\(\\mathbb{R}^q\\), and equation \\(\\ref{multiple-integral}\\) holds.</p> Proof <p>(i) \\(f\\) is a simple function, which holds naturally, bacause Linearity of Lebesgue integral.</p> <p>(ii) \\(f\\) is a non-negative measurable funcion. Notice that</p> \\[ f(x)=\\lim_{k\\rightarrow \\infty}\\varphi_k(x,y) \\] <p>where \\(\\varphi_k\\) are non-negative simple functions, and monotonically increase.</p> <p>Example. Assume \\(f\\) is a measurable function on \\(E\\), then </p> \\[ \\int_E |f|dx=\\int_{(0,\\infty)}d_f(\\alpha)d\\alpha \\] <p>where \\(d_f(\\alpha)=m(\\{f&gt;\\alpha\\})\\).</p> Proof \\[ \\begin{align*} \\int_{(0,\\infty)}d_f(\\alpha)d\\alpha&amp;=\\int_{(0,\\infty)}m(\\{f&gt;\\alpha\\})d\\alpha\\\\ &amp;=\\int_{0,\\infty}\\left[\\int_E \\chi_{\\{f&gt;\\alpha\\}}(x)dx\\right]d\\alpha\\\\ &amp;=\\int_E dx\\int_{(0,\\infty)} \\left[\\int_E \\chi_{\\{f&gt;\\alpha\\}}(x)d\\alpha\\right]\\quad \\text{by Tonelli Theorem}\\\\ &amp;=\\int_E dx \\int_{(0,|f|)} \\left[\\int_E \\chi_{\\{f&gt;\\alpha\\}}(x)d\\alpha\\right]\\\\ &amp;=\\int_E |f|dx. \\end{align*} \\]"},{"location":"Math/Real_Analysis/Lebesgue_Integral/#fubini-theorem","title":"Fubini Theorem","text":"<p>Fubini Theorem</p> <p>Assume \\(f(x,y)\\) is a L integrable function with \\((x,y)\\in \\mathbb{R}^p\\times \\mathbb{R}^q\\), then </p> <p>(i) \\(\\forall a.e.y\\in \\mathbb{R}^q\\), \\(f(x,y)\\), as a function of \\(x\\in \\mathbb{R}^q\\) is L integrable.</p> <p>(ii) \\(F(y):=\\int_{\\mathbb{R}^p}f(x,y)dx\\) is L integrable on \\(\\mathbb{R}^q\\), and equation \\(\\ref{multiple-integral}\\) holds.</p> Proof <p>Notice that \\(f(x,y)=f(x,y)^+-f(x,y)^-\\), which are non-negative measurable function. So by Tonelli Theorem, we are done.</p> <p>Example. Calculate L integral \\((0&lt; a &lt; b)\\)</p> \\[ I=\\int_{(0,\\infty)}\\frac{\\sin x}{x}[e^{-ax}-e^{-bx}]dx \\] Answer <p>Notice that</p> \\[ I=\\int_0^\\infty \\sin x dx\\int_{a}^b e^{-xy}dy \\] <p>Check whether \\(f=\\sin x e^{-xy}\\) is L integrable, i.e. for \\(|f|\\),</p> \\[ \\begin{align*} \\int_{(0,\\infty)}\\int_a^b |f|dxdy&amp;\\leq \\int_{(0,\\infty)}\\int_a^b e^{-xy}dxdy\\\\ &amp;=\\int_a^b dy\\int_{(0,\\infty)}e^{-xy}dx\\quad\\text{by Tonelli Theorem}\\\\ &amp;=\\int_a^b \\frac{1}{x}dx=\\ln (b/a)&lt;\\infty \\end{align*} \\] <p>which is L integrable. So by Fubini Theorem,</p> \\[ \\begin{align*} I&amp;=\\int_a^b dy\\int_{(0,\\infty)}e^{-xy}\\sin x dx\\\\ &amp;=\\int_a^b \\frac{1}{1+y^2}dy=\\arctan b-\\arctan a \\end{align*} \\] <p>Example. prove</p> \\[ \\int_{(0,\\infty)^2}e^{-xy}\\sin x\\sin ydxdy=\\int_{(0,\\infty)}\\sin y dy\\int_{(0,\\infty)}e^{-xy}\\sin xdx=\\int_{(0,\\infty)}\\sin x dx\\int_{(0,\\infty)}e^{-xy}\\sin ydy \\]"},{"location":"Math/Real_Analysis/Lebesgue_Measure/","title":"Lebesgue Measure","text":"<p>This chapter we focus on the measure of bounded set on \\(\\mathbb{R}\\).</p> <p>Our goal: extend the concept of length on intervals to a more general set of real numbers. </p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#lebesgue-outer-measure","title":"Lebesgue Outer Measure","text":"<p>Definition of Lebesgue outer measure</p> <p>Assume \\(E \\subset \\mathbb{R}\\), we define the Lebesgue outer measure to be</p> \\[ m^*(E)=\\inf \\left\\{\\sum_n l(I_n): \\{I_n\\}_{n\\geq 1} \\text{ are open intervals and } E\\subset \\bigcup_n I_n \\right\\} \\] <p>where \\(l(I_n)\\) denotes the length of intevals.</p> <p></p> <p>Example. For denumerable set \\(E=\\{x_n\\}_{n\\geq 1}\\), its Lebesgue outer measure </p> \\[ m^*(E)=0 \\] Proof <p>Choose open intervals </p> \\[ I_n=\\left(x_n-\\frac{\\varepsilon}{2^{n+1} }, x_n+\\frac{\\varepsilon}{2^{n+1} } \\right) \\] <p>and show the summation of length of \\(\\{I_n\\}\\) are \\(\\varepsilon\\), which could be sufficiently small.</p> <p>Note that of Lebesgue outer measure of \\(\\varnothing\\) is also \\(0\\).</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#properties","title":"Properties","text":"<p>Properties of Lebesgue Outer Measure</p> <p>(i) Monotonicity.</p> <p>If \\(E_1\\subset E_2\\), then </p> \\[ m^*(E_1)\\leq m^*(E_2) \\] <p>(ii) If \\(I\\) is an interval, then </p> \\[ m^*(I)=l(I) \\] <p>(iii) subadditivity </p> <p>If \\(\\{E_n\\}_{n\\geq 1}^{\\infty}\\) is a sequence of subset of \\(\\mathbb{R}\\), then</p> \\[ m^*\\left(\\bigcup_{n=1}^\\infty E_n\\right) \\leq \\sum_{n=1}^\\infty m^*(E_n) \\] Proof for (i)Proof for (ii)Proof for (iii) <p>(i) \\(\\forall \\varepsilon&gt;0, \\exists \\{I_n\\}\\), s.t. \\(E_2\\subset \\bigcup\\limits_{n=1}^\\infty I_n\\), and </p> \\[ \\begin{equation} \\sum_{n=1}^\\infty l(I_n)\\leq m^*(E_2)+\\varepsilon \\quad\\text{by property of infimum for $E_2$}\\label{ieq1} \\end{equation} \\] <p>then because \\(E_1\\subset E_2\\), we have \\(E_1\\subset \\bigcup\\limits_{n=1}^\\infty I_n\\), so</p> \\[ \\begin{equation} m^*(E_1)\\leq \\sum_{n=1}^\\infty l(I_n)\\quad \\text{by property of infimum $E_1$}\\label{ieq2} \\end{equation} \\] <p>combine inequation \\(\\ref{ieq1}\\) and \\(\\ref{ieq2}\\), we have</p> \\[ m^*(E_1)\\leq m^*(E_2)+\\varepsilon \\] <p>because \\(\\varepsilon\\) could be sufficiently small, we have</p> \\[ m^*(E_1)\\leq m^*(E_2) \\] <p>(ii) </p> <ul> <li>We firstly show that the conclusion for closed intervals \\(I=[a,b]\\). </li> </ul> <p>Closed set has a great property, that is, using Heine-Borel Theorem, if \\(I \\subset \\bigcup_{n=1}^\\infty I_n\\), then \\(\\exists N\\), such that \\(I\\subset \\bigcup_{n=1}^N I_n\\). Thus</p> \\[ l(I)&lt;\\sum_{n=1}^N l(I_n) &lt; \\sum_{n=1}^\\infty l(I_n)\\leq m^*(I)+\\varepsilon \\] <p>which means \\(l(I)\\leq m^*(I)\\) (let \\(\\varepsilon\\rightarrow 0\\)). This is one direction. </p> <p>The opposite direction could be explained as below. We try to choose a special open interval. \\(\\forall \\varepsilon&gt;0\\), \\(I\\subset (a-\\varepsilon, b+\\varepsilon)\\), which means </p> \\[ m^*(I)\\leq l((a-\\varepsilon, b+\\varepsilon))=b-a+2\\varepsilon \\] <p>So let \\(\\varepsilon\\rightarrow 0\\), we have </p> \\[ m^*(I)\\leq b-a=l(I) \\] <ul> <li>Now we consider \\(I=(a,b]\\) (similar logic for \\([a,b)\\)).</li> </ul> <p>Note that we can use a closed interval to contain \\(I\\)</p> \\[ I=(a,b]\\subset [a,b] \\] <p>So by Monotonicity property of Lebesgue outer measure, we have</p> \\[ m^*(I)\\leq m^*([a,b])=l([a,b])=l((a,b])=l(I) \\] <p>For another direction, we can choose a closed set which is contained by \\(I\\), i.e. \\(\\forall \\varepsilon&gt;0\\), \\([a+\\varepsilon,b]\\subset (a,b]\\), by Monotonicity property of Lebesgue outer measure, we have</p> \\[ b-a-\\varepsilon=m^*([a+\\varepsilon,b])\\leq m^*(I) \\] <p>let \\(\\varepsilon\\rightarrow 0\\), we have</p> \\[ l(I)=b-a\\leq m^*(I) \\] <ul> <li>Then we consider \\((a,b)\\), by using </li> </ul> \\[ [a+\\varepsilon,b-\\varepsilon] \\subset I \\subset [a-\\varepsilon,b+\\varepsilon] \\] <p>which means</p> \\[ b-a-2\\varepsilon=m^*([a+\\varepsilon,b-\\varepsilon]) \\leq m^*(I)\\leq m^*([a-\\varepsilon,b+\\varepsilon])=b-a+2\\varepsilon \\] <p>let \\(\\varepsilon\\rightarrow 0\\), and we are done.</p> <ul> <li>Finally we consider unbounded intervals \\(I=[a,+\\infty]\\).</li> </ul> <p>In a similar logic, we choose \\([a,b]\\) (\\(\\forall b&gt;a\\)), we have \\([a,b] \\subset [a,\\infty)=I\\), so</p> \\[ b-a=m^*([a,b])\\leq m^*(I) \\] <p>let \\(b\\rightarrow +\\infty\\), we have </p> \\[ +\\infty\\leq m^*(I)\\leq +\\infty \\] <p>and we are done.</p> <p>We have a sequence to add up, so we have to use a similar strategy in Example of measure of Denumerable set, i.e. use \\(\\varepsilon/{2^{n}}\\). </p> <ul> <li> <p>if \\(\\sum_{n=1}^\\infty E_n=\\infty\\), then (iii) holds naturally.</p> </li> <li> <p>Consider \\(\\sum_{n=1}^\\infty E_n&lt;\\infty\\). \\(\\forall \\varepsilon&gt;0\\), \\(\\forall E_n (n\\geq 1)\\), we have sequence of open intervals \\(\\{I^{(n)}_k\\}_{k=1}^\\infty\\), such that \\(E_n\\subset \\bigcup_{n=1}^\\infty I_k^{(n)}\\) and</p> </li> </ul> \\[ \\sum_{k=1}^\\infty I_k^{(n)}\\leq m^*(E_n)+\\frac{\\varepsilon}{2^n} \\] <p>Show that these sequences of open intervals could compose a covering for \\(\\bigcup_{n=1}^\\infty E_n\\), i.e.</p> \\[ \\bigcup_{n=1}^\\infty E_n \\subset \\bigcup_{n=1}^\\infty\\bigcup_{k=1}^\\infty I_k^{(n)} \\] <p>by Monotonicity property of Lebesgue outer measure, we have</p> \\[ \\begin{align*} m^*\\left(\\bigcup_{n=1}^\\infty E_n\\right) &amp;\\leq m^*\\left(\\bigcup_{n=1}^\\infty\\bigcup_{k=1}^\\infty I_k^{(n)}\\right)\\\\ &amp;=\\sum_{n=1}^\\infty \\sum_{n=1}^\\infty l(I_k^{(n)}) \\quad \\text{by property of measure for intervals}\\\\ &amp;\\leq \\sum_{n=1}^\\infty \\left[m^*(E_n)+\\frac{\\varepsilon}{2^n}\\right]\\\\ &amp;=\\sum_{n=1}^\\infty m^*(E_n) + \\varepsilon \\end{align*} \\] <p>let \\(\\varepsilon \\rightarrow 0\\), and we are done.</p> <p>Example. Prove another definition of Lebesgue outer measure</p> \\[ m^*(E)'=\\inf\\{m(Q): E\\subset Q, Q \\text{ is an open set}\\} \\] <p>where \\(m(\\cdot)\\) means the Lebesgue measure, which we will discuss in more details in the following part. Here we use it because for open intervals, its outer measure and measure are of the same.</p> Proof <ul> <li>\\(m^*(E) \\leq m^*(E)'\\). </li> </ul> \\[ \\forall \\varepsilon&gt;0, \\exists \\{I_n\\} \\text{ s.t } E\\subset \\bigcup_n I_n \\text{ &amp; } \\sum_n l(I_n)\\leq m^*(E)+\\varepsilon \\] <p>let \\(Q=\\bigcup_n I_n\\), then \\(E\\subset Q\\) and \\(m(Q)=\\sum_n l(I_n)\\leq m^*(E)+\\varepsilon\\). Let \\(\\varepsilon\\rightarrow 0\\), and get</p> \\[ m^*(E)'\\leq m^*(E) \\] <ul> <li>\\(m^*(E)\\geq m^*(E)'\\).</li> </ul> <p>This part is easy. Because we have an arbitrary open set \\(Q\\supset E\\), \\(m^*(E)\\leq m^*(Q)=m(Q)\\). Apply \"inf\" to both sides we have</p> \\[ m^*(E)\\leq \\inf_{Q} {m(Q)}=m^*(E)' \\] <p></p> <p>Example. Assume \\(E\\subset \\mathbb{R}\\), \\(0 &lt; m^*(E)&lt;\\infty\\), prove: </p> \\[ f(x):=m^*((-\\infty,x)\\cap E) \\] <p>is a consistently continuous function with respect to \\(x\\).</p> Proof <p>\\(\\forall x&lt;y\\), we have</p> \\[ (-\\infty,y)\\cap E=((-\\infty,x)\\cap E )\\cup([x,y)\\cap E), \\] <p>thus</p> \\[ \\begin{align*} m^*((-\\infty,y)\\cap E)&amp;=m^*[((-\\infty,x)\\cap E )\\cup([x,y)\\cap E) ]\\\\ f(y)&amp;\\leq f(x)+m^*[([x,y)\\cap E)]\\\\ f(y)&amp;\\leq f(x)+y-x\\\\ \\Rightarrow 0\\leq f(y)-f(x)&amp;\\leq y-x. \\end{align*} \\] <p>which means \\(f(x)\\) is a consistently continuous function.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#lebesgue-measure_1","title":"Lebesgue Measure","text":"<p>Definition of Lebesgue Measure</p> <p>Set \\(E\\subset \\mathbb{R}\\) is Lebesgue measurable, if \\(\\forall A\\subset \\mathbb{R}\\), </p> \\[ \\begin{align} m^*(A)\\geq m^*(A\\cap E)+m^*(A\\cap E^c). \\label{caratheo} \\end{align} \\] <p>which is also called Carath\u00e9odory condition (or criterion). and we have Lebesgue measure denoted by \\(m(A)=m^*(A)\\). Denote \\(\\Omega\\) as a set of all measurable set.</p> <p>Note that \\(A=(A\\cap E)\\cup (A\\cap E^c)\\), so by subadditivity of Lebesgue outer measure, we have equality holds in inequation \\(\\ref{caratheo}\\). Here \\(A\\) is also called test set.</p> <p></p> <p>Another condition for measurable set</p> <p>Set \\(E\\subset \\mathbb{R}\\) and \\(E\\in \\Omega\\), iff \\(\\forall A\\subset E\\), \\(b\\subset E^c\\), such that</p> \\[ \\begin{align} m^*(A\\cup B)=m^*(A)+m^*(B) \\label{Cara-eq} \\end{align} \\] Proof <p>This is quite simple if we could see the relationship between condition \\(\\ref{Cara-eq}\\) and Carath\u00e9eodory condition.</p> <ul> <li>\\(\\Rightarrow\\). If \\(E\\) satisfies Caratheodory condition, then \\(\\forall T\\subset \\mathbb{R}\\), </li> </ul> \\[ m^*(T)\\geq m^*(T\\cap E)+m^*(T\\cap E^c). \\] <p>Let \\(A=T\\cap E\\subset E\\), \\(B=T\\cap E^c\\subset E^c\\), then \\(T=(T\\cap E)\\cup (T\\cap E^c)=A\\cup B\\), so the above condition \\(\\ref{Cara-eq}\\) holds.</p> <ul> <li>\\(\\Leftarrow\\). If \\(E\\) satisfies condition \\(\\ref{Cara-eq}\\), then \\(\\forall A\\subset E\\), \\(B\\subset E^c\\), </li> </ul> \\[ m^*(A\\cup B)=m^*(A)+m^*(B) \\] <p>let \\(T = A\\cup B\\), then \\(A = (A\\cup B)\\cap E=T\\cap E\\), \\(B=(A\\cup B)\\cap E^c=T\\cap E^c\\), then Carath\u00e9odory condition holds.</p> <p>This condition \\(\\ref{Cara-eq}\\) is also quite useful in the following proof.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#measurable-sets","title":"Measurable Sets","text":"<p>Lebesgue Measurable Sets</p> <p>(i) If \\(m^*(E)=0\\), then \\(E\\in \\Omega\\), and \\(m(E)=0\\).</p> <p>(ii) If \\(E\\) is an interval, then \\(E\\in \\Omega\\), and \\(m(E)=m^*(E)\\).</p> Proof for (i)Proof for (ii) <p>Because \\(E \\supset (A\\cap E)\\), so \\(0=m^*(E)\\geq m^*(A\\cap E)\\geq 0\\), which means \\(m^*(A\\cap E)=0\\).</p> <p>then by \\(A\\supset (A\\cap E^c)\\), we have</p> \\[ \\begin{align*} m^*(A)&amp;\\geq m^*(A\\cap E^c)\\\\ &amp;=m^*(A\\cap E^c)+0\\\\ &amp;=m^*(A\\cap E^c)+m^*(A\\cap E) \\end{align*} \\] <p>which satisfies Carath\u00e9odory condition.</p> <p>We need to prove that intervals satisfies Carath\u00e9odory condition, i.e. \\(\\forall A\\subset \\mathbb{R}\\), </p> \\[ m^*(A)\\geq m^*(A\\cap E)+m^*(A\\cap E^c) \\] <p>We focus on using open intervals of \\(A\\). That is, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists {I_n}\\) s.t. \\(A\\subset \\bigcup_n I_n\\) and \\(\\sum_n l(I_n)\\leq m^*(A)+\\varepsilon\\). We want to show that \\(m^*(A\\cap E)+m^*(A\\cap E^c)\\leq \\sum_n l(I_n)\\).</p> <p>Notice that \\(A\\cap E\\subset \\left[(\\bigcup_n I_n)\\cap E\\right]\\), so</p> \\[ \\begin{align*} m^*(A\\cap E)&amp;\\leq m^*\\left(\\left(\\bigcup_n I_n\\right)\\cap E\\right)\\\\ &amp;=m^*\\left(\\bigcup_n \\left(I_n\\cap E\\right)\\right)\\quad\\text{by distributive law}\\\\ &amp;\\leq \\sum_n m^*(I_n\\cap E) \\quad \\text{by subadditivity}\\\\ &amp;=\\sum_n l(I_n\\cap E) \\quad\\text{$I_n\\cap E$ is also intervals} \\end{align*} \\] <p>We have \\(E^c=E_1\\cup E_2\\) where \\(E_1\\) and \\(E_2\\) are also intervals, because \\(E\\) is an interval. Apply same logic to \\(E_1\\) and \\(E_2\\) gives </p> \\[ m^*(A\\cap E_1)\\leq \\sum_n l(I_n\\cap E_1) \\] \\[ m^*(A\\cap E_2)\\leq \\sum_n l(I_n\\cap E_2) \\] <p>adding the above three inequation up and get</p> \\[ \\begin{align*} m^*(A\\cap E)+ m^*(A\\cap E_1)+m^*(A\\cap E_1)&amp;\\leq \\sum_n l(I_n\\cap E)+l(I_n\\cap E_1)+l(I_n\\cap E_2)\\\\ &amp;=\\sum_n l(I_n) \\quad\\text{additivity of intervals} \\end{align*} \\] <p>Notice that \\((A\\cap E^c)=(A\\cap E_1)\\cup (A\\cap E_2)\\), then also by subadditivity of Lebesgue outer measure</p> \\[ m^*(A\\cap E^c)\\leq m^*(A\\cap E_1)+m^*(A\\cap E_2) \\] <p>So we have</p> \\[ \\begin{align*} m^*(A\\cap E)+m^*(A\\cap E^c)&amp;\\leq m^*(A\\cap E)+m^*(A\\cap E_1)+m^*(A\\cap E_2)\\\\ &amp;\\leq \\sum_n l(I_n)\\\\ &amp;\\leq  m^*(A)+\\varepsilon \\end{align*} \\] <p>Let \\(\\varepsilon\\rightarrow 0\\), so \\(E\\) satisfies Carath\u00e9odory condition.</p> <p>From (i) we could know that denumerable sets have zero measure. From (ii) we could know that any open, closed intervals are measurable.</p> <p>Example. Cantor set has zero measure.</p> Answer <p>Because in configuration of Cantor set, the measure of \\(G\\) </p> \\[ \\begin{align*} m(G)&amp;=\\frac{1}{3}+2\\cdot \\frac{1}{3^2}+2^2\\frac{1}{3^3}+\\cdots\\\\ &amp;=\\frac{1}{3} \\frac{[1-(\\frac{2}{3})^n]}{1-\\frac{2}{3}}\\rightarrow 1 (n\\rightarrow \\infty) \\end{align*} \\] <p>So \\(m(C)=m([0,1])-m(G)=0\\).</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#measure-operations","title":"Measure Operations","text":"<p>This part we aim to show denumerable additivity of measure. So we have to give some foundations.</p> <p>Measure operations</p> <p>Assume \\(E_1\\in \\Omega\\), \\(E_2\\in \\Omega\\), then</p> <p>(i) \\(E_1^c\\in \\Omega\\).</p> <p>(ii) \\(E_1\\cup E_2\\in \\Omega\\).</p> <p>(iii) \\(E_1\\cap E_2\\in \\Omega\\).</p> <p>(iv) \\(E_1 \\backslash E_2 \\in \\Omega\\).</p> Proof for (i)Proof for (ii)Proof for (iii)Proof for (iv) <p>This is quite easy, cause we can substitute \\(E=(E^c)^c\\) into Carath\u00e9odory condition and we could get the result.</p> <p>Our goal is \\(\\forall A\\subset \\mathbb{R}\\),</p> \\[ m^*(A)\\geq m^*(A\\cap (E_1\\cup E_2))+m^*(A\\cap (E_1\\cup E_2)^c) \\] <p>From what we already know, we have</p> \\[ \\begin{align*} m^*(A)\\geq m^*(A\\cap E_1)+m^*(A\\cap E_1^c)\\\\ m^*(A)\\geq m^*(A\\cap E_2)+m^*(A\\cap E_2^c) \\end{align*} \\] <p>The following is a little tricky. Let \\(A=A\\cap E_1^c\\), then the second of above two becomes</p> \\[ \\begin{align*} m^*[(A\\cap E_1^c)\\cap E_2]+m^*[(A\\cap E_1^c)\\cap E_2^c]&amp;\\leq m^*(A\\cap E_1^c)\\\\ m^*[(A\\cap E_1^c)\\cap E_2]+m^*[A\\cap (E_1\\cup E_2)^c]&amp;\\leq m^*(A)-m^*(A\\cap E_1)\\\\ m^*[(A\\cap E_1^c)\\cap E_2]+m^*(A\\cap E_1)+m^*[A\\cap (E_1\\cup E_2)^c]&amp;\\leq m^*(A) \\end{align*} \\] <p>Now we only need to show that \\(m^*[(A\\cap E_1^c)\\cap E_2]+m^*(A\\cap E_1)=m^*[A\\cap (E_1\\cup E_2)]\\). Here we could use another definition of measurable set. Notice that \\((A\\cap E_1)\\subset E_1\\), \\([(A\\cap E_1^c)\\cap E_2]\\subset E_1^c\\), \\(E_1\\in \\Omega\\), so </p> \\[ \\begin{align*} m^*[(A\\cap E_1^c)\\cap E_2]+m^*(A\\cap E_1)&amp;=m^*([(A\\cap E_1^c)\\cap E_2]\\cup (A\\cap E_1))\\\\ &amp;=m^*([(A\\cap E_1^c)\\cup(A\\cap E_1)]\\cap [E_2\\cup (A\\cap E_1)])\\\\ &amp;=m^*(A\\cap[(E_2\\cup A)\\cap(E_2\\cup E_1)])\\\\ &amp;=m^*(A\\cap(E_1\\cup E_2)) \\end{align*} \\] <p>Or we have </p> \\[ \\begin{align*} [(A\\cap E_1^c)\\cap E_2]\\cup (A\\cap E_1)&amp;=[A\\cap (E_1^c\\cap E_2)]\\cup (A\\cap E_1)\\\\ &amp;=[(E_1^c\\cap E_2)\\cup E_1]\\cap A\\\\ &amp;=[(E_2\\cup E_1)\\cap (E_1^c\\cup E_1)]\\cap A\\\\ &amp;=[(E_1\\cup E_2)\\cap \\mathbb{R}]\\cap A\\\\ &amp;=(E_1\\cup E_2)\\cap A \\end{align*} \\] <p>which gives the same answer.</p> <p>Using (i) and (ii)</p> <p>Notice \\(E_1\\cap E_2=(E_1^c\\cup E_2^c)^c\\). Because operations of complementation and union both holds for measurable, operation of intersection also holds for measurable.</p> <p>Same logic with (iii). Notice that \\(E_1\\backslash E_2=E_1\\cap E_2^c\\), since operation of intersection and complementation holds for measurable, operation of difference also holds for measurable.</p> <p></p> <p>Corollary</p> <p>Assume \\(E_n\\in \\Omega\\), \\(k=1,2\\cdots\\), then \\(\\forall N&gt;0\\) and \\(N&lt;\\infty\\),</p> \\[ \\bigcup_{n=1}^N E_n\\in \\Omega,\\quad \\bigcap_{n=1}^N E_n\\in \\Omega \\] <p>Using the above corollary we could have a great result for operations of sequence of measurable sets.</p> <p></p> <p>Theorem for union of sequence of measurable sets</p> <p>If \\(\\{E_n\\}_{n\\geq 1}\\) are mutually disjoint and \\(E_n\\in \\Omega\\), then </p> \\[ \\bigcup_n E_n\\in \\Omega \\] Proof <p>Notice that \\(\\forall N&gt;0\\), using corollary</p> \\[ \\begin{align*} m^*(A)&amp;\\geq m^*\\left[A\\cap \\left(\\bigcup_{n=1}^N E_n\\right)\\right]+m^*\\left[A\\cap \\left(\\bigcup_{n=1}^N E_n\\right)^c\\right]\\\\ \\end{align*} \\] <p>Here we could not let \\(N\\rightarrow \\infty\\), due to \\(N\\) is embedded in the operation of sets, so we need to get it out. By additivity for mutually disjoint sets, we have</p> \\[ \\begin{align*} m^*(A)&amp;=\\left[\\sum_{n=1}^N m^*(A\\cap E_n)\\right] + m^*\\left[A\\cap \\left(\\bigcup_{n=1}^N E_n\\right)^c\\right]\\quad\\\\ &amp;\\geq \\left[\\sum_{n=1}^N m^*(A\\cap E_n)\\right] + m^*\\left[A\\cap \\left(\\bigcup_{n=1}^\\infty E_n\\right)^c\\right] \\end{align*} \\] <p>let \\(N\\rightarrow \\infty\\), and get \\(\\infty\\) back into operations of sets</p> \\[ \\begin{align*} m^*(A)&amp;\\geq\\left[\\sum_{n=1}^\\infty m^*(A\\cap E_n)\\right] + m^*\\left[A\\cap \\left(\\bigcup_{n=1}^\\infty E_n\\right)^c\\right]\\\\ &amp;\\geq m^*\\left[\\bigcup_{n=1}^\\infty (A\\cap E_n)\\right]+ m^*\\left[A\\cap \\left(\\bigcup_{n=1}^\\infty E_n\\right)^c\\right]\\quad \\text{by subadditivity}\\\\ &amp;=m^*\\left[A\\cap \\left(\\bigcup_{n=1}^\\infty E_n\\right)\\right] +m^*\\left[A\\cap \\left(\\bigcup_{n=1}^\\infty E_n\\right)^c\\right] \\end{align*} \\] <p>and we are done.</p> <p>In a similar way, we have</p> <p>Corollary</p> <p>If \\(\\{E_n\\}_{n\\geq 1}\\) are mutually disjoint and \\(E_n\\in \\Omega\\), then </p> \\[ \\bigcap_n E_n\\in \\Omega \\] <p>the above two theorems both hold if we remove the condition \"mutually disjoint\".</p> <p>Theorem for operations of denumerable sets</p> <p>Assume \\(E_n\\in \\Omega\\), \\(n=1,2,\\cdots\\), then </p> \\[ \\bigcup_n E_n\\in \\Omega,\\quad \\bigcap_n E_n\\in \\Omega \\] Proof <p>We could use the conclusion from the above two theorem. That is, construct mutually disjoint sets to help prove. We would use some conclusion from Sets.</p> <p>Define \\(A_1=E_1\\), \\(A_n=E_n-\\bigcup_{k=1}^{n-1}E_n (n\\geq 2)\\) so we have a sequence of mutually disjoint sets \\(\\{A_n\\}\\) which satisfies \\(\\bigcup_n A_n=\\bigcup_n E_n\\).</p> <p>Because \\(\\bigcup_n A_n\\) are measurable due to Theorem for union of sequence of measurable sets, so \\(\\bigcup_n E_n\\in \\Omega\\).</p> <p>To be more specific, we have a good property for measurable sets, which is denumerable additivity, or \\(\\sigma\\)-additiviity.</p> <p>\\(\\sigma\\)-additivity</p> <p>If \\(\\{E_n\\}_{n\\geq 1}\\) are mutually disjoint and \\(E_n\\in \\Omega\\), then \\(\\forall A\\subset \\mathbb{R}\\),</p> \\[ \\begin{align} m^*\\left[A\\cap \\left(\\bigcup_{n=1}^\\infty E_n\\right)\\right]=\\sum_{n=1}^\\infty m^*(A\\cap E_n)\\label{sigma-additivity} \\end{align}\\] <p>If we let \\(A=\\mathbb{R}\\), then equation \\(\\ref{sigma-additivity}\\) becomes</p> \\[ m^*\\left(\\bigcup_{n=1}^\\infty E_n\\right)=\\sum_{n=1}^\\infty m^*(E_n) \\] Proof <p>The proof has been given in proof of Theorem for union of sequence of measurable sets.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#limit-operation","title":"Limit Operation","text":"<p>Now we introduce limit operation which might be useful in future proof.</p> <p></p> <p>Limit Operations</p> <p>If a sequence of measurable sets \\(\\{E_n\\}_{n\\geq 1}\\) satisfies either of the following condition </p> <p>(i) \\(\\{E_n\\}\\) is monotonically increasing.</p> <p>(ii) \\(\\{E_n\\}\\) is monotonically decreasing, and \\(m(E_1)&lt;\\infty\\).</p> <p>then </p> \\[ \\lim_{n\\rightarrow \\infty} m(E_n) = m\\left(\\lim_{n\\rightarrow \\infty} E_n\\right) \\] Proof for (i)Proof for (ii) <p>(i) means \\(E_1\\subset E_2\\subset\\cdots\\), so </p> \\[ m\\left( \\lim_{n\\rightarrow \\infty} E_n\\right)=m\\left( \\bigcup_{n=1}^\\infty E_n \\right) \\] <p>and </p> \\[ \\lim_{n\\rightarrow \\infty} m(E_n)=\\lim_{N\\rightarrow \\infty}m\\left( \\bigcup_{n=1}^N E_n \\right) \\] <p>and the proposition to be proved equals to </p> \\[ m\\left( \\bigcup_{n=1}^\\infty E_n \\right)=\\lim_{N\\rightarrow \\infty}m\\left( \\bigcup_{n=1}^N E_n \\right) \\] <p>Let \\(A_1=E_1\\), \\(A_n=E_n-\\bigcup_{k=1}^{n-1}E_n (n\\geq 2)\\), so \\(\\{A_n\\}\\) are mutually disjoint and satisfies \\(\\bigcup_n A_n=\\bigcup_n E_n\\). So</p> \\[ \\begin{align*} m\\left( \\bigcup_{n=1}^\\infty E_n \\right)&amp;=m\\left( \\bigcup_{n=1}^\\infty A_n \\right)\\\\ &amp;=\\sum_{n=1}^\\infty m(A_n)\\quad\\text{by $\\sigma$-additivity}\\\\ &amp;=\\lim_{N\\rightarrow \\infty}\\sum_{n=1}^N m(A_n)\\quad\\text{holds for series}\\\\ &amp;=\\lim_{N\\rightarrow \\infty}m\\left( \\bigcup_{n=1}^N A_n \\right)\\\\ &amp;=\\lim_{N\\rightarrow \\infty}m\\left( \\bigcup_{n=1}^N E_n \\right)\\\\ \\end{align*} \\] <p>and we are done.</p> <p>(ii) means \\(E_1\\supset E_2\\supset \\cdots\\). We could use the conclusion from (i), that is, let \\(A_n=E_1\\backslash E_n (n\\geq 1)\\) so it is easy to see that</p> \\[ A_1\\subset A_2\\subset \\cdots \\text{ and } \\bigcup_n A_n=E_1\\backslash \\left(\\bigcup_n E_n\\right) \\] <p>So by operation of difference, </p> \\[ \\begin{align*} m(E_1)-m\\left(\\bigcup_n E_n\\right)&amp;=m\\left(\\bigcup_n A_n\\right)\\\\ m(E_1)&amp;=\\lim_n m(A_n)+m\\left(\\bigcup_n A_n\\right)\\\\ \\Rightarrow \\lim_n [m(E_1)-m(A_n)]&amp;=m\\left(\\bigcup_n A_n\\right) \\\\ \\lim_n [m(E_1\\backslash A_n)]&amp;=m\\left(\\bigcup_n A_n\\right)\\quad\\text{by operation of difference}\\\\ \\lim_n m(E_n)&amp;=m\\left(\\bigcup_n A_n\\right) \\end{align*} \\] <p>and we are done.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#translation-invariance","title":"Translation Invariance","text":"<p>Definition of Translation</p> <p>Assume \\(E\\subset \\mathbb{R}\\) and \\(y\\in \\mathbb{R}\\), then </p> \\[ E_y=\\{x+y: x\\in E\\} \\] <p>is called a translation of \\(E\\) about \\(y\\).</p> <p>About translation, we have the following property.</p> <p>Property of Translation</p> <p>Assume \\(E, F\\subset \\mathbb{R}\\), then \\(\\forall y\\in \\mathbb{R}\\),</p> <p>(i) \\(E\\cap E_y=(E_{-y}\\cap F)_y\\).</p> <p>(ii) \\((E^c)_y=(E_y)^c\\).</p> <p>(iii) \\(m^*(E)=m^*(E_y)\\).</p> <p>Translation Invariance of Measure</p> <p>Assume \\(E \\in \\Omega\\), then \\(\\forall y\\in \\mathbb{R}\\), \\(E_y\\in \\Omega\\) and \\(m(E_y)=m(E)\\).</p> Proof <p>Use the above property to transfer Carath\u00e9odory condition of \\(E\\) into \\(E_y\\).</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#approximate-measurable-set-with-open-closed-sets","title":"Approximate Measurable set with open &amp; closed sets","text":"<p>Theorem of Approximation</p> <p>The following proposition are equivalent.</p> <p>(i) \\(E\\in \\Omega\\).</p> <p>(ii) \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\text{ open set }G\\supset E\\), such that \\(m^*(G-E)&lt;\\varepsilon\\).</p> <p>(iii) \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\text{ closed set } F \\subset E\\), such that \\(m^*(E-F)&lt;\\varepsilon\\).</p> (i) \\(\\Rightarrow\\) (ii)(ii) \\(\\Rightarrow\\) (i)(i) \\(\\Rightarrow\\) (iii) <p>We could prove this on \\(\\mathbb{R}^n\\).</p> <p>Define </p> \\[ B_n:=\\{x\\in \\mathbb{R} : n-1 \\leq |x|&lt; n\\},\\quad n\\in \\mathbb{N}^+ \\] <p>because open set is measurable, whose subtraction is also measurable, i.e. \\(B_n \\in \\Omega\\).</p> <p>We here want to use these sequence of half-open cubes. That is, define \\(E_n:=B_n\\cap E \\in \\Omega\\), and it is easy to see that \\(\\{E_n\\}\\) are mutually disjoint. If we assume \\(E(n)&lt;\\infty\\) (if not, the proposition also holds), then by \\(E=\\bigcup_n E_n\\), we have sigma-additivity</p> \\[ m(E)=\\sum_n m(E_n) \\] <p>Then we want to use open covering of \\(E_n\\) to construct the so-called open set \\(G\\). That is, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\{I^{(n)}_k\\}_{k\\geq 1}\\) such that \\(E_n\\subset \\bigcup_k I^{(n)}_k\\) and \\(\\sum_k l(I_k^{(n)})\\leq m(E_n)+\\varepsilon/2^n\\). Then the following construction is quite natural.</p> <p>Define \\(G_n:=\\bigcup_k I_k^{(n)}\\supset E_n\\), so by operation of difference, we have</p> \\[ \\begin{align*} m(G_n\\backslash E_n)&amp;=m(G_n)-m(E_n)\\\\ &amp;\\leq \\sum_km(I_k^{(n)})-m(E_n) \\quad \\text{by subadditivity}\\\\ &amp;&lt; \\frac{\\varepsilon}{2^n} \\end{align*} \\] <p>Define \\(G:=\\bigcup_n G_n \\supset \\bigcup_n E_n=E\\), so by operation of difference, we have</p> \\[ \\begin{align*} m(G)-m(E)&amp;=m(G\\backslash E)\\\\ &amp;=m\\left[ \\left(\\bigcup_n G_n\\right) \\backslash \\left(\\bigcup_n E_n\\right)\\right]\\\\ &amp;\\leq m\\left[\\bigcup_n (G_n \\backslash E_n)\\right]\\quad \\text{by subtraction relation}\\\\ &amp;\\leq \\sum_n m(G_n\\backslash E_n)&lt;\\varepsilon \\end{align*} \\] <p>and we are done.</p> <p>Here we utilize \\(\\varepsilon\\) to formulate a covering of open sets and hope to show \\(G\\backslash E\\) has zero measure. That is, \\(\\forall \\varepsilon=\\frac{1}{n}\\), \\(\\exists G_n \\supset E\\) s.t. \\(m^*(G_n \\backslash E)&lt;\\varepsilon=\\frac{1}{n}\\). Define \\(G:=\\bigcap_n G_n\\) which is measurable. By \\(G\\backslash E \\subset G_n\\backslash E\\), we have</p> \\[ m^*(G\\backslash E) = m^*(G_n\\backslash E)\\leq \\frac{1}{n} \\] <p>Let \\(n\\rightarrow \\infty\\), we have \\(m^*(G\\backslash E)=0\\). Because \\(G\\backslash E \\in \\Omega\\) and \\(G\\in \\Omega\\), we have \\(E\\in \\Omega\\).</p> <p>WE could utilize conclusions from (i) and (ii). Assume \\(E\\in \\Omega\\), then \\(E^c\\in \\Omega\\). By (ii), \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\text{ open set }G\\supset E^c\\), such that \\(m(G\\backslash E^c)&lt;\\varepsilon\\). Since \\(G\\) is an open set, \\(G^c\\) is a closed set. If we let \\(F=G^c\\), then </p> \\[ E\\backslash F=E\\backslash G^c=E\\cap G = G\\backslash E^c \\] <p>which means \\(m(E\\backslash F)=m(G\\backslash E^c)&lt;\\varepsilon\\).</p> <p>Corollary</p> <p>The following statements are equivalent.</p> <p>(i) \\(E\\in \\Omega\\).</p> <p>(ii) \\(\\forall \\varepsilon&gt;0\\), \\(\\exists \\text{ open set }G\\) and \\(\\text{ closed set } F\\), such that</p> \\[ F\\subset E\\subset G \\text{ and } m(G\\backslash F)&lt;\\varepsilon. \\] <p>(iii) \\(\\exists G=G_\\delta \\supset E\\), such that \\(m^*(G-E)=0\\)</p> <p>(iv) \\(\\exists F=F_\\sigma \\subset E\\), such that \\(m^*(E-F)=0\\)</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#borel-set","title":"Borel Set","text":"<p>Check the definition of \\(\\sigma\\)-algebra and its properties in elementary probability theory, and \\(\\sigma\\)-algebra generated by set.</p> <p>Definition of Borel Algebra and Borel Set</p> <p>A \\(\\sigma\\)-algebra generated by the whole open intervals on \\(\\mathbb{R}\\) is called Borel Algebra, denoted by \\(\\mathscr{B}\\), or \\(\\mathscr{B}(\\mathbb{R})\\). The element of \\(\\mathscr{B}\\) is called Borel Set. Apparently, Borel set is measurable.</p> <p>Properties of Borel Set</p> <p>(i) Open set \\(G\\in \\mathscr{B}\\)</p> <p>(ii) Closed set \\(F\\in \\mathscr{B}\\).</p> <p>(iii) single point set \\(\\{a\\}\\in \\mathscr{B}\\).</p> <p>(iv) half open intervals \\(H\\in \\mathscr{B}\\).</p> Proof <p>(i) \\(G\\) is denumerable union of open intervals.</p> <p>(ii) \\(F\\) is the complementary set of open set \\(G\\).</p> <p>(iii) single point set is the denumerable intersection of open intervals, i.e.</p> \\[ \\{a\\}=\\bigcap_{n=1}^\\infty\\left(a-\\frac{1}{n}, a+\\frac{1}{n}\\right). \\] <p>(iv) half open intervals are union of an interval and a single point set.</p> <p></p> <p>Equivalent configuration of Borel Algebra</p> <p>The following are equivalent definition of \\(\\mathscr{B}\\).</p> <p>(i) \\(\\sigma(\\{(-\\infty,x]: x\\in \\mathbb{R}\\})\\),</p> <p>(ii) \\(\\sigma(\\{(-\\infty,x): x\\in \\mathbb{R}\\})\\),</p> <p>(iii) \\(\\sigma(\\{(-\\infty,x]: x\\in \\mathbb{Q}\\})\\).</p> <p>Actually, \\(\\mathscr{B}\\) could be generated by the whole closed set, or the whole half open sets, but not single point set. Check the following example.</p> <p>Example. (Simgle point set) Assume \\(Q\\) is a non-empty set and \\(S\\) is a set composed by all single point in \\(Q\\), i.e.</p> \\[ S=\\{\\{x\\}: x\\in Q\\}. \\] <p>Show that \\(\\sigma(S)=\\mathscr{A}\\), where \\(\\mathscr{A}\\) is defined by</p> \\[ \\mathscr{A}:=\\{A\\subset Q: \\text{either }A \\text{ or } A^c \\text{ is denumerable.}\\} \\] Proof <p>We use \\(\\mathscr{C}\\) to be the whole denumerable sets on \\(Q\\).</p> <ul> <li>\"\\(\\subset\\)\". Firstly show that \\(\\mathscr{A}\\) is a \\(\\sigma\\)-algebra. Apparently \\(\\varnothing, Q\\in \\mathscr{A}\\). We show the following two condition.</li> </ul> <p>(i) \\(\\forall A\\in \\mathscr{A}\\), \\(A^c \\in\\mathscr{A}\\). This is apparently, because \\(\\forall A\\in \\mathscr{A}\\), there are two cases: </p> <p>if \\(A\\in \\mathscr{C}\\), then \\((A^c)^c=A\\in \\mathscr{C}\\), so \\(A^c\\in\\mathscr{A}\\); </p> <p>else if \\(A^c\\in \\mathscr{C}\\), then \\(A^c\\in \\mathscr{A}\\).</p> <p>(ii) \\(\\forall \\{A_n\\}_{n\\geq 1}\\in \\mathscr{A}\\), \\(\\bigcup_nA_n\\in\\mathscr{A}\\). There are also two cases.</p> <p>If for all \\(A_n\\), we have \\(A_n\\in\\mathscr{C}\\), then by union of denumerable sets is denumerable, \\(\\bigcup_nA_n\\in\\mathscr{C}\\), which means \\(\\bigcup_nA_n\\in\\mathscr{A}\\); </p> <p>else if there exists \\(j\\) such that \\(A_j^c\\in \\mathscr{C}\\), then \\((\\bigcup_n A_n)^c=\\bigcap_n A_n^c\\in \\mathscr{C}\\), so \\(\\bigcup_n A_n\\in \\mathscr{A}\\).</p> <p>Combining the above two, we have \\(A\\) is a \\(sigma\\)-algebra. By definition of \\(\\sigma(S)\\), we have \\(\\sigma(C)\\subset \\mathscr{A}\\). </p> <ul> <li>\"\\(\\supset\\)\". \\(\\forall A\\in \\mathscr{A}\\), there are two cases.</li> </ul> <p>If \\(A\\in \\mathscr{C}\\), then \\(A=\\{x_n\\}_{n\\geq 1}=\\bigcup_n x_n\\). Since \\(\\{x_n\\}\\in S\\in \\sigma(S)\\), and \\(\\sigma(S)\\) is closed for denumerable union, so \\(A\\in \\sigma(S)\\).</p> <p>Else if \\(A^c\\in \\mathscr{C}\\), then \\(A^c\\in \\sigma(S)\\). Since \\(\\sigma(S)\\) is closed for complementation, we have \\(A\\in \\sigma(S)\\).</p> <p>In a nutshell, we have \\(\\mathscr{A}\\subset \\sigma(S)\\). And we are done.</p>"},{"location":"Math/Real_Analysis/Lebesgue_Measure/#measure-of-direct-product","title":"Measure of Direct Product","text":"<p>This part is useful in deducing multiple integral formula.</p> <p>Theorem for measurability of Direct Product</p> <p>Assume \\(P\\) and \\(Q\\) are measurable sets in \\(\\mathbb{R}^p\\) and \\(\\mathbb{R}^q\\) respectively, then \\(P\\times Q\\) is a measurable set in \\(\\mathbb{R}^{p+q}\\), and</p> \\[ m(P\\times Q)=m(P)\\cdot m(Q). \\] Proof <p>(i) \\(P\\), \\(Q\\) are cubes in \\(\\mathbb{R}^p\\) and \\(\\mathbb{R}^q\\) respectively.</p> <p>(ii) \\(P\\), \\(Q\\) are open sets in \\(\\mathbb{R}^p\\) and \\(\\mathbb{R}^q\\) respectively.</p> <p>Use Configuration of open sets in high dimension space.</p> <p>(iii) \\(P\\), \\(Q\\) are measure-finite sets in \\(\\mathbb{R}^p\\) and \\(\\mathbb{R}^q\\) respectively.</p> <p>Use Approximation with open and closed sets.</p> <p>(iv) \\(P\\), \\(Q\\) are general measurable sets in \\(\\mathbb{R}^p\\) and \\(\\mathbb{R}^q\\) respectively. </p> <p>Use series of measure-finite sets like \\(\\{x\\in \\mathbb{R}^p: k-1\\leq \\|x\\|&lt; k\\}\\) to intersect to construct cases that could use conclusion in (iii).</p>"},{"location":"Math/Real_Analysis/Lp_Space/","title":"\\(L^p\\) Space","text":"<p>Modern Real analysis focuses on digging into the structure of Space combined by these functions. Lebesgue Integral Theory is the most successful one.</p> <p>Definitions</p> <p>(i) Assume \\(E\\) is a measurable set, \\(1\\leq p&lt;\\infty\\), denote</p> \\[ L^p(E)=\\{f: f\\text{ is measurable on } E \\text{ and } \\int_E |f(x)|^pdx&lt;\\infty \\}. \\] <p>for \\(f\\in L^p(E)\\), denote</p> \\[ \\|f\\|_p=\\left(\\int_E |f(x)|^pdx\\right)^{\\frac{1}{p}} \\] <p>as norm of \\(f\\). Notice that \\(L^1(E)\\) equals \\(L(E)\\).</p> <p>(ii) Assume \\(m(E)&gt;0\\). If there exists \\(M&gt;0\\), such that \\(|f(x)|\\leq M,\\quad a.e.x\\in E\\), then we call \\(f\\) is essentially bounded on \\(E\\), \\(M\\) is called the essential upper bound of \\(f(x)\\). A set composed by all essentially bounded function is denoted as \\(L^\\infty(E)\\). If \\(f\\in L^{\\infty}(E)\\), define</p> \\[ \\|f\\|_{\\infty}:=\\inf\\{M: |f(x)|\\leq M,\\quad a.e.x\\in E\\} \\] <p>as the norm or essential bound of \\(f\\).</p> <p>Example. Prove: when \\(0&lt; m(E)&lt;\\infty\\), we have</p> \\[ \\lim_{p\\rightarrow \\infty}\\|f\\|_p=\\|f\\|_\\infty. \\] Proof <p>Let \\(M=\\|f\\|_\\infty\\), then choose \\(M'&lt;M\\), and define set </p> \\[ A=\\{x\\in E: |f(x)|&gt;M'\\} \\] <p>which has positive measure. Then by</p> \\[ \\|f\\|_p=\\left(\\int_E |f(x)|^pdx\\right)^{\\frac{1}{p}}\\geq M' [m(A)]^{\\frac{1}{p}} \\] <p>let \\(p\\rightarrow \\infty\\), we have</p> \\[ \\underline{\\lim}\\limits_{p\\rightarrow \\infty}\\|f\\|_p\\geq M' \\] <p>let \\(M'\\rightarrow M\\), we have \\(\\underline{\\lim}\\limits_{p\\rightarrow \\infty}\\|f\\|_p\\geq M\\).</p> <p>On the other hand, we have</p> \\[ \\|f\\|_p\\leq \\left(\\int_E M^pdx\\right)^{\\frac{1}{p}}=M[m(E)]^{\\frac{1}{p}} \\] <p>let \\(p\\rightarrow \\infty\\), we have</p> \\[ \\overline{\\lim}\\limits_{p\\rightarrow \\infty}\\|f\\|_p\\leq M. \\] <p>So we are done.</p> <p>Theorem</p> <p>If \\(m(E)&lt;\\infty\\), then \\(L^p(E)\\subset L^1(E)\\).</p> Proof <p>Let \\(A=\\{f\\geq 1\\}\\cap E\\), \\(B=\\{f&lt;1\\}\\cap E\\), if \\(f\\in L^p(E)\\), then</p> \\[ \\begin{align*} \\int_E |f|^p dx&lt;\\infty. \\end{align*} \\] <p>So</p> \\[ \\begin{align*} \\int_E |f| dx&amp;=\\int_A |f|dx+\\int_B|f|dx\\\\ &amp;&lt;\\int_A |f|^pdx+m(B)\\\\ &amp;&lt;\\int_E |f|^pdx+m(B)&lt;\\infty. \\end{align*} \\] <p>Example. \\(E=(0,\\infty)\\), \\(f(x)=\\frac{1}{1+x}\\), then \\(f\\in L^p(E)\\) but \\(f\\notin L^1(E)\\).</p> <p>\\(L^p(E)\\) is a linear space</p> <p>Assume \\(f,g\\in L^p(E)\\), \\(0&lt; p\\leq \\infty\\), \\(\\alpha,\\beta\\in \\mathbb{R}\\), then</p> \\[ \\alpha f + \\beta g\\in L^p(E). \\] Proof <ul> <li>\\(0&lt;p&lt;\\infty\\), </li> </ul> \\[ |\\alpha f(x)+\\beta g(x)|^p\\leq 2^p(|\\alpha|^p|f(x)|^p+|\\beta|^p|g(x)|^p). \\] <ul> <li>\\(p=\\infty\\).</li> </ul> \\[ |\\alpha f(x)+\\beta g(x)|\\leq |\\alpha|\\|f\\|_\\infty+|\\beta|\\|g\\|_\\infty, \\quad a.e.x\\in E. \\] <p>so</p> \\[ \\|\\alpha f+\\beta g\\|_\\infty\\leq |\\alpha|\\|f\\|_\\infty+|\\beta|\\|g\\|_\\infty. \\]"},{"location":"Math/Real_Analysis/Lp_Space/#properties-of-lp-space","title":"Properties of \\(L^p\\) Space","text":"<p>H\u00f6lder Inequation</p> <p>If \\(1&lt;p,q&lt;\\infty\\), and </p> \\[ \\frac{1}{p}+\\frac{1}{q}=1, \\] <p>then we call \\(p\\) and \\(q\\) are conjugate indexes.</p> <p>If \\(f\\in L^p(E)\\), \\(g\\in L^q(E)\\), then \\(fg\\in L(E)\\) and </p> \\[ \\|fg\\|_1\\leq \\|f\\|_p\\cdot \\|g\\|_q. \\] Proof <p>Use inequation </p> \\[ ab\\leq \\frac{a^p}{p}+\\frac{b^q}{q}. \\] <p>When \\(p=2\\) in the above inequation, it is called Schwartz Inequation.</p> <p> Example. Assume \\(m(E)&lt;\\infty\\), and \\(1\\leq p&lt; q &lt;\\infty\\), then \\(L^q(E)\\subset L^p(E)\\), and  \\[ \\|f\\|_p\\leq [m(E)]^{\\frac{1}{p}-\\frac{1}{q}}\\|f\\|_q. \\] Proof <p>Use H\u00f6lder inequation. Let \\(r=\\frac{q}{p}\\) and \\(r'=\\frac{r}{r-1}=\\frac{q}{q-p}\\).</p> \\[ \\begin{align*} \\int_E |f|^p\\cdot 1dx&amp;\\leq \\| |f|^p\\|_r\\cdot \\|1\\|_{r'}\\\\ \\|f\\|_p^p&amp;=\\left(\\int_E |f|^{p\\cdot r}dx\\right)^\\frac{1}{r}\\cdot \\left(\\int_E 1dx\\right)^\\frac{1}{r'}\\\\ \\|f\\|_p^p&amp;=\\left(\\int_E |f|^{q}dx\\right)^\\frac{p}{q}\\cdot \\left(m(E)\\right)^\\frac{q-p}{q}\\\\ \\|f\\|_p^p&amp;=\\|f\\|_q^p\\cdot \\left(m(E)\\right)^\\frac{q-p}{q}\\\\ \\Rightarrow \\|f\\|_p&amp;=\\|f\\|_q\\cdot\\left(m(E)\\right)^{\\frac{1}{p}-\\frac{1}{q}} \\end{align*} \\] <p>which means \\(L^q(E)\\subset L^p(E)\\).</p> <p>Example. Assume \\(f\\in L^r(E)\\cap L^s(E)\\), where \\(1\\leq r&lt; q &lt; s \\leq \\infty\\), and </p> \\[ \\frac{1}{p}=\\frac{\\lambda}{r}+\\frac{1-\\lambda}{s},\\quad 0&lt;\\lambda&lt;1. \\] <p>then</p> \\[ \\|f\\|_p\\leq \\|f\\|_r^\\lambda \\|f\\|_s^{1-\\lambda}. \\] Proof <p></p> <p>Minkowski Inequation</p> <p>Assume \\(f,g\\in L^p(E)\\), \\(1\\leq p\\leq\\infty\\), then \\(f+g\\in L^p(E)\\), and</p> \\[ \\|f+g\\|_p\\leq \\|f\\|_p+\\|g\\|_p. \\] Proof <p>Example. Assume \\(1\\leq p\\leq \\infty\\). If \\(f_k(x)\\in L^p(E), k=1,2,\\cdots\\), and \\(\\sum\\limits_{k=1}^\\infty f_k(x)\\) converges \\(a.e.x\\in E\\), then </p> \\[ \\left\\|\\sum_{k=1}^\\infty f_k\\right\\|_p\\leq \\sum_{k=1}^\\infty \\|f_k\\|_p. \\] Proof <ul> <li>\\(p=\\infty\\).</li> </ul> <p>Notice that</p> \\[ \\left|\\sum\\limits_{k=1}^\\infty f_k(x)\\right|\\leq \\sum\\limits_{k=1}^\\infty \\left|f_k(x)\\right|\\leq \\sum_{k=1}^\\infty \\left\\|f_k(x)\\right\\|_\\infty \\] <p>then by definition of \\(\\left\\|\\sum\\limits_{k=1}^\\infty f_k\\right\\|_\\infty\\), we have</p> \\[ \\begin{align*} \\left\\|\\sum_{k=1}^\\infty f_k\\right\\|_\\infty&amp;\\leq \\sum_{k=1}^\\infty \\left\\|f_k\\right\\|_\\infty. \\end{align*} \\] <ul> <li>\\(1\\leq p&lt;\\infty\\).</li> </ul> \\[ \\begin{align*} \\left\\|\\sum_{k=1}^\\infty f_k\\right\\|_p^p&amp;=\\int_E \\left|\\sum_{k=1}^\\infty f_k\\right|^pdx\\\\ &amp;\\leq \\int_E \\left(\\sum_{k=1}^\\infty \\left|f_k\\right|\\right)^p dx\\\\ &amp;=\\int_E \\lim_{n\\rightarrow \\infty} \\left(\\sum_{k=1}^n \\left|f_k\\right|\\right)^p dx\\\\ &amp;=\\lim_{n\\rightarrow \\infty} \\int_E \\left(\\sum_{k=1}^n \\left|f_k\\right|\\right)^p dx\\quad \\text{Levi Theorem}\\\\ &amp;=\\lim_{n\\rightarrow \\infty} \\left\\|\\sum_{k=1}^n \\left|f_k\\right|\\right\\|_p^p\\\\ &amp;\\leq \\lim_{n\\rightarrow \\infty}\\left(\\sum_{k=1}^n \\left\\|f_k\\right\\|_p\\right)^p \\quad \\text{Minkowski} \\end{align*} \\] <p>To introduce distance in \\(L^p(E)\\), we assume that if \\(f,g\\in L^p(E)\\), we have \\(f(x)=g(x),a.e.x\\in E\\), then we denote this relationship as \\(f=g\\).</p>"},{"location":"Math/Real_Analysis/Lp_Space/#use-p-norm-as-metric","title":"Use p-norm as Metric","text":"<p>Now in the following parts, we assume \\(1\\leq p\\leq \\infty\\).</p> <p>Definitions</p> <p>Assume \\(f,g\\in L^p(E)\\), we introduce distance (or to be more specific, metric) </p> \\[ \\rho(f,g)=\\|f-g\\|_p,\\quad 1\\leq p\\leq \\infty. \\] <p>then \\(\\rho\\) satisfies three conditions of metric, and \\((L^p(E),\\rho)\\) is a metric space.</p>"},{"location":"Math/Real_Analysis/Lp_Space/#convergence","title":"Convergence","text":"<p>With metric \\(\\rho\\), we could introduce the limit in a sense of \\(\\rho\\). This is a special case of convergence.</p> <p>Definition of Convergence in \\(L^p\\) Space</p> <p>Assume \\(f_n\\in L^p(E)\\), if there exists \\(f\\in L^P(E)\\), such that</p> \\[ \\lim_{n\\rightarrow \\infty} \\rho(f_n,f)=\\lim_{n\\rightarrow \\infty}\\|f_n-f\\|_p=0, \\] <p>then we call \\(\\{f_n\\}\\) converges to \\(f\\) in a sense of \\(L^p(E)\\), which is denoted by \\(f_n\\rightarrow f(L^p)\\).</p> <p>Properties of Convergence in \\(L^p\\) Space</p> <p>(i) Uniqueness. If \\(\\lim\\limits_{n\\rightarrow \\infty} \\rho(f_n,f)=0=\\lim\\limits_{n\\rightarrow \\infty} \\rho(f_n,g)\\), then </p> \\[ f(x)=g(x),\\quad a.e.x\\in E. \\] <p>(ii) If \\(\\lim\\limits_{n\\rightarrow \\infty} \\rho(f_n,f)=0\\), then </p> \\[ \\lim\\limits_{n\\rightarrow \\infty} \\|f_n\\|_p=\\|f\\|_p. \\] Proof <p>(i) easy to see.</p> <p>(ii) We have</p> \\[ |\\|f_k\\|_p-\\|f\\|_p|\\leq \\|f_k-f\\|_p. \\] <p>Example. Still the same function of Example. We could know that</p> \\[ f_n(x)\\nrightarrow 0,\\quad a.e.x\\in E, \\] <p>but \\(f_n\\overset{L^p}{\\rightarrow}0\\), since</p> \\[ \\int_0^1 |f_n-f|^pdx=\\int_{\\left[\\frac{i}{2^k},\\frac{i+1}{2^k}\\right]}1^pdx=\\frac{1}{2^k}\\rightarrow 0(n\\rightarrow \\infty). \\] <p>Example. \\(f_k(x)=k\\chi_{(0,\\frac{1}{k})}\\), \\(E=(0,\\infty)\\), then </p> \\[ f_k(x)\\rightarrow 0,\\quad \\forall x\\in E. \\] <p>but \\(f_k\\overset{L^p}{\\nrightarrow} f\\), since</p> \\[ \\int_E |f_n-f|^pdx=\\int_{(0,\\frac{1}{k})}|k|^pdx=|k|^{p-1}\\rightarrow \\infty. \\] <p>Similar to Cauchy sequence in \\(\\mathbb{R}\\), we have the following Cauchy sequence in a sense of \\(L^p(E)\\).</p> <p>Definition of Cauchy Sequence in \\(L^p\\) Space</p> <p>Assume \\(f_k\\in L^p(E)\\), if</p> \\[ \\lim_{n,m\\rightarrow \\infty}\\rho(f_m,f_n)=\\lim_{n,m\\rightarrow \\infty}\\|f_m-f_n\\|_p=0, \\] <p>then we call \\(\\{f_k\\}\\) is Cauchy Sequence in \\(L^p(E)\\).</p> <p></p> <p>Property of Cauchy Sequence</p> <p>Assume \\(f_n\\) is a Cauchy Sequence in \\(L^p(E)\\). If there exists a subsequence \\(\\{f_{n_j}\\}_{j\\geq 1}\\) which is convergent in a sense of \\(L^p\\), then \\(f_n\\) itself also converges in a sense of \\(L^p\\).</p> Proof \\[ \\|f_n-f\\|_p\\leq \\|f_n-f_{n_j}\\|_p+\\|f_{n_j}-f\\|_p \\]"},{"location":"Math/Real_Analysis/Lp_Space/#completeness","title":"Completeness","text":"<p>Similar to proof in \\(\\mathbb{R}\\), we could prove that a convergent sequence must be Cauchy Sequence. Readers could use Minkowski inequation</p> \\[ \\|f_{n+l}-f_{n}\\|_p\\leq \\|f_{n+1}-f\\|_p+\\|f_n-f\\|_p\\rightarrow 0 (n\\rightarrow \\infty) \\] <p>To show its completeness, we have the following lemma. </p> <p>Lemma: find convergent function</p> <p>Assume \\(\\{f_k\\}\\subset L^p(E)\\), \\(1\\leq p&lt;\\infty\\). If </p> \\[ \\begin{equation} \\sum_{k=1}^\\infty\\|f_k\\|_p&lt;\\infty,\\label{lemma-condition} \\end{equation} \\] <p>then there exists \\(f\\in L^p(E)\\), such that</p> \\[ \\sum_{k=1}^\\infty f_k\\overset{L^p}{=}f. \\] Proof <p>Let \\(g_k(x)=\\sum\\limits_{j=1}^k |f_j|\\), and \\(g=\\lim\\limits_{k\\rightarrow \\infty} g_k(x)=\\sum\\limits_{k=1}^\\infty |f_k|\\). so </p> \\[ g_k^p(x)=\\left(\\sum\\limits_{j=1}^k |f_j|\\right)^p\\rightarrow g^p=\\left(\\sum\\limits_{k=1}^\\infty |f_k|\\right)^p. \\] <p>Since \\(g^p_k\\) are non-negative and monotonically increasing, by Levi Theorem, we have</p> \\[ \\begin{align*} \\int_E g^pdx&amp;=\\lim_{k\\rightarrow \\infty}\\int_E g_k^pdx\\\\ &amp;=\\lim_{k\\rightarrow \\infty}\\|g_k\\|^p_p\\\\ &amp;\\leq \\lim_{k\\rightarrow \\infty} \\left(\\sum_{j=1}^k \\|f_j\\|_p\\right)^p\\quad \\text{Minkowski}\\\\ &amp;=\\left(\\sum_{j=1}^\\infty \\|f_j\\|_p\\right)^p \\end{align*} \\] <p>by condition \\(\\ref{lemma-condition}\\) of this lemma, we have \\(g(x)&lt;\\infty\\), i.e. \\(\\sum\\limits_{k=1}^\\infty f_k\\) converges absolutely \\(a.e.x\\in E\\). So there exists measurable function \\(f\\), such that </p> \\[ f=\\sum\\limits_{k=1}^\\infty f_k, \\quad a.e.x\\in E. \\] <p>Notice this limit function is just measurable, we have to prove that \\(f\\in L^p(E)\\). Notice that</p> \\[ \\left|\\sum\\limits_{j=1}^k f_j\\right|^p\\leq \\left(\\sum\\limits_{j=1}^k |f_j|\\right)^p\\leq \\left(\\sum\\limits_{j=1}^\\infty |f_j|\\right)^p=g^p \\] <p>where the last item \\(g^p\\) is L integral, so \\(F_k^p=\\left(\\sum\\limits_{j=1}^k f_j\\right)^p\\) is controlled by \\(g^p\\). Since \\(|f|^p=\\lim\\limits_{k\\rightarrow \\infty}|F_k|^p\\), by LDCT, we have</p> \\[ \\begin{align*} \\int_E |f|^pdx=\\lim_{k\\rightarrow \\infty}\\int_E \\left|\\sum\\limits_{j=1}^k f_j\\right|^pdx\\leq \\int_E g^pdx&lt;\\infty. \\end{align*} \\] <p>which means \\(f\\in L^p(E)\\). Finally, we have to show that \\(F_k\\) converges to \\(f\\) in a sense of \\(L^p(E)\\). That is, Notice that</p> \\[ |f-F_k|^p=\\left|\\sum_{j=k+1}^\\infty f_j\\right|\\leq \\left(\\sum_{j=k+1}^\\infty|f_j|\\right)^p\\leq g^p \\in L^1(E), \\] <p>by LDCT, we have</p> \\[ \\lim_{k\\rightarrow \\infty}\\int_E |f-F_k|^pdx=\\int_E \\lim_{k\\rightarrow \\infty} |f-F_k|^pdx=0. \\] <p>Now we have to prove Cauchy Sequence must be a convergent sequence, and its limit must be in \\(L^p(E)\\).</p> <p>Riesz-Ficher: Completeness</p> <p>\\(L^p(E)\\) is a complete metric space. That is, Cauchy sequence in \\(L^p(E)\\) must converges.</p> Proof <ul> <li>\\(1\\leq p&lt;\\infty\\).</li> </ul> <p>By definition of Cauchy sequence, \\(\\forall m\\geq 1\\), \\(\\exists j_m\\), such that</p> \\[ \\|f_{j_m}-f_k\\|_p&lt;\\frac{1}{2^m},\\quad \\forall k&gt;j_m. \\] <p>Choose \\(k=j_m+1\\), so we get a subsequence \\(\\{f_{j_m}\\}\\) such that</p> \\[ \\|f_{j_m}-f_{j_m+1}\\|_p&lt;\\frac{1}{2^m} \\] <p>Now let \\(g_1=f_{j_1}\\), \\(g_k=f_{j_k}-f_{j_{k-1}}, k\\geq 2\\). Then we have</p> \\[ \\begin{align*} \\sum_{k=1}^\\infty\\|g_k\\|_p&amp;=\\|f_{j_1}\\|_p+\\sum_{k=2}^\\infty \\|f_{j_k}-f_{j_{k-1}}\\|_p\\\\ &amp;&lt;\\|f_{j_1}\\|_p+\\sum_{k=2}^\\infty\\frac{1}{2^k}&lt;\\infty \\end{align*} \\] <p>which satisfies the condition \\(\\ref{lemma-condition}\\) of lemma. So there exsits \\(f\\in L^p(E)\\) such that</p> \\[ G_k:=\\sum_{j=1}^k g_j=f_{j_k}\\rightarrow f(L^p). \\] <p>Then by Property of Cauchy Sequence, convergence of subseuquence could deduce original sequence \\(f_k\\) converges to \\(f\\) in a sense of \\(L^p(E)\\).</p> <ul> <li>\\(p=\\infty\\).</li> </ul> <p>Assume \\(\\{f_k\\}\\in L^\\infty(E)\\) is a Cauchy sequence, then by its definition, \\(\\forall m\\in \\mathbb{N}\\), \\(\\exists k_m\\), such that</p> \\[ \\|f_j-f_k\\|_\\infty&lt;\\frac{1}{2^m},\\quad \\forall j,k&gt;k_m. \\] <p>By definition of \\(L^\\infty\\), there exsits a zero-measure set \\(Z_{j,k,m}\\), such that</p> \\[ |f_j(x)-f_k(x)|\\leq \\|f_j-f_k\\|_\\infty&lt;\\frac{1}{2^m},\\quad \\forall x\\in E-Z_{j,k,m},j,k&gt;k_m. \\] <p>Let \\(Z=\\bigcup_{j,k,m\\in \\mathbb{N}}Z_{j,k,m}\\) which is also zero-measure, and</p> \\[ \\begin{align} |f_j(x)-f_k(x)|&lt;\\frac{1}{2^m},\\quad x\\in E-Z, j,k&gt;k_m.\\label{completeness} \\end{align} \\] <p>The above inequation implies \\(\\{f_n\\}\\) is Cauchy number sequence (\\(\\mathbb{R}\\)), so </p> \\[ \\lim_{n\\rightarrow \\infty}f_n(x)=f(x),\\quad x\\in E-Z. \\] <p>let \\(j\\rightarrow \\infty\\) in inequation \\(\\ref{completeness}\\), we have</p> \\[ |f(x)-f_k(x)|\\leq\\frac{1}{2^m},\\quad x\\in E-Z, k&gt;k_m. \\] <p>which still by definition of \\(L^\\infty\\), we have</p> \\[ \\|f(x)-f_k(x)\\|_\\infty \\leq\\frac{1}{2^m},\\quad x\\in E-Z, k&gt;k_m. \\] <p>which means \\(f\\in L^\\infty(E)\\) and \\(f_k\\overset{L^\\infty}{\\rightarrow} f\\).</p> <p>Lebesgue's dominated control theorem in \\(L^p\\) Space</p> <p>Assume \\(f_k\\), \\(f\\) are measurable function on \\(E\\). If \\(f_k\\rightarrow f,a.e.x\\in E\\), and there exists a function \\(F(x)\\in L^p(E)\\) such that</p> \\[ |f_k(x)|\\leq F(x),\\quad a.e.x\\in E, \\] <p>then \\(f_k\\overset{L^p}{\\rightarrow}f\\).</p> Proof <p>Let </p> \\[ g_n(x)=|f_k(x)-f(x)|^p\\rightarrow 0(k\\rightarrow \\infty). \\] <p>Notice that </p> \\[ |g_k(x)|\\leq (|f_k|+|f|)^p\\leq 2^p|f|^p\\in L^1(E),  \\] <p>So by LDCT in \\(L^1(E)\\), we have</p> \\[ \\lim_{k\\rightarrow \\infty}\\int_E |f_k-f|^p=\\int_E 0dx=0. \\]"},{"location":"Math/Real_Analysis/Lp_Space/#separability","title":"Separability","text":"<p>Definitions of Separations</p> <p>(i) Assume \\(\\varGamma\\subset L^p(E)\\). If \\(\\forall f\\in L^p(E)\\), \\(\\forall\\varepsilon&gt;0\\), \\(\\exists g\\in \\varGamma\\), such that</p> \\[ \\|f-g\\|_p&lt;\\varepsilon, \\] <p>then we call \\(\\varGamma\\) is a dense subset of \\(L^p(E)\\). Apparently, its equivalent definition is \\(\\forall f\\in L^p(E)\\), \\(\\exists \\{g_n\\}\\subset \\varGamma\\), such that \\(g_n\\rightarrow f(L^p)\\), i.e.</p> \\[ \\lim_{n\\rightarrow \\infty}\\|f-g_n\\|_p=0. \\] <p>(ii) If \\(L^p(E)\\) has dense subsets whose elements are denumerable, then we call \\(L^p(E)\\) is separable.</p> <p>Separability of \\(L^p(E)\\)</p> <p>If \\(1\\leq g&lt;\\infty\\), then \\(L^p(E)\\) is separable.</p> Proof"},{"location":"Math/Real_Analysis/Measurable_Func/","title":"Measurable Function","text":"<p>Functions discussed in this chapter are defined on measurable sets, whose value could be generalized real number, i.e. \\(\\mathbb{R}\\) including \\(\\pm \\infty\\).</p> <p>Definition of Measurable Function</p> <p>Assume function \\(f\\) has defined on measurable set \\(D\\). If \\(\\forall \\alpha\\in \\mathbb{R}\\), set</p> \\[ \\{f&gt;\\alpha\\}=D(f&gt;\\alpha):=\\{x\\in D: f(x)&gt; \\alpha\\} \\] <p>is also measurable, then we call \\(f\\) is a measurable function on \\(D\\).</p> <p>Example. Continuous function on an interval \\(D\\subset \\mathbb{R}\\) is a measurable function.</p> Proof <ul> <li> <p>For an open interval \\(D\\), this is easy.</p> </li> <li> <p>For an arbitrary interval \\(D\\), the original image of \\({f&gt;\\alpha}\\) would differ from open sets with at most \\(2\\) points.</p> </li> </ul> <p>Characteristic Function</p> <p>Assume \\(A\\subset X\\), then define a characteristic function \\(A\\) as</p> \\[ \\chi_A(x)=\\begin{cases}1,\\quad x\\in A\\\\  0, \\quad x\\in X\\backslash A \\end{cases} \\] <p>Measurability of Characteristic Function</p> <p>For measurable set \\(D\\), denote \\(\\chi_D\\) as its characteristic function, then </p> \\[ \\{\\chi_D&gt;\\alpha\\}=\\begin{cases} \\varnothing,\\quad \\alpha\\geq 1\\\\ D,\\quad 0\\leq\\alpha&lt;1 \\\\ \\mathbb{R},\\quad \\alpha&lt;0 \\end{cases} \\] <p>So we could prove \\(\\chi_D\\) is measurable function.</p>"},{"location":"Math/Real_Analysis/Measurable_Func/#properties","title":"Properties","text":"<p>Properties of Measurable Function</p> <p>Assume \\(f(x)\\) is measurable on \\(E\\), then the following sets are all measureable.</p> <p>(i) \\(\\{f\\leq \\alpha\\}\\) (ii) \\(\\{f\\geq \\alpha\\}\\)</p> <p>(iii) \\(\\{f&lt;\\alpha\\}\\) (iv) \\(\\{f=\\alpha\\}\\)</p> <p>(v) \\(\\{f&lt;+\\infty\\}\\) (vi) \\(\\{f=+\\infty\\}\\)</p> <p>(vii) \\(\\{f&gt;-\\infty\\}\\) (viii) \\(\\{f=-\\infty\\}\\)</p> Proof <ul> <li> <p>(i) \\(=E-\\{f&gt;\\alpha\\}\\)</p> </li> <li> <p>(ii) \\(=\\bigcap_{k=1}^\\infty\\{f&gt;\\alpha-\\frac{1}{k}\\}\\) (length decrease, so we choose inferior)</p> </li> <li> <p>(iii) using (ii) with same logic of proving (i)</p> </li> <li> <p>(iv) using (i) amd (ii)</p> </li> <li> <p>(v) \\(=\\bigcup_{k=1}^\\infty\\{f&lt;k\\}\\) (length increase, so we choose superior)</p> </li> <li> <p>(vi) use (v) with same logic of proving (i)</p> </li> <li> <p>(vii) same logic of proving (i)</p> </li> <li> <p>(viii) use (vii) with same logic of proving (i).</p> </li> </ul> <p>The above (i), (ii) and (iii) are equivalent definition of measurable function, but (iv) is not. </p> <p> Example. \\(f\\) is measurable on measurable set \\(D\\), iff \\(\\forall r\\in \\mathbb{Q}\\), \\(\\{f&gt;r\\}\\) is measurable. </p> Proof <ul> <li> <p>\"\\(\\Rightarrow\\)\". Let \\(\\forall \\alpha\\), \\(\\alpha=r\\).</p> </li> <li> <p>\"\\(\\Leftarrow\\)\". Notice that \\(\\forall \\alpha, \\exists \\{r_n\\}\\), such that \\(r_n\\in \\mathbb{R}\\), and \\(r_1&gt;r_2&gt;\\cdots\\), and \\(r_n\\rightarrow \\alpha\\), so </p> </li> </ul> \\[ \\{f&gt;\\alpha\\}=\\bigcup_{n=1}^\\infty\\{f&gt;r_n\\} \\] <p>which means \\(f\\) is measurable.</p> <p>When \\(\\{f=r\\}\\) is measurable, we could not deduce the same result. Bacause let \\(A\\) to be a non-measurable set, define </p> \\[ f=\\begin{cases} \\sqrt{3},\\quad x\\in A\\\\ \\sqrt{2},\\quad x\\notin A \\end{cases} \\] <p>then \\(\\forall r\\in \\mathbb{Q}\\), \\(\\{f=r\\}=\\varnothing\\) is measurable, but let \\(\\alpha=1.5\\),</p> \\[ \\{f&gt;1.5\\}=A \\] <p>is not measurable, so \\(f\\) is not measurable.</p> <p>Region Extension</p> <p>(i) Assume generalized real function \\(f(x)\\) is defined on \\(D_1\\cup D_2\\). If \\(f(x)\\) is measurable on \\(D_1\\) and \\(D_2\\), then \\(f(x)\\) is also measurable on \\(D_1\\cup D_2\\).</p> <p>(ii) Assume \\(f(x)\\) is measurable on \\(D\\). If \\(A\\subset D\\) is also measurable, then \\(f(x)\\) is also measurable on \\(A\\).</p> Proof <p>(i) \\(\\{x\\in D_1\\cup D_2:f&gt;\\alpha\\}=\\{x\\in D_1: f&gt;\\alpha\\}\\cup \\{x\\in D_2: f&gt;\\alpha\\}\\) and by measure operation.</p> <p>(ii) \\(\\{x\\in A: f&gt;\\alpha\\}=\\{x\\in D: f&gt;\\alpha\\}\\cap A\\) and by measure operation.</p> <p></p> <p>Lemma of functions</p> <p>Assume \\(f(x)\\), \\(g(x)\\) are measurable on \\(E\\), then \\(\\{f&gt;g\\}\\) is also measurable.</p> Proof <p>\\(\\forall \\varepsilon_r=\\frac{1}{r},r\\in \\mathbb{N}^+\\), we have</p> \\[ \\{f&gt;g\\} = \\bigcup_{r=1}^\\infty\\{f&gt; \\varepsilon_r\\}\\cap\\{\\varepsilon_r &gt;g\\} \\] <p>so \\(\\{f&gt;g\\}\\) is measurable.</p>"},{"location":"Math/Real_Analysis/Measurable_Func/#basic-operations","title":"Basic Operations","text":"<p>Basic Operations of Measurable Function</p> <p>Assume \\(f(x)\\), \\(g(x)\\) are measurable function on \\(D\\), then the following functions are measurable.</p> <p>(i) \\(cf(x) (c\\in \\mathbb{R})\\). (ii) \\(f(x)+g(x)\\). (iii) \\(f(x)\\cdot g(x)\\).</p> Proof <p>(i) move \\(c\\) to the side of \\(\\alpha\\).</p> <p>(ii) same logic with Lemma.</p> <p>(iii) use \\(4f(x)\\cdot g(x)=[f(x)+g(x)]^2-[f(x)-g(x)]^2\\). You only need to consider whether \\(f^2(x)\\) are measurable, which is easy to see. Notice that \\(\\forall\\alpha\\geq 0\\),</p> \\[ \\{f^2&gt;\\alpha\\}=\\{f&gt;\\sqrt{\\alpha}\\}\\cup \\{f&lt;-\\sqrt{\\alpha}\\} \\] <p>is measurable, and \\(\\forall \\alpha&lt;0\\),</p> \\[ \\{f^2&gt;\\alpha\\}=D \\] <p>is measurable.</p> <p>Example. Assume \\(f\\) is defined on measurable set \\(D\\), prove: if \\(f^2\\) is measurable on \\(D\\) and \\(\\{f&gt;0\\}\\) is measurable, then \\(f\\) is measurable on \\(D\\).</p> Proof <ul> <li>\\(\\forall \\alpha\\geq 0\\),</li> </ul> \\[ \\{f&gt;\\alpha\\}=\\{f^2&gt;\\alpha^2\\}\\cap \\{f&gt;0\\} \\] <p>is measurable.</p> <ul> <li>\\(\\forall \\alpha&lt;0\\),</li> </ul> \\[ \\{f&gt;\\alpha\\}=\\{f&gt;0\\}\\cup \\{\\alpha&lt;f\\leq 0\\}=\\{f&gt;0\\}\\cup \\left[\\{f^2&lt;\\alpha^2\\}\\cap \\{f\\leq 0\\}\\right] \\] <p>is measurable.</p> <p>Example. Assume \\(f\\) is a measurable function on \\(D\\), prove: for all open set \\(G\\subset \\mathbb{R}\\) and closed set \\(F\\subset \\mathbb{R}\\), </p> \\[ f^{-1}(G),\\quad f^{-1}(F) \\] <p>are measurable.</p> Proof <p>Known from configuration of set, \\(\\forall\\) open set \\(G\\subset \\mathbb{R}\\), there exists \\(\\{a_n\\}\\) and \\(\\{b_n\\}\\), such that \\(a_n&lt;b_n\\), and \\(G=\\bigcup\\limits_{n=1}^\\infty(a_n,b_n)\\), so</p> \\[ f^{-1}(G)=\\bigcup_{n=1}^\\infty\\{a_n&lt;f&lt;b_n\\} \\] <p>is measurable. Notice that \\(f^{-1}(F)=f^{-1}(\\mathbb{R}-F^c)=D-f^{-1}(F^c)\\) is also measurable.</p>"},{"location":"Math/Real_Analysis/Measurable_Func/#limit-superior-limit-inferior","title":"Limit superior &amp; limit inferior","text":"<p>Limit superior &amp; limit inferior</p> <p>Sequence of Function.</p> <p>Similarly, given a sequence of function \\(\\{f_n(x)\\}_{n\\geq 1}\\) we have limit superior</p> \\[ \\overline{\\lim}\\limits_{n\\rightarrow \\infty}f_n=\\inf_{n\\geq 1} \\{\\sup_{k\\geq n}\\{f_k(x)\\}\\} \\] <p>and limit inferior</p> \\[ \\underline{\\lim}\\limits_{n\\rightarrow \\infty}f_n=\\sup_{n\\geq 1} \\{\\inf_{k\\geq n}\\{f_k(x)\\}\\} \\] <p></p> <p>Theorem for measurability of Limit</p> <p>Assume \\(\\{f_n(x)\\}_{n\\geq 1}\\) is a sequence of measurable functions on measurable set \\(D\\), then the following functions are all measurable.</p> \\[ \\begin{align} \\sup_{n\\geq 1}f_n(x),\\quad \\inf_{n\\geq 1}f_n(x)\\label{supinf-function}\\\\ \\overline{\\lim}\\limits_{n\\rightarrow \\infty}f_n(x),\\quad \\underline{\\lim}\\limits_{n\\rightarrow \\infty}f_n(x) \\label{limit-supinf-function} \\end{align} \\] Proof <ul> <li>For expression \\(\\ref{supinf-function}\\), we can write it as</li> </ul> \\[ \\left\\{\\sup_{n\\geq 1}f_n(x)&gt;\\alpha\\right\\}=\\bigcup_n^\\infty\\{f_n(x)&gt;\\alpha\\} \\] <p>where the latter one is measurable.</p> <ul> <li>For expression \\(\\ref{limit-supinf-function}\\), we could use conclusion from \\(\\ref{supinf-function}\\).</li> </ul> <p>Example. Assume \\(f_n\\) is a measurable function on \\(D\\). Prove: Set </p> \\[ \\{x\\in D: f_n \\text{ converges}.\\} \\] <p>is measurable.</p> Proof <p>\\(f_n\\) converges, iff its limit superior and inferior are of the same. So we define </p> \\[ F(x)=\\overline{\\lim}\\limits_{n\\rightarrow \\infty}f_n,\\quad f(x)=\\underline{\\lim}\\limits_{n\\rightarrow \\infty}f_n, \\] <p>both of which are measurable by Theorem for limit. Then consider </p> \\[ \\{F=f\\}=D-\\{F&gt;f\\} \\] <p>since the latter is measurable according to Lemma of functions, \\(\\{F=f\\}\\) is measurable.</p>"},{"location":"Math/Real_Analysis/Measurable_Func/#almost-everywhere","title":"Almost Everywhere","text":"<p>Definition of Almost Everywhere of a Proposition</p> <p>Assume \\(D\\) is measurable, and proposition \\(P(x)\\) is related with points in \\(D\\). If there exists a zero-measure set \\(E\\subset D\\), such that \\(P(x)\\) holds on \\(D\\backslash E\\), then we call \\(P(x)\\) holds almost everywhere on \\(D\\). </p> <p>We denote this statement as </p> \\[ P(x) \\ a.e.x \\in D. \\]"},{"location":"Math/Real_Analysis/Measurable_Func/#approximation","title":"Approximation","text":""},{"location":"Math/Real_Analysis/Measurable_Func/#approximation-with-simple-function","title":"Approximation with Simple Function","text":"<p>Simple Function</p> <p>Assume \\(f\\) is a function on measurable set \\(D\\). If \\(f(D)\\) is composed of finite points \\(\\{a_k\\}_{k=1}^n\\), and sets \\(E_k=\\{f=a_k\\} (k=1,2,\\cdots, n)\\) are all measurable, then we call \\(f\\) is a simple function on \\(D\\). </p> <p>Apparently, here \\(f(x)\\) could be expressed by finite linear combination of characteristic functions, i.e.</p> \\[ f(x)=\\sum_{k=1}^n a_k \\chi_{E_k}(x) \\] <p></p> <p>Approximate measurable function with sequence of simple functions</p> <p>If \\(f(x)\\) is measurable on \\(D\\), then there exists a sequence of simple functions \\(\\{f_k\\}_{k=1}^\\infty\\), such that for every point \\(x\\in D\\), \\(\\{f_k\\}_{k\\geq 1}\\) converges to \\(f(x)\\). Specially, if</p> <p>(i) \\(f\\) is non-negative, then \\(\\{f_k\\}_{k\\geq 1}\\) is monotonically increasing.</p> <p>(ii) \\(f\\) is bounded, then \\(\\{f_k\\}_{k\\geq 1}\\) converges uniformly to \\(f(x)\\).</p> HintsProof <p>Use a special construction.</p> <p>Formulate a sequence of function which converges to \\(f(x)\\). This is a little tricky.</p> <p>Define</p> \\[ f_n(x)=\\begin{cases} n,\\quad f(x)&gt;n\\\\ \\frac{k-1}{2^n},\\quad \\frac{k-1}{2^n}\\leq f(x)&lt;\\frac{k}{2^n}, k=-n2^n+1,-n2^n\\cdots,n2^n\\\\ -n,\\quad f(x)&lt;n \\end{cases} \\] <p>Look the following image for intuitional impression.</p> <p><p> </p></p> <p>Now we show that \\(f_n(x)\\) corvergese to \\(f(x)\\). For fixed point \\(x\\in D\\), if</p> <p>(i) \\(f(x)=\\infty\\). Then \\(f_n(x)=n\\rightarrow \\infty=f(x)\\). The same for \\(f(x)=-\\infty\\).</p> <p>(ii) \\(-\\infty&lt;f(x)&lt;+\\infty\\). Then for sufficient large number \\(n\\), there exists unique \\(k_n\\) such that \\(-n 2^n+1\\leq k_n \\leq n2^n\\) and </p> \\[ \\frac{k_n-1}{2^n}\\leq f(x)\\leq \\frac{k_n}{2^n} \\] <p>which means \\(f_n(x)=\\frac{k_n-1}{2^n}\\), satisfying</p> \\[ 0\\leq f(x)-f_n(x)\\leq \\frac{1}{2^n}. \\] <p>Let \\(n\\rightarrow \\infty\\), we have \\(f_n(x)\\) converges to \\(f(x)\\).</p> <ul> <li>Specially, if \\(f(x)\\) is non-negative, for \\(f(x)=+\\infty\\), then \\(f_n(x)=n\\) is monotonically increasing. </li> </ul> <p>For \\(f(x)&lt;\\infty\\), if \\(f(x)&lt;n\\), we have \\(f(x)=\\frac{k-1}{2^n}\\) for some \\(k\\) such that \\(\\frac{k-1}{2^n}&lt;f(x)&lt;\\frac{k}{2^n}\\), then for \\(f_{n+1}(x)\\), we partition this interval into two parts, which are </p> \\[ \\left[\\frac{2k-2}{2^{n+1}},\\frac{2k-1}{2^{n+1}}\\right), \\left[\\frac{2k-1}{2^{n+1}}, \\frac{2k}{2^{n+1}}\\right) \\] <p>the former part \\(f_{n+1}(x)=f_n(x)=\\frac{k-1}{2^n}\\), while the latter part gives \\(f_{n+1}(x)=\\frac{2k-1}{2^{n+1}}&gt;f_n(x)\\), so it is monotonically increasing.</p> <p>for \\(\\infty&gt;f(x)\\geq n\\), this is similar to \\(\\infty\\).</p> <ul> <li>Specailly, if \\(f(x)\\) is bounded, denote its upper bound as \\(M\\), so when \\(n&gt;M\\), we have \\(|f(x)-f_n(x)|&lt;\\frac{1}{2^n}\\) on all \\(x\\in D\\), which means \\(f_n(x)\\rightrightarrows f(x)\\).</li> </ul> <p>Example. Assume \\(f\\) is differentiable on \\(\\mathbb{R}\\), prove \\(f'(x)\\) is measurable.</p> Proof <p>\\(f\\) is differentiable, so \\(f\\) and \\(f(x+\\frac{1}{n})\\) is continuous, thus measurable. So </p> \\[ f'(x)=\\lim_{n\\rightarrow \\infty} \\frac{f(x+\\frac{1}{n})-f(x)}{\\frac{1}{n}}\\overset{\\Delta}{=}f_n(x) \\] <p>\\(f_n\\) is measurable, so \\(f'\\) is also measurable.</p>"},{"location":"Math/Real_Analysis/Measurable_Func/#uniform-convergnce-ae","title":"Uniform Convergnce a.e.","text":"<p>Lemma: use language of Set to describe convergence</p> <p>Assume \\(\\{f_k(x)\\}_{k\\geq 1}\\), \\(f(x)\\) are measurable function on \\(a.e.x \\in E\\), and \\(m(E)&lt;\\infty\\). If \\(f_k(x)\\rightarrow f(x), a.e.x \\in E\\), then \\(\\forall \\varepsilon&gt;0\\), let \\(E_k(\\varepsilon)=\\{x\\in E: |f_k(x)-f(x)|\\geq \\varepsilon\\}\\), then we have</p> \\[ \\lim_{j\\rightarrow \\infty} m\\left(\\bigcup_{k=j}^\\infty E_k(\\varepsilon)\\right)=0. \\] Proof <p>We have the following equivalent statement for point-wise convergence.</p> \\[ \\begin{align} \\nonumber &amp;f_k(x)\\rightarrow f(x), a.e.x \\in E \\\\ \\nonumber \\Leftrightarrow &amp; \\forall \\varepsilon&gt;0, \\exists N&gt;0, \\forall k\\geq N, |f_k(x)-f(x)|&lt;\\varepsilon\\\\ \\nonumber \\Leftrightarrow &amp; \\forall \\varepsilon&gt;0, \\exists N&gt;0, x\\in \\bigcap_{k=N}^\\infty \\{x\\in E: |f_k(x)-f(x)|&lt;\\varepsilon \\}\\\\ \\Leftrightarrow &amp; \\forall \\varepsilon&gt;0, x\\in \\bigcup_{N=1}^\\infty \\bigcap_{k=N}^\\infty \\{x\\in E: |f_k(x)-f(x)|&lt;\\varepsilon \\} \\label{equiv-pointwise-convergence}\\\\ \\end{align} \\] <p>Note that we could have an equivalent statement at the last</p> \\[ x\\in \\bigcap_{\\varepsilon&gt;0}\\bigcup_{N=1}^\\infty \\bigcap_{k=N}^\\infty \\{x\\in E: |f_k(x)-f(x)|&lt;\\varepsilon \\} \\] <p>but \\(\\bigcap_{\\varepsilon&gt;0}\\) is not denumerable, so the in the following proof, we have to choose a denumerable set like \\(\\mathbb{Q}\\) or \\(\\frac{1}{r}\\).</p> <p>Now we have a equivalent statement of \\(f_k(x)\\rightarrow f(x), a.e.x\\in E\\), that is, expression \\(\\ref{equiv-pointwise-convergence}\\). Then its opponent proposition \\(f_k(x)\\nrightarrow f(x)\\) could be expressed by</p> \\[ \\exists \\varepsilon&gt;0, x\\in \\bigcap_{N=1}^\\infty \\bigcup_{k=N}^\\infty \\{x\\in E: |f_k(x)-f(x)|\\geq \\varepsilon \\} \\] <p>That is, </p> \\[ \\begin{align} \\{x\\in E: f_k(x)\\nrightarrow f(x)\\}=\\bigcup_{\\varepsilon&gt;0}\\bigcap_{N=1}^\\infty \\bigcup_{k=N}^\\infty E_k(\\varepsilon) \\label{non-convergence} \\end{align} \\] <p>which has zero-measure according to \\(a.e.x\\in E\\), so by limit operations in measure, we have</p> \\[ \\lim_{N\\rightarrow \\infty}m\\left(\\bigcup_{k=N}^\\infty E_k(\\varepsilon)\\right)=m\\left(\\lim_{N\\rightarrow \\infty} \\bigcup_{k=N}^\\infty E_k(\\varepsilon)\\right)=m\\left( \\bigcap_{N=1}^\\infty \\bigcup_{k=N}^\\infty E_k(\\varepsilon)\\right)=0. \\] <p>The first equation holds because \\(E_k(\\varepsilon)\\) is measurable (\\(f_k, f\\) are all measurable functions) and \\(\\bigcup_{k=N}^\\infty E_k(\\varepsilon)\\) is monotonically decreasing when \\(N\\rightarrow \\infty\\). The last equation holds because the left side of equation \\(\\ref{non-convergence}\\) has zero-measure (which means the item to be unioned should have zero-measure).</p> <p></p> <p>Egoroff Theorem</p> <p>Assume \\(\\{f_k(x)\\}_{k\\geq 1}\\) is a measurable function which is finite on \\(a.e.x\\in E\\), and \\(m(E)&lt;\\infty\\). If \\(f_k(x)\\rightarrow f(x), a.e.x\\in E\\), then \\(\\forall \\delta&gt;0\\), \\(\\exists \\text{ measurable set } E_\\delta\\subset E\\), such that \\(m(E_\\delta)\\leq \\delta\\) and </p> \\[ f_k(x)\\rightrightarrows f(x), \\quad \\forall x\\in E-E_\\delta. \\] Proof <p>Choose \\(\\varepsilon=\\frac{1}{r}\\) and define </p> \\[ E_k\\left(\\frac{1}{r}\\right)=\\{x\\in E: |f_k(x)-f(x)|\\geq \\frac{1}{r}\\}, \\] <p>then by condition of the theorem and Lemma, \\(\\forall \\delta&gt;0\\), \\(\\forall r\\in \\mathbb{R}\\), \\(\\exists N_r\\), \\(\\forall N \\geq N_r\\), </p> \\[ m\\left[\\bigcup_{k=N}^\\infty E_k\\left(\\frac{1}{r}\\right)\\right]&lt;\\frac{\\delta}{2^r}. \\] <p>Then we have to sum up there measure. That is, let \\(E_\\delta=\\bigcup_{r=1}^\\infty \\bigcup_{k=N}^\\infty E_k\\left(\\frac{1}{r}\\right)\\), which satisfies</p> \\[ m(E_\\delta)\\leq \\sum_{r=1}^\\infty m\\left[\\bigcup_{k=N}^\\infty E_k\\left(\\frac{1}{r}\\right)\\right]&lt;\\delta. \\] <p>Check that </p> \\[ E\\backslash E_\\delta=\\bigcap_{r=1}^\\infty \\bigcap_{k=N}^\\infty E_k^c\\left(\\frac{1}{r}\\right)=\\bigcap_{r=1}^\\infty \\bigcap_{k=N}^\\infty \\{x\\in E: |f_k(x)-f(x)|&lt; \\frac{1}{r}\\} \\] <p>Notice that \\(f_k\\rightrightarrows f, a.e.x\\in A\\), iff \\(\\sup_{x\\in A}|f_k(x)-f(x)|\\rightarrow 0 (k\\rightarrow \\infty)\\). If we let \\(A=E\\backslash E_\\delta\\), this holds apparently.</p> <p>Note that the condition \\(m(E)&lt;\\infty\\) in the above theorem could not be removed.</p> <p>Example. \\(f_n(x)=x^n\\) converges to \\(f(x)=\\begin{cases}0,\\quad 0&lt;x&lt;1 \\\\ 1,\\quad x=1 \\end{cases}\\) point-wisely, but not uniformly. If we remove a sufficiently small set \\((1-\\delta,1]\\), then \\(f_n(x)\\rightrightarrows f(x)\\) uniformly. </p>"},{"location":"Math/Real_Analysis/Measurable_Func/#extension-of-measurable-function","title":"Extension of Measurable Function","text":"<p>The following theorem is the main result of this part. </p> <p>Lusin Theorem</p> <p>Assume \\(f(x)\\) is a measurable function \\(a.e.\\) on \\(E\\), then \\(\\forall \\delta&gt;0\\), \\(\\exists\\) closed set \\(F\\subset E\\), s.t. \\(m(E\\backslash F)&lt;\\delta\\), and \\(f(x)\\) is a continuous function on \\(F\\).</p> HintsProof <p>Using Theorem in Mathematical Analysis for uniform convergent function, that is, the convergent function of uniformly convergent function sequence is continuous(Here notice we have to cut off a closed set from \\(E\\), which might be infinite). So we have to make use of Egoroff Theorem.</p> <p>(i) \\(f\\) is a simple function on \\(E\\). </p> <p>From conclusion in Property of Simple Function, there exists measurable sets \\(E_i(i=1,2,\\cdots,N)\\), such that \\(E=\\bigcup_{i=1}^N E_n\\), \\(E_i\\cap E_j=\\varnothing\\), and </p> \\[ f(x)=\\sum_{i=1}^N \\alpha_i \\chi_{E_i}(x) \\] <p>From Theorem of Approximation for measurable sets, \\(\\forall \\delta&gt;0\\), because \\(E_i\\) is measurable, \\(\\exists\\) closed set \\(F_i\\subset E_i\\), such that \\(m(E_i\\backslash F_i)&lt;\\delta/N\\), and \\(\\chi_{E_i}(x)\\in C(F_i)\\), or to be more specific, is constant on \\(F_i\\). So from \\(F:=\\bigcup_{i=1}^N F_i\\subset E=\\bigcup_{i=1}^N E_i\\), we have</p> \\[ \\begin{align*} m(E\\backslash F)&amp;=m\\left[\\left(\\bigcup_{i=1}^N E_i \\right)\\backslash \\left(\\bigcup_{i=1}^N F_i\\right)\\right]\\\\ &amp;\\leq m\\left[\\bigcup_{i=1}^N \\left( E_i\\backslash F_i\\right)\\right]\\\\ &amp;\\leq \\sum_{i=1}^N m(E_i\\backslash F_i)&lt;\\delta \\end{align*} \\] <p>(ii) \\(m(E)&lt;\\infty\\).</p> <p>From conclusion in Approximation with simple function, there exists a sequence of simple function \\(\\{\\psi_k\\}_{k\\geq 1}\\) such that \\(\\psi_k(x)\\rightarrow f(x)\\). </p> <p>Then since \\(m(E)&lt;\\infty\\), we could use Egoroff Theorem for the above function sequence. That is, \\(\\forall \\delta&gt;0\\), \\(\\exists \\text{ measurable function }E_\\delta\\subset\\), such that \\(m(E\\backslash E_\\delta)&lt;\\delta/2\\) and </p> \\[ \\psi_k(x)\\rightrightarrows f(x),\\quad \\forall x\\in E\\backslash E_\\delta. \\] <p>The following method is a little tricky. \\(\\forall k\\), from (i) we could know for simple function \\(\\psi_k(x)\\), there exists closed set \\(F_k\\subset E\\backslash E_\\delta\\), such that \\(m((E\\backslash E_\\delta)\\backslash F_k)&lt;\\delta/2^{k+1}\\) and \\(\\psi_k(x)\\in C(F_k)\\). Define \\(F=\\bigcap_{k=1}^\\infty F_k\\) which is still a closed set.</p> <p>Now we have to prove that the part we remove is small enough. That is, since </p> \\[ E\\backslash F=E\\backslash \\left(\\bigcap_{k=1}^\\infty E_k\\right) \\subset [E_\\delta] \\cup \\left[(E\\backslash E_\\delta)\\backslash \\left(\\bigcap_{k=1}^\\infty E_k\\right)\\right] \\] <p>so </p> \\[ \\begin{align*} m\\left[E\\backslash F\\right]&amp;\\leq m\\left[E_\\delta\\right] + m\\left[(E\\backslash E_\\delta)\\backslash \\left(\\bigcap_{k=1}^\\infty E_k\\right)\\right]\\\\ &amp;\\leq \\frac{\\delta}{2}+m\\left[\\bigcup_{k=1}^\\infty \\left((E\\backslash E_\\delta)\\backslash F_k\\right)\\right]\\\\ &amp;\\leq \\frac{\\delta}{2} + \\sum_{k=1}^\\infty m\\left[\\left((E\\backslash E_\\delta)\\backslash F_k\\right)\\right]\\\\ &amp;\\leq \\frac{\\delta}{2} + \\frac{\\delta}{2}=\\delta \\end{align*} \\] <p>Thus, \\(\\psi_k(x)\\) uniformly converges to \\(f(x)\\) on \\(F\\), by Theorem in Mathematical Analysis for uniform convergent function, meaning \\(f(x)\\in C(F)\\).</p> <p>(iii) Last situation. \\(m(E)=\\infty\\).</p> <p>We have to transform this problem into (ii), that is, use finite-measure set to intersect.</p> <p>Define \\(E_k:=E\\cap \\{x\\in E: k-1 \\leq |x|&lt; k\\}\\). Then \\(E=\\bigcup_{k=1}^\\infty E_k\\), and \\(E_i\\cap E_j=\\varnothing\\).</p> <p>So \\(\\forall k\\), there exists closed sets \\(F_k\\subset E_k\\), such that \\(m(E_k\\backslash F_k)&lt;\\delta/2^{k+1}\\), and \\(f\\in C(F_k)\\). Consider define \\(E_\\delta=\\bigcup_{k=1}^\\infty F_k\\) (see difference with (ii)) which is just a measurable set, or to be more specific, a \\(F_\\sigma\\) set. But again by using Theorem of Approximation for measurable sets, there exists a closed set \\(F\\subset E_\\delta\\), such that \\(m(E_\\delta-F)&lt;\\delta/2\\). </p> <p>Now we check the measure of the removal part. Since </p> \\[ E\\backslash F \\subset (E\\backslash E_\\delta)\\cup (E_\\delta\\backslash F), \\] <p>we have</p> \\[ \\begin{align*} m(E\\backslash F)&amp;\\leq m\\left[E\\backslash E_\\delta\\right]+m\\left[E_\\delta \\backslash F\\right]\\\\ &amp;=m\\left[E\\backslash \\left(\\bigcup_{k=1}^\\infty F_k\\right)\\right]+m\\left[E_\\delta \\backslash F\\right]\\\\ &amp;\\leq m\\left[\\bigcup_{k=1}^\\infty (E_k\\backslash F_k)\\right] + \\frac{\\delta}{2}\\\\ &amp;\\leq \\sum_{k=1}^\\infty m(E_k\\backslash F_k)+\\frac{\\delta}{2}\\\\ &amp;=\\frac{\\delta}{2}+\\frac{\\delta}{2}=\\delta \\end{align*}  \\] <p>Thus we are done.</p> <p>Continuous Extension Theorem</p> <p>Assume \\(f(x)\\) is a measurable function finite \\(a.e.\\) on measurable set \\(E\\subset \\mathbb{R}\\), then \\(\\forall \\delta&gt;0\\), there \\(\\exists\\) closed set \\(F_\\delta\\subset E\\) and continuous function \\(g(x)\\) on whole space \\(\\mathbb{R}\\), such that \\(m(E-F_\\delta)&lt;\\delta\\) and \\(g(x)=F(x),\\forall x\\in F_\\delta\\), and</p> \\[ \\begin{equation} \\sup_{x\\in \\mathbb{R}}g(x)=\\sup_{x\\in F_\\delta}f(x),\\quad \\inf_{x\\in \\mathbb{R}}g(x)=\\inf_{x\\in F_\\delta}f(x).\\label{boundary-condition} \\end{equation} \\] <p>which naturally satisfies the condition in the theorem.</p> Proof <p>By Lusin Theorem, \\(\\forall \\delta&gt;0\\), there exists closed set \\(F\\subset E\\), such that \\(m(E-F)&lt;\\delta\\), and \\(f\\in C(F)\\). Now the only problem is, how to extend this continuous function on \\(F\\) to all space \\(E\\) such that it satisfy the so-called boundary condition \\(\\ref{boundary-condition}\\).</p> <p>By Composition of closed set, \\(F\\) could be expressed by \\(\\mathbb{R}-G\\), where \\(G\\) is a union of denumerable open intervals which are mutually disjoint. That is, </p> \\[ F^c=\\mathbb{R}-F=\\bigcup_{i=1}^\\infty(a_i,b_i). \\] <p>where at most \\(2\\) open intervals are infinite intervals. From here we formulate a function on \\(\\mathbb{R}\\)</p> \\[ g(x)=\\begin{cases} f(x),\\quad &amp;x\\in F,\\\\ f(a_i)+\\frac{f(b_i)-f(a_i)}{b_i-a_i}(x-a_i),\\quad &amp; x\\in (a_i,b_i),\\\\ f(a_i),\\quad &amp;x\\in (a_i,+\\infty),\\\\ f(b_i),\\quad &amp;x\\in (-\\infty, b_i). \\end{cases} \\]"},{"location":"Math/Real_Analysis/Measurable_Func/#convergence-in-measure","title":"Convergence in Measure","text":"<p>Example. \\(\\forall n\\in \\mathbb{N}^+\\), \\(\\exists ! k,i\\in \\mathbb{N}^+\\), such that \\(n=2^k+i\\), where \\(0\\leq u\\leq 2^k\\). Formulate a function sequence </p> \\[ f_n(x)=\\chi_{\\left[\\frac{i}{2^k},\\frac{i+1}{2^k}\\right]}(x),\\quad n=1,2,\\cdots,\\quad x\\in [0,1] \\] <p>So for every point \\(x\\in [0,1]\\), \\(f_n(x)\\) does not converge.</p> <p>For the above example, we could consider its convergence in measure, or probability.</p> <p>Still the above example. </p> <p>If we define a set</p> \\[ E_n(\\varepsilon)=\\{x\\in [0,1]: |f_n(x)-0|\\geq \\varepsilon\\}, \\] <p>then its measure </p> \\[ m(E_n(\\varepsilon))=\\frac{1}{2^k}\\rightarrow 0 \\] <p>which means for sufficient large \\(n\\), the frequency of \\(0\\) is close to \\(1\\). So we call \\(\\{f_n(x)\\}\\) converges to \\(f(x)=0\\) in measure.</p> <p>Definition of convergence in measure</p> <p>Assume function \\(f\\) and function sequence \\(\\{f_n\\}_{n\\geq 1}\\) are measurable function finite \\(a.e\\) on \\(D\\). If \\(\\forall \\delta&gt;0\\), we have</p> \\[ \\lim_{n\\rightarrow \\infty} m(\\{|f_n-f|\\geq \\delta\\})=0 \\] <p>then we call \\(\\{f_n\\}\\) converges to \\(f\\) in measure on region \\(D\\).</p> <p>Using \\(\\varepsilon-N\\) language, we have </p> \\[ \\forall \\varepsilon&gt;0, \\sigma&gt;0,\\exists N\\in \\mathbb{N}^+, \\forall n\\geq N, m(\\{|f_n-f|&gt;\\sigma\\})&lt;\\varepsilon. \\] <p>Generally speaking, \"convergence point-wisely\" has no apparent relationship with \"convergence in measure\", readers could see the following exmaple.</p> <p>Example. Define \\(f_n(x)=\\chi_{(n,\\infty)}(x)\\), \\(f(x)=0\\), then \\(\\forall x\\in \\mathbb{R}\\), we have \\(f_n(x)\\rightarrow f(x)\\). But if we let \\(\\delta=1/2\\), then \\(\\{|f_n-f|\\geq 1/2\\}=(n,\\infty)\\), whose measure \\(m(\\{|f_n-f|\\geq 1/2\\})=\\infty\\), which does not converges to \\(f(x)\\) in measure.</p>"},{"location":"Math/Real_Analysis/Measurable_Func/#uniqueness-of-convergent-function","title":"Uniqueness of Convergent Function","text":"<p>Uniqueness of Convergent Function</p> <p>If \\(f_n(x)\\) converges to \\(f(x)\\) and \\(g(n)\\) on \\(E\\) in measure, then </p> \\[ f(x)=g(x),\\quad a.e.x\\in E. \\] Proof <p>Because \\(|f(x)-g(x)|\\leq |f(x)-f_n(x)|+|f_n(x)-g(x)|\\) (at least one set of the right side should be larger then \\(\\varepsilon/2\\) if \\(|f(x)-g(x)|&gt;\\varepsilon\\)), so </p> \\[ \\{|f(x)-g(x)|&gt;\\varepsilon\\}\\subset \\left\\{|f(x)-f_n(x)|&gt;\\frac{\\varepsilon}{2}\\right\\}\\cup \\left\\{|g(x)-f_n(x)|&gt;\\frac{\\varepsilon}{2}\\right\\} \\] <p>So by subadditivity, we have</p> \\[ m(\\{|f(x)-g(x)|&gt;\\varepsilon\\})\\leq m\\left(\\left\\{|f(x)-f_n(x)|&gt;\\frac{\\varepsilon}{2}\\right\\}\\right)+m\\left(\\left\\{|g(x)-f_n(x)|&gt;\\frac{\\varepsilon}{2}\\right\\}\\right)\\rightarrow 0,\\quad n\\rightarrow \\infty \\]"},{"location":"Math/Real_Analysis/Measurable_Func/#relationship-between-two-convergences","title":"Relationship between two Convergences","text":"<p>Lebesgue's Theorem: Relationship between two Convergences</p> <p>Assume \\(m(E)&lt;\\infty\\), \\(\\{f_n(x)\\}\\) is a measurable function sequence \\(a.e.\\) on \\(E\\). If \\(\\{f_n\\}\\) converges to function \\(f(x)\\) \\(a.e.\\) on \\(E\\), where \\(f(x)\\) is a finite function \\(a.e.x\\in E\\), then \\(f_n(x)\\) converges to \\(f(x)\\) in measure.</p> HintsProof <p>Use \\(m(E)&lt;\\infty\\) to show that \\(f_n(x) \\rightrightarrows f(x)\\), which could gives convergence in measure.</p> <p>By Egoroff Theorem, because \\(m(E)&lt;\\infty\\), we have point-wise convergent function sequence \\(\\{f_n\\}\\) also converges uniformly to \\(f(x)\\). That is, we have \\(\\forall \\delta&gt;0\\), \\(\\exists\\) measurable set \\(E_\\delta\\), such that \\(m(E_\\delta)&lt;\\delta\\) and</p> \\[ f_n(x)\\rightrightarrows f(x),\\quad \\forall x\\in E\\backslash E_\\delta. \\] <p>Then by definition of uniform convergence, we have \\(\\forall \\varepsilon&gt;0\\), \\(\\exists N&gt;0\\), \\(\\forall n\\geq N\\), </p> \\[ |f_n(x)-f(x)|&lt;\\varepsilon,\\quad x\\in E\\backslash E_\\delta  \\] <p>which means the set</p> \\[ \\{x\\in E: |f_n(x)-f(x)|\\geq \\varepsilon\\}\\subset E_\\delta \\] <p>so</p> \\[ m(\\{x\\in E: |f_n(x)-f(x)|\\geq \\varepsilon\\})\\leq m(E_\\delta)&lt;\\delta. \\] <p>and we are done.</p> <p>Note that \\(m(E)&lt;\\infty\\) is not necessary, see that we use the uniform convergence to prove, which is the most important.</p> <p></p> <p>Riesz Theorem</p> <p>Assume function \\(f\\) and function sequence \\(\\{f_n\\}\\) are measurable function \\(a.e.\\) on \\(E\\), where \\(f\\) and \\(\\{f_n\\}\\) are finite \\(a.e.\\) on \\(E\\). Then </p> <p>(i) If \\(\\{f_n\\}\\) converge to \\(f\\) in measure, then there exists subsequence \\(\\{f_{n_j}\\}\\) convergent to \\(f\\) \\(a.e.x\\in E\\).</p> <p>(ii) If \\(m(E)&lt;\\infty\\), and \\(\\{f_n\\}\\) converge to \\(f\\) \\(a.e.x\\in E\\), then \\(\\{f_n\\}\\) converge to \\(f\\) in measure.</p> Proof for (i)Proof for (ii) <p>We hope to use \\(\\delta\\) to get a subsequence of \\(f_n\\).</p> <p>\\(\\forall k\\in \\mathbb{N}^+\\), let \\(\\delta_k=\\frac{1}{2^k}\\), so \\(f_n\\overset{m}{\\rightarrow}f\\) implies</p> \\[ \\lim_{n\\rightarrow \\infty}m(\\{|f_n-f|\\geq \\delta_k\\})=0. \\] <p>So there exsits \\(n_k\\), such that </p> \\[ m(\\{|f_{n_k}-f|\\geq \\delta_k\\})&lt;\\frac{1}{2^k}. \\] <p>Notice \\(\\{m(\\{|f_n-f|\\geq \\delta_k\\})\\}_{n\\geq 1}\\) is a number sequence, so \\(n_1&lt;n_2&lt;\\cdots\\).</p> <p>Define </p> \\[ E:=\\bigcap_{p=1}^\\infty\\bigcup_{k=p}^\\infty\\left\\{|f_{n_k}-f|\\geq \\frac{1}{2^k}\\right\\} \\] <p>and for every \\(p\\), measure</p> \\[ m(E)\\leq m\\left(\\left\\{|f_{n_k}-f|\\geq \\frac{1}{2^k}\\right\\}\\right)&lt;\\sum_{k=p}^\\infty\\frac{1}{2^k}=\\frac{1}{2^{p-1}}. \\] <p>let \\(p\\rightarrow \\infty\\), we have \\(m(E)=0\\). Thus for every \\(x\\in D-E\\), i.e.</p> \\[ x\\in \\bigcup_{p=1}^\\infty\\bigcap_{k=p}^\\infty\\left\\{|f_{n_k}-f|&lt; \\frac{1}{2^k}\\right\\} \\] <p>which means, \\(\\exists p_0\\geq 1\\), such that </p> \\[ |f_{n_k}-f|&lt; \\frac{1}{2^k},\\quad \\forall k\\geq p_0 \\] <p>meaning \\(\\lim\\limits_{k\\rightarrow \\infty}f_{n_k}=f\\).</p> <p>\\(\\forall \\delta&gt;0\\), \\(\\forall \\varepsilon&gt;0\\), since \\(m(D)&lt;\\infty\\), by Egoroff Theorem, there exists measurable set \\(E\\subset D\\), such that \\(m(D-E)&lt;\\varepsilon\\), and \\(f_n\\rightrightarrows f, a.e.x\\in E\\). That is, there exsits \\(N\\), such that</p> \\[ |f_n(x)-f(x)|&lt;\\delta,\\quad x\\in E,n\\geq N. \\] <p>which means</p> \\[ m(\\{|f_n(x)-f(x)|\\geq \\delta\\})=M(D-E)&lt;\\varepsilon,\\quad n&gt;N \\] <p>meaning </p> \\[ \\lim_{n\\rightarrow \\infty}m(\\{|f_n(x)-f(x)|\\geq \\delta\\})=0. \\] <p>Lemma: Sufficient and Necessary Condition for convergence in measure</p> <p>Assume function \\(f\\) and function sequence \\(\\{f_n\\}\\) are measurable on \\(E\\) where \\(m(E)&lt;\\infty\\). Then \\(f_n(x)\\) converges to \\(f(x)\\) in measure, iff \\(\\forall\\) subsequence \\(\\{f_{n_j}\\}\\), \\(\\exists\\) a subsequence \\(\\{f_{n_{j_i}}\\}\\) such that </p> \\[ f_{n_{j_i}}\\rightarrow f(x), a.e.x\\in E. \\] Proof <ul> <li>\"\\(\\Rightarrow\\)\".</li> </ul> <p>We prove \\(\\forall\\) subsequence \\(\\{f_{n_j}\\}\\), \\(f_{n_j}\\) converges to \\(f\\) in measure, which holds apparently because \\(f_n(x)\\) converges to \\(f(x)\\) in measure.</p> <p>Then apply Riesz Theorem to \\(f_{n_j}\\) and we are done.</p> <ul> <li>\"\\(\\Leftarrow\\)\". We make use of contradiction. That is, if \\(\\{f_n\\}\\) does not converges to \\(f\\) in measure, i.e. \\(\\exists \\varepsilon_0&gt;0\\), </li> </ul> \\[ \\lim_{n\\rightarrow \\infty} m(\\{|f_n-f|\\geq \\varepsilon_0\\})\\neq 0. \\] <p>So \\(\\forall j, \\exists n_j\\), s.t.</p> \\[ \\begin{align} m(\\{|f_{n_j}-f|\\geq \\varepsilon_0\\})&gt;0.\\label{contradiction} \\end{align} \\] <p>But if there exists a subsequence \\(\\{f_{n_{j_i}}\\}\\) such that </p> \\[ f_{n_{j_i}}\\rightarrow f(x), a.e.x\\in E. \\] <p>then by Lebesgue's Theorem (\\(m(E)&lt;\\infty\\)), we have \\(f_{n_{j_i}}\\overset{m}{\\rightarrow}f\\), which contradicts inequation \\(\\ref{contradiction}\\).</p>"},{"location":"Math/Real_Analysis/Sets/","title":"Sets","text":""},{"location":"Math/Real_Analysis/Sets/#basic-operations-on-sets","title":"Basic operations on sets","text":"<p>Theorem of Operations on sets</p> <p>Let \\( A \\), \\( B \\), and \\( C \\) be sets. Then the following properties hold:</p> <p>(i)  </p> \\[ A \\cup B = B \\cup A, \\quad A \\cap B = B \\cap A \\] <p>(ii)  </p> \\[ (A \\cup B) \\cup C = A \\cup (B \\cup C), \\quad (A \\cap B) \\cap C = A \\cap (B \\cap C) \\] <p>(iii)  </p> \\[ (A \\cup B) \\cap C = (A \\cap C) \\cup (B \\cap C), \\quad (A \\cap B) \\cup C = (A \\cup C) \\cap (B \\cup C) \\] Proof <p>We only prove (iii). </p> \\[ \\begin{aligned} (A \\cup B) \\cap C &amp;= \\{ x \\mid (x \\in A \\cup B) \\land (x \\in C) \\} \\\\ &amp;= \\{ x \\mid (x \\in A \\lor x \\in B) \\land (x \\in C) \\} \\\\ &amp;= \\{ x \\mid (x \\in A \\land x \\in C) \\lor (x \\in B \\land x \\in C) \\} \\\\ &amp;= \\{ x \\mid (x \\in A \\cap C) \\lor (x \\in B \\cap C) \\} \\\\ &amp;= (A \\cap C) \\cup (B \\cap C) \\end{aligned} \\] <p>The other part is similar:</p> \\[ \\begin{aligned} (A \\cap B) \\cup C &amp;= \\{ x \\mid (x \\in A \\cap B) \\lor (x \\in C) \\} \\\\ &amp;= \\{ x \\mid (x \\in A \\land x \\in B) \\lor (x \\in C) \\} \\\\ &amp;= \\{ x \\mid (x \\in A \\lor x \\in C) \\land (x \\in B \\lor x \\in C) \\} \\\\ &amp;= \\{ x \\mid (x \\in A \\cup C) \\land (x \\in B \\cup C) \\} \\\\ &amp;= (A \\cup C) \\cap (B \\cup C) \\end{aligned} \\]"},{"location":"Math/Real_Analysis/Sets/#de-morgan-formula","title":"De Morgan formula","text":"<p>Theorem of De Morgan Formula</p> <p>Let \\( \\{ A_\\lambda : \\lambda \\in \\Lambda \\} \\) be a family of sets on set \\( X \\). Then:</p> \\[ \\left( \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda \\right)^c = \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda^c, \\quad \\left( \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda \\right)^c = \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda^c \\] Proof <p>Firstly, we prove the first part:</p> \\[ \\begin{aligned} \\forall x \\in \\left( \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda \\right)^c &amp;\\Leftrightarrow \\forall x \\in X \\text{ and } x \\notin \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda \\\\ &amp;\\Leftrightarrow \\forall x \\in X, \\neg (\\exists \\lambda \\in \\Lambda, x \\in A_\\lambda) \\quad \\text{(by definition of union)} \\\\ &amp;\\Leftrightarrow \\forall x \\in X, \\forall \\lambda \\in \\Lambda, x \\notin A_\\lambda \\\\ &amp;\\Leftrightarrow \\forall \\lambda \\in \\Lambda, \\forall x \\in X, x \\notin A_\\lambda \\quad \\text{(change order of } \\forall \\text{)} \\\\ &amp;\\Leftrightarrow \\forall \\lambda \\in \\Lambda, x \\in A_\\lambda^c \\quad \\text{(by definition of complement)} \\\\ &amp;\\Leftrightarrow \\forall x \\in \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda^c \\quad \\text{(by definition of intersection)} \\end{aligned} \\] <p>Then, we prove the second part using similar logic. The following is an incorrect proof:</p> \\[ \\begin{aligned} \\forall x \\in \\left( \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda \\right)^c &amp;\\Leftrightarrow \\forall x \\in X \\text{ and } x \\notin \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda \\\\ &amp;\\Leftrightarrow \\forall x \\in X, \\neg (\\forall \\lambda \\in \\Lambda, x \\in A_\\lambda) \\quad \\text{(by definition of intersection)} \\\\ &amp;\\Leftrightarrow \\forall x \\in X, \\exists \\lambda \\in \\Lambda, x \\notin A_\\lambda \\\\ &amp;\\Leftrightarrow \\exists \\lambda \\in \\Lambda, \\forall x \\in X, x \\notin A_\\lambda \\quad \\text{(cannot change order!)} \\\\ &amp;\\Leftrightarrow \\exists \\lambda \\in \\Lambda, x \\in A_\\lambda^c \\quad \\text{(by definition of complement)} \\\\ &amp;\\Leftrightarrow \\forall x \\in \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda^c \\quad \\text{(by definition of union)} \\end{aligned} \\] <p>Using the first part, we can prove the second part correctly:</p> \\[ \\begin{aligned} \\left( \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda \\right)^c &amp;= \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda^c \\quad \\text{(the first part proved)} \\\\ \\Rightarrow \\left( \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda^c \\right)^c &amp;= \\bigcap_{\\lambda \\in \\Lambda} \\left( A_\\lambda^c \\right)^c \\quad \\text{(substitute } A_\\lambda \\text{ with } A_\\lambda^c \\text{)} \\\\ \\Rightarrow \\left( \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda^c \\right)^c &amp;= \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda \\quad \\text{(use } (A^c)^c = A \\text{)} \\\\ \\Rightarrow \\left( \\left( \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda^c \\right)^c \\right)^c &amp;= \\left( \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda \\right)^c \\quad \\text{(operate both sides with complement)} \\\\ \\Rightarrow \\bigcup_{\\lambda \\in \\Lambda} A_\\lambda^c &amp;= \\left( \\bigcap_{\\lambda \\in \\Lambda} A_\\lambda \\right)^c \\quad \\text{(exactly what we want)} \\end{aligned} \\] <p>Corollary of Set Difference</p> <p>For any sets \\( A \\) and \\( B \\), the following holds:</p> \\[ A - B = A \\cap B^c = B^c - A^c \\] <p></p> <p>Example.  Prove:</p> \\[ \\bigcup_{n=1}^{\\infty} A_n - \\bigcup_{n=1}^{\\infty} B_n \\subset \\bigcup_{n=1}^{\\infty} (A_n - B_n) \\] Proof \\[ \\begin{aligned} \\bigcup_{n=1}^{\\infty} A_n - \\bigcup_{n=1}^{\\infty} B_n &amp;= \\left( \\bigcup_{n=1}^{\\infty} A_n \\right) \\cap \\left( \\bigcup_{n=1}^{\\infty} B_n \\right)^c \\\\ &amp;= \\left( \\bigcup_{n=1}^{\\infty} A_n \\right) \\cap \\left( \\bigcap_{n=1}^{\\infty} B_n^c \\right) \\quad \\text{(by De Morgan's formula)} \\\\ &amp;= \\left( A_1 \\cup A_2 \\cup \\dots \\right) \\cap \\left( \\bigcap_{n=1}^{\\infty} B_n^c \\right) \\\\ &amp;= \\left[ A_1 \\cap \\left( \\bigcap_{n=1}^{\\infty} B_n^c \\right) \\right] \\cup \\left[ A_2 \\cap \\left( \\bigcap_{n=1}^{\\infty} B_n^c \\right) \\right] \\cup \\dots \\\\ &amp;\\subset \\left( A_1 \\cap B_1^c \\right) \\cup \\left( A_2 \\cap B_2^c \\right) \\cup \\dots \\\\ &amp;= \\bigcup_{n=1}^{\\infty} \\left( A_n \\cap B_n^c \\right) \\\\ &amp;= \\bigcup_{n=1}^{\\infty} \\left( A_n - B_n \\right) \\end{aligned} \\] <p></p> <p>Example.  </p> <p>Assume we have a list of sets \\( \\{ A_n \\}_{n \\geq 1} \\), let \\( B_1 = A_1 \\), and for \\( n \\geq 2 \\), define:</p> \\[ B_n = A_n - \\bigcup_{k=1}^{n-1} A_k \\] <p>Then prove:</p> <p>(i) \\( \\{ B_n \\}_{n \\geq 1} \\) are mutually disjoint.</p> <p>(ii) \\( \\bigcup_{n=1}^{\\infty} B_n = \\bigcup_{n=1}^{\\infty} A_n \\).</p> Proof for (i)Proof for (ii) <p>Use contradiction. \\(\\forall 0&lt;i&lt;j\\), if \\(\\exists x\\in B_i\\cap B_j\\), then </p> \\[ x\\in A_j-\\bigcup_{k=1}^{j-1} A_k=A_j\\cap \\left(\\bigcup_{k=1}^{j-1}A_k\\right)^c = A_j\\cap \\left(\\bigcap_{k=1}^{j-1}A_k^c\\right) \\] <p>which means \\(x\\in \\bigcap_{k=1}^{j-1}A_k^c\\), so \\(x\\notin A_k\\), \\((k=1,2,\\cdots, j-1)\\), which means \\(x\\notin A_i\\), while \\(x\\in B_i=A_i-\\bigcup_{k=1}^{i-1} A_k\\) shows that \\(x\\in A_i\\), contradicting!</p> <p>Use induction.</p> <ul> <li> <p>\\(n=1\\). \\(A_1=B_1\\) holds apparently.</p> </li> <li> <p>Assume \\(n=k\\) the proposition holds, then </p> </li> </ul> \\[ \\bigcup_{n=1}^{k} B_n = \\bigcup_{n=1}^{k} A_n  \\] <p>so</p> \\[ \\begin{align*} \\bigcup_{n=1}^{k+1} B_n &amp;= \\left(\\bigcup_{n=1}^{k} B_n\\right) \\cup B_{k+1}\\\\ &amp;=\\left(\\bigcup_{n=1}^{k} A_n\\right)\\cup \\left(A_{k+1}-\\bigcup_{n=1}^{k} A_n\\right)\\\\ &amp;= \\bigcup_{n=1}^{k+1} A_n \\\\ \\end{align*} \\] <p> Example. Prove \\[ \\bigcup_{n=1}^\\infty (A-A_n)=A-\\bigcap_{n=1}^\\infty A_n \\] Proof <p>Just use De Morgan formula.</p> \\[ \\begin{align*} A-\\bigcap_{n=1}^\\infty A_n&amp;=A\\cap \\left[\\bigcap_{n=1}^\\infty A_n\\right]^c\\\\ &amp;=A\\cap \\bigcup_{n=1}^\\infty A_n^c\\\\ &amp;=\\bigcup_{n=1}^\\infty (A\\cap A_n^c)\\\\ &amp;=\\bigcup_{n=1}^\\infty (A-A_n) \\end{align*} \\] <p>The above conclusions are quite useful in configuration of sets when we encounter measurable sets.</p>"},{"location":"Math/Real_Analysis/Sets/#limit-operation","title":"Limit Operation","text":"<p>limit superior and inferior</p> <p>Sequence of Sets.</p> <p>Given a sequence of sets \\(\\{A_n\\}_{n\\leq 1}\\), we could construct two new sets</p> \\[ B_n=\\bigcup_{k=n}^\\infty A_k,\\quad  C_n=\\bigcap_{k=n}^\\infty A_k \\] <p>So we have \\(B_1\\supset B_2\\supset \\cdots\\), and \\(C_1\\subset C_2\\subset \\cdots\\). Then define limit superior</p> \\[ \\overline{\\lim}\\limits_{n\\rightarrow \\infty}A_n:=\\bigcap_{n=1}^\\infty B_n=\\bigcap_{n=1}^\\infty \\bigcup_{k=n}^\\infty A_k \\] <p>and limit inferior</p> \\[ \\underline{\\lim}\\limits_{n\\rightarrow \\infty}A_n:=\\bigcup_{n=1}^\\infty C_n=\\bigcup_{n=1}^\\infty \\bigcap_{k=n}^\\infty A_k \\] <p>If the limit superior and inferior of \\(\\{A_n\\}\\) are the same, then we call \\(\\{A_n\\}\\) has limit and its value is denoted as</p> \\[ \\lim_{n\\rightarrow \\infty}A_n=\\overline{\\lim}\\limits_{n\\rightarrow \\infty}A_n=\\underline{\\lim}\\limits_{n\\rightarrow \\infty}A_n \\] <p>Theorem of limit operations</p> <p>Assume \\(\\{A_n\\}_{n\\geq 1}\\) is a sequence of sets, then</p> <p>(i) \\(x\\in \\overline{\\lim}\\limits_{n\\rightarrow \\infty}A_n=\\bigcap_{n=1}^\\infty \\bigcup_{k=n}^\\infty A_k\\) iff \\(\\forall N\\), \\(\\exists n\\geq N\\), such that \\(x\\in A_n\\).</p> <p>(ii) \\(x\\in \\underline{\\lim}\\limits_{n\\rightarrow \\infty}A_n=\\bigcup_{n=1}^\\infty \\bigcap_{k=n}^\\infty A_k\\) iff \\(\\exists N_x\\), \\(\\forall n&gt;N_x\\), such that \\(x\\in A_n\\).</p> <p>(iii) \\(\\underline{\\lim}\\limits_{n\\rightarrow \\infty}A_n\\subset\\overline{\\lim}\\limits_{n\\rightarrow \\infty}A_n\\).</p> <p>(iv) When \\(\\{A_n\\}_{n\\geq 1}\\) monotonically increases or decreases, it has limit and its value is</p> \\[ \\lim_{n\\rightarrow \\infty}A_n=\\begin{cases} \\displaystyle\\bigcup_{n\\rightarrow \\infty}A_n,\\quad \\{A_n\\} \\nearrow\\\\ \\displaystyle\\bigcap_{n\\rightarrow \\infty}A_n,\\quad \\{A_n\\} \\searrow \\end{cases} \\] Proof <p>We only prove (iv).</p> <p>when \\(\\{A_n\\}\\) monotonically increase, we have</p> \\[ \\bigcup_{k=n}^\\infty A_k = \\bigcup_{k=1}^\\infty A_k,\\quad \\bigcap_{k=n}^\\infty A_k=A_n \\] <p>so </p> \\[ \\overline{\\lim}\\limits_{n\\rightarrow \\infty}A_n=\\bigcap_{n=1}^\\infty \\bigcup_{k=n}^\\infty A_k=\\bigcap_{n=1}^\\infty \\bigcup_{k=1}^\\infty A_k=\\bigcup_{k=1}^\\infty A_k,\\quad  \\underline{\\lim}\\limits_{n\\rightarrow \\infty}A_n=\\bigcup_{n=1}^\\infty \\bigcap_{k=n}^\\infty A_k=\\bigcup_{n=1}^\\infty A_n \\] <p>Both of the above are of the same. Similar logic for \\(\\{A_n\\}_{n\\geq 1}\\) monotonically decreasing.</p>"},{"location":"Math/Real_Analysis/Sets/#equalibrium-of-sets","title":"Equalibrium of Sets","text":"<p>Cantor-Bernstein Theorem</p> <p>If \\( A \\sim B_1 \\subset B \\) and \\( B \\sim A_1 \\subset A \\), then \\( A \\sim B \\).</p> Proof <p>From what we already know, there exist two bijections \\( f : A \\to B_1 \\) and \\( g : B \\to A_1 \\) such that \\( B_1 = f(A) \\) and \\( A_1 = g(B) \\). We aim to prove that \\( A \\sim B \\).</p> <p>Define the following sequences of sets:</p> \\[ \\begin{aligned} A_2 &amp;= g(B_1), &amp; B_2 &amp;= f(A_1), \\\\ A_3 &amp;= g(B_2), &amp; B_3 &amp;= f(A_2), \\\\ &amp;\\vdots \\\\ A_n &amp;= g(B_{n-1}), &amp; B_n &amp;= f(A_{n-1}). \\end{aligned} \\] <p>From this construction, we observe the following properties:</p> \\[ A_n \\sim A_{n+2}, \\quad B_n \\sim B_{n+2}. \\] <p>If we define \\( A = A_0 \\), then:</p> \\[ \\begin{aligned} A_0 &amp;\\sim A_2 \\sim A_4 \\sim \\dots \\sim A_{2n}, \\\\ A_1 &amp;\\sim A_3 \\sim A_5 \\sim \\dots \\sim A_{2n+1}. \\end{aligned} \\] <p>Now, consider the decomposition of \\( A \\) and \\( A_1 \\):</p> \\[ \\begin{aligned} A &amp;= (A \\setminus A_1) \\cup A_1 \\\\   &amp;= (A \\setminus A_1) \\cup (A_1 \\setminus A_2) \\cup A_2 \\\\   &amp;= (A \\setminus A_1) \\cup (A_1 \\setminus A_2) \\cup (A_2 \\setminus A_3) \\cup A_3 \\\\   &amp;= \\bigcup_{i=0}^n (A_i \\setminus A_{i+1}) \\cup A_{n+1} \\\\   &amp;= \\left( \\bigcup_{i=0}^\\infty (A_i \\setminus A_{i+1}) \\right) \\cup \\left( \\bigcap_{i=1}^\\infty A_i \\right), \\end{aligned} \\] <p>and</p> \\[ A_1 = \\left( \\bigcup_{i=1}^\\infty (A_i \\setminus A_{i+1}) \\right) \\cup \\left( \\bigcap_{i=1}^\\infty A_i \\right). \\] <p>Note that for all \\( n \\in \\mathbb{N} \\), \\( A_{2n} \\setminus A_{2n+1} \\sim A_{2n+2} \\setminus A_{2n+3} \\) (this follows from \\( A_{2n} \\sim A_{2n+2} \\) and \\( A_{2n+1} \\sim A_{2n+3} \\)).</p> <p>Thus:</p> \\[ \\bigcup_{i=0}^\\infty (A_{2i} \\setminus A_{2i+1}) \\sim \\bigcup_{i=0}^\\infty (A_{2i+2} \\setminus A_{2i+3}). \\] <p>We can now show that:</p> \\[ \\begin{aligned} A_0 &amp;= (A_0 \\setminus A_1) \\cup (A_1 \\setminus A_2) \\cup (A_2 \\setminus A_3) \\cup \\dots \\cup \\left( \\bigcap_i A_i \\right), \\\\ A_1 &amp;= (A_1 \\setminus A_2) \\cup (A_2 \\setminus A_3) \\cup (A_3 \\setminus A_4) \\cup \\dots \\cup \\left( \\bigcap_i A_i \\right). \\end{aligned} \\] <p>The \\( i \\)-th term in the first row is equivalent to the \\( (i+1) \\)-th term in the second row, and the \\( i \\)-th term in the second row is equivalent to the \\( (i+1) \\)-th term in the first row. Since the final term in both rows is the same, we conclude that \\( A = A_0 \\sim A_1 \\).</p> <p>Corollary of Cantor-Bernstein Theorem</p> <p>Assume there are three sets \\( A_0 \\), \\( A_1 \\), and \\( A_2 \\) such that \\( A_2 \\subset A_1 \\subset A_0 \\). If \\( A_0 \\sim A_2 \\), then \\( A_0 \\sim A_1 \\).</p>"},{"location":"Math/Real_Analysis/Sets/#denumerability","title":"Denumerability","text":"<p>Definition of Denumerability</p> <p>A set \\( E \\) is said to be denumerable if \\( E \\sim \\mathbb{N} \\). Moreover, we write that \\( E \\) has base \\( \\aleph_0 \\).</p> <p></p> <p>Theorem of Basic Facts on Denumerable Sets</p> <ol> <li>Every infinite set has a denumerable subset.</li> <li>Every infinite subset of a denumerable set is denumerable.</li> <li>The denumerable union of denumerable sets is denumerable.</li> </ol> Proof <p>We only prove (iii). This method is known as diagonal method.</p> <p>Assume \\( \\{ A_n \\}_{n \\geq 1} \\) is a collection of mutually exclusive denumerable sets. Because we can use Example: construct mutually disjoint sets to convert an arbitrary sequence of sets into a sequence of mutually disjoint sets. (If \\( \\bigcup_{n=1}^\\infty A_n \\) is denumerable, then by combining the same items, we can easily obtain a denumerable set.)</p> <p>For each \\( n \\), the denumerable set \\( A_n \\) can be written as:</p> \\[ A_n = \\{ a_1^{(n)}, a_2^{(n)}, \\dots \\}, \\quad n = 1, 2, \\dots \\] <p>Define a new collection of sets \\( \\{ B_m \\}_{m \\geq 1} \\) as:</p> \\[ B_m = \\{ a_1^{(m)}, a_2^{(m-1)}, \\dots, a_m^{(1)} \\}, \\quad m = 1, 2, \\dots \\] <p>We observe that:</p> \\[ \\bigcup_{m=1}^\\infty B_m = \\bigcup_{n=1}^\\infty A_n \\] <p>Since \\( \\bigcup_{m=1}^\\infty B_m \\) is a denumerable union of finite sets, it is denumerable. Therefore, \\( \\bigcup_{n=1}^\\infty A_n \\) is also denumerable.</p> <p>Corollary</p> <p>\\( \\mathbb{Q} \\) is denumerable.</p> Proof <p>Define \\( A_n = \\{ 1/n, 2/n, \\dots \\} \\), which is denumerable. Then:</p> \\[ \\mathbb{Q}^+ = \\bigcup_{n=1}^\\infty A_n \\] <p>is denumerable (by Theorem (iii)). Similarly, \\( \\mathbb{Q}^- \\) is also denumerable. Thus:</p> \\[ \\mathbb{Q} = \\mathbb{Q}^+ \\cup \\{ 0 \\} \\cup \\mathbb{Q}^- \\] <p>is denumerable.</p> <p></p> <p>Example.  Prove: The set of mutually exclusive open intervals is denumerable.</p> Proof <p>Let \\( \\{ I_\\lambda \\}_{\\lambda \\in \\Lambda} \\subset \\mathbb{R} \\) be a collection of mutually exclusive open intervals. For each \\( \\lambda \\in \\Lambda \\), choose a rational number \\( r_\\lambda \\in I_\\lambda \\) and define:</p> \\[ A = \\{ r_\\lambda : \\lambda \\in \\Lambda \\} \\] <p>Then \\( \\{ I_\\lambda \\}_{\\lambda \\in \\Lambda} \\sim A \\). Since \\( A \\) is a subset of \\( \\mathbb{Q} \\), which is denumerable, \\( \\{ I_\\lambda \\}_{\\lambda \\in \\Lambda} \\) is also denumerable.</p> <p>The above denumerability is quite important in proving. Readers could check the following examples.</p> <p></p> <p>Example.  Some facts about functions on \\( \\mathbb{R} \\):</p> <p>(i) Prove: The discontinuity points of a monotonic function on \\( \\mathbb{R} \\) are at most denumerable.</p> <p>(ii) Prove: The first case discontinuity points of a real function on \\( \\mathbb{R} \\) are at most denumerable.</p> <p>(iii) Let \\( f \\) be a real function on \\( \\mathbb{R} \\). If there exists \\( M &gt; 0 \\) such that for any finite set of distinct points \\( x_1, x_2, \\dots, x_n \\), \\( \\left| \\sum_{k=1}^n f(x_k) \\right| \\leq M \\), prove: \\( \\{ x : f(x) \\neq 0 \\} \\) is at most denumerable.</p> Proof for (i)Proof for (ii) <p>Let \\(f\\) to be monotonically increasing. Firstly we prove that the discontinuity point is the first class, i.e. its left and right limits exists.</p> <p>Let \\(A=\\{f(t): x_1&lt;t&lt;x_0\\}\\), then \\(f(x_0)&lt;\\infty\\) is an upper bound of \\(A\\). Denote \\(\\alpha=\\sup A\\), then by its definition, \\(\\forall \\varepsilon&gt;0\\), \\(\\exists x_2\\in (x_1,x_0)\\), such that </p> \\[ \\alpha-\\varepsilon&lt;f(x_2)&lt;\\alpha. \\] <p>which means \\(|f(x_2)-\\alpha|&lt;\\varepsilon\\). Since \\(f\\) is monotonically increasing, so \\(\\forall x\\in (x_2,x_0)\\), \\(|f(x)-\\alpha|&lt;\\varepsilon\\), which means </p> \\[ f(x_0^-)=\\lim_{x\\rightarrow x_0^-}f(x)=\\alpha\\leq f(x_0)&lt;\\infty \\] <p>meaning left limit exists. Same for right limit. </p> <p>Then we have to show that these discontinuity points are denumerable. Let \\(B\\) to be the set composed by all the discontinuity points and \\(C\\) to be a set composed by open intervals in \\(\\mathbb{R}\\). Let</p> \\[ \\varphi:B\\mapsto C, \\quad x\\mapsto (f(x^-),f(x^+)). \\] <p>which is a bijection. Because \\(f\\) is monotonically increasing, range of \\(\\varphi\\) is mutually disjoint. By Example of mutually disjoint open intervals, the range of \\(\\varphi\\) is at most denumerable, so its original image is also denumerable.</p> <p>Let \\(A\\) to denote the set composed by all the first class discontinuity points of \\(f\\), then </p> \\[ A=\\{x: |f(x)-f(x^+)|&gt;0\\}\\cup \\{x: |f(x)-f(x^-)|&gt;0\\} \\] <p>If we define</p> \\[ E_n:=\\{x: |f(x)-f(x^+)|&gt;\\frac{1}{n}\\},\\quad F_m:=\\{x: |f(x)-f(x^-)|&gt;\\frac{1}{m}\\}, \\] <p>then </p> \\[ A=\\left(\\bigcup_{n=1}^\\infty E_n\\right)\\cup \\left(\\bigcup_{m=1}^\\infty F_m\\right) \\] <p>Now we want to show that \\(E_n\\) is a denumerable set. \\(\\forall x\\in E_n\\), by definition of right limit, \\(\\exists \\delta_x&gt;0\\), such that when \\(t\\in (x,x+\\delta_x)\\), we have</p> \\[ \\begin{equation} |f(t)-f(x^+)|&lt;\\frac{1}{4n}.\\label{def-limit} \\end{equation} \\] <p>So \\(\\forall x_1,x_2\\in (x,x+\\delta_x)\\), we have</p> \\[ \\begin{align} |f(x_1)-f(x_2)|&lt;|f(x_1)-f(x^+)|+|f(x_2)-f(x^+)|&lt;\\frac{1}{2n}.\\label{def-limit2} \\end{align} \\] <p>We have to show that these open intervals \\(I_x:=(x,x+\\delta_x),x\\in E_n\\) are mutually disjoint. We prove it by contradiction. Assume there exists \\(x_3\\in E_n\\) such that \\(x_3\\in (x,x+\\delta)\\), then from inequation \\(\\ref{def-limit2}\\), \\(\\forall t\\in (x,x+\\delta_x)\\), \\(x_3\\) follows inequation \\(\\ref{def-limit}\\)</p> \\[ \\begin{align} |f(x_3)-f(t)|&lt;\\frac{1}{2n}.\\label{x3-t} \\end{align} \\] <p>However on the other hand, since \\(x_3\\in E_n\\), it follows </p> \\[ \\begin{align} |f(x_3)-f(x_3^+)|&gt;\\frac{1}{n}\\label{x3-def} \\end{align} \\] <p>and also by definition of right limit, \\(\\exists\\delta_{x_3}\\) such that when \\(t\\in (x_3,x_3+\\delta_{x_3})\\), we have</p> \\[ \\begin{align} |f(t)-f(x_3^+)|&lt;\\frac{1}{2n}.\\label{x3-limit} \\end{align} \\] <p>We could choose \\(\\delta_{x_3}&lt;\\delta_x\\), then \\(\\forall t\\in (x_3,x_3+\\delta_{x_3})\\subset (x,x+\\delta_x)\\), by inequation \\(\\ref{x3-def}\\) and \\(\\ref{x3-limit}\\), we have</p> \\[ |f(x_3)-f(t)|\\geq|f(x_3)-f(x_3^+)|-|f(t)-f(x_3^+)|&gt;\\frac{1}{n}-\\frac{1}{2n}=\\frac{1}{2n}, \\] <p>which contradicts inequation \\(\\ref{x3-t}\\)! So \\(\\{I_x,x\\in E_n\\}\\) are mutually disjoint, so still by example of denumerable open set, \\(E_n\\) is denumerable. The same logic for \\(F_m\\), so \\(A\\) is also denumerable.</p> <p>Example.  Some facts about sets:</p> <ol> <li>Let \\( E \\subset \\mathbb{R}^3 \\). If for any distinct \\( x_1, x_2 \\in E \\), the distance \\( d(x_1, x_2) \\in \\mathbb{Q} \\), prove: \\( E \\) is at most denumerable.</li> <li>Let \\( A \\subset \\mathbb{R} \\) be a non-denumerable set. Prove: There exists \\( x \\in A \\) such that for any \\( \\delta &gt; 0 \\), the neighborhood \\( V(x, \\delta) \\cap A \\neq \\varnothing \\), and the set of all such \\( x \\) is also non-denumerable.</li> </ol> <p>Example.  Fact about isolated points:  </p> <p>Prove: If \\( E \\subset \\mathbb{R}^n \\), then the number of isolated points in \\( E \\) is denumerable.</p>"},{"location":"Math/Real_Analysis/Sets/#continuous-base","title":"Continuous Base","text":"<p>Definition of Continuous Base</p> <p>A set \\( E \\) is said to have a continuous base if \\( E \\sim [0, 1] \\). Moreover, we write that \\( E \\) has base \\( \\aleph \\).</p> <p>Corollary</p> <p>\\( \\mathbb{R} \\) has a continuous base.</p>"},{"location":"Math/Real_Analysis/Sets/#topology-of-set","title":"Topology of Set","text":""},{"location":"Math/Real_Analysis/Sets/#neighborhood","title":"Neighborhood","text":"<p>Definition of Neighbourhood</p> <p>(i) The \\( \\varepsilon \\) neighbourhood of \\( \\pmb{x} \\in \\mathbb{R}^n \\) is defined by:</p> \\[ V(\\pmb{x}, \\varepsilon) = \\{ \\pmb{y} \\in \\mathbb{R}^n : d(\\pmb{x}, \\pmb{y}) &lt; \\varepsilon \\} \\] <p>(ii) A set \\( E \\subset \\mathbb{R}^n \\) is called a neighbourhood of \\( \\pmb{x} \\in \\mathbb{R}^n \\) if there exists \\( \\varepsilon &gt; 0 \\) such that:</p> \\[ V(\\pmb{x}, \\varepsilon) \\subset E \\]"},{"location":"Math/Real_Analysis/Sets/#interior-point","title":"Interior Point","text":"<p>Definition of Interior Point</p> <p>A point \\( \\pmb{x} \\in \\mathbb{R}^n \\) is an interior point of a set \\( E \\subset \\mathbb{R}^n \\) if there exists \\( \\delta &gt; 0 \\) such that:</p> \\[ V(\\pmb{x}, \\delta) \\subset E \\] <p>Pay attention to the fact that interior points of \\( E \\) are definitely in \\( E \\).</p> <p>We can also define outer point of a set. That is, there exists \\( \\delta &gt; 0 \\) such that:</p> \\[ V(\\pmb{x}, \\delta) \\subset E^c \\quad \\text{(or equivalently, } V(\\pmb{x}, \\delta) \\cap E = \\varnothing \\text{)} \\] <p>And we can define the boundary point of a set. That is, for all \\( \\delta &gt; 0 \\):</p> \\[ V(\\pmb{x}, \\delta) \\cap E \\neq \\varnothing \\quad \\text{and} \\quad V(\\pmb{x}, \\delta) \\cap E^c \\neq \\varnothing \\] <p></p> <p>Corollary of Interior Point and Neighbourhood</p> <p>Assume \\( \\pmb{x} \\in \\mathbb{R}^n \\) and \\( E \\subset \\mathbb{R}^n \\), then:</p> \\[ \\pmb{x} \\text{ is an interior point of } E \\iff E \\text{ is a neighbourhood of } \\pmb{x}. \\] <p></p> <p>Corollary of Neighbourhood Interior Points</p> <p>The set \\( V(\\pmb{x}, \\varepsilon) \\) is a neighbourhood of all its points. That is:</p> \\[ \\forall \\pmb{y} \\in V(\\pmb{x}, \\varepsilon), \\pmb{y} \\text{ is an interior point of } V(\\pmb{x}, \\varepsilon). \\] Proof <p>For any \\( \\pmb{y} \\in V(\\pmb{x}, \\varepsilon) \\), we have \\( d(\\pmb{y}, \\pmb{x}) &lt; \\varepsilon \\). Choose \\( \\delta = \\varepsilon - d(\\pmb{y}, \\pmb{x}) \\). Then:</p> \\[ V(\\pmb{y}, \\delta) \\subset V(\\pmb{x}, \\varepsilon) \\] <p>because for any \\( \\pmb{z} \\in V(\\pmb{y}, \\delta) \\):</p> \\[ d(\\pmb{z}, \\pmb{x}) \\leq d(\\pmb{z}, \\pmb{y}) + d(\\pmb{y}, \\pmb{x}) &lt; \\delta + d(\\pmb{y}, \\pmb{x}) = \\varepsilon \\] <p>which implies \\( \\pmb{z} \\in V(\\pmb{x}, \\varepsilon) \\).</p> <p>Therefore, for any point \\( \\pmb{y} \\in V(\\pmb{x}, \\varepsilon) \\), \\( V(\\pmb{x}, \\varepsilon) \\) is a neighbourhood of \\( \\pmb{y} \\).</p>"},{"location":"Math/Real_Analysis/Sets/#kernel-of-set","title":"Kernel of Set","text":"<p>Definition of Kernel of Set</p> <p>The kernel of a set \\( E \\) is a set made up of all its interior points, denoted as \\( E^o \\). Obviously:</p> \\[ E^o \\subset E \\] <p>Correspondingly, the set of all outer points of \\( E \\) can be denoted as \\( (E^c)^o \\).</p>"},{"location":"Math/Real_Analysis/Sets/#open-set","title":"Open Set","text":"<p>Definition of Open Set</p> <p>A set \\( E \\subset \\mathbb{R}^n \\) is an open set if:</p> \\[ \\forall \\pmb{x} \\in E, \\pmb{x} \\text{ is an interior point of } E. \\] <p>By Corollary of Interior Point and Neighbourhood, we can have another equivalent statement:</p> \\[ \\forall \\pmb{x} \\in E, E \\text{ is the neighbourhood of } \\pmb{x}. \\] <p>Specifically, we define \\( \\varnothing \\) as an open set.</p> <p>Corollary of Open Sets</p> <p>(i) \\( V(\\pmb{x}, \\varepsilon) \\) is an open set. (ii) \\( \\mathbb{R}^n \\) is an open set. (iii) \\( E^o \\) is an open set.</p> Proof <p>We only prove (iii).  </p> <p>For any \\( \\pmb{x} \\in E^o \\), there exists \\( \\delta &gt; 0 \\) such that:</p> \\[ V(\\pmb{x}, \\delta) \\subset E \\] <p>We aim to prove that \\( \\pmb{x} \\) is an interior point of \\( E^o \\), that is:</p> \\[ \\forall \\pmb{x} \\in E^o, \\exists \\delta &gt; 0, V(\\pmb{x}, \\delta) \\subset E^o \\] <p>Note that by Corollary of Interior Point and Neighbourhood, for any \\( \\pmb{y} \\in V(\\pmb{x}, \\delta) \\), there exists \\( \\zeta &gt; 0 \\) such that:</p> \\[ V(\\pmb{y}, \\zeta) \\subset V(\\pmb{x}, \\delta) \\] <p>By the definition of \\( E^o \\), we have:</p> \\[ V(\\pmb{y}, \\zeta) \\subset E \\] <p>This implies that \\( \\pmb{y} \\) is an interior point of \\( E \\). Therefore:</p> \\[ V(\\pmb{x}, \\delta) \\subset E^o \\] <p>Thus, \\( E^o \\) is an open set.</p> <p></p> <p>Theorem of Properties of Open Sets</p> <p>(i) Infinite union of open sets is an open set. (ii) Finite intersection of open sets is an open set.</p> <p>Infinite intersection of open sets might not be an open set.</p> <p>Example. Notice that </p> \\[ \\bigcap_{n=1}^\\infty \\left(1-\\frac{1}{n},2+\\frac{1}{n}\\right)=[1,2]. \\]"},{"location":"Math/Real_Analysis/Sets/#accumulation-point","title":"Accumulation Point","text":"<p>Definition of Accumulation Point, Derived Set, and Closure of a Set</p> <p>A point \\( \\pmb{x} \\in \\mathbb{R}^n \\) is an accumulation point of a set \\( E \\subset \\mathbb{R}^n \\) if:</p> \\[ \\forall \\varepsilon &gt; 0, \\quad \\overline{V}(\\pmb{x}, \\varepsilon) \\cap E \\neq \\varnothing \\] <p>The Derived Set of \\( E \\) is defined as the set composed of all accumulation points of \\( E \\), denoted as \\( E' \\).</p> <p>The Closure of \\( E \\) is the union of \\( E \\) and its derived set, denoted as:</p> \\[ \\overline{E} = E \\cup E' \\] <p>Obviously, \\( E \\subset \\overline{E} \\).</p> <p>Regularly, the points of \\( E \\setminus E' \\) are called isolated points. That is:</p> \\[ \\exists \\varepsilon &gt; 0, \\quad \\overline{V}(\\pmb{x}, \\varepsilon) \\cap E = \\varnothing \\] <p>Pay attention to the fact that accumulation points of \\( E \\) are not necessarily in \\( E \\).</p> <p>Obviously, the interior points of \\( E \\) must be accumulation points of \\( E \\). Therefore, every point of an open set \\( E \\) must be an accumulation point of \\( E \\).</p> <p>Example.  Prove:</p> <p>A point \\( \\pmb{x} \\in \\mathbb{R}^n \\) is an accumulation point of a set \\( E \\subset \\mathbb{R}^n \\), or equivalently, \\( \\pmb{x} \\in E' \\), if and only if there exists a sequence \\( \\{ \\pmb{x}_k \\} \\subset E \\) with \\( \\pmb{x}_k \\neq \\pmb{x} \\) such that:</p> \\[ \\lim_{k \\to \\infty} \\pmb{x}_k = \\pmb{x} \\] <p>Theorem of Facts about Kernel, Derived Set, and Closure</p> <p>Let \\( A \\) and \\( B \\) be subsets of \\( \\mathbb{R}^n \\). Then:</p> <p>(i) If \\( A \\subset B \\), then:</p> \\[ A' \\subset B', \\quad A^o \\subset B^o, \\quad \\overline{A} \\subset \\overline{B} \\] <p>(ii) The following hold:</p> \\[ (A \\cup B)' = A' \\cup B', \\quad \\overline{A \\cup B} = \\overline{A} \\cup \\overline{B} \\] <p>but:</p> \\[ (A \\cup B)^o \\supset A^o \\cup B^o \\] <p>(iii) The following hold:</p> \\[ (A \\cap B)^o = A^o \\cap B^o \\] <p>but:</p> \\[ (A \\cap B)' \\subset A' \\cap B', \\quad \\overline{A \\cap B} \\subset \\overline{A} \\cap \\overline{B} \\] Proof for (i)Proof for (ii)Proof for (iii) <p>(i) If \\( A \\subset B \\), then:</p> <ul> <li> <p>For any \\( \\pmb{x} \\in A^o \\), there exists \\( \\delta &gt; 0 \\) such that \\( V(\\pmb{x}, \\delta) \\subset A \\subset B \\), which implies \\( \\pmb{x} \\in B^o \\).</p> </li> <li> <p>For any \\( \\pmb{x} \\in A' \\), for all \\( \\delta &gt; 0 \\), \\( \\overline{V}(\\pmb{x}, \\delta) \\cap A \\neq \\varnothing \\), which implies \\( \\overline{V}(\\pmb{x}, \\delta) \\cap B \\neq \\varnothing \\), so \\( \\pmb{x} \\in B' \\).</p> </li> <li> <p>With \\( A' \\subset B' \\), we have:</p> </li> </ul> \\[ \\overline{A} = A \\cup A' \\subset B \\cup B' = \\overline{B} \\] <p>(ii) It is easy to see that:</p> <ul> <li>Since \\( A \\subset A \\cup B \\) and \\( B \\subset A \\cup B \\), we have \\( A' \\subset (A \\cup B)' \\) and \\( B' \\subset (A \\cup B)' \\), so:</li> </ul> \\[ A' \\cup B' \\subset (A \\cup B)' \\] <ul> <li>Similarly:</li> </ul> \\[ \\overline{A \\cup B} \\supset \\overline{A} \\cup \\overline{B}, \\quad (A \\cup B)^o \\supset A^o \\cup B^o \\] <ul> <li>To prove \\( (A \\cup B)' = A' \\cup B' \\), note that for any \\( \\pmb{x} \\in (A \\cup B)' \\), \\( \\pmb{x} \\) is an accumulation point of either \\( A \\) or \\( B \\). If \\( \\pmb{x} \\notin A' \\cup B' \\), then \\( \\pmb{x} \\in A'^c \\cap B'^c \\), which contradicts the definition of accumulation points. Thus:</li> </ul> \\[ (A \\cup B)' = A' \\cup B' \\] <ul> <li>Using this result:</li> </ul> \\[ \\overline{A} \\cup \\overline{B} = A \\cup A' \\cup B \\cup B' = (A \\cup B) \\cup (A' \\cup B') = (A \\cup B) \\cup (A \\cup B)' = \\overline{A \\cup B} \\] <ul> <li>For the opposite example, consider \\( A = (0, 1] \\) and \\( B = (1, 2] \\). Then:</li> </ul> \\[ A^o = (0, 1), \\quad B^o = (1, 2), \\quad (A \\cup B)^o = (0, 2) \\neq (0, 1) \\cup (1, 2) = A^o \\cup B^o \\] <p>(iii) Since \\( A \\cap B \\subset A \\) and \\( A \\cap B \\subset B \\), we have:</p> \\[ (A \\cap B)^o \\subset A^o \\cap B^o, \\quad (A \\cap B)' \\subset A' \\cap B', \\quad \\overline{A \\cap B} \\subset \\overline{A} \\cap \\overline{B} \\] <ul> <li>To prove \\( (A \\cap B)^o = A^o \\cap B^o \\), note that for any \\( \\pmb{x} \\in A^o \\cap B^o \\), there exist \\( \\delta_1, \\delta_2 &gt; 0 \\) such that \\( V(\\pmb{x}, \\delta_1) \\subset A \\) and \\( V(\\pmb{x}, \\delta_2) \\subset B \\). Let \\( \\delta = \\min\\{\\delta_1, \\delta_2\\} \\), then:</li> </ul> \\[ V(\\pmb{x}, \\delta) \\subset A \\cap B \\] <p>which implies \\( \\pmb{x} \\in (A \\cap B)^o \\).</p> <ul> <li>For the last two statements, consider \\( A = \\mathbb{Q} \\) and \\( B = \\mathbb{Q}^c \\). Then:</li> </ul> \\[ A' \\cap B' = \\mathbb{R} \\neq \\varnothing = (A \\cap B)', \\quad \\overline{A} \\cap \\overline{B} = \\mathbb{R} \\neq \\varnothing = \\overline{A \\cap B} \\]"},{"location":"Math/Real_Analysis/Sets/#closed-set","title":"Closed Set","text":"<p>Definition of Closed Set</p> <p>Assume a set \\( F \\subset \\mathbb{R}^n \\). If \\( F^c \\) is an open set, then \\( F \\) is a closed set.</p> <p>Note that every point of an closed set \\(E\\) may not be the accumulation point of \\(E\\).</p> <p>Theorem of Properties of Closed Sets</p> <p>(i) Infinite intersection of closed sets is a closed set. (ii) Finite union of closed sets is a closed set.</p> Proof <p>The proof follows directly from De Morgan's formula and the properties of open sets (De Morgan Formula and Theorem of Properties of Open Sets).</p> <ul> <li>For (i):  </li> </ul> <p>Let \\( \\{ F_\\lambda \\}_{\\lambda \\in \\Lambda} \\) be a collection of closed sets. Then:</p> \\[ \\left( \\bigcap_{\\lambda \\in \\Lambda} F_\\lambda \\right)^c = \\bigcup_{\\lambda \\in \\Lambda} F_\\lambda^c \\] <p>Since each \\( F_\\lambda^c \\) is open, their union is also open. Therefore, \\( \\bigcap_{\\lambda \\in \\Lambda} F_\\lambda \\) is closed.</p> <ul> <li>For (ii):  </li> </ul> <p>Let \\( F_1, F_2, \\dots, F_n \\) be closed sets. Then:</p> \\[ \\left( \\bigcup_{k=1}^n F_k \\right)^c = \\bigcap_{k=1}^n F_k^c \\] <p>Since each \\( F_k^c \\) is open, their finite intersection is also open. Therefore, \\( \\bigcup_{k=1}^n F_k \\) is closed.</p> <p>Infinite union of closed sets might not be a closed set.</p> <p>Example. Notice that </p> \\[ \\bigcup_{n=1}^\\infty \\left[1+\\frac{1}{n},2-\\frac{1}{n}\\right]=(1,2). \\] <p></p> <p>Theorem of Closed Set and Derived Set Condition</p> <p>A set \\( E \\) is a closed set if and only if \\( E' \\subset E \\).</p> Proof <ul> <li>\"\\(\\Rightarrow\\)\":  </li> </ul> <p>If \\( E \\) is a closed set, then \\( E^c \\) is an open set. That is:</p> \\[ \\forall \\pmb{x} \\in E', \\forall \\varepsilon &gt; 0, \\quad \\overline{V}(\\pmb{x}, \\varepsilon) \\cap E \\neq \\varnothing \\] <p>If \\( \\pmb{x} \\in E^c \\), then \\( \\pmb{x} \\) is an interior point of \\( E^c \\), which means:</p> \\[ \\exists \\delta &gt; 0, \\quad V(\\pmb{x}, \\delta) \\subset E^c \\quad \\Rightarrow \\quad V(\\pmb{x}, \\delta) \\cap E = \\varnothing \\] <p>This contradicts the definition of accumulation points. Therefore, \\( \\pmb{x} \\notin E^c \\), which implies \\( \\pmb{x} \\in E \\). Thus:</p> \\[ E' \\subset E \\] <ul> <li>\"\\(\\Leftarrow\\)\":  </li> </ul> <p>If \\( E' \\subset E \\), then \\( E'^c \\supset E^c \\). For any \\( \\pmb{x} \\in E^c \\), \\( \\pmb{x} \\in E'^c \\), which means \\( \\pmb{x} \\) is not an accumulation point of \\( E \\). That is:</p> \\[ \\exists \\delta &gt; 0, \\quad \\overline{V}(\\pmb{x}, \\delta) \\cap E = \\varnothing \\] <p>This implies:</p> \\[ \\overline{V}(\\pmb{x}, \\delta) \\subset E^c \\] <p>Therefore, \\( E^c \\) is an open set, and \\( E \\) is a closed set.</p> <p>Corollary</p> <p>If \\( E \\) is a closed set, then:</p> \\[ \\overline{E} = E \\] <p>Theorem of Derived Set Closure</p> <p>The derived set of any set is a closed set.</p> Proof Version OneProof Version Two <p>Given a set \\( E \\), we prove that \\( E'^c \\) is an open set.  </p> <p>For any \\( \\pmb{x} \\in E'^c \\), \\( \\pmb{x} \\notin E' \\), which means \\( \\pmb{x} \\) is not an accumulation point of \\( E \\). That is:</p> \\[ \\exists \\delta &gt; 0, \\quad \\overline{V}(\\pmb{x}, \\delta) \\cap E = \\varnothing \\quad \\Rightarrow \\quad \\overline{V}(\\pmb{x}, \\delta) \\subset E^c \\] <p>With the same logic as Corollary of Neighbourhood Interior Points, we can show that for any \\( \\pmb{y} \\in \\overline{V}(\\pmb{x}, \\delta) \\), there exists \\( \\zeta &gt; 0 \\) such that:</p> \\[ V(\\pmb{y}, \\zeta) \\subset V(\\pmb{x}, \\delta) \\subset E^c \\] <p>This means that every point in \\( \\overline{V}(\\pmb{x}, \\delta) \\) is not an accumulation point of \\( E \\). By Definition of accumulation point:</p> \\[ \\overline{V}(\\pmb{x}, \\delta) \\subset E'^c \\quad \\Rightarrow \\quad V(\\pmb{x}, \\delta) \\subset E'^c \\] <p>Thus, \\( \\pmb{x} \\) is an interior point of \\( E'^c \\), so \\( E'^c \\) is an open set, and \\( E' \\) is a closed set.</p> <p>Now we use Theorem of Closed Set and Derived Set Condition to show that \\( (E')' \\subset E' \\).  </p> <p>Assume \\( E' \\neq \\varnothing \\), and we aim to show that for any \\( \\pmb{x} \\in (E')' \\), for all \\( \\delta &gt; 0 \\):</p> \\[ \\overline{V}(\\pmb{x}, \\delta) \\cap E' \\neq \\varnothing \\quad \\Rightarrow \\quad \\overline{V}(\\pmb{x}, \\delta) \\cap E \\neq \\varnothing \\] <p>For any \\( \\pmb{y} \\in \\overline{V}(\\pmb{x}, \\delta) \\cap E' \\), let \\( \\delta &gt; \\zeta &gt; 0 \\) such that:</p> \\[ \\overline{V}(\\pmb{y}, \\zeta) \\cap E \\neq \\varnothing \\] <p>Then:</p> \\[ \\overline{V}(\\pmb{x}, \\delta) \\cap E \\supset \\overline{V}(\\pmb{y}, \\zeta) \\cap E \\neq \\varnothing \\] <p>So \\( \\pmb{x} \\) is an accumulation point of \\( E \\), which implies \\( \\pmb{x} \\in E' \\). Therefore, \\( (E')' \\subset E' \\), and by Theorem of Closed Set and Derived Set Condition, \\( E' \\) is a closed set.</p> <p>Corollary</p> <p>The closure \\( \\overline{E} \\) is a closed set.</p> A Bad ProofAnother Proof <p>We prove that \\( \\overline{E}^c \\) is an open set.  </p> <p>For any \\( \\pmb{x} \\in \\overline{E}^c = \\mathbb{R}^n - \\overline{E} = \\mathbb{R}^n - E - E' \\), \\( \\pmb{x} \\notin E \\) and \\( \\pmb{x} \\) is not an accumulation point of \\( E \\). That is:</p> \\[ \\exists \\delta &gt; 0, \\quad \\overline{V}(\\pmb{x}, \\delta) \\cap E = \\varnothing \\] <p>Additionally, we can show that:</p> \\[ \\exists \\zeta &gt; 0, \\quad \\overline{V}(\\pmb{x}, \\zeta) \\cap E' = \\varnothing \\] <p>(Because if \\( \\forall \\zeta &gt; 0 \\), \\( \\overline{V}(\\pmb{x}, \\zeta) \\cap E' \\neq \\varnothing \\), then \\( \\pmb{x} \\) would be an accumulation point of \\( E' \\), which contradicts \\( \\pmb{x} \\notin E' \\).)</p> <p>Therefore:</p> \\[ V(\\pmb{x}, \\delta) \\subset \\overline{E}^c \\] <p>This implies that \\( \\overline{E}^c \\) is an open set, so \\( \\overline{E} \\) is a closed set.</p> <p>We use Theorem of Closed Set and Derived Set Condition to show that \\( \\overline{E}' \\subset \\overline{E} \\).  </p> <p>Since:</p> \\[ \\overline{E} = E \\cup E' \\] <p>we have:</p> \\[ \\overline{E}' = (E \\cup E')' = E' \\cup (E')' \\] <p>Because \\( E' \\) and \\( (E')' \\) are both closed sets, their union \\( \\overline{E}' \\) is also a closed set. Therefore, \\( \\overline{E} \\) is a closed set.</p> <p>Theorem of Closure Facts</p> <p>(i) The closure of the closure of a set \\( E \\) is equal to the closure of \\( E \\):</p> \\[ \\overline{\\overline{E}} = \\overline{E} \\] <p>Example.  </p> <p>Assume \\( I = (0, 1) \\), and a real function \\( f(x) \\) is defined on \\( I \\). Then:</p> \\[ f(x) \\text{ is continuous on } I \\iff \\forall \\text{ open set } G \\subset (-\\infty, \\infty), \\quad f^{-1}(G) \\text{ is an open set.} \\] Proof <ul> <li>\"\\(\\Rightarrow\\)\":  </li> </ul> <p>If \\( f(x) \\) is continuous, and assume \\( f^{-1}(G) \\) is not empty, then for any \\( x_0 \\in f^{-1}(G) \\), we have \\( f(x_0) \\in G \\). Since \\( G \\) is an open set, there exists \\( \\delta &gt; 0 \\) such that:</p> \\[ V(f(x_0), \\delta) \\subset G \\] <p>By the definition of a continuous function, for any \\( \\varepsilon &gt; 0 \\), there exists \\( \\delta' &gt; 0 \\) such that for all \\( x \\in V(x_0, \\delta') \\):</p> \\[ -\\varepsilon &lt; f(x) - f(x_0) &lt; \\varepsilon \\] <p>Let \\( \\varepsilon &lt; \\delta \\). Then for all \\( x \\in V(x_0, \\delta') \\):</p> \\[ |f(x) - f(x_0)| &lt; \\varepsilon &lt; \\delta \\] <p>This implies:</p> \\[ V(f(x_0), \\varepsilon) \\subset G \\] <p>Therefore:</p> \\[ V(x_0, \\delta') \\subset f^{-1}(G) \\] <p>This shows that \\( x_0 \\) is an interior point of \\( f^{-1}(G) \\), so \\( f^{-1}(G) \\) is an open set.</p> <ul> <li>\"\\(\\Leftarrow\\)\":  </li> </ul> <p>If for every open set \\( G \\subset (-\\infty, \\infty) \\), \\( f^{-1}(G) \\) is an open set, then for any \\( x_0 \\in f^{-1}(G) \\) and any \\( \\varepsilon &gt; 0 \\), \\( V(x_0, \\varepsilon) \\) is an open set. Therefore:</p> \\[ f^{-1}(V(f(x_0), \\varepsilon)) \\text{ is an open set.} \\] <p>Since \\( x_0 \\in f^{-1}(V(f(x_0), \\varepsilon)) \\), there exists \\( \\delta &gt; 0 \\) such that:</p> \\[ V(x_0, \\delta) \\subset f^{-1}(V(f(x_0), \\varepsilon)) \\] <p>This implies:</p> \\[ \\forall x \\in V(x_0, \\delta), \\quad f(x) \\in V(f(x_0), \\varepsilon) \\] <p>Thus, \\( f(x) \\) is continuous at \\( x_0 \\), and since \\( x_0 \\) is arbitrary, \\( f(x) \\) is continuous on \\( I \\).</p> <p>Theorem of Biggest Open Set and Smallest Closed Set</p> <p>(i) The kernel \\( E^o \\) is the biggest open set among all sets \\( \\subset E \\). (ii) The closure \\( \\overline{E} \\) is the smallest closed set among all sets \\( \\supset E \\).</p> <p>Example.  Prove the following statements are equivalent:</p> <ol> <li>Set \\( E \\subset \\mathbb{R}^n \\) is a closed set.  </li> <li>For every accumulation point \\( \\pmb{x} \\) of \\( E \\), \\( \\pmb{x} \\in E \\), that is, \\( E' \\subset E \\). (We can also get \\( \\overline{E} \\supset E \\).)  </li> <li>For every sequence \\( \\{ \\pmb{x}_k \\} \\subset E \\), if \\( \\lim_{k \\to \\infty} \\pmb{x}_k = \\pmb{x} \\), then \\( \\pmb{x} \\in E \\).</li> </ol> <p>Example.  Some facts about kernel and closure of a set:</p> <ol> <li>If \\( A \\cap B = \\varnothing \\), then:</li> </ol> \\[ \\overline{A} \\cap B^o = \\varnothing \\] <ol> <li>The closure of the complement of \\( A \\) is equal to the complement of the kernel of \\( A \\):</li> </ol> \\[ \\overline{A^c} = (A^o)^c \\] <ol> <li>Assume \\( A \\) is an open set. Prove:</li> </ol> \\[ A \\subset B \\iff \\forall \\text{ nonempty open set } S \\subset A, \\quad S \\cap B \\neq \\varnothing \\]"},{"location":"Math/Real_Analysis/Sets/#complete-set","title":"Complete Set","text":"<p>Definition of Complete Set</p> <p>A set \\( E \\) is called a complete set if:</p> \\[ E' = E \\] <p>Example.  </p> <p>If \\( E \\) is a closed set and does not have isolated points, then \\( E \\) is a complete set.</p>"},{"location":"Math/Real_Analysis/Sets/#sparse-set-dense-set","title":"Sparse Set &amp; Dense Set","text":"<p>Definition of Sparse Set and Dense Set</p> <ul> <li>A set \\( E \\subset \\mathbb{R}^n \\) is called a sparse set if for every nonempty open set \\( S \\subset \\mathbb{R}^n \\), there exists a nonempty open subset \\( S_0 \\subset S \\) such that:</li> </ul> \\[ S_0 \\cap E = \\varnothing \\] <ul> <li>A set \\( E \\subset \\mathbb{R}^n \\) is called a dense set if for every nonempty open set \\( S \\subset \\mathbb{R}^n \\):</li> </ul> \\[ S \\cap E \\neq \\varnothing \\] <p>Theorem of Sparse Set and Dense Set</p> <p>(i) A set \\( E \\subset \\mathbb{R}^n \\) is a sparse set if and only if:</p> \\[ \\left( \\overline{E} \\right)^o = \\varnothing \\] <p>(ii) A set \\( E \\subset \\mathbb{R}^n \\) is a dense set if and only if:</p> \\[ \\overline{E} = \\mathbb{R}^n \\] <p>Corollary</p> <p>Some common sets:</p> <ol> <li>\\( \\mathbb{Z} \\) is a sparse set.  </li> <li>\\( \\mathbb{Q} \\) is a dense set.</li> </ol>"},{"location":"Math/Real_Analysis/Sets/#configuration-of-sets","title":"Configuration of Sets","text":"<p>Definition of Composing Interval</p> <p>Assume \\( G \\subset \\mathbb{R} \\) is a bounded open set. For any \\( x_0 \\in G \\), there exists an interval \\( (\\alpha, \\beta) \\) such that:</p> <p>(i) \\( x_0 \\in (\\alpha, \\beta) \\). (ii) \\( (\\alpha, \\beta) \\subset G \\). (iii) \\( \\alpha \\notin G \\) and \\( \\beta \\notin G \\).</p> <p>We define \\( (\\alpha, \\beta) \\) as the composing interval of set \\( G \\).</p> <p>Theorem of Decomposition of Bounded Open Sets</p> <p>Assume \\( G \\subset \\mathbb{R} \\) is a bounded open set. Then:</p> \\[ G = \\bigcup_{n=1}^\\infty (\\alpha_n, \\beta_n) \\] <p>where \\( (\\alpha_n, \\beta_n) \\) are mutually exclusive (disjoint) open intervals.</p> Proof <p>Easy to see that:</p> \\[ \\alpha = \\inf\\{x : (x, x_0) \\subset G\\}, \\quad \\beta = \\sup\\{x : (x_0, x) \\subset G\\} \\] <p>Every point \\( x \\in G \\) has a composing interval. That is, for any \\( x \\neq y \\in G \\), there exist intervals \\( (\\alpha_x, \\beta_x) \\) and \\( (\\alpha_y, \\beta_y) \\subset G \\) that are their composing intervals, respectively.</p> <ul> <li> <p>If \\( (\\alpha_x, \\beta_x) \\) and \\( (\\alpha_y, \\beta_y) \\) have common points, then they must be the same interval. Otherwise, one of \\( \\alpha \\) or \\( \\beta \\) would lie within the other interval, which contradicts Definition of Composing Interval.</p> </li> <li> <p>If \\( (\\alpha_x, \\beta_x) \\) and \\( (\\alpha_y, \\beta_y) \\) share no common points, then their intersection is empty:</p> </li> </ul> \\[ (\\alpha_x, \\beta_x) \\cap (\\alpha_y, \\beta_y) = \\varnothing \\] <p>Therefore, \\( G \\) can be written as a union of mutually exclusive open intervals. Now we need to prove that the number of such intervals is denumerable, which follows directly from Example of denumerable open sets.</p> <p>We can have similar conclusion in higher dimensions as below. </p> <p>Theorem of Decomposition of Open Sets in \\( \\mathbb{R}^n \\)</p> <p>Assume \\( G \\subset \\mathbb{R}^n \\) is an open set. Then:</p> \\[ G \\text{ is a union of denumerable half-open cubes which are mutually exclusive.} \\] <p></p> <p>Theorem of Complete Set in \\( \\mathbb{R} \\)</p> <p>A set \\( E \\subset \\mathbb{R} \\) is a complete set if and only if:</p> \\[ F^c = \\mathbb{R} - F \\text{ is a denumerable union of open sets which are mutually exclusive and share no common endpoints.} \\] <p>Definition of \\( G_\\delta \\) Set and \\( F_\\sigma \\) Set</p> <ul> <li>A \\( G_\\delta \\) set is a set composed of the denumerable intersection of open sets. That is:</li> </ul> \\[ G_\\delta = \\bigcap_{k=1}^\\infty G_k, \\quad \\text{where } G_k \\text{ is an open set.} \\] <ul> <li>A \\( F_\\sigma \\) set is a set composed of the denumerable union of closed sets. That is:</li> </ul> \\[ F_\\sigma = \\bigcup_{k=1}^\\infty F_k, \\quad \\text{where } F_k \\text{ is a closed set.} \\]"},{"location":"Math/Real_Analysis/Sets/#cantor-set","title":"Cantor Set","text":"<p>Definition of Cantor Set Configuration</p> <p>The construction of the Cantor set proceeds as follows:</p> <ul> <li>Step 1: Remove an open interval of length \\( \\frac{1}{3} \\) from \\( [0, 1] \\).  </li> <li>Step 2: Remove two open intervals of length \\( \\frac{1}{9} \\) from the remaining set, specifically \\( \\left( \\frac{1}{9}, \\frac{2}{9} \\right) \\) and \\( \\left( \\frac{7}{9}, \\frac{8}{9} \\right) \\).  </li> <li>Step 3: Continue this process.  </li> <li>Step n: Remove \\( 2^{n-1} \\) open intervals of length \\( \\frac{1}{3^n} \\) from the remaining set. Denote these intervals as \\( I_{n,k} \\) where \\( 1 \\leq k \\leq 2^{n-1} \\). The remaining closed sets are \\( 2^n \\) in number, each of length \\( \\frac{1}{3^n} \\), denoted by \\( F_n \\). Note that the endpoints of \\( I_{n,k} \\) are never removed.</li> </ul> <p>Finally, we obtain:</p> \\[ \\begin{align*} G_0 = &amp;\\left( \\frac{1}{3}, \\frac{2}{3} \\right) \\cup \\\\ &amp;\\left( \\frac{1}{9}, \\frac{2}{9} \\right) \\cup \\left( \\frac{7}{9}, \\frac{8}{9} \\right) \\cup \\\\ &amp;\\left( \\frac{1}{27}, \\frac{2}{27} \\right) \\cup \\left( \\frac{7}{27}, \\frac{8}{27} \\right) \\cup \\left( \\frac{19}{27}, \\frac{20}{27} \\right) \\cup \\left( \\frac{25}{27}, \\frac{26}{27} \\right) \\cup \\dots \\end{align*} \\] <p>This set \\( G_0 \\) is an open set (by Theorem of Properties of Open Sets). The Cantor set is defined as:</p> \\[ P_0 = [0, 1] \\setminus G_0 \\] <p>which is a closed set. Additionally, we can express \\( P_0 \\) as:</p> \\[ P_0 = \\bigcap_{n=1}^\\infty F_n \\] <p><p> </p></p> <p>Theorem of Facts about Cantor Set</p> <p>(i) The Cantor set is a closed set, meaning it has no interior points. (ii) The Cantor set is a complete set. (iii) The Cantor set has base \\( \\aleph \\). (iv) The Cantor set is a sparse set.</p> Proof for (i)Proof for (ii) \u2160Proof for (ii) \u2161Proof for (iii) <p>(i) This is easily seen from the definition of the Cantor set. Specifically, for any \\( x \\in P_0 \\), there does not exist \\( \\delta &gt; 0 \\) such that:</p> \\[ (x - \\delta, x + \\delta) \\subset P_0 \\] <p>First Perspective: Show that \\( (P_0)' = P_0 \\). Since \\( P_0 \\) is a closed set, we have \\( (P_0)' \\subset P_0 \\) (by Theorem of Closed Set and Derived Set Condition). We only need to show that \\( P_0 \\subset (P_0)' \\), meaning every point in \\( P_0 \\) is an accumulation point of \\( P_0 \\). That is:</p> \\[ \\forall x \\in P_0, \\forall \\varepsilon &gt; 0, \\quad (x - \\varepsilon, x + \\varepsilon) \\cap P_0 \\setminus \\{x\\} \\neq \\varnothing \\] <p>We use the endpoints of \\( I_{n,k} \\). For any \\( \\varepsilon &gt; 0 \\), there exists \\( k_0 &gt; 0 \\) such that:</p> \\[ \\frac{1}{3^{k_0}} &lt; \\varepsilon \\] <p>Since \\( x \\in P_0 = \\bigcap_{n=1}^\\infty F_n \\), we have \\( x \\in F_{k_0} \\), which is the union of \\( 2^{k_0} \\) closed intervals of length \\( \\frac{1}{3^{k_0}} \\). Assume \\( x \\in I = [\\alpha, \\beta] \\), a closed interval of length \\( \\frac{1}{3^{k_0}} \\). Then:</p> \\[ I \\subset (x - \\varepsilon, x + \\varepsilon) \\] <p>The endpoints \\( \\alpha \\) and \\( \\beta \\) must be in \\( P_0 \\). Thus:</p> \\[ (x - \\varepsilon, x + \\varepsilon) \\cap P_0 \\setminus \\{x\\} \\supset \\{\\alpha, \\beta\\} \\neq \\varnothing \\] <p>Second Perspective: Show that \\( P_0 \\) has no isolated points. Assume \\( P_0 \\) has an isolated point \\( x \\). Since \\( 0 \\) and \\( 1 \\) are accumulation points of \\( P_0 \\), assume \\( x \\neq 0, 1 \\). By the definition of an isolated point:</p> \\[ \\exists \\delta &gt; 0, \\quad (x - \\delta, x + \\delta) \\cap P_0 \\setminus \\{x\\} = \\varnothing \\] <p>This implies:</p> \\[ (x - \\delta, x) \\cup (x, x + \\delta) \\subset G_0 \\] <p>However, this contradicts the construction of \\( P_0 \\), as \\( x \\notin G_0 \\).</p> <p>(iii) By the closed intervals embedding theorem, we can express the Cantor set using ternary expansions. Define:</p> \\[ x = \\sum_{n=1}^\\infty \\frac{a_n}{3^n}, \\quad a_n \\in \\{0, 1, 2\\} \\] <p>The series converges, and:</p> <ul> <li>If \\( a_1 = 0 \\), then:</li> </ul> \\[ 0 \\leq x &lt; \\sum_{n=2}^\\infty \\frac{2}{3^n} = \\frac{1}{3} \\] <ul> <li>If \\( a_1 = 1 \\), then:</li> </ul> \\[ \\frac{1}{3} \\leq x &lt; \\frac{1}{3} + \\sum_{n=2}^\\infty \\frac{2}{3^n} = \\frac{2}{3} \\] <ul> <li>If \\( a_1 = 2 \\), then:</li> </ul> \\[ \\frac{2}{3} \\leq x &lt; \\frac{2}{3} + \\sum_{n=2}^\\infty \\frac{2}{3^n} = 1 \\] <p>If \\( a_n \\neq 1 \\) for all \\( n \\), then \\( x \\in P_0 \\).</p>"},{"location":"Math/StochasticProcesses/","title":"Stochastic Processes","text":"<p>Reference</p> <ul> <li> <p>\u968f\u673a\u8fc7\u7a0b\u53ca\u5176\u5e94\u7528, \u8363\u817e\u4e2d</p> </li> <li> <p>\u6982\u7387\u8bba, \u5e94\u575a\u521a</p> </li> </ul>"},{"location":"Math/StochasticProcesses/#elementary-probability-theory","title":"Elementary Probability Theory","text":""},{"location":"Math/StochasticProcesses/#basic-processes","title":"Basic Processes","text":""},{"location":"Math/StochasticProcesses/#poisson-process","title":"Poisson Process","text":""},{"location":"Math/StochasticProcesses/#markov-process-chain","title":"Markov Process &amp; Chain","text":""},{"location":"Math/StochasticProcesses/#brownian-motion","title":"Brownian Motion","text":""},{"location":"Math/StochasticProcesses/#stationary-process","title":"Stationary Process","text":""},{"location":"Math/StochasticProcesses/#martingale","title":"Martingale","text":""},{"location":"Math/StochasticProcesses/Basic_Pro/","title":"Basic Processes","text":""},{"location":"Math/StochasticProcesses/Basic_Pro/#introductions","title":"Introductions","text":"<p>Definition of Stochastic Process</p> <p>Assume \\((\\Omega,\\mathscr{F}, \\mathbb{P})\\) is a probability space, \\(T\\) is an index set of real parameters. \\(\\forall t\\in T\\), there exists a random variable \\(X_t(\\omega)\\) defined on \\((\\Omega,\\mathscr{F}, \\mathbb{P})\\), then we call the collection of random variables \\(\\{X_t: t\\in T\\}\\) Stochastic Process.</p> <p>Fixing \\(t\\), \\(X_t(\\omega)\\) is a general random variable. However, when fixing \\(\\omega\\), \\(X_t(\\omega)\\) is a function on \\(T\\), which is called sample path. Different from random vector, the random variables of stochastic process are defined on the same probability space.</p> <p>According to the number of \\(T\\) and range \\(S\\) of \\(X_t\\), we have the following categories.</p> \\(S\\) denumerable \\(S\\) are intervals \\(T\\) denumerable discrete time and state discrete time, continuous state \\(T\\) are intervals continuous time, discrete state continuous time and state"},{"location":"Math/StochasticProcesses/Basic_Pro/#mathematical-characteristics","title":"Mathematical characteristics","text":"<p>Usually \\(T\\) is not countable, so we could not use all the union distribution to describe the statistic law. </p> <p>Definition of finite-dimensional distribution</p> <p>Assume \\(\\{X_t: t\\in T\\}\\) is a stochastic process. \\(\\forall n\\geq 1\\), \\(\\forall t_1,t_2,\\cdots,t_n\\in T\\), random vector \\((X_{t_1},\\cdots,X_{t_n})\\) has its union distribution function</p> \\[ F_{t_1\\cdots t_n}(x_1,\\cdots,x_n)=\\mathbb{P}\\{X_{t_1}&lt;x_1,\\cdots,X_{t_n}&lt;x_n\\} \\] <p>we make all these function up to </p> \\[ \\mathscr{H}=\\{F_{t_1\\cdots t_n}(x_1,\\cdots,x_n):  t_1,t_2,\\cdots,t_n\\in T,\\forall n\\geq 1\\} \\] <p>which is called the Finite-dimensional Distribution collection of \\(\\{X_t: t\\in T\\}\\).</p> <p>Properties of Finite-dimensional Distribution</p> <p>(i) Transverse Consistency. For any permutation of sequence \\(\\{1,2,\\cdots,n\\}\\), denoted by \\(\\{j_1,\\cdots,j_n\\}\\), </p> \\[ F_{t_1\\cdots t_n}(x_1,\\cdots,x_n)=F_{t_{j_1}\\cdots t_{j_n}}(x_{j_1},\\cdots,x_{j_n}). \\] <p>(ii) Longitudinal Consistency. \\(\\forall m&lt;n \\in \\mathbb{N}^+\\),</p> \\[ F_{t_1\\cdots t_m\\cdots t_n}(x_1,\\cdots,x_m,+\\infty,\\cdots,+\\infty)=F_{t_1\\cdots t_m}(x_1,\\cdots,x_m). \\] <p>Kolmogorov Theorem</p> <p>Assume we have a finite-dimensional distribution collection \\(\\mathscr{H}\\). If it satisfies Transverse and longitudinal Consistency, then there exsits a probability space \\((\\Omega,\\mathscr{F}, \\mathbb{P})\\) and a stochastic process {X_t} defined on that, such that \\(\\forall n\\geq 1\\), \\(\\forall t_1,\\cdots,t_n\\in T\\),</p> \\[ \\mathbb{P}\\{X_{t_1}&lt;x_1,\\cdots,X_{t_n}&lt;x_n\\}=F_{t_1\\cdots t_n}(x_1,\\cdots,x_n) \\] <p>Mathematical Characteristics for one-dimensonal distribution</p> <p>Assume \\(X_t\\) has a distribution function \\(F_t(x)\\). \\(\\forall t\\in T\\), define mean function </p> \\[ \\mu_X(t)=\\mathbb{E}(X_t), \\] <p>and variance function </p> \\[ \\sigma^2_X(t)=D(X_t). \\] <p>This is similar to definition of random variables.</p> <p>Mathematical Characteristics for two-dimensonal distribution</p> <p>Assume \\((X_{t}, X_s)\\) has a distribution function \\(F_{t,s}(x_t,x_s)\\), \\(\\forall t,s\\in T\\), define autocorrelation function</p> \\[ r_X(t,s)=\\mathbb{E}(X_tX_s), \\] <p>and covariance function</p> \\[ C_X(t,s)=\\text{Cov}(X_t,X_s). \\] <p>This is similar to definitions in Random Vector.</p> <p>The following theorem depicts the sufficient and necessary condition for having these mathematical characteristics exist.</p> <p>Theorem for existence of two-dimensional mathematical characteristics</p> <p>Stochastic process \\(\\{X_t\\}\\) has two-dimensional mathematical characteristics, iff </p> \\[ \\mathbb{E}X^2_t&lt;\\infty,\\quad \\forall t\\in T. \\] <p>If \\(\\{X_t\\}\\) satisfies the above condition, we call it Second-order Moment Process. </p> Proof <ul> <li>\"\\(\\Rightarrow\\)\". Easy to see. Let \\(t=s\\), and using autocorrelation function </li> </ul> \\[ R_X(t,t)=\\mathbb{E}(X_t^2)&lt;\\infty. \\] <ul> <li>\"\\(\\Leftarrow\\)\". Using Cauchy-Schwarz Inequation. If \\(\\mathbb{E}X_t^2&lt;\\infty\\), then </li> </ul> \\[ |\\mu_X(t)| \\leq \\mathbb{E}|X_t|\\cdot 1 \\leq \\sqrt{\\mathbb{E}X_t^2 \\cdot \\mathbb{E}1^2}&lt;\\infty, \\] <p>and </p> \\[ |R_X(t,s)|\\leq \\mathbb{E}|X_tX_s|\\leq \\sqrt{\\mathbb{E}X_t^2 \\cdot\\mathbb{E}X_s^2}&lt;\\infty. \\] <p>then use the above function to prove the existence of other items. Like \\(|C_X(t,s)|=|R_X(t,s)-\\mu_X(t)\\mu_X(s)|&lt;\\infty\\), \\(|\\sigma_X(t)|=|\\mathbb{E}(X_t^2)-(\\mathbb{E}X_t)^2|&lt;\\infty\\).</p>"},{"location":"Math/StochasticProcesses/Basic_Pro/#independent-process","title":"Independent Process","text":"<p>Definition for independent process</p> <p>Assume \\(\\{X_t\\}\\) is a stochastic process. If \\(\\forall n\\geq 1\\), \\(\\forall t_1,\\cdots,t_n\\in T\\) and \\(x_1,\\cdots,x_n\\in \\mathbb{R}\\), </p> \\[ \\mathbb{P}(X_{t_1}&lt;x_1,\\cdots,X_{t_n}&lt;x_n)=\\prod_{i=1}^n \\mathbb{P}(X_{t_i}&lt;x_i), \\] <p>or in terms of distribution function</p> \\[ F_{t_1,\\cdots,t_n}(x_1,\\cdots,x_n)=\\prod_{i=1}^n F_{t_i}(x_i), \\] <p>then we call \\(\\{X_t\\}\\) is an independent process.</p> <p>This is really hard to get an independent process in nature. So we introduce a White Noise Process.</p> <p>Definition for White Noise Process</p> <p>Assume \\(\\{X_t\\}\\) is a stochastic process. If it satisfies</p> \\[ \\begin{cases} \\mu_X(t)=0,\\quad &amp;\\forall t\\in T \\\\ C_X(t,s)=0,\\quad &amp;\\forall t\\neq s\\in T, \\end{cases} \\] <p>then we call \\(\\{X_t\\}\\) White Noise Process.</p> <p>Note the second condition could be be interpreted as \\(R_X(t,s)=\\mu_X(t)^2=0, \\forall t\\neq s\\in T\\).</p>"},{"location":"Math/StochasticProcesses/Basic_Pro/#independent-increment-process","title":"Independent Increment Process","text":"<p>Definition for Independent Increment Process</p> <p>Assume \\(\\{X_t\\}\\) is a stochastic process. If \\(\\forall n\\geq 3\\), \\(t_1&lt;t_2&lt;\\cdots&lt;t_n\\in T\\), random variables</p> \\[ X_{t_2}-X_{t_1},\\cdots,X_{t_n}-X_{t_{n-1}} \\] <p>are independent mutually, then we call \\(\\{X_t\\}\\) Independent Increment Process.</p> <p>Properties of independent increment process</p> <p>(i) Normalization. Assume \\(\\{X_t: t\\in [a,b]\\}\\) is a independent increment process, then \\(Y_t:=X_t-X_a\\) is still a independent increment process.</p> <p>(ii) Convolution.</p> Proof <p>(i) \\(\\forall a\\leq t_1\\leq t_2\\leq\\cdots\\leq t_n\\leq b\\), </p> \\[ Y_{t_i}-Y_{t_{i-1}}=X_{t_i}-X_{t_{i-1}},\\quad i=2,\\cdots,n \\] <p>are still independent mutually.</p>"},{"location":"Math/StochasticProcesses/Basic_Pro/#stationary-increment-process","title":"Stationary Increment Process","text":"<p>Stationary Increment Process</p> <p>Assume \\(\\{X_t: t\\in [a,b]\\}\\) is a stochastic process. If \\(\\forall t,s,h\\in [a,b]\\) with \\(t+h,s+h\\in [a,b]\\), </p> \\[ X_{t+h}-X_t,\\quad X_{s+h}-X_s \\] <p>are of the same distribution, then we call \\(\\{X_t\\}\\) Stationary Increment Process.</p> <p>Note that the above condition also means \\(\\forall t&gt;s\\), \\(X_t-X_s\\) is merely dependent on \\(t-s\\), i.e h.</p> <p>Usually we consider a process which is both independent and stationary increment process. It has some good properties.</p>"},{"location":"Math/StochasticProcesses/Basic_Pro/#gaussian-process","title":"Gaussian Process","text":"<p>Definition for Gaussian Process</p> <p>Assume \\(\\{X_t\\}\\) is a stochastis process. If forall \\(n\\geq 1\\), \\(\\forall t_1,\\cdots,t_n\\in T\\), random vector \\((X_{t_1},\\cdots,X_{t_n})\\) satisfies Gaussian distribution, then we call \\(\\{X_t\\}\\) Gaussian Process.</p> <p>Note that Gaussian process is a second-order moment process, the finite-dimensional distribution is totally determined by its mean function and covariance functions.</p> <p>Linear invariance of Gaussian Process</p> <p>Assume \\(\\{X_t\\}\\) is a stochastis process. It is a Gaussian Process, iff \\(\\forall n\\geq 1\\), \\(\\forall t_1,\\cdots,t_n\\in T\\), \\(\\forall a_1,\\cdots,a_n\\in \\mathbb{R}\\), such that </p> \\[ Y=\\sum_{i=1}^na_i X_{t_i} +a_0 \\] <p>is a normal random variable, where \\(a_0,\\cdots,a_n\\) do not equal \\(0\\) at the same time.</p>"},{"location":"Math/StochasticProcesses/Basic_Pro/#stationary-process","title":"Stationary Process","text":"<p>Linear index set</p> <p>Index set \\(T\\) is a linear index set, if \\(\\forall t_1,t_2\\in T\\), \\(t_1+t_2\\in T\\).</p> <p>The above definition is to guarantee the closeness of calculation for time in index set.</p> <p>Strictly Stationary Process</p> <p>Assume \\(\\{X_t: t\\in T\\}\\) is a stochastis process, and \\(T\\) is a linear index set. If \\(\\forall n\\geq 1\\), \\(\\forall t_1,\\cdots,t_n, h\\in T\\), random vector \\((X_{t_1},\\cdots, X_{t_n})\\) and \\((X_{t_1+h},\\cdots,X_{t_n+h})\\) share the same distribution, denoted by</p> \\[ X_{t_i}\\overset{d}{=}X_{t_i+h}, \\quad \\forall i=1,\\cdots,n, \\quad \\forall h\\in T \\] <p>then we call \\(\\{X_t\\}\\) is a Strictly Stationary Process. We usually say it has translation invariance.</p> <p>Properties of Strictly Stationary Process</p> <p>(i) \\(\\{X_t\\}\\) is an Identically Distributed Process.</p> <p>(ii) If \\(\\{X_t\\}\\) is a second-order moment process, then \\(\\forall t\\in T\\),</p> \\[ \\mu_X(t)=\\mu, \\quad \\sigma^2_X(t)=\\sigma^2, \\] <p>and </p> \\[ R_X(t,t+\\tau)=R_X(0,t+\\tau)=R(\\tau). \\] Proof <p>(i) By definition, let \\(n=1\\), we have</p> \\[ F_t(x)=F_{t+h}(x),\\quad \\forall t,h\\in T. \\] <p>(ii) Easy to see. Since \\(X_t\\overset{d}{=}X_s,(t\\neq s\\in T)\\), their mathematical characteristics \\(\\mathbb{E}(X_t)=\\mathbb{E}(X_s)\\), then \\(\\mu_X(t)=\\mu_X(s)=C\\).</p> <p>It is hard to ask a process to have its distribution invariant with time. In practice, we have the following looser process model, by emphasizing its mathematical characteristics.</p> <p>Wide-sense Stationary Process</p> <p>Assume \\(\\{X_t: t\\in T\\}\\) is a stachostic process, and \\(T\\) is a linear index set. If \\(\\{X_t\\}\\) is a second-order moment process, and \\(\\forall t,\\tau\\in T\\), </p> \\[ \\mu_X(t)=\\mu,\\quad R_X(t,t+\\tau)=R(\\tau). \\] <p>then we call \\(\\{X_t\\}\\) a wide-sense Stationary process, or weakly stationary process.</p> <p>Relationship of strictly and weakly stationary processes</p> <p>(i) A strictly stationary process with second-order moment is a weakly stationary process.</p> <p>(ii) For Gaussian process, strictly and weakly stationary processes are equivalent.</p> Proof <p>(i) Easy to see. Check the property of strictly stationary process.</p> <p>(ii) For gaussian process \\(\\{X_t\\}\\), it must be a second-order moment process, so if it is a strict stationary process, then it must be a weakly stationary process.</p> <p>On the other hand, if Gaussian process \\(\\{X_t\\}\\) is a weakly stationary process, then its mathematical characteristics are determined, then its finite-dimensional distribution is determined, so it is a strictly stationary process. </p>"},{"location":"Math/StochasticProcesses/Brownian/","title":"Brownian Motion","text":"<p>The following definition is similar to definition of Poisson Process.</p> <p>Brownian Motion (Wiener Process)</p> <p>Assume \\(B_t\\) is a real-valued Stochastic Process, it is called Brownian motion with para \\(\\sigma^2\\) if</p> <p>(i) \\(B(0)=0\\),</p> <p>(ii) Independent increment. \\(\\forall n\\geq 1, 0&lt;t_1&lt;\\cdots&lt;t_n\\), </p> \\[ B(t_1), B(t_2)-B(t_1),\\cdots,B(t_n)-B(t_{n-1}) \\] <p>are mutually independent.</p> <p>(iii) Stationary increment. For \\(s&lt;t\\), \\(B(t)-B(s)\\) and \\(B(t-s)\\) have the same distribution.</p> <p>(iv) Normal distribution. \\(\\forall t\\), \\(B(t)\\sim N(0,\\sigma^2 t)\\).</p> <p>If \\(\\sigma^2=1\\), we call the above process standard Brownian motion.</p> <p>Properties of Brownian Process</p> <p>Now we assume \\(\\sigma^2=1\\).</p> <p>(i) MC. \\(\\mu(B_t)=0\\), \\(Var(B_t)=\\sigma^2t=t\\).</p> <p>(ii) Self-related coefficient. If \\(s&lt;t\\), we have</p> \\[ r_B(s,t)=EB_s B_t=EB_s [B(t-s)+B(s)]=s. \\] <p>So \\(r_B(s,t)=s\\wedge t\\). This could also be used to define a Brownian motion.</p> <p>(iii) Distribution. Since \\(B_t\\sim N(0,t)\\), we have density function</p> \\[ p(t,x)=\\frac{1}{\\sqrt{2\\pi t}}e^{-\\frac{x^2}{2t}}. \\] <p>In actual calculation, we usually define \\(Z\\sim N(0,1)\\), then \\(B_t=\\sqrt{t} Z\\).</p> <p>(iv) Brownian motion is a Gaussian Process. To elaborate, \\(\\forall n, 0&lt;t_1&lt;\\cdots&lt;t_n\\), group 1</p> \\[ B(t_1), B(t_2)-B(t_1),\\cdots, B(t_n)-B(t_{n-1}) \\] <p>are mutually independent and follow Gaussian distribution, so their joint distribution is also Gaussian distribution. Since group 2</p> \\[ B(t_1),\\cdots,B(t_n) \\] <p>are a linear combination of group 1, so the joint distribution of group 2 are also Gaussian distribution. And moreover, \\((B(t_1),\\cdots,B(t_n))\\sim N(\\pmb{0}, \\pmb{\\Sigma}_n)\\), where</p> \\[ \\pmb{\\Sigma}_n=(t_i\\wedge t_j)_{n\\times n}. \\]"},{"location":"Math/StochasticProcesses/Brownian/#test-criterion","title":"Test Criterion","text":"<p>Equivalent Proposition for Brownian Motion</p> <p>Assume \\(X_t\\) is a real-valued Gaussian process, if \\(EX_t=0\\), \\(r_X(s,t)=s\\wedge t\\), then \\(X\\) is a Brownian motion.</p> Proof <ul> <li> <p>\\(X_0=0\\). This is by \\(X_0\\sim N(0,0)\\) which is a decreased Gaussian distribution, so \\(X_0=0\\) a.s.</p> </li> <li> <p>Independent increment. \\(\\forall n\\), \\(0&lt;t_1&lt;\\cdots&lt;t_n\\), we have \\(X(t_1), X(t_2)-X(t_1),\\cdots, X(t_n)-X(t_{n-1})\\) are mutually independent because their joint distribution is Gaussian distribution and covariance is zero for any two of them.</p> </li> <li> <p>Stationary increment. For \\(s&lt;t\\), we have \\(X(t)-X(s)\\sim N(0, t-s)\\sim X(t-s)\\), Readers could calculate its variance.</p> </li> <li> <p>Normal distribution. \\(X_t\\sim N(0,t)\\).</p> </li> </ul> <p>Corollary: some transformation of Brownian motion</p> <p>Assume \\(B_t\\) is a Brownian motion, then the following transformation is also Brownian motion.</p> <p>(i) Given \\(t_0&gt;0\\), a motion starting at \\(t_0\\) </p> \\[ X_t=B(t+t_0)-B(t_0). \\] <p>(ii) Self-Similarity. Given a constant \\(c&gt;0\\), </p> \\[ X_t=\\frac{1}{c}B(c^2t). \\] <p>(iii) Symmetry of \\(0\\) and \\(\\infty\\).</p> \\[ X_t=\\tilde{B}_t=\\begin{cases} tB(1/t),\\quad &amp;t&gt;0,\\\\ 0,\\quad &amp;t=0. \\end{cases} \\] Proof <p>Just use the above test criterion. For (i) it is actually a translation. </p> <p>For (ii), it is a scaling transformation. </p> <p>For (iii), we first prove it is Gaussian Process. For all \\(n\\), \\(0&lt;t_1&lt;\\cdots&lt;t_n\\), since joint distribution of </p> \\[ B(1/t_1),B(1/t_2),\\cdots, B(1/t_n) \\] <p>is Gaussian distribution by assumption, so their linear combination</p> \\[ t_1 B(1/t_1), t_2B(1/t_2)-t_1B(1/t_1),\\cdots , t_nB(1/t_n)-t_{n-1}B(1/t_{n-1}) \\] <p>is Gaussian distribution. Easy to check its MC. That is, \\(EX_t=EtB(1/t)=0\\), </p> \\[ \\begin{align*} r_X(s,t)=EX_tX_s&amp;=stEB(1/t)B(1/s)\\\\ &amp;=stE\\{B(1/t)[B(1/s)-B(1/t)+B(1/t)]\\}\\\\ &amp;=st EB(1/t)[B(1/s)-B(1/t)]+EB(1/t)^2\\\\ &amp;=st \\cdot \\frac{1}{t}=s \\end{align*} \\] <p>that is, \\(r_X(s,t)=s\\wedge t\\).</p> <p>Here we give some Related Processes derived from standard Brownian motion.</p> <p>Related Processes</p> <p>Assume \\(B_t\\) is a standard Brownian motion, then the following processes are common.</p> <p>(i) Brownian Bridge.</p> \\[ B^0(t)=B(t)-B(1)t, \\quad 0\\leq t\\leq 1. \\] <p>(ii) Reflected Brownian motion.</p> \\[ X_t=|B_t|,\\quad t\\geq 0. \\] <p>(iii) Geometric Brownian motion. Gien \\(\\alpha, \\beta\\in \\mathbb{R}\\), </p> \\[ X_t=e^{\\alpha t +\\beta B_t},\\quad t\\geq 0. \\] <p>(iv) Integrated process.</p> <p>We give the distribution of the above process.</p> Distribution <p>(i) \\(X_t\\) is still a Gaussian process. \\(EX_t=0-t\\cdot 0=0\\), and for \\(0&lt;s&lt;t&lt;1\\)</p> \\[ \\begin{align*} r_X(s,t)&amp;=E[B(s)-B(1)s][B(t)-B(1)t]\\\\ &amp;=EB_sB_t-sEB(1)B(t)-tEB(1)B(s)+stEB_1^2\\\\ &amp;=s-st-st+st=s-st \\end{align*} \\] <p>so \\(r_X(s,t)=(s\\wedge t)(1-s\\lor t)\\).</p> <p>(ii) For this one, we could get its distribution</p> \\[ F_{X}(x)=\\begin{cases} 0,\\quad &amp;x\\leq 0\\\\ P(|B_t|\\leq x)\\quad &amp;x&gt;0. \\end{cases} \\] <p>while for \\(x&gt;0\\),</p> \\[ P(|B_t|\\leq x)=P(-x\\leq B_t\\leq x)=2\\Phi(x/\\sqrt{t})-1. \\] <p>with its density function (by taking derivative of the above CDF)</p> \\[ p(t,x)=\\begin{cases} 0,\\quad &amp;x\\leq 0\\\\ \\sqrt{\\frac{2}{\\pi t}}e^{-\\frac{x^2}{2t}},\\quad &amp;x&gt;0. \\end{cases} \\] <p>So its ME</p> \\[ EX_t=\\int_\\mathbb{R} |x|\\frac{1}{\\sqrt{2\\pi t}}e^{-\\frac{x^2}{2t}}dx=\\frac{1}{2t}\\int_0^\\infty \\frac{1}{\\sqrt{2\\pi t}}e^{-\\frac{x^2}{2t}}d\\frac{x^2}{2t}=\\sqrt{2t/\\pi}. \\] <p>(iii) Distribution is a little tedious. But for ME, we have \\(Z\\sim N(0,1)\\)</p> \\[ Ee^{\\alpha t+\\beta Z}=e^{\\alpha t}\\int_\\mathbb{R} e^{\\beta x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx=e^{\\alpha t+\\beta^2/2} \\] <p>So \\(Ee^{\\alpha t+\\beta B_t}=Ee^{\\alpha t+\\beta \\sqrt{t}Z}=Ee^{\\alpha t+(\\beta\\sqrt{t}) Z}=e^{\\alpha t+\\beta^2t/2}\\).</p>"},{"location":"Math/StochasticProcesses/Brownian/#maximum-first-hitting-distribution","title":"Maximum &amp; First hitting distribution","text":""},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/","title":"Elementary Probability Theory","text":""},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#probability-space","title":"Probability Space","text":"<p>Definitions of \\(\\sigma\\)-algebra</p> <p>A family of subset \\(\\mathscr{F}\\) of non-empty set \\(\\Omega\\) is called a \\(\\pmb{\\sigma}\\)-algebra on \\(\\Omega\\), if it satisfies</p> <p>(i) \\(\\varnothing, \\Omega\\in \\mathscr{F}\\),</p> <p>(ii) If \\(A\\in \\mathscr{F}\\), then \\(A^c\\in \\mathscr{F}\\),</p> <p>(iii) If a sequence of sets \\(\\{A_n\\}_{n\\geq 1}\\in \\mathscr{F}\\), then \\(\\bigcup\\limits_{n=1}^\\infty A_n\\in \\mathscr{F}\\).</p> <p>Easy to see that for set \\(\\Omega\\), the smallest family of set \\(\\mathscr{F}=\\{\\varnothing, \\Omega\\}\\), and its biggest one is composed by all its subsets (or power set), denoted as \\(2^\\Omega\\).</p> <p>Properties of \\(\\sigma\\)-algebra</p> <p>Assume \\(\\mathscr{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\), then </p> <p>(i) If \\(A,B\\in \\mathscr{F}\\), then \\(A\\cap B, A\\cup B, A-B \\in \\mathscr{F}\\),</p> <p>(ii) If \\(\\{A_n\\}_{n\\geq 1}\\in \\mathscr{F}\\), then \\(\\bigcap\\limits_{n=1}^\\infty A_n\\in \\mathscr{F}\\).</p> Proof <p>(i) \\(A\\cup B=A\\cup B\\cup \\varnothing\\cup \\cdots \\cup \\varnothing\\in \\mathscr{F}\\). \\(A\\cap B=(A^c\\cup B^c)^c\\in \\mathscr{F}\\). \\(A-B=A\\cap B^c\\in \\mathscr{F}\\).</p> <p>(ii) Using De Morgan formula, i.e. \\(\\bigcap\\limits_{n=1}^\\infty A_n=\\left(\\bigcup\\limits_{n=1}^\\infty A_n^c\\right)^c \\in \\mathscr{F}\\).</p> <p></p> <p>Example. (Discrete) Assume sequence of events \\(\\{\\Omega_n\\}_{n\\geq 1}\\) is a parition of \\(\\Omega\\), then </p> \\[ \\mathscr{A}:=\\left\\{\\bigcup_{i\\in I}\\Omega_i: I\\subset \\{1,2,\\cdots\\}\\right\\} \\] <p>is a sub sigma-algebra of \\(\\mathscr{F}\\).</p> <p>Kolmogorov: Definition of probability measure</p> <p>Assume \\(\\Omega\\) is a sample space, \\(\\mathscr{F}\\) is a \\(\\sigma\\)-algebra on \\(\\Omega\\). A function \\(\\mathbb{P}\\) on \\(\\mathscr{F}\\) is called probability measure, if it satisfies</p> <p>(i) Non-negative. \\(\\forall A\\in \\mathscr{F}\\), \\(\\mathbb{P}\\geq 0\\).</p> <p>(ii) Normalization. \\(\\mathbb{P}(\\Omega)=1\\).</p> <p>(iii) Sigma-Additivity. Assume \\(\\{A_n\\}_{n\\geq 1}\\in \\mathscr{F}\\), \\(A_i\\cap A_j=\\varnothing, \\forall i\\neq j\\), then </p> \\[ \\mathbb{P}\\left(\\bigcup_{n\\geq 1}A_n\\right)=\\sum_{n\\geq 1}\\mathbb{P}(A_n). \\] <p>In this case, \\(\\mathbb{P}(A)\\) is called probability that event \\(A\\) happens.</p> <p>We combine the above definition \\(\\mathscr{F}\\), \\(\\mathbb{P}\\) and \\(\\Omega\\) as \\((\\Omega, \\mathscr{F}, \\mathbb{P})\\), which is called Probability Space.</p> <p>Because of Sigma-Additivity, we could use all the tools and results from Measure Theory. And apart from Normalization, the definition of probability measure is the same as measure. However, in probability theory, there are some unique phenomena and methods.</p> <p>Regarding \\(\\sigma\\)-algebra, we could not always choose its power set, because \\(\\Omega\\) might have non-denumerable elements.</p> <p>Properties of Probability Measure</p> <p>(i) \\(\\mathbb{P}(\\varnothing)=0\\).</p> <p>(ii) If \\(A,B\\in \\mathscr{F}\\), \\(A\\cap B=0\\), then \\(\\mathbb{P}(A\\cup B)=\\mathbb{P}(A)+\\mathbb{P}(B)\\).</p> <p>(iii) If \\(A,B\\in \\mathscr{F}\\), \\(A\\subset B\\), then \\(\\mathbb{P}(B-A)=\\mathbb{P}(B)-\\mathbb{P}(A)\\), so \\(\\mathbb{P}(A)\\leq \\mathbb{P}(B)\\).</p> <p>(iv) \\(\\mathbb{P}(A^c)=1-\\mathbb{P}(A)\\).</p> <p>(v) Sub-sigma-additivity. If \\(\\{A_n\\}_{n\\geq 1}\\in \\mathscr{F}\\), then </p> \\[ \\mathbb{P}\\left(\\bigcup_n A_n\\right)\\leq \\sum_n \\mathbb{P}(A_n). \\] <p>(vi) Inferior continuity. If \\(\\{A_n\\}_{n\\geq 1}\\in \\mathscr{F}\\) and monotonically increase, then </p> \\[ \\mathbb{P}\\left(\\bigcup_n A_n\\right)=\\lim_n \\mathbb{P}(A_n). \\] <p>(vi) Superior continuity. If \\(\\{A_n\\}_{n\\geq 1}\\in \\mathscr{F}\\) and monotonically decrease, then </p> \\[ \\mathbb{P}\\left(\\bigcap_n A_n\\right)=\\lim_n \\mathbb{P}(A_n). \\] Proof <p>Note Probability should not be defined randomly because it is a function defined on \\(\\sigma\\)-algebra. When \\(\\Omega\\) has denumerable elements, probability space is easy to construct.</p> <p>Discrete Probability Space</p> <p>(i) \\((\\Omega,\\{\\varnothing, A, A^c, \\Omega\\},\\mathbb{P})\\) is called Bernoulli probability space, if</p> \\[ \\mathbb{P}(\\varnothing)=0,\\quad\\mathbb{P}(A)=p, \\quad\\mathbb{P}(A^c)=1-p, \\quad\\mathbb{P}(\\Omega)=1. \\] <p>(ii) Same as Example, we have \\(\\mathbb{P}\\) defined by</p> \\[ \\mathbb{P}\\left(\\bigcup_{i\\in I,\\atop I\\subset\\{1,2,\\cdots\\}}\\Omega_i\\right)=\\sum_{i\\in I,\\atop I\\subset\\{1,2,\\cdots\\}}\\mathbb{P}(\\Omega_i) \\] <p>then \\((\\Omega, \\mathscr{A}, \\mathbb{P})\\) is a probability space, also called Discrete Probability Space.</p> <p>Example. Assume \\(\\Omega\\) has denumerable elements. Choose its power set as \\(\\mathscr{F}\\). Then for every sample point \\(\\omega\\in\\Omega\\), formulate a function \\(p:\\Omega\\mapsto \\mathbb{R}\\) as \\(p(\\omega)\\), it satisfies \\(\\sum\\limits_{\\omega\\in \\Omega}p(\\omega)=1\\) and for all \\(A\\subset \\Omega\\), define</p> \\[ \\mathbb{P}(A):=\\sum_{\\omega\\in A}p(\\omega). \\] <p>Show that \\((\\Omega, \\mathscr{F}, \\mathbb{P})\\) is a probability space. If \\(|\\Omega|&lt;\\infty\\), then </p> \\[ \\mathbb{P}(\\{\\omega\\})=\\frac{1}{|\\Omega|}, \\] <p>which is classical model.</p> <p>If \\(\\Omega\\) has non-denumerable elements, then it is not easy to formulate its \\(\\sigma\\)-algebra.</p> <p>Example. Assume \\(\\Omega=[0,1]\\), \\(\\mathscr{F}\\) is a Borel set \\(\\mathscr{B}([0,1])\\). For \\(A\\in \\mathscr{F}\\), \\(\\mathbb{P}(A)=|A|\\), this is exactly Lebesgue measure of \\(A\\). Then show that \\((\\Omega, \\mathscr{F}, \\mathbb{P})\\) is a probability space, also called Geometric Probability Spcae.</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#random-variable","title":"Random Variable","text":"<p>This part we have a similar path as we have in measurable function, but just gives specific definitions.</p> <p>Simply speaking, random variables is a measurable function on \\(\\Omega\\), to endow basic events with a number. </p> <p>Definition of Random Variable</p> <p>Assume \\((\\Omega, \\mathscr{F}, \\mathbb{P})\\) is a probability space. \\(\\xi:\\Omega\\rightarrow \\mathbb{R}\\), a function defined on \\(\\Omega\\), is called Random Variable, if \\(\\forall \\alpha\\in \\mathbb{R}\\)</p> \\[ \\{\\xi\\leq \\alpha\\}:=\\{\\omega\\in \\Omega: \\xi(\\omega)\\leq \\alpha\\}\\in \\mathscr{F}. \\] <p>Note: for a random variable \\(\\xi\\), we say \\(\\xi\\in A\\), if \\(\\forall \\omega\\in \\Omega\\), \\(\\xi(\\omega)\\in A\\). If \\(\\mathbb{P}(\\xi\\in A)=1\\) (usually \\(&lt;1\\)), then we call \\(\\xi\\) is distributed on \\(A\\). </p> <p>Readers could check this definition with measurable functions and its equivalent definitions. Here measurability of \\(\\xi\\) means that information of a \\(\\sigma\\)-algebra is enough to find \\(\\xi\\).</p> <p>Properties of Random Variable</p> <p>(i) random variable of \\(\\sigma\\)-algebra constructs a linear space. Note its addition is similar to proof in measurable function.</p> Proof <p>(i) We prove for addition. For random variables \\(\\xi, \\eta\\), \\(\\forall \\alpha\\in \\mathbb{R}\\), we have</p> \\[ \\{\\xi+\\eta&lt;\\alpha\\}=\\bigcup_{r\\in \\mathbb{Q}}\\left(\\{\\xi&lt;r\\}\\cap\\{\\eta&gt;r+\\alpha\\}\\right) \\] <p>where the right side is a denumerable union of subsets in \\(\\sigma\\)-algebra, which still lies in \\(\\mathscr{F}\\) by its definition.</p> <p>Similar in measure theory, we have characteristic function. For \\(A\\subset \\Omega\\), </p> \\[ 1_A(x)=\\begin{cases} 1,\\quad x\\in A\\\\ 0,\\quad x\\notin A. \\end{cases} \\] <p>Example. Set \\(A\\subset \\Omega\\) has characteristic function \\(1_A\\), then</p> \\[ \\{1_A\\leq \\alpha\\}=\\begin{cases} \\Omega,\\quad &amp;\\alpha&gt;1,\\\\ A^c,\\quad &amp;\\alpha\\in[0,1),\\\\ \\varnothing,\\quad &amp;\\alpha&lt;0. \\end{cases} \\] <p>Notice here we use \\(\\{\\xi\\leq \\alpha\\}\\) rather than \\(\\{\\xi&gt;\\alpha\\}\\), because of practical meaning of the former one.</p> <p>Definitions of discrete random variables</p> <p>If random variable \\(\\xi\\) is distributed on a set with denumerable elements, i.e. the range of \\(\\xi\\) is denumerable, then we call \\(\\xi\\) discrete random variable, whose range is denoted by \\(R(\\xi)\\). If \\(R(\\xi)\\) has finite elements, then we call \\(\\xi\\) simple random variable. In terms of form, we have</p> \\[ \\xi=\\sum_{x\\in R(\\xi)}x 1_{\\{\\xi=x\\}}. \\]"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#distribution-function","title":"Distribution Function","text":"<p>Definition of Distribution function</p> <p>Assume \\(\\xi\\) is a random variable, then \\(\\forall x\\in \\mathbb{R}\\), </p> \\[ F_\\xi(x):=\\mathbb{P}(\\xi\\leq x) \\] <p>is called Distribution Function os \\(\\xi\\).</p> <p>Properties of Distribution Function</p> <p>Assume \\(F_\\xi\\) is a distribution function, then</p> <p>(i) \\(F_\\xi\\) monotonically increases,</p> <p>(ii) \\(F_\\xi\\) is right continuous,</p> <p>(iii) \\(\\lim\\limits_{x\\rightarrow -\\infty}F_\\xi(x)=0,\\quad \\lim\\limits_{x\\rightarrow +\\infty}F_\\xi(x)=1\\).</p> <p>Example.  (Bernoulli Distribution). A random experiment with only two results are usually called Bernoulli Experiment. Denote the success probability of an event \\(A\\) is \\(\\mathbb{P}(A)=p\\), counterpart is \\(q=1-p\\), so the index of success \\(\\xi\\) is a random variable with distribution</p> \\[ \\left(\\begin{array}{ccc} \\xi &amp; 0&amp;1\\\\ \\mathbb{P} &amp;q &amp;p \\end{array}\\right). \\] <p>The index \\(1_A\\) of an event \\(A\\) is Bernoulli distribution. Any Bernoulli distribution must be an index of an event.</p> <p>We care more about distribution rather than random variables. Because, only defined on the same probability space, random variable \\(\\xi,\\eta\\) could chances to equal. But defined on different probability space, we could see their resemblance by checking their distribution variables, since both its demain of definition \\((R(\\xi))\\)and range \\((F_\\xi(x))\\) could be measured on \\(\\mathbb{R}\\).</p> <p>Definition of Same distribution</p> <p>Two random variables (might be defined on different probability space) \\(\\xi\\) and \\(\\eta\\) are called to have Same Distribution, if their distribution functions are of the same.</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#condition-probability","title":"Condition Probability","text":"<p>Definitions of Independence</p> <p>Events \\(A, B\\) is said to be independent, if</p> \\[ \\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\mathbb{P}(B). \\] <p>Events sequence \\(\\{A_n\\}_{n\\geq 1}\\) are said to be independent mutually, if \\(\\forall\\) finite number of events \\(\\{A_{n_j}\\}_{1\\leq j\\leq k}\\),    </p> \\[ \\mathbb{P}\\left(\\bigcap_{j=1}^k A_{n_j}\\right)=\\prod_{j=1}^k \\mathbb{P}(A_{n_j}). \\] <p>Random variables \\(\\{\\xi_i\\}_{1\\leq i\\leq n}\\) are said to be independent mutually, if \\(\\forall x_i\\in \\mathbb{R} (1\\leq i\\leq n)\\), </p> \\[ \\mathbb{P}(\\xi_1\\leq x_1,\\cdots, \\xi_n\\leq x_n)=\\prod_{i=1}^n \\mathbb{P}(\\xi_i\\leq x_i). \\] <p>Definitions of Conditional Probability</p> <p>Assume \\((\\Omega, \\mathscr{F}, \\mathbb{P})\\) is a probability space, \\(A,B\\in \\mathscr{F}\\), and \\(\\mathbb{P}(A)&gt;0\\). Conditional probability of \\(B\\) on \\(A\\) is </p> \\[ \\mathbb{P}(B|A):=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(A)}. \\] <p>which is a mapping \\(B\\mapsto \\mathbb{P}(B|A)\\), is a probability on \\((\\Omega, \\mathscr{F})\\), and also a probability on a shrinked space \\((A,A\\cap \\mathscr{F})\\), where \\(A\\cap \\mathscr{F}\\) is a \\(\\sigma\\)-algebra on \\(A\\).</p> <p>Properties of Conditional Probability</p> <p>(i) Random variables \\(\\xi_1,\\cdots,\\xi_n\\) are mutually independent, iff \\(\\forall x_i\\leq y_i, 1\\leq i\\leq n\\), </p> \\[ \\mathbb{P}(x_1&lt;\\xi_1\\leq y_1,\\cdots,x_n&lt;\\xi_n\\leq y_n)=\\mathbb{P}(x_1&lt;\\xi_1\\leq y_1)\\cdots\\mathbb{P}(x_n&lt;\\xi_n\\leq y_n). \\] <p>(ii) If the random variables are discrete, then they are independent iff</p> \\[ \\mathbb{P}(\\xi_1=x_1,\\cdots,\\xi_n=x_n)=\\mathbb{P}(\\xi_1=x_1)\\cdots\\mathbb{P}(\\xi_n=x_n). \\] <p>(iii) \\(\\mathbb{P}((C|B)|A)=\\mathbb{P}(C|A\\cap B)\\).</p> <p>Example. Toss coin infinite times could be achieved  is equivalent to uniform distribution could be achieved.</p> Proof <ul> <li>\\(\\Rightarrow\\). If random variable \\(\\xi\\) defined on probability space \\((\\Omega, \\mathscr{F},\\mathbb{P})\\) is uniformly distributed on \\([0,1]\\). Then denote its \\(n\\) bit of number as \\(\\xi_n\\), with \\(\\xi_n \\in {0,1}\\). Then the first \\(n\\) bits number could fall into an interval with length \\(\\frac{1}{2^n}\\), i.e.</li> </ul> \\[ \\mathbb{P}(\\xi_1=a_1,\\cdots,\\xi_n=a_n)=\\frac{1}{2^n} \\] <p>which means \\(\\{\\xi_n\\}_{n\\geq 1}\\) are mutually independent, demonstrating that it described coin-toss problem.</p> <ul> <li>\\(\\Leftarrow\\). If random variables \\(\\{\\xi_n\\}_{n\\geq 1}\\) defined on \\((\\Omega, \\mathscr{F},\\mathbb{P})\\) which are mutually independent. Then define</li> </ul> \\[ \\xi:=\\sum_{n=1}^\\infty\\frac{\\xi_n}{2^n}. \\] <p>Then \\(\\xi\\) is a binary number. Then \\(\\forall n\\geq 1, 0\\leq k\\leq 2^n-1\\), we have</p> \\[ \\mathbb{P}(\\xi\\in [\\frac{k}{2^n},\\frac{k+1}{2^n}])=\\frac{1}{2^n}, \\] <p>meaning \\(\\xi\\) is a uniform distribution. <p>\\(\\square\\)</p></p> <p>Total Probability Formula</p> <p>Assume events \\(\\{\\Omega_n\\}_{n\\geq 1}\\subset \\mathscr{F}\\) is a partition of \\(\\Omega\\), then \\(\\forall A\\in \\mathscr{F}\\), </p> \\[ \\mathbb{P}(A)=\\sum_{n\\geq 1}\\mathbb{P}(A\\cap \\Omega_n)=\\sum_{n\\geq 1}\\mathbb{P}(A|\\Omega_n)\\mathbb{P}(\\Omega_n). \\] Proof <p>Since \\(A=A\\cap \\Omega=\\bigcup_{n\\geq 1}(A\\cap\\Omega_n)\\), so by sigma-additivity</p> \\[ \\mathbb{P}(A)=\\mathbb{P}\\left(\\bigcup_{n\\geq 1}(A\\cap\\Omega_n)\\right)=\\sum_{n\\geq 1}\\mathbb{P}(A\\cap\\Omega_n)=\\sum_{n\\geq 1}\\mathbb{P}(A|\\Omega_n)\\mathbb{P}(\\Omega_n). \\] <p><p>\\(\\square\\)</p></p> <p>Definition of Bayes Formula</p> <p>If \\(A\\) happens, then we could calculate probability of every categories \\(\\Omega_n\\)</p> \\[ \\mathbb{P}(\\Omega_n|A)=\\frac{\\mathbb{P}(A|\\Omega_n)\\mathbb{P}(\\Omega_n)}{\\mathbb{P}(A)}. \\] <p>where \\(\\mathbb{P}(A)\\) is calculated using Total probability formula. The above formula is called Bayes Formula.</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#mathematical-expectation","title":"Mathematical Expectation","text":"<p>This part we also have a similar path as we have in Lebesgue Integral.</p> <p>Definitions of ME for simple RV</p> <p>(i) Assume \\(\\xi\\) is a simple random variable</p> \\[ \\xi=\\sum_{x\\in R(\\xi)}x\\cdot 1_{\\{\\xi=x\\}} \\] <p>define its mathematical expectation as a weighted average</p> \\[ \\mathbb{E}\\xi=\\sum_{x\\in R(\\xi)}x\\mathbb{P}(\\xi=x). \\] <p>Actually, it is Lebesgue integral of simple function.</p> <p>Region irrelevance of ME for simple RV</p> <p>If \\(x_1,\\cdots,x_n\\in \\mathbb{R}\\), and \\(\\Omega_1,\\cdots,\\Omega_n\\) are finite partition of \\(\\Omega\\), then</p> \\[ \\mathbb{E}\\left(\\sum_{i=1}^n x_i 1_{\\Omega_i}\\right)=\\sum_{i=1}^nx_i\\mathbb{P}(\\Omega_i). \\] <p>Note here \\(\\sum\\limits_{i=1}^n\\mathbb{P}(\\Omega_n)=1\\).</p> Proof <p>Use the intersection to achieve transition from partition of range to partition os definition domain. Let \\(\\xi=\\sum_i x_i 1_{\\Omega_i}\\), then</p> \\[ \\begin{align*} \\mathbb{E}\\xi&amp;=\\sum_{y\\in R(\\xi)}y\\mathbb{P}(\\xi=y)\\\\ &amp;=\\sum_{y\\in R(\\xi)} y\\sum_{i=1}^n \\mathbb{P}(\\xi=y, \\Omega_i)\\\\ &amp;=\\sum_{i=1}^n\\sum_{y\\in R(\\xi)} y \\mathbb{P}(\\xi=y, \\Omega_i)\\\\ &amp;=\\sum_{i=1}^n x_i \\mathbb{P}(\\xi=x_i, \\Omega_i)\\quad \\text{ for } j\\neq i, \\mathbb{P}(\\xi=x_j, \\Omega_i)=0\\\\ &amp;=\\sum_{i=1}^n x_i \\mathbb{P}(\\Omega_i). \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p> </p> <p>Properties of simple RV</p> <p>Assume \\(\\xi,\\eta\\) are simple random variables,</p> <p>(i) if \\(\\xi\\geq 0\\), then \\(\\mathbb{E}\\xi\\geq 0\\).</p> <p>(ii) Homogeneity. \\(\\forall a\\in \\mathbb{R}\\), \\(\\mathbb{E}(a\\xi)=a\\mathbb{E}\\xi\\).</p> <p>(iii) Linearity. \\(\\mathbb{E}(\\xi+\\eta)=\\mathbb{E}(\\xi)+\\mathbb{E}(\\eta)\\).</p> <p>(iv) Characteristic function. If \\(A\\in \\mathscr{F}\\), then \\(\\mathbb{E}1_A=\\mathbb{P}(A)\\).</p> <p>(v) Zero a.s.. If \\(\\mathbb{P}(\\xi\\neq 0)=0\\), then \\(\\mathbb{E}\\xi=0\\).</p> <p>(vi) Monotonicity. If \\(\\xi\\leq \\eta\\), then \\(\\mathbb{E}\\xi\\leq \\mathbb{E}\\eta\\).</p> <p>(vii) Independence property. If \\(\\xi\\) and \\(\\eta\\) are independent, then \\(\\mathbb{E}(\\xi\\cdot \\eta)=\\mathbb{E}\\xi\\cdot\\mathbb{E}\\eta\\).</p> <p>Corollary: using linearity</p> <p>Choose arbitrary events \\(\\{A_k\\}_{1\\leq k\\leq n}\\) and real number \\(\\{x_k\\}_{1\\leq k\\leq n}\\), then </p> \\[ \\mathbb{E}\\left(\\sum_{k=1}^n x_k1_{A_k}\\right)=\\sum_{k=1}^n x_k\\mathbb{P}(A_k). \\] <p>Here comes the mathematical expectation of non-negative random variables.</p> <p>Definition of ME for Non-negative random variables</p> <p>Assume \\(\\xi\\) is a non-negative randon variable, define is Mathematical Expectation to be </p> \\[ \\mathbb{E}\\xi=\\sup\\{\\mathbb{E}\\eta: 0\\leq\\eta\\leq \\xi, \\eta\\text{ is a simple RV}\\}. \\] <p>If \\(\\mathbb{E}\\xi&lt;\\infty\\), we call \\(\\xi\\) is integrable(L). If \\(A\\) is an event, then we use \\(\\mathbb{E}(\\xi; A)\\) to denote \\(\\mathbb{E}(\\xi\\cdot 1_A)\\), the ME of \\(\\xi\\) limited on event \\(A\\). </p> <p>Readers could compare the following contents with similar results in Lebesgue Integral.</p> <p></p> <p>L\u00e9vi Monotonic Convergence Theorem</p> <p>(i) If random variables \\(\\xi,\\eta\\) satisfy \\(0\\leq \\eta\\leq \\xi\\), then \\(\\mathbb{E}\\eta\\leq\\mathbb{E}\\xi\\).</p> <p>(ii) If a sequence of non-negative random variables \\(\\{\\xi_n\\}_{n\\geq 1}\\) monotinically increase and converge to \\(\\xi\\), then \\(\\lim\\limits_{n\\rightarrow\\infty}\\mathbb{E}\\xi_n=\\mathbb{E}\\xi\\).</p> <p>(iii) Non-negative random variables could always be represented by the limit of s sequence of non-negative random variables.</p> Proof <p><p>\\(\\square\\)</p></p> <p>We use \\(\\mathscr{F}_+\\) to denote all the non-negative random variables on \\((\\Omega, \\mathscr{F})\\), then we know \\(\\mathbb{E}\\) is a function defined on \\(\\mathscr{F}_+\\).</p> <p>Relationship between Probability and Mathematical Expectation</p> <p>By definition of non-negative random variables and L\u00e9vi Theorem, we have the following results.</p> <p>(i) Additivity of random variables. Assume \\(\\{\\xi_n\\}_{n\\geq 1} \\subset \\mathscr{F}_+\\) , then </p> \\[ \\mathbb{E}\\sum_n\\xi_n=\\sum_n \\mathbb{E}\\xi_n. \\] <p>(ii) Characteristic function. \\(\\forall A\\in \\mathscr{F}\\), we have \\(\\mathbb{E}1_A=\\mathbb{P}(A)\\). </p> <p>Conversely, if there exists a non-negative function \\(\\mathbb{E}\\) defined on \\(\\mathscr{F}_+\\), such that it satisfies additivity and \\(\\mathbb{E}1=1\\), then \\(\\forall A\\in \\Omega\\), we could use \\(\\mathbb{E}\\) to define probability measure</p> \\[ \\mathbb{P}(A)=\\mathbb{E}1_A. \\] <p>Definitions of ME for genaral random variables</p> <p>For a general random variable \\(\\xi\\), we separate it into positive add negetive parts (non-negative random variables)</p> \\[ \\xi^+=\\xi\\cdot 1_{\\{\\xi&gt;0\\}}=\\max\\{\\xi,0\\} ,\\quad  \\xi^-=\\xi\\cdot 1_{\\{\\xi&lt;0\\}}=\\max\\{-\\xi,0\\}. \\] <p>Then \\(\\xi=\\xi^+-\\xi^-\\), \\(|\\xi|=\\xi^+ +\\xi^-\\). If \\(\\mathbb{E}|\\xi|&lt;\\infty\\), then we call \\(\\xi\\) is integrable. If \\(\\xi\\) is integrable, then we could define is Mathematical Expectation</p> \\[ \\mathbb{E}\\xi=\\mathbb{E}\\xi^+ -\\mathbb{E}\\xi^-. \\] <p>Properties of non-negative random variables</p> <p>Almost the same as (i) (ii) (iii) in Properties of simple RV.</p> <p>(iv) Independence. Assume \\(\\xi,\\eta\\) are independent. If they are non-negative, or they are integrable and their multiplication \\(\\xi\\eta\\) is also integrable, then \\(\\mathbb{E}\\xi\\eta=\\mathbb{E}\\xi\\cdot \\mathbb{E}\\eta\\).</p> <p>Fatou Lemma</p> <p>Assume \\(\\{\\xi_n\\}_{n\\geq 1}\\) are non-negative, then \\(\\mathbb{E}\\lim\\inf_n\\xi_n\\leq \\lim\\inf_n\\mathbb{E}\\xi_n\\).</p> <p>Lebesgue's Dominated Convergence Theorem</p> <p>Assume \\(\\{\\xi_n\\}_{n\\geq 1}\\) satisfies \\(\\lim\\limits_{n\\rightarrow\\infty}\\xi_n=\\xi\\), and there exists an integrable non-negative simple function \\(\\eta\\) such that \\(|\\xi_n|\\leq \\eta\\), then \\(\\mathbb{E}\\xi=\\lim\\limits_{n\\rightarrow \\infty}\\mathbb{E}\\xi_n\\).</p> <p>Similar to convergence almost everywhere, we have an allege holds almost surely, denoted by \\(a.s.\\). Like \\(\\xi=0, a.s.\\) means \\(\\mathbb{P}(\\xi\\neq 0)=0\\).</p> <p>Properties of general Random Variables</p> <p>Assume \\(\\xi\\) is a random variable on probability space \\((\\Omega,\\mathscr{F},\\mathbb{P})\\), </p> <p>(i) If \\(\\xi=0, a.s.\\), then \\(\\xi\\) is integrable, and \\(\\mathbb{E}\\xi=0\\);</p> <p>(ii) If \\(\\mathbb{P}(A)=0\\), then \\(\\mathbb{E}(\\xi; A)=0\\);</p> <p>(iii) If \\(\\xi\\) is integrable, then \\(\\xi=0, a.s.\\) iff \\(\\forall A\\in \\mathbb{F}\\), \\(\\mathbb{E}(\\xi;A)=0\\);</p> <p>(iv) If \\(\\xi\\) is non-negative, then \\(\\mathbb{E}\\xi=0\\) implies \\(\\xi=0, a.s.\\)</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#calculations","title":"Calculations","text":"<p>Theorem for calculating ME of discrete random variables</p> <p>A discrete random variable \\(\\xi\\) is integrable, iff </p> \\[ \\mathbb{E}(|\\xi|)=\\mathbb{E}\\left(\\sum_{x\\in R(\\xi)}|x|1_{\\{\\xi=x\\}}\\right)=\\sum_{x\\in R(\\xi)}|x|\\mathbb{P}(\\xi=x)&lt;\\infty \\] <p>and holds</p> \\[ \\mathbb{E}(\\xi)=\\sum_{x\\in R(\\xi)}x\\mathbb{P}(\\xi=x)&lt;\\infty. \\] Proof <p>Using Levy Theorem. Note \\(\\xi&gt;0\\), \\(R(\\xi)=\\{x_n\\}_{n\\geq 1}\\), so \\(\\xi_n=\\sum_{i=1}^n x_i 1_{\\{\\xi=x_i\\}}\\nearrow \\xi\\).</p> <p>Corollary: ME for function of discrete random variable</p> <p>Assume \\(\\xi\\) is a discrete random variable, and \\(\\phi:\\mathbb{R}\\rightarrow \\mathbb{R}\\). Then \\(\\phi(\\xi)\\) is integrable, iff</p> \\[ \\mathbb{E}|\\phi(\\xi)|=\\sum_{x\\in R(\\xi)}|\\phi(x)|\\mathbb{P}(\\xi=x)&lt;\\infty, \\] <p>and </p> \\[ \\mathbb{E}\\phi(\\xi)=\\sum_{x\\in R(\\xi)}\\phi(x)\\mathbb{P}(\\xi=x). \\] Proof <p>Easy to show that \\(\\phi(\\xi)\\) is also a discrete random variable. \\(\\{\\phi^{-1}(\\{y\\}): y\\in R(\\phi(\\xi))\\}\\) is a partition of \\(R(\\xi)\\). So for \\(y\\in R(\\phi(\\xi))\\),</p> \\[ \\mathbb{P}(\\phi(\\xi)=y)=\\sum_{x\\in \\phi^{-1}(y)}\\mathbb{P}(\\xi=x). \\] <p>Since </p> \\[ \\begin{align*} \\sum_{x\\in R(\\xi)}|\\phi(x)|\\mathbb{P}(\\xi=x)&amp;=\\sum_{y\\in R(\\phi(\\xi))}\\sum_{x\\in \\phi^{-1}(y)}|\\phi(x)|\\mathbb{P}(\\xi=x)\\quad \\text{ using partition of }R(\\xi)\\\\ &amp;=\\sum_{y\\in R(\\phi(\\xi))}|y| \\sum_{x\\in \\phi^{-1}(y)}\\mathbb{P}(\\xi=x)\\\\ &amp;=\\sum_{y\\in R(\\phi(\\xi))}|y|\\mathbb{P}(\\phi(\\xi)=y)&lt;\\infty. \\end{align*} \\] <p>So actual integral without absolute sign, would follow inversely.</p> <p><p>\\(\\square\\)</p></p> <p>Theorem for calculation of general random variables</p> <p>Assume \\(\\xi\\) is a random variable, \\(F\\) is its distribution function. If \\(\\phi\\) is non-negaitve continuous or bounded continuous function, then </p> \\[ \\mathbb{E}\\phi(\\xi)=\\int_\\mathbb{R}\\phi(x)dF(x). \\] Proof <p>Assume \\(\\phi\\geq 0\\). We first prove in finite intervals \\([a,b]\\). </p> <p>(Actually we could only prove in measurable function) Assume $\\Delta: a=x_1&lt;\\cdots&lt;x_n=b $ is a partition of \\([a,b]\\), let \\(m_i=\\inf\\limits_{x\\in(x_{-1},x_{i}]}\\phi\\), \\(g^\\Delta:=\\sum\\limits_{i=1}^n m_i 1_{(x_{i-1},x_i]}\\), \\(\\lambda=\\max\\limits_{1\\leq i\\leq n}{|x_{i}-x_{i-1}|}\\). So</p> \\[ g^\\Delta\\rightarrow \\phi\\cdot 1_{(a,b]} \\quad(\\lambda\\rightarrow 0) \\] <p>so </p> \\[ g^\\Delta (\\xi) \\rightarrow \\phi(\\xi)\\cdot 1_{(a,b]}(\\xi)\\quad (\\lambda\\rightarrow 0) \\] <p>Since \\(g^\\Delta (\\xi)\\) is a simple random variable, then its limit is also random variable and </p> \\[ \\begin{align*} \\mathbb{E}\\phi(\\xi)\\cdot 1_{(a,b]}(\\xi)&amp;=\\lim_{\\lambda\\rightarrow 0}\\mathbb{E}g^\\Delta (\\xi)\\\\ &amp;=\\lim_{\\lambda\\rightarrow 0}\\sum_{i=1}^n m_i \\mathbb{E} 1_{(x_{i-1},x_i]}(\\xi)\\\\ &amp;=\\lim_{\\lambda\\rightarrow 0} \\sum_{i=1}^n m_i \\mathbb{P}(x_{i-1}&lt;\\xi\\leq x_i)\\\\ &amp;=\\lim_{\\lambda\\rightarrow 0} \\sum_{i=1}^n m_i (F(x_i)-F(x_{i-1}))=\\int_{(a,b]} \\phi(x)dF(x). \\end{align*} \\] <p>Then for \\(\\mathbb{R}\\), we use \\(\\phi(\\xi)\\cdot 1_{(-n,n]}(\\xi)\\nearrow \\phi(\\xi)\\), and let \\(n\\rightarrow \\infty\\).</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#variance","title":"Variance","text":"<p>The following inequation is essentially the same as we have discussed in Real analysis. </p> <p>Chebyshev Inequation (Markov Inequation)</p> <p>Assume \\(\\xi\\) is a random variable, \\(\\alpha&gt;0\\), then \\(\\forall m&gt;0\\), we have</p> \\[ \\mathbb{P}(|\\xi|&gt;m)\\leq \\frac{1}{m^\\alpha}\\mathbb{E}|\\xi|^\\alpha.  \\] Proof <p>By definition</p> \\[ \\mathbb{E}|\\xi|^\\alpha\\geq \\mathbb{E}(|\\xi|^\\alpha:|\\xi|&gt;m)\\geq m^\\alpha \\mathbb{P}(|\\xi|&gt;m). \\] <p>Cauchy-Schwarz Inequation</p> <p>Assume \\(\\xi,\\eta\\) are random variables, then</p> \\[ |\\mathbb{E}\\xi\\eta|^2\\leq \\mathbb{E}\\xi^2\\cdot \\mathbb{E}\\eta^2. \\] Proof <p>Just as the most popular method, i.e. using quadratic function.</p> <p>If \\(\\mathbb{E}\\xi^2&lt;\\infty\\), then \\(\\xi\\) is square integrable, and implies \\(\\xi\\) is integrable. Here we could define its Variance as</p> \\[ D\\xi:=\\mathbb{E}(\\xi-\\mathbb{E}\\xi)^2. \\] <p>Properties of Variance</p> <p>(i) \\(D\\xi\\geq 0\\). \\(D\\xi=0\\) iff \\(\\xi=C, a.s.\\)</p> <p>(ii) Calculation.\\(D\\xi=\\mathbb{E}\\xi^2-(\\mathbb{E}\\xi)^2\\).</p> <p>(iii) \\(D(a\\xi)=a^2D\\xi\\).</p> <p>(iv) If \\(\\xi,\\eta\\) are independent, then \\(D(\\xi+\\eta)=D\\xi+D\\eta\\).</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#bernoullis-law-of-large-numbers","title":"Bernoulli's Law of large numbers","text":"<p>We could now discuss the relationship between probability and frequency.</p> <p>Bernoulli's Law of large numbers</p> <p>Assume we carry out a Bernoulli experiment with rate of success \\(p\\), and use \\(\\xi_n\\) to denote the number of success within first \\(n\\) times experiments. So \\(\\frac{\\xi_n}{n}\\) is the frequency of first \\(n\\) times experiments, which is also a random variable. Then \\(\\forall \\varepsilon&gt;0\\), </p> \\[ \\lim_{n\\rightarrow\\infty}\\mathbb{P}\\left(\\left|\\frac{\\xi_n}{n}-p\\right|&gt;\\varepsilon\\right)=0. \\] <p>which means that \\(\\eta_n=\\frac{\\xi_n}{n}\\) converges to \\(p\\) in a sense of probability.</p> Proof <p>Using Chebyshev Inequation.</p> \\[ \\begin{align*} \\mathbb{P}\\left(\\left|\\frac{\\xi_n}{n}-p\\right|&gt;\\varepsilon\\right)\\leq \\frac{1}{\\varepsilon^\\alpha}\\mathbb{E}\\left|\\frac{\\xi_n}{n}-p\\right|^\\alpha \\end{align*} \\] <p>Choose \\(\\alpha=2\\), and by \\(\\mathbb{E}\\xi_n=np\\), \\(\\mathbb{E}\\xi_n^2=np(1-p)+(np)^2\\)</p> \\[ \\begin{align*} \\mathbb{P}\\left(\\left|\\frac{\\xi_n}{n}-p\\right|&gt;\\varepsilon\\right)&amp;\\leq \\frac{1}{\\varepsilon^2}\\mathbb{E}\\left|\\frac{\\xi_n}{n}-p\\right|^2\\\\ &amp;=\\frac{1}{\\varepsilon^2}\\frac{\\mathbb{E}(\\xi_n^2)-2np\\mathbb{E}\\xi_n+(np)^2}{n^2}\\\\ &amp;=\\frac{1}{\\varepsilon^2}\\frac{p-p^2}{n}\\rightarrow 0(n\\rightarrow \\infty). \\end{align*} \\] <p>Total probability formula has a expectation form.</p> <p>Conditional Mathematical Expectation</p> <p>Assume integrable random variable \\(\\xi_n\\) is defined on probability space \\((\\Omega,\\mathscr{F}, \\mathbb{P})\\). For an arbitrary event \\(A\\) with positive probability, define Conditional Mathematical Expectation</p> \\[ \\mathbb{E}(\\xi|A):=\\frac{\\mathbb{E}(\\xi;A)}{\\mathbb{P}(A)}. \\] <p>So with the above definition, we have an extension of total probability formula. </p> <p>Expectation form of Total probability formula</p> <p>Assume \\(\\{\\Omega_n\\}_{n\\geq 1}\\subset \\mathscr{F}\\) is a paritition of \\(\\Omega\\), then for all random variable \\(\\xi\\), we have</p> \\[ \\mathbb{E}(\\xi)=\\sum_{n=1}^\\infty \\mathbb{E}(\\xi;\\Omega_n)\\mathbb{P}(\\Omega_n). \\]"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#continuous-random-variables","title":"Continuous Random Variables","text":""},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#measurability","title":"Measurability","text":"<p>In a discrete sample space \\(\\Omega_d\\), we could choose its power set as its \\(\\sigma\\)-algebra, but in continuous sample space \\(\\Omega_c\\), it has been proved contradicted when we choose its power set. In fact, apart from the smallest and biggest family of set, it is usually not easy to find a \\(\\sigma\\)-algebra directly, but often generated indirectly.</p> <p>Properties of \\(\\sigma\\)-algebra</p> <p>Assume \\(\\{\\mathscr{F}_\\lambda\\}_{\\lambda\\in\\Lambda}\\) are \\(\\sigma\\)-algebra on \\(\\Omega\\), then </p> \\[ \\bigcap_{\\lambda\\in \\Lambda}\\mathscr{F}_\\lambda \\] <p>is also a \\(\\sigma\\)-algebra. </p> <p></p> <p>Definition of \\(\\sigma\\)-algebra generated</p> <p>For a family set of \\(\\Omega\\), denoted by \\(\\mathscr{A}\\), define \\(\\sigma\\)-algebra generated by \\(\\mathscr{A}\\) to be the intersection of family sets of \\(\\Omega\\) which contain \\(\\mathscr{A}\\), denoted by</p> \\[ \\sigma(\\mathscr{A}):=\\bigcap_{\\lambda\\in \\Lambda\\atop \\mathscr{A}\\subset \\mathscr{F}_\\lambda}\\mathscr{F}_\\lambda. \\] <p>Example. Still the Discrete-exmaple, we could write its generated \\(\\sigma\\)-algebra</p> \\[ \\sigma(\\{\\Omega_n\\}_{n\\geq 1})=\\left\\{\\bigcup_{i\\in I}\\Omega_i: I\\subset \\mathbb{N}\\right\\}. \\] <p>To go through the following theorem, we have to introduce some properties of inverse image. That is, assume \\(f:X\\rightarrow Y\\) is a mapping, \\(X\\) and \\(Y\\) are domain of definition and range. If \\(B\\subset Y\\), define inverse image</p> \\[ f^{-1}(B)=\\{x\\in X:f(x)=B\\}. \\] <p>which is also denoted by \\(\\{f\\in B\\}\\).</p> <p></p> <p>Properties of Inverse Image</p> <p>Assume \\(f:X\\rightarrow Y\\) is a mapping, then </p> <p>(i) \\(f^{-1}(\\varnothing)=\\varnothing\\), \\(f^{-1}(Y)=X\\).</p> <p>(ii) If \\(B\\subset Y\\), then \\(f^{-1}(B^c)=f^{-1}(B)^c\\).</p> <p>(iii) If \\(\\{B_\\lambda\\}_{\\lambda\\in\\Lambda}\\subset Y\\), then </p> \\[ \\begin{align*} f^{-1}\\left(\\bigcap_{\\lambda\\in\\Lambda}B_\\lambda\\right)&amp;=\\bigcap_{\\lambda\\in\\Lambda}f^{-1}\\left(B_\\lambda\\right)\\\\ f^{-1}\\left(\\bigcup_{\\lambda\\in\\Lambda}B_\\lambda\\right)&amp;=\\bigcup_{\\lambda\\in\\Lambda}f^{-1}\\left(B_\\lambda\\right). \\end{align*} \\] <p>With the following properties, we have the following theorem.</p> <p>Theorem for high-dimension \\(\\sigma\\)-Algebra</p> <p>Assume \\(\\xi:\\Omega\\rightarrow \\mathbb{R}^n\\), and a family set \\(\\mathscr{B}_0\\). If \\(B\\subset \\mathbb{R}^n\\), define </p> \\[ \\xi^{-1}(\\mathscr{B}_0)=\\{\\xi^{-1}(B): B\\in \\mathscr{B}_0\\} \\] <p>to be the inverse of family set \\(\\mathscr{B}_0\\). Then if \\(\\mathscr{B}_0\\) is a \\(\\sigma\\)-algebra, then \\(\\xi^{-1}(\\mathscr{B}_0)\\) is also a \\(\\sigma\\)-algebra.</p> Proof <p>Using the properties of inverse, and transform the problem into image.</p> <p>Sufficient and necessary condition for random variable</p> <p>Assume \\((\\Omega,\\mathscr{F}, \\mathbb{P})\\) is a probability space, then \\(\\xi\\) defined on \\(\\Omega\\) is a random variable, iff</p> \\[ \\xi^{-1}(\\mathscr{B})\\subset \\mathscr{F}. \\] Proof <ul> <li>\"\\(\\Leftarrow\\)\".</li> </ul> <p>Just check the condition for \\(\\xi\\) to be a random variable.</p> <ul> <li>\"\\(\\Rightarrow\\)\".</li> </ul> <p>Denote \\(\\mathscr{B}'\\) as the whole set \\(B\\subset \\mathbb{R}\\), which satisfies \\(\\{\\xi\\in B\\}\\in \\mathscr{F}\\), i.e.</p> \\[ \\mathscr{B}'=\\{B\\subset \\mathbb{R}: \\{\\xi\\in B\\}\\in \\mathscr{F}\\}. \\] <p>Assume \\(\\xi\\) is a random variable. then by its definition, we have </p> \\[ \\{(-\\infty, x]: x\\in \\mathbb{R}\\}\\subset \\mathscr{B}' \\] <p>If \\(\\mathscr{B}'\\) is a \\(\\sigma\\)-algebra, then by another configuration of Borel algebra, then \\(\\mathscr{B}'\\) it is a \\(\\sigma\\)-algebra containing \\(\\mathscr{B}\\), then \\(\\xi^{-1}(\\mathscr{B})\\subset \\mathscr{F}\\).</p> <p>Check that</p> <p>(i) total space \\(\\mathbb{R}\\in\\mathscr{B}'\\), since \\(\\xi^{-1}(\\mathbb{R})=\\Omega\\in \\mathscr{F}\\), so here \\(B=\\mathbb{R}\\in \\mathscr{B}'\\).</p> <p>(ii) Assume \\(A\\in \\mathscr{B}'\\), then by definition of \\(\\mathscr{B}'\\), we have \\(\\xi^{-1}(A)\\in \\mathscr{F}\\), then by operations of inverse mapping, we have </p> \\[ \\xi^{-1}(A^c)=(\\xi^{-1}(A))^c\\in \\mathscr{F}. \\] <p>So here \\(B=A^c\\in \\mathscr{B}'\\).</p> <p>(iii) Assume \\(\\{A_n\\}_{n\\geq 1}\\in \\mathscr{B}'\\), then \\(\\xi^{-1}(A_n)\\in \\mathscr{F}\\), then by operations of inverse mapping, we have</p> \\[ \\xi^{-1}\\left(\\bigcup_n A_n\\right)=\\bigcup_n \\xi^{-1}(A_n)\\in\\mathscr{F}. \\] <p>So here \\(B=\\bigcup_n A_n\\in \\mathscr{F}\\).</p> <p>In a nutshell, combined with (i) (ii) and (iii), we have \\(\\mathscr{B}'\\) is a \\(\\sigma\\)-algebra.</p> <p>From the above deduction, we could see that \\(\\xi\\) is a random variable, iff there exists any family set \\(\\mathscr{A}\\) generated by \\(\\mathscr{B}\\), such that</p> \\[ \\xi^{-1}(\\mathscr{A})\\in \\mathscr{F}. \\] <p>So if \\(\\xi\\) is a random variable on \\(\\sigma\\)-algebra \\(\\mathscr{A}\\), then it must be a random variable on any \\(\\sigma\\)-algebra \\(\\mathscr{A}'\\supset \\mathscr{A}\\). From these logic, we could see that there exists a smallest \\(\\sigma\\)-algebra denoted by \\(\\sigma(\\xi)\\), such that \\(\\xi\\) is a random variable on \\(\\sigma(\\xi)\\). It is easy to see that </p> \\[ \\sigma(\\xi)=\\xi^{-1}(\\mathscr{B}). \\] <p>Definition of Borel measurable function</p> <p>For a special case, \\(\\Omega=\\mathbb{R}\\), we call \\(\\xi=f\\) is a Borel measurable function, if \\(\\forall x\\in \\mathbb{R}\\), </p> \\[ \\{f\\leq x\\}\\in \\mathscr{B}. \\] <p>It is natural to have the following properties.</p> <p>Properties of Borel measurable function</p> <p>(i) Assume \\(f\\in C(\\mathbb{R})\\) is Borel measurable function.</p> <p>(ii) Assume \\(\\xi\\) is a random variable, \\(f\\) is a Borel measurable function, then \\(f(\\xi)\\) is a random variable.</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#achievement-of-distribution-function","title":"Achievement of Distribution function","text":"<p>With the help of Borel algebra, we have the following extra properties of distribution function.</p> <p>Properties of Distribution Function</p> <p>(i) \\(\\forall a&lt;b\\), \\(\\mathbb{P}(\\xi\\in (a,b])=F(b)-F(a)\\).</p> <p>(ii) \\(\\forall x\\in \\mathbb{R}\\), \\(\\mathbb{R}(\\xi=x)=F(x)-F(x^-)\\).</p> <p>(iii) \\(\\forall x\\in\\mathbb{R}\\), \\(\\mathbb{P}(\\xi&gt;x)=1-F(x)\\).</p> <p>(iv) Two distribution functions \\(F(x)=G(x),\\quad \\forall x\\in \\mathbb{R}\\), iff they are the same on a dense subset of \\(\\mathbb{R}\\).</p> <p>Theorem for achievement of DF</p> <p>An arbitrary distribution function \\(F\\) on \\(\\mathbb{R}\\) could be achieved.</p> Proof <p>Using generalized inverse. The above proof is not applicable to multi-dimension distribution function.</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#density-function","title":"Density Function","text":"<p>Continuous DF &amp; Density Function</p> <p>A distribution function \\(F\\) is said to be continuous, if there exists a non-negative L integrable function \\(f\\), such that \\(\\forall x\\in \\mathbb{R}\\),</p> \\[ F(x)=\\int_{-\\infty}^x f(t)dt. \\] <p>and \\(f\\) is called the Density Function. In this case, we also call its corresponding random variable \\(\\xi\\) to be continuous.</p> <p>Readers could see that \\(F\\) is absolute continuous, by properties of indefinite integral. Density function is not unique, since they could differ in zero-measure sets.</p> <p>Calculation for ME of Continuous distribution</p> <p>Assume \\(F\\) is a continuous distribution function of random variable \\(\\xi\\), with its density function \\(f\\), then for a non-negative continuous or bounded continuous function \\(\\phi(\\xi)\\), </p> \\[ \\mathbb{E}\\phi(\\xi)=\\int_\\mathbb{R}\\phi(x)f(x)dx. \\]"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#random-vector","title":"Random Vector","text":"<p>Definition of joint distribution function</p> <p>A joint distribution function of random vector \\(X=(\\xi_1,\\cdot,\\xi_n)\\) is defined by</p> \\[ F_X(x_1,\\cdots,x_n)=\\mathbb{P}(\\xi_1\\leq x_1,\\cdots,\\xi_n\\leq x_n). \\] <p>if \\(\\phi\\) is a continuous function on \\(\\mathbb{R}^n\\), and \\(\\phi(X)\\) is integrable, then </p> \\[ \\mathbb{E}\\phi(X)=\\int_{\\mathbb{R}^n}\\phi(x_1,\\cdots,x_n)d_n F(x_1,\\cdots,x_n). \\] <p>Properties of joint distribution function</p> <p>(i) Easy to see that marginal distribution function of one random variable</p> \\[ F_{\\xi_i}(x_i)=F_X(\\infty,\\cdots,x_i,\\cdots,\\infty). \\] <p>(ii) If \\(\\xi_1,\\cdots,\\xi_n\\) are mutually independent, then </p> \\[ F_X(x_1,\\cdots,x_n)=F_{\\xi_1}(x_1)\\cdots F_{\\xi_n}(x_n). \\]"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#covariance","title":"Covariance","text":"<p>Definition of Covariance</p> <p>Assume \\(\\xi, \\eta\\) are two random variables, and their Covariance is defined by</p> \\[ \\text{cov}(\\xi,\\eta):=\\mathbb{E}[(\\xi-\\mathbb{\\xi})(\\eta-\\mathbb{E}\\eta)]=\\mathbb{E}\\xi\\eta-\\mathbb{E}\\xi\\cdot\\mathbb{E}\\eta. \\] <p>Properties of Covariance</p> <p>Assume \\(\\xi, \\eta\\) are two random variables, then</p> <p>(i) \\(\\text{cov}(\\xi,\\xi)=D\\xi\\geq 0\\);</p> <p>(ii) \\(\\text{cov}(\\xi,\\eta)=\\text{cov}(\\eta,\\xi)\\);</p> <p>(iii) Linearity. \\(\\forall c_1,c_2\\in \\mathbb{R}\\), </p> \\[ \\text{cov}(c_1\\xi_1+c_2\\xi_2,\\eta)=c_1\\text{cov}(\\xi_1,\\eta)+c_2\\text{cov}(\\xi_2,\\eta). \\] <p>Corresponding to covariance, we have its normalized quantity correlation coefficient.</p> <p>Definition of Correlation Coefficient</p> <p>Assume \\(\\xi,\\eta\\) are two random variables, their Correlation Coefficient is defined by</p> \\[ \\rho(\\xi,\\eta):=\\frac{\\text{cov}(\\xi,\\eta)}{\\sqrt{D\\xi\\cdot D\\eta}}. \\] <p>when the denumerator equals \\(0\\), it is accustomed to \\(1\\).</p> <p>Properties of Correlation Coefficient</p> <p>(i) \\(|\\rho(\\xi,\\eta)|\\leq 1\\).</p> <p>(ii) \\(|\\rho(\\xi,\\eta)|=1\\), iff \\(\\xi, \\eta\\) are linearly relevant, i.e. \\(\\exists a,b,c\\in \\mathbb{R}, a,b\\neq 0\\), such that</p> \\[ \\mathbb{P}(a\\xi+b\\eta=c)=1. \\] <p>Definition of Covariance Matrix</p> <p>Assume \\(X=(\\xi_1,\\cdots,\\xi_n)\\), \\(Y=(\\eta_1,\\cdots,\\eta_m)\\) is a random vector, then define its Covariance Matrix $$to be</p> \\[ \\pmb{\\text{cov}}(X,Y):=(\\text{cov}(\\xi_i,\\eta_j))_{n\\times m}. \\] <p>For \\(Y=X\\), we have a phalanx</p> \\[ \\pmb{\\text{cov}}(X,X)=\\mathbb{E}[X^TX]-(\\mathbb{E}X)^T(\\mathbb{E}X). \\] <p>Properties of Covariance Matrix of a random vector \\(X\\)</p> <p>Assume cov\\((X,X)\\) is a covariance matrix of \\(X=(\\xi_1,\\cdots,\\xi_n)\\), then cov\\((X,X)\\) is a symmetric non-negative definite matrix.</p> Proof <p>\\(\\forall (x_1,\\cdots,x_n)\\in\\mathbb{R}^n\\), </p> \\[ \\sum_{1\\leq i,j\\leq n}x_i\\text{cov}(\\xi_i,\\xi_j)x_j=\\left(\\sum_{i=1}^nx_i\\xi_i,\\sum_{j=1}^n x_j\\xi_j\\right)=D\\left(\\sum_{i=1}^n x_i\\xi_i\\right)\\geq 0. \\]"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#function-of-random-variables","title":"Function of random variables","text":"<p>We focus on random variables with density functions. </p> <p>Fubini Theorem</p> <p>For Borel measurable function \\(h\\) defined on \\(\\mathbb{R}\\times \\mathbb{R}\\), we have</p> \\[ \\int_{\\mathbb{R}^2}h(x,y) dF_\\xi(x) dF_\\eta(y)=\\int_{\\mathbb{R}}dF_\\xi(x) \\int_{\\mathbb{R}} h(x,y)dF_\\eta(y). \\] <p>sum of random variable</p> <p>Assume \\(\\xi\\), \\(\\eta\\) are two independent random variables, with distribution function \\(F\\) and \\(G\\), then by Fubini Theorem, </p> \\[ \\mathbb{P}(\\xi+\\eta\\leq x)=\\int_\\mathbb{R}dG(v)\\int_{u+v\\leq x} dF(u)=\\int_\\mathbb{R} F(x-v)dG(v). \\] <p>If \\(\\xi\\) and \\(\\eta\\) are continuous random variables, with its density function \\(f\\), \\(g\\), then </p> \\[ \\begin{align*} \\mathbb{P}(\\xi+\\eta\\leq x)&amp;=\\int_{-\\infty}^\\infty g(v)dv\\int_{-\\infty}^{x-v} f(u) du\\\\ &amp;=\\int_{-\\infty}^\\infty g(v)dv\\int_{-\\infty}^{x} f(z-v) dz\\\\ &amp;=\\int_{-\\infty}^{x} dz \\int_{-\\infty}^\\infty g(v)f(z-v) dv \\end{align*} \\] <p>so the density function of \\(\\xi+\\eta\\) is </p> \\[ f * g (x)= \\int_{-\\infty}^\\infty g(v)f(x-v) dv. \\] <p>Actually, if only one of \\(\\xi\\) and \\(\\eta\\) are continuous, let \\(G\\) has a density function \\(g\\), then the density function of \\(\\xi+\\eta\\) is</p> \\[ \\int_{-\\infty}^\\infty g(x-v)dF(v). \\]"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#convergence-for-sequence-of-random-variables","title":"Convergence for Sequence of random variables","text":"<p>Definition of convergence</p> <p>Assume \\(\\{\\xi_n\\}_{n\\geq 1}\\) is a sequence of random variables, \\(\\xi\\) is a random variable. We call</p> <p>(i) \\(\\{\\xi_n\\}\\) converges to \\(\\xi\\) in a sense of probability, if \\(\\forall \\varepsilon&gt;0\\), </p> \\[ \\lim_{n\\rightarrow \\infty}\\mathbb{P}(\\{\\omega\\in\\Omega: |\\xi_n(\\omega)-\\xi(\\omega)|&gt;\\varepsilon\\})=0. \\] <p>which is denoted by \\(\\xi_n\\overset{p}{\\rightarrow}\\xi\\).</p> <p>(ii) \\(\\{x_n\\}\\) almost surely converges to \\(\\xi\\), if</p> \\[ \\mathbb{P}(\\{\\omega\\in\\Omega: \\lim_{n\\rightarrow\\infty}\\xi_n(\\omega)=\\xi(\\omega)\\})=1 \\] <p>or \\(\\forall \\varepsilon&gt;0\\), \\(\\exists A\\subset \\Omega\\), s.t. \\(\\mathbb{P}(A)=0\\) and</p> \\[ \\lim_{n\\rightarrow\\infty} \\xi_n(\\omega)=\\xi(\\omega),\\quad \\forall \\omega\\in \\Omega-A. \\] <p>which is denoted by \\(\\xi_n\\overset{a.s.}{\\rightarrow}\\xi\\).</p> <p></p> <p>Law of large numbers</p> <p>Assume \\(\\{\\xi_n\\}_{n\\geq 1}\\) is a sequence of random variables. Let partial summation \\(S_n=\\sum\\limits_{i=1}^n\\xi_i\\), \\(m_n=\\mathbb{E}(S_n)\\), \\(s_n^2=DS_n\\). Then \\(\\{\\xi_n\\}\\) satisfies</p> <p>(i) Law of large number, if \\(\\frac{S_n-m_n}{n}\\overset{P}{\\rightarrow} 0(n\\rightarrow \\infty)\\), i.e. \\(\\forall \\varepsilon&gt;0\\),</p> \\[ \\lim_{n\\rightarrow \\infty}\\mathbb{P}\\left(\\left\\{\\frac{\\sum\\limits_{i=1}^n\\xi_i-m_n}{n}&gt;\\varepsilon\\right\\}\\right)=0. \\] <p>(ii) Strong law of large number, if \\(\\frac{S_n-m_n}{n}\\overset{a.s.}{\\rightarrow} 0(n\\rightarrow \\infty)\\), i.e.</p> \\[ \\mathbb{P}\\left(\\left\\{\\lim_{n\\rightarrow \\infty}\\frac{\\sum\\limits_{i=1}^n\\xi_i-m_n}{n}=0\\right\\}\\right)=1. \\] <p>Borel-Cantelli Theorem</p> <p>Assume \\(\\{A_n\\}\\) is a sequence of events, </p> <p>(i) If \\(\\sum\\limits_{n=1}^\\infty\\mathbb{P}(A_n)&lt;\\infty\\), then </p> \\[ \\mathbb{P}(\\lim\\sup_nA_n)=0. \\] <p>(ii) If \\(\\{A_n\\}\\) are independent events, and \\(\\sum\\limits_{n=1}^\\infty\\mathbb{P}(A_n)=\\infty\\), then</p> \\[ \\mathbb{P}(\\lim\\sup_nA_n)=1. \\] <p>The following is the probability form of Riesz theorem.</p> <p>Riesz Theorem for Probability</p> <p>Assume \\(\\{\\xi_n\\}_{n\\geq 1}\\) is a sequence of random variables, \\(\\xi\\) is a random variable, then</p> <p>(i) If \\(\\xi_n\\overset{P}{\\rightarrow}\\xi\\), then there exists a subsequence \\(\\{\\xi_{n_k}\\}\\) such that \\(\\xi_{n_k}\\overset{a.s.}{\\rightarrow} \\xi\\).</p> <p>(ii) If \\(\\xi_{n}\\overset{a.s.}{\\rightarrow} \\xi\\), then \\(\\xi_n\\overset{P}{\\rightarrow}\\xi\\).</p> <p>The following theorem is a sufficient condition for almost surely convergence.</p> <p>Sufficient condition for almost surely convergence</p> <p>Assume \\(\\{\\xi_n\\}_{n\\geq 1}\\) is a sequence of random variables, \\(\\xi\\) is a random variable. If \\(\\forall \\varepsilon&gt;0\\), </p> \\[ \\sum_{n=1}^\\infty\\mathbb{P}(|\\xi_n-\\xi|&gt;\\varepsilon)&lt;\\infty, \\] <p>then \\(\\xi_{n}\\overset{a.s.}{\\rightarrow} \\xi\\).</p>"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#uniform-convergence","title":"Uniform Convergence","text":"<p>This is the expectation form of H\u00f6lder inequation.</p> <p>H\u00f6lder Inequation</p> <p>Assume \\(1&lt;p&lt;\\infty\\), \\(1&lt;q&lt;\\infty\\), \\(\\frac{1}{p}+\\frac{1}{q}=1\\), then</p> \\[ \\mathbb{E}|\\xi\\eta|\\leq (\\mathbb{E}|\\xi|^p)^{\\frac{1}{p}}(\\mathbb{E}|\\eta|^q)^{\\frac{1}{q}}. \\] <p>a order absolute moment</p> <p>Assume \\(\\xi\\) is a ranodm variable and \\(a&gt;0\\), we call \\(\\mathbb{E}|\\xi|^a\\) \\(a\\) order absolute moment.</p> <p>Properties of absolute moment</p> <p>Assume \\(0&lt;a&lt;b\\), then</p> \\[ (\\mathbb{E}|\\xi|^a)^{\\frac{1}{a}}\\leq (\\mathbb{E}|\\xi|^b)^{\\frac{1}{b}} \\] Proof <p>Using H\u00f6lder inequation, let \\(p=\\frac{a}{b}\\).</p> \\[ \\mathbb{E}|\\xi|^a\\cdot 1\\leq (\\mathbb{E}(|\\xi|^a)^\\frac{a}{b})^{\\frac{b}{a}}\\cdot (\\mathbb{E}(1)^{\\frac{a}{a-b}})^{\\frac{a-b}{a}} \\] <p>\\(L^r\\) space for probability</p> <p>Assume \\(\\{\\xi_n\\}_{n\\geq 1}\\) is a sequence of random variables, \\(\\xi\\) is a random variable. \\(\\{\\xi_n\\}\\) is said to converge to \\(\\xi\\) in a sense of \\(r\\) order absolute moment, or \\(L^r\\) (\\(r\\geq 1\\)), if \\(\\mathbb{E}|\\xi_n|^r\\) and \\(\\mathbb{E}|\\xi|^r\\) is finite, and </p> \\[ \\lim_{n\\rightarrow \\infty}\\mathbb{E}|\\xi_n-\\xi|^r=0, \\] <p>which is denoted by \\(\\xi_n\\overset{L^r}{\\rightarrow}\\xi\\).</p> <p>Definition of uniform convergence</p> <p>Assume integrable family of random variables \\(\\{\\xi_\\lambda\\}_{\\lambda\\in\\Lambda}\\) is uniformly convergent, if</p> \\[ \\lim_{N\\rightarrow \\infty}\\sup_\\Lambda\\mathbb{E}(|\\xi_\\lambda|; |\\xi|\\geq N)=0. \\]"},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#convergence-in-distribution","title":"Convergence in distribution","text":""},{"location":"Math/StochasticProcesses/Ele_Prob_Theo/#analysis-tool","title":"Analysis Tool","text":"<p>Probability Generating Function</p> <p>Assume the distribution sequence of discrete variable \\(\\xi\\) is \\(\\mathbb{P}(\\xi=k)=p_k\\), \\(k=0,1,\\cdots\\). We call a function of real variable \\(s\\)</p> \\[ \\psi_\\xi(s)=\\mathbb{E}(s^\\xi)=\\sum_{k=0}^\\infty p_k s^k, |s|&lt;1. \\] <p>Probability Generating Function.</p> <p>Similar to \\(z\\)-Transfer in signal analysis.</p> <p>Properties for generating function</p> <p>(i) \\(|\\psi_\\xi(s)|\\leq \\psi_\\xi(1)=1\\).</p> <p>(ii) Assume \\(\\{\\xi_i\\}_{1\\leq i\\leq n}\\) are mutually independent, and have generating function \\(\\psi_{\\xi_i}(s)\\), then \\(\\eta=\\sum_{i=1}^n \\xi_i\\) has generating function</p> \\[ \\psi_\\eta(s)=\\prod_{i=1}^n \\psi_{\\xi_i}(s). \\] <p>(iii) \\(p_k=\\mathbb{P}(\\xi=k)=\\frac{\\psi^{(k)}(0)}{k!}\\), \\(k=0,1,\\cdots\\)</p> <p> Example. Calculate the generating function of the following distribution corresponding to a random variable. <p>(i) Binary distribution \\(\\xi\\sim B(n,p)\\).</p> <p>(ii) Possion distribution \\(\\xi\\sim \\pi(\\lambda)\\). </p>"},{"location":"Math/StochasticProcesses/Markov/","title":"Markov Process &amp; chain","text":"<p>Definition of Markov Process</p> <p>Assume \\(\\{X(t): t\\in T\\}\\) is a stochastic process. If \\(\\forall n\\), \\(t_i\\in T\\), \\(i=1,2,\\cdots,n\\), and \\(t_1&lt;\\cdots&lt;t_n\\), we have state \\(x_1,\\cdots,x_n\\in S\\), the conditional probability \\(X(t_n)\\) satisfies</p> \\[ \\begin{align*} P&amp;(X(t_n)&lt;x_n\\mid X(t_0)=x_0,\\cdots X(t_{n-1})=x_{n-1})\\\\&amp;=P(X(t_n)&lt;x_n\\mid X(t_{n-1})=x_{n-1}),\\quad x_n\\in \\mathbb{R}, \\end{align*} \\] <p>then \\(X(t)\\) is called Markov Process.</p> <p>When the state space \\(S\\) is discrete, then we call the above process Markov Chain.</p> <p>When both the state space \\(S\\) and time space \\(T\\) are discrete, then the above process is called discrete Markov Chain.</p> <p>Transition Probability Distribution</p> <p>We call the following conditional distribution </p> \\[ F(t_{n-1}, x_{n-1}; t_n, x_n)=P(X(t_n)&lt;x_n\\mid X(t_{n-1})=x_{n-1}). \\] <p>trransition probability distribution.</p> <p>For discrete Markov chain, \\(\\forall n,m \\in T\\), \\(i,j \\in S\\), we define transition probability</p> \\[ p_{ij}(m,n)=P(X(n)=j\\mid X(m)=i) \\] <p>which means the probability of transiting from state \\(i\\) at time \\(m\\) to state \\(j\\) at time \\(n\\). Apparently, we have \\(p_{ij}(m,n)\\geq 0\\) and </p> \\[ \\sum_{j=1}^\\infty p_{ij}(m,n)=1.  \\] <p>Define transition matrix of \\(n\\) steps</p> \\[ P(m,m+n)= [p_{ij}(m, m+n)]_{|S|\\times |S|} \\] <p>Specifically, we focus on transition matrix of one step. If \\(P(m,m+1)\\) is irrelevant with \\(m\\), then we call the above Markov chain time-homogeneous. So we denote </p> \\[ p_{ij}=p_{ij}(1)=p_{ij}(m,m+1). \\]"},{"location":"Math/StochasticProcesses/Markov/#c-k-equation","title":"C-K Equation","text":"<p>Chapman-Kolmogorov Equation</p> <p>Assume \\(\\{X_n\\}\\) is a discrete Markov Chain, and its state space is \\(S\\). Then \\(\\forall h,l\\in \\mathbb{N}^+\\), and \\(\\forall m\\in \\mathbb{N}\\), </p> \\[ p_{ij}(m,m+h+l)=\\sum_{r=0}^\\infty p_{ir}(m,m+h) \\cdot p_{rj}(m+h,m+h+l). \\] <p>which is called Chapman-Kolmogorov Equation.</p> Proof <p>By definition,</p> \\[ \\begin{align*} p_{ij}&amp;(m,m+h+l)\\\\ &amp;=P(X_{m+h+l}=j\\mid X_m=i)\\\\ &amp;=\\frac{P(X_{m+h+l}=j, X_m=i)}{P(X_m=i)}\\\\ &amp;=\\frac{\\sum_{r=0}^\\infty P(X_{m+h+l}=j, X_{m+h}=r, X_m=i)}{P(X_m=i)}\\\\ &amp;=\\sum_{r=0}^\\infty \\frac{P(X_{m+h+l}=j, X_{m+h}=r, X_m=i)}{P(X_{m+h}=r, X_m=i)}\\frac{P(X_{m+h}=r, X_m=i)}{P(X_m=i)}\\\\ &amp;=\\sum_{r=0}^\\infty P(X_{m+h+l}=j\\mid X_{m+h}=r, X_m=i)\\cdot P(X_{m+h}=r\\mid X_m=i)\\quad \\text{Totel probability formula}\\\\ &amp;=\\sum_{r=0}^\\infty P(X_{m+h+l}=j\\mid X_{m+h}=r) P(X_{m+h}=r\\mid X_m=i)\\quad\\text{By Markov property}\\\\ &amp;=\\sum_{r=0}^\\infty p_{ir}(m,m+h) \\cdot p_{rj}(m+h,m+h+l). \\end{align*} \\] <p>Corollary: Multiple transition Probability</p> <p>Transition probablity of one step of a discrete Markov chain could determine the transition probability of multiple steps.</p> Proof <p>By induction. For \\(h=2\\), we have</p> \\[ p_{ij}(m,m+2)=\\sum_{r=0}^\\infty p_{ir}(m,m+1)\\cdot p_{rj}(m+1,m+2). \\] <p>Assume for \\(h=k\\), the theorem holds. Then for \\(h=k+1\\), we have</p> \\[ p_{ij}(m,m+k+1)=\\sum_{r=0}^\\infty p_{ir}(m, m+k)\\cdot p_{rj}(m+k, m+k+1). \\] <p>If we write a matrix, then </p> \\[ P(m,m+k+1)=P(m,m+k)P(m+k,m+k+l). \\] <p>Note here is the premultiplication of matrices.</p> <p>Determination of the distribution of discrete Markov Chain</p> <p>Assume \\(\\{X_n\\}\\) is a discrete Markov Chain, and its state space is \\(S\\). The finite distribution of \\(X_n\\) is determined if all the transition probability of one step \\(p_{ij}(m,m+1)\\) and the initial distribution \\(p_i=P(X_0=i)\\) is determined. </p> Proof <p>\\(\\forall k\\in \\mathbb{N}^+\\), \\(\\forall 0&lt;m_1&lt;\\cdots&lt;x_k\\in T\\), and \\(i_1,\\cdots,i_k\\in S\\), we have</p> \\[ \\begin{align*} P(&amp;X_{m_k}=i_k,\\cdots,X_{m_1}=i_1)\\\\ &amp;=\\sum_{i=0}^\\infty P(X_{m_k}=i_k,\\cdots,X_{m_1}=i_1, X_{0}=i)\\\\ &amp;=\\sum_{i=0}^\\infty P(X_{m_k}=i_k\\mid X_{m_{k-1}}=i_{k-1}, \\cdots, X_{m_1}=i_1, X_{0}=i)\\cdot P(X_{m_{k-1}}=i_{k-1},\\cdots,X_{m_1}=i_1, X_{0}=i)\\\\ &amp;=\\sum_{i=0}^\\infty p_{i_{k-1}i_{k}} (m_{k-1}, m_k) \\cdot P(X_{m_{k-1}}=i_{k-1} \\mid X_{m_{k-2}}=i_{k-2}\\cdots,X_{m_1}=i_1, X_{0}=i)\\\\ &amp;=\\sum_{i=0}^\\infty p_i \\cdot p_{i i_1}(0,m_1)\\cdot \\prod_{j=1}^{k-1} p_{i_j i_{j+1}}(m_{j}, m_{j+1})  \\end{align*} \\]"},{"location":"Math/StochasticProcesses/Markov/#state-space","title":"State Space","text":"<p>Now we focus on time-homogeneous discrete Markov Chain.</p> <p>Invariance of Time</p> <p>Assume \\(X_n\\) is a time-homogeneous discrete Markov Chain, then its transition probability of \\(h\\) steps</p> \\[ p_{ij}(m,m+h) \\] <p>is irrelevant with time \\(m\\).</p> Proof <p>Use C-K equation</p> \\[ \\begin{align*} p_{ij}(m,m+h)&amp;=\\sum_{r_1=0}^\\infty p_{ir_1}(m,m+1)\\cdot p_{r_1j}(m+1,m+h)\\\\ &amp;=\\sum_{r_1=0}^\\infty p_{ir_1} \\sum_{r_2=0}^\\infty p_{r_1r_2}\\cdot  p_{r_2j}(m+2,m+h)\\\\ &amp;=\\sum_{r_1=0}^\\infty p_{ir_1}\\sum_{r_2=0}^\\infty p_{r_1r_2}\\cdots \\sum_{r_{h-1}=0}^\\infty p_{r_{h-1}r_{h}} \\end{align*} \\] <p>which is only dependent on steps \\(h\\). This is similar to stationary increment property.</p>"},{"location":"Math/StochasticProcesses/Markov/#accessibility-communicating-class","title":"Accessibility &amp; Communicating Class","text":"<p>Accessibility of state</p> <p>If there exsits \\(N&gt;0\\) such that \\(p_{ij}(N)&gt;0\\), then we call that state \\(i\\) could access state \\(j\\), denoted by</p> \\[ i\\rightarrow j. \\] <p>If \\(i\\rightarrow j\\) and \\(j\\rightarrow i\\), then we call \\(i\\), \\(j\\) communicate, denoted by </p> \\[ i\\leftrightarrow j. \\] <p></p> <p>Properties of accessibility</p> <p>(i) If \\(i\\rightarrow j\\) and \\(j\\rightarrow k\\), then \\(i\\rightarrow k\\).</p> <p>(ii) Reflexivity. If \\(i\\leftrightarrow j\\), then \\(j\\leftrightarrow i\\).</p> <p>(iii) Transitivity. If \\(i\\leftrightarrow j\\) and \\(j\\leftrightarrow i\\), then \\(i\\leftrightarrow i\\).</p> Proof <p>(i) There exsits \\(N_1,N_2&gt;0\\), s.t. \\(p_{ij}(N_1)&gt;0\\) and \\(p_{jk}(N_2)&gt;0\\), then by C-K equation</p> \\[ p_{ik}(N_1+N_2)=\\sum_{r=0}^\\infty p_{ir}(N_1)p_{rk}(N_2)\\geq p_{ij}(N_1)p_{jk}(N_2)&gt;0. \\] <p>By Properties of accessibility, the following communicating class defines an equivalent class.</p> <p>Communicating class</p> <p>We denote \\(C(j)\\) to be the communicating class of state \\(j\\), which is actually the collection of all states mutually accessible from \\(j\\), which has the following properties.</p> <p>(i) If \\(i\\leftrightarrow j\\), then \\(C(i)=C(j)\\).</p> <p>(ii) If two state set \\(C_1\\), \\(C_2\\) are communicating classes, then either \\(C_1=C_2\\) or \\(C_1\\cap C_2=\\varnothing\\).</p> <p>If any two states of a state space \\(S\\) communicate, i.e. all states are in one common communicating class, then we call the Markov chain irreducible. This is the smallest unit we shall focus on.</p> Proof <p>(i) Easy to see. \\(\\forall k\\in C(i)\\), we have \\(k\\leftrightarrow i\\). Since \\(i\\leftrightarrow j\\), we have \\(k\\leftrightarrow j\\), so \\(k\\in C(j)\\). The other direction is the same.</p> <p>(ii) By definition of communicating class, there exists \\(i\\), \\(j\\) such that </p> \\[ C_1=C(i),\\quad C_2=C(j). \\] <p>If \\(C_1\\cap C_2\\neq \\varnothing\\), then choose one \\(g\\in C_1\\cap C_2\\), which satisfies \\(g\\leftrightarrow i\\) and \\(g\\leftrightarrow j\\). So</p> \\[ i\\leftrightarrow j. \\] <p>which means \\(C_1=C(i)=C(j)=C_2\\) by (i).</p> <p><p>\\(\\square\\)</p></p> <p>Some books also defines the following notions, which does not help actually. But readers could check it and make it as an example. </p> Other notions of states <p>If \\(C(j)\\neq\\varnothing\\), then we call \\(j\\) a return state. If \\(C(j)= \\varnothing\\), then we call \\(j\\) a non-return state. </p> <p>In further study, we would see a non-return state must be a transient state, but a return state should be decomposed carefully.</p> <p>In the following discussion, we would notice that a communicating class partitions the categories of states, which is for transitivity of period, Transitivity of Recurrent state, Transitivity of positive and null recurrent states.</p> <p>The following definition is a criterion for checking whether a state is recurrent, which would be of importance in Relationship between communicating class and recurrent state after we introduce limit theorem.</p> <p>Closed state class</p> <p>We call set of state \\(C\\) a closed state set, if for all state \\(j\\not\\in C\\), and \\(i\\in C\\), \\(j\\not\\leftrightarrow i\\).</p> <p>If a closed state set has only one element \\(j\\), then we call \\(j\\) an absorbing state.</p>"},{"location":"Math/StochasticProcesses/Markov/#period-of-state","title":"Period of State","text":"<p>Now we focus on return state, other than absorbing state or non-return state, since both of which does not have periodical phenomenon.</p> <p>Definition of period of state</p> <p>We denote \\(d(j)\\) to be the period of a return state \\(j\\), defined by</p> \\[ d(j)=gcd\\{n: p_{jj}(n)&gt;0, n\\geq 1\\}. \\] <p>if \\(d(j)=1\\), we call \\(j\\) non-periodical state, which includes the absorbing state \\(k\\), whose \\(d(k)=1\\).</p> <p>Usually, if state \\(i\\) has period \\(d\\), then for all \\(n\\neq 0(\\text{mod } d)\\), we have \\(p_{ii}(n)=0\\). Conversely speaking, if \\(n\\equiv 0(\\text{mod } d)\\), \\(p_{ii}(n)&gt;0\\) do not hold necessarily.</p> <p>To be elaborate, we have the following theorem.</p> <p>Theorem for period</p> <p>If state \\(i\\) has a period \\(d\\), then there exsits \\(M\\in\\mathbb{N}^+\\), such that for all \\(n&gt;M\\), </p> \\[ p_{ii}(nd)&gt;0. \\] <p>Here we have the first transitivity of communicating class, which is for period.</p> <p></p> <p>Transitivity of period</p> <p>If \\(i\\leftrightarrow j\\), then \\(d(i)=d(j)\\).</p> Proof <p>Use a special loop. Since \\(i\\leftrightarrow j\\), so there exsits \\(n,m&gt;0\\) such that \\(p_{ij}(n)&gt;0\\) and \\(p_{ji}(m)&gt;0\\), so</p> \\[ p_{ii}(n+m)\\geq p_{ij}(n)p_{ji}(m)&gt;0. \\] <p>So \\(d(i)\\mid n+m\\).</p> <p>By definition of \\(d(j)=gcd\\{n_1,n_2,\\cdots,n_k,\\cdots\\}\\), we have \\(p_{jj}(n_k)&gt;0\\) for all \\(k=1,\\cdots\\). So for each \\(k\\),</p> \\[ p_{ii}(n+m+n_k)\\geq p_{ij}(n)p_{jj}(n_k)p_{ji}(m)&gt;0. \\] <p>So \\(d(i)\\mid n+m+n_k\\). Combined with the above two, we have \\(d(i)\\mid n_k\\), which means \\(d(i)\\mid d(j)\\).</p> <p>Similarly we have \\(d(j)\\mid d(i)\\).</p>"},{"location":"Math/StochasticProcesses/Markov/#ergodicity-of-state","title":"Ergodicity of State","text":"<p>Here we dig deeper into the probability itself for return states. Notice some return states are possible not to return in the long run, while others are always possible to return. So here comes two equivalent languages to describe the phenomenon.</p>"},{"location":"Math/StochasticProcesses/Markov/#first-passage-probability-of-state-in-n-steps","title":"First-passage Probability of State in n Steps","text":"<p>Definition of passage probability of State</p> <p>Define the first-passage probability from state \\(i\\) to \\(j\\) after \\(n\\) steps.</p> \\[ f_{ij}(n)=P(X_n=j,X_{n-1}\\neq j,\\cdots, X_1\\neq j \\mid X_0=i). \\] <p>Define Ultimate probability from state \\(i\\) to \\(j\\)</p> \\[ f_{ij}=P(X_m=j,\\exists m\\mid X_0=i). \\] <p>Note that </p> \\[ f_{ij}=\\sum_{n=0}^\\infty f_{ij}(n), \\] <p>which is the core of our analysis.</p> <p>The following relationship is useful in proving limit theorem, though it might not help in distinguishing return states in this part.</p> <p></p> <p>Theorem for relationship between first-passage probability and passage probability</p> <p>For any state \\(i,j\\in S\\), \\(\\forall n\\geq 1\\), </p> \\[ p_{ij}(n)=\\sum_{m=1}^n f_{ij}(m) p_{ij}(n-m). \\] Proof \\[ \\begin{align*} p_{ij}(n)&amp;=P(X_n=j\\mid X_0=i)\\\\ &amp;=\\sum_{m=1}^n P(X_n=j,X_m=j,X_{m-1}\\neq j,\\cdots, X_1\\neq j\\mid X_0=i)\\quad \\text{This is a partition}\\\\ &amp;=\\sum_{m=1}^n \\frac{P(X_n=j,X_m=j,X_{m-1}\\neq j,\\cdots, X_1\\neq j, X_0=i)}{P(X_0=i)}\\\\ &amp;=\\sum_{m=1}^n P(X_n=j\\mid X_m=j,X_{m-1}\\neq j,\\cdots, X_1\\neq j, X_0=i)\\cdot P(X_m=j, \\cdots,X_1\\neq j \\mid X_0=i)\\\\ &amp;=\\sum_{m=1}^n p_{jj}(n-m) f_{ij}(m). \\end{align*} \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Markov/#first-hitting-time-as-random-variable","title":"First Hitting Time as Random Variable","text":"<p>Another version using First Hitting Time</p> <p>We define </p> \\[ T_j=\\inf\\{n\\geq 1: X_n=j\\} \\] <p>to be First Hitting Time of \\(j\\).</p> <p>Then </p> \\[ P(T_j=n\\mid X_0=i)=f_{ij}(n). \\] <p>and</p> \\[ P(T_j&lt;\\infty\\mid X_0=i)=f_{ij}. \\] <p>Notice that the above \"\\(&lt;\\infty\\)\" means the chain could reach state \\(j\\) from \\(i\\) in finitely many steps.</p>"},{"location":"Math/StochasticProcesses/Markov/#recurrence-transient","title":"Recurrence &amp; Transient","text":"<p>Now we could come to the core defintion in this part, here we use from the above two equivalent definitions.</p> <p>Definition of Recurrent State, self-reachable</p> <p>If \\(f_{jj}=1\\), or \\(P(T_j&lt;\\infty\\mid X_0=j)=1\\), then state \\(j\\) is called recurrent state.</p> <p>If \\(f_{jj}&lt;1\\), or \\(P(T_j&lt;\\infty\\mid X_0=j)&lt;1\\), then state \\(j\\) is called transient state.</p> <p>For recurrent state \\(j\\), we have \\(\\sum_{n=1}^\\infty f_{jj}(n)=1\\). So \\(f_{jj}(n)\\) becomes a discrete distribution.</p> <p>Now we could talk about the mathematical characteristics of the above distribution and its practical meaning. From Another version using First Hitting Time, we have</p> \\[ P(T_j=n\\mid X_0=j)=f_{jj}(n), \\quad n=1,2,\\cdots \\] <p>Definition of Mean Recurrence Time</p> <p>For recurrent state \\(j\\), Mean recurrence time is defined as ME of \\(T_j\\)</p> \\[ \\tau_i=\\mu_i=E(T_j\\mid X_0=j)=\\sum_{ij1}^\\infty nf_{jj}(n). \\] <p>If \\(\\mu_i&lt;+\\infty\\), then \\(j\\) is called Positive recurrent state. If \\(\\mu_i=+\\infty\\), then \\(i\\) is called Null recurrent state.</p> <p>A non-periodic positive recurrent state is also called Ergodic state.</p> Another version using an equation <p>Define \\(r_0=1\\), \\(r_k=P(T_i&gt;k\\mid X_0=i)\\), \\(k\\geq 1\\), then</p> \\[ \\mu_i=E(T_i\\mid X_0=i)=\\sum_{k=0}^\\infty r_k. \\] <p>This could be explained by this. We use the definition of expectation. Since</p> \\[ P(T_i&gt;k\\mid X_0=i)=\\sum_{n=k+1}^\\infty P(T_i=n\\mid X_0=i)=\\sum_{n=k+1}^\\infty f_{ii}(n) \\] <p>So</p> \\[ \\begin{align*} \\sum_{k=0}^\\infty r_k&amp;=\\sum_{k=0}^\\infty P(T_i&gt;k\\mid X_0=i)\\\\ &amp;=\\sum_{k=0}^\\infty \\sum_{n=k+1}^\\infty f_{ii}(n)\\\\ &amp;=\\sum_{n=0}^\\infty n f_{ii}(n)\\quad \\text{by triangle choice}\\\\ &amp;=\\mu_i \\end{align*} \\]"},{"location":"Math/StochasticProcesses/Markov/#green-function","title":"Green Function","text":"<p>Actually, it is really hard to calculate \\(f_{ii}(n)\\) in some complex Markov chain. So we have to give another description of recurrent state. The core function is called Green Function</p> \\[ G(j)=\\sum_{n=1}^\\infty p_{jj}(n). \\]"},{"location":"Math/StochasticProcesses/Markov/#rth-passage-probability-of-state-in-infinite-steps","title":"rth-passage Probability of State in Infinite Steps","text":"<p>rth-passage Probability</p> <p>Define rth-passage probability</p> \\[ Q_{ij}(r)=P(\\forall R\\geq r, \\exists \\{r_i\\}_{1\\leq i\\leq R}, r_i\\geq 1, s.t. X_{r_i}=j\\mid X_0=i) \\] <p>which means the probability for the chain to pass \\(j\\) in at least \\(r\\) times with initial state \\(i\\). Easy to have</p> \\[ f_{ij}=Q_{ij}(1), \\] <p>so rth passage probability is based on steps \\(n\\rightarrow \\infty\\).</p> <p>Define \\(Q_{ij}:=\\lim\\limits_{r\\rightarrow \\infty}Q_{ij}(r)\\) to mean the probability that the chain reaches \\(j\\) in infinitely many times from initial state \\(i\\), which would be interpreted by nth hitting time more naturally.</p> <p>Properties of rth-passage probability</p> <p>(i) Monotonically decreasing.</p> \\[ Q_{ij}(1)\\geq Q_{ij}(2)\\geq \\cdots \\] <p>(ii) If \\(Q_{ij}=1\\), which means starting from state \\(i\\), we would reach state \\(j\\) infinitely many times. So by property (i), we have \\(Q_{ij}(n)=1\\).</p> <p>(iii) If \\(Q_{jj}=1\\), then \\(Q_{jj}(1)=f_{jj}=1\\), state \\(j\\) is a recurrent state.</p> <p></p> <p>Relationship between \\(f_{ij}\\) and \\(Q_{ij}\\)</p> <p>For any state \\(i,j\\in S\\), and parameter \\(r\\geq 1\\), we have the iterative equation</p> \\[ Q_{ij}(r+1)=f_{ij} Q_{jj}(r) \\] Proof <p>Still use first-passage to partition the space.</p> \\[ \\begin{align*} Q_{ij}(r+1)&amp;=P(\\forall R\\geq r+1, \\exists \\{r_i\\}_{1\\leq i\\leq R}, r_i\\geq 1, s.t. X_{r_i}=j\\mid X_0=i)\\\\ &amp;=\\sum_{n=1}^\\infty P(\\forall R\\geq r, \\exists \\{r_i\\}_{1\\leq i\\leq R}, r_i\\geq n+1, s.t. X_{r_i}=j, X_n=j, X_{n-1}\\neq j,\\cdots, X_1\\neq j \\mid X_0=i)\\\\ &amp;=\\sum_{n=1}^\\infty P(\\forall R\\geq r, \\exists \\{r_i\\}_{1\\leq i\\leq R}, r_i\\geq n+1, s.t. X_{r_i}=j \\mid X_n=j )\\cdot \\\\ &amp;\\quad P(X_n=j, X_{n-1}\\neq j,\\cdots, X_1\\neq j \\mid X_0=i)\\quad \\text{by Markov property}\\\\ &amp;=\\sum_{n=1}^\\infty P(\\forall R\\geq r, \\exists \\{r_i\\}_{1\\leq i\\leq R}, r_i\\geq 1, s.t. X_{r_i}=j \\mid X_0=j ) f_{ij}(n)\\quad\\text{by time-homogeneous}\\\\ &amp;=\\sum_{n=1}^\\infty Q_{jj}(r)f_{ij}(n)=f_{ij}Q_{jj}(r). \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>Corollary</p> <p>We could inductively show that </p> \\[ Q_{jj}(r)=(f_{jj})^r. \\] Proof <p>Apply Relationship between \\(f_{ij}\\) and \\(Q_{ij}\\) to \\(Q_{jj}(m)\\) iteratively.</p> \\[ Q_{jj}(r)=f_{jj} Q_{jj}(r-1)=\\cdots=(f_{jj})^{r-1} Q_{jj}(1)=(f_{jj})^{r}. \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Markov/#rth-hitting-time-as-random-variables","title":"rth Hitting Time as Random Variables","text":"<p>rth Hitting Time</p> <p>Since random variable first hitting time is defined by</p> \\[ T_{jj}(\\omega)=\\inf\\{k\\geq 1: X_k(\\omega)=j\\}. \\] <p>So we have the inductive definition for rth hitting time</p> \\[ S_{jj}^{(0)}=0, \\quad S_{jj}^{(1)}=T_{jj},\\quad S_{jj}^{(r)}=\\inf\\{k\\geq S_{jj}^{(r-1)}+1: X_k(\\omega)=j\\}. \\] <p>and the rth interval time </p> \\[ T_{jj}^{(0)}=0, \\quad T_{jj}^{(r)}=S_{jj}^{(r)}-S_{jj}^{(r-1)}. \\] <p>check the following image.</p> <p><p> </p></p> <p>It is easy to have</p> \\[ P(S_{jj}(r)&lt;\\infty)=Q_{jj}(r). \\] <p>Properties of rth hitting time and interval time</p> <p>\\(\\{T_{jj}(r)\\}\\) are mutually independent and follow the same distribution. Moreover, they are independent with \\(S_{jj}(r)\\) and </p> \\[ P(T_{jj}(r+1)=n\\mid S_{jj}(r)&lt;\\infty)=P(T_{jj}(1)=n). \\] Proof <p>By Markov property.</p> <p>The following theorem is the same with Relationship between \\(f_{ij}\\) and \\(Q_{ij}\\) essentially.</p> <p>Relationship between first &amp; rth hitting time</p> \\[ P(S_{jj}(r)&lt;\\infty)=P(T_{jj}&lt;\\infty)^r. \\] Proof <p>We prove here with induction. For \\(r=0\\), \\(1=1\\) and the proposition holds.</p> <p>Assume for \\(r=k\\) the proposition holds, i.e. \\(P(S_{jj}(k)&lt;\\infty)=P(T_{jj}&lt;\\infty)^k\\). Then</p> \\[ \\begin{align*} P(S_{jj}(k+1)&lt;\\infty)&amp;=P(T_{jj}(k+1)&lt;\\infty, S_{jj}(k)&lt;\\infty)\\\\ &amp;=P(T_{jj}(k+1)&lt;\\infty\\mid S_{jj}(k)&lt;\\infty )P(S_{jj}(k)&lt;\\infty)\\\\ &amp;=P(T_{jj}(1)&lt;\\infty)P(T_{jj}&lt;\\infty)^k\\\\ &amp;=P(T_{jj}&lt;\\infty)^{k+1}. \\end{align*} \\] <p><p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Markov/#number-of-visits-in-n-steps","title":"Number of visits in n steps","text":"<p>Number of visits in n steps</p> <p>Define</p> \\[ N_{jj}(n)=\\sum_{i=1}^n 1_{\\{X_n=j\\mid X_0=j\\}} \\] <p>to denote the number of time the chain reaches state \\(j\\) from initial state \\(j\\) in \\(n\\) steps. Here \\(1_{\\{X_n=j\\mid X_0=j\\}}\\) denotes the event whether the chain reaches state \\(j\\) from initial state \\(j\\) in exactly \\(n\\) steps.</p> <p>So</p> \\[ EN_{jj}(n)=\\sum\\limits_{i=1}^n p_{jj}(i),\\quad EN_{jj}=\\lim_{n\\rightarrow\\infty}EN_{jj}(n)=\\sum\\limits_{n=1}^\\infty p_{jj}(n). \\] Proof <p>We could take another perspective from number of visits in n steps. Notice a property of ME</p> \\[ \\sum_{i=1}^\\infty P(A_i)=E\\sum_{i=1}^\\infty 1_{A_i} \\] <p>where \\(\\sum\\limits_{i=1}^\\infty 1_{A_i}\\) is actually the total number of events \\(A_1,\\cdots,A_i,\\cdots\\) happening. Note that \\(A_i\\) are not necessarily mutually disjoint, but it does not impede our analysis.</p> <p>Then we have the relationship between \\(S_{jj}(r)\\) and \\(N_{jj}(n)\\).</p> <p>Relationship between n-visit probability \\(S_{jj}(n)\\) and \\(N_{jj}(n)\\)</p> <p>Easy to see that </p> \\[ P(N_{jj}(n)&gt;r)=P(S_{jj}(r)&lt;n) \\] <p>So let \\(n\\rightarrow \\infty\\), we have</p> \\[ P(N_{jj}&gt;r)=P(S_{jj}(r)&lt;\\infty). \\] <p>Now we give the main result of this part. </p> <p></p> <p>Recurrent state description with passage probability</p> <p>(i) State \\(j\\) is a recurrent state, iff \\(\\sum\\limits_{n=1}^\\infty p_{jj}(n)=+\\infty\\).</p> <p>(ii) State \\(j\\) is a transient state, iff \\(\\sum\\limits_{n=1}^\\infty p_{jj}(n)&lt;+\\infty\\).</p> Proof using number of visitsProof using squeeze method <p>we make use of the relationship between the first hitting time and nth hitting time.</p> \\[ \\begin{align*} \\sum\\limits_{n=1}^\\infty p_{jj}(n)&amp;=EN_{jj}=\\sum_{r=0}^\\infty P(N_{jj}&gt;r)\\\\ &amp;=\\sum_{r=0}^\\infty P(S_{jj}(r)&lt;\\infty)=\\sum_{r=0}^\\infty Q_{jj}(r)\\\\ &amp;=\\sum_{r=0}^\\infty f_{jj}^r=\\frac{1}{1-f_{jj}}. \\end{align*} \\] <p>So if \\(j\\) is recurrent, then \\(f_{jj}\\rightarrow 1\\), the series diverges. Otherwise \\(j\\) is transient, so \\(f_{jj}&lt;1\\), the series converges.</p> <p>The proof is done by leveraging Theorem for relationship between first-passage probability and passage probability.</p> <ul> <li>Sufficient for (i) and necessary for (ii).</li> </ul> <p>We have</p> \\[ \\begin{align*} p_{jj}(n)&amp;=\\sum_{m=1}^n f_{jj}(m)p_{jj}(n-m)\\\\ \\Rightarrow \\quad \\sum_{n=1}^N p_{jj}&amp;=\\sum_{n=1}^N \\sum_{m=1}^n f_{jj}(m)p_{jj}(n-m)\\\\ &amp;=\\sum_{m=1}^N \\sum_{n=m}^N  f_{jj}(m)p_{jj}(n-m)\\\\ &amp;=\\sum_{m=1}^N f_{jj}(m)  \\sum_{n=m}^N p_{jj}(n-m)\\\\ &amp;\\leq \\sum_{m=1}^N f_{jj}(m) \\sum_{n=0}^N p_{jj}(n) \\end{align*} \\] <p>so we have </p> \\[ \\frac{\\sum_{n=1}^N p_{jj}(n)}{p_{jj}(0)+\\sum_{n=1}^N p_{jj}(n)}\\leq \\sum_{m=1}^N f_{jj}(m) \\] <p>let \\(N\\rightarrow\\infty\\), we have</p> \\[ 1\\leq f_{jj} \\] <p>which means \\(f_{jj}=1\\), so it is recurrent state. So if state \\(j\\) is transient, then it must have \\(\\sum\\limits_{n=1}^\\infty p_{jj}(n)&lt;+\\infty\\).</p> <ul> <li>Necessary for (i) and sufficient for (ii).</li> </ul> <p>Still using the same formula, we choose a smaller one \\(M\\leq N\\)</p> \\[ \\begin{align*} \\sum_{n=1}^N p_{jj}(n)&amp;=\\sum_{m=1}^N f_{jj}(m)\\sum_{n=0}^{N-m} p_{jj}(n)\\\\ &amp;\\geq \\sum_{m=1}^M f_{jj}(m)\\sum_{n=0}^{N-m} p_{jj}(n)\\\\ &amp;\\geq \\sum_{m=1}^M f_{jj}(m)\\sum_{n=0}^{N-M} p_{jj}(n) \\end{align*} \\] <p>So</p> \\[ \\sum_{m=1}^M f_{jj}(m)\\leq \\frac{\\sum_{n=1}^{N} p_{jj}(n)}{p_{jj}(0) +\\sum_{n=1}^{N-M} p_{jj}(n)} \\] <p>let \\(N\\rightarrow \\infty\\), if \\(\\sum_{n=1}^{N} p_{jj}(n)&lt;\\infty\\), then the right-hand limit is strictly less than \\(1\\), i.e.</p> \\[ \\sum_{m=1}^M f_{jj}(m)\\leq \\delta&lt;1 \\] <p>so let \\(M\\rightarrow \\infty\\), we have a contradiction. Actually, if \\(\\sum_{n=1}^{N} p_{jj}(n)&lt;\\infty\\), then \\(f_{jj}&lt;1\\), which means state \\(j\\) is transient state.</p> <p>Here we use the Green function to give the second transitivity of communicating class for recurrent states.</p> <p></p> <p>Corollary: Transitivity of Recurrent state</p> <p>If state \\(i\\leftrightarrow j\\), and \\(j\\) is a recurrent state, then \\(i\\) is also a recurrent state.</p> Proof <p>Since \\(i\\leftrightarrow j\\), \\(\\exists m,n\\geq 1\\), such that</p> \\[ p_{ij}(m)&gt;0, \\quad p_{ji}(n)&gt;0. \\] \\[ \\begin{align*} \\sum_{k=1}^\\infty p_{ii}(k)&amp;\\geq \\sum_{k=1}^\\infty p_{ii}(n+m+k)\\\\ &amp;\\geq \\sum_{k=1}^\\infty p_{ij}(m)p_{jj}(k)p_{ji}(n)\\\\ &amp;= p_{ij}(m)p_{ji}(n) \\sum_{k=1}^\\infty p_{jj}(k)=+\\infty. \\end{align*} \\] <p>we are done. <p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Markov/#limit-theorem","title":"Limit Theorem","text":"<p>We now focus on the outer characteristics of recurrent, transient and ergodic state, the limit of \\(p_{jj}(n)\\).</p> <p>The following theorem has considered the influence of period. If a positive recurrent state has a period, it might not have the limit of \\(p_{jj}(n)\\) (this is actually the limit distribution item we will talk about in the next part), but just a convergent subsequence. So a more general statement of the following Theorem for limit of passage probability of three states (i) and (ii) considering period is </p> <p></p> <p>Lemma: subsequence convergence and Ces\u00e0ro limit</p> <p>Assume the Markov chain is irreducible. If state \\(j\\) is positive recurrent, then</p> \\[ \\limsup_{n\\rightarrow\\infty}p_{jj}(n)&gt;0 \\] <p>or we have its Ces\u00e0ro limit (take an average of the first \\(n\\) items)</p> \\[ \\lim_{n\\rightarrow \\infty}\\frac{1}{n}\\sum_{m=1}^n p_{jj}(m)=\\frac{1}{\\mu_j}&gt;0, \\] <p>without considering period.</p> Proof <ul> <li>Define some variables. In the following discussion, we assume the probability is conditioned on \\(X_0=j\\).</li> </ul> <p>In number of visits in n steps, we have already defined \\(N_{j}(n)\\) to be the number of reaching \\(j\\) in \\(n\\) steps, which is also called Sojourn number.</p> <p>Define \\(T_1\\) to be the number of steps for reaching \\(j\\) at the first time, and \\(T_k\\) to be the number of steps for reaching \\(j\\) at \\(k\\) times after reaching \\(j\\) at \\(k-1\\) times. So by time-homogeneous property, we have \\(T_1,\\cdots,T_k,\\cdots\\) are mutually independent and follow the same distribution, i.e. first hitting time \\(T_j\\), that is,</p> \\[ P(T_k=n)=f_{jj}(n), \\quad n=1,2,\\cdots,\\forall k=1,2,\\cdots \\] <p>and satisfies \\(E(T_k)=\\mu_j\\).</p> <p>Define accumulated time \\(S_k=\\sum_{i=1}^k T_i\\). Then we have the relationship </p> \\[ N_j(n)=\\max\\{k\\geq 1: S_k \\leq n\\}. \\] <p>or </p> \\[ S_k=\\min\\{n\\geq 1: N_j(n)=k\\}. \\] <p>Note that the above relationship also holds in Poisson Process.</p> <ul> <li>Prove first \\(\\lim\\limits_{n\\rightarrow \\infty} \\frac{N_j(n)}{n}=\\frac{1}{\\mu_j}\\)</li> </ul> <p>Notice their relationship</p> \\[ S_{N_j(n)}\\leq n\\leq S_{N_j(n)+1} \\] <p>so </p> \\[ \\frac{S_{N_j(n)}}{N_j(n)}\\leq \\frac{n}{N_j(n)}\\leq \\frac{S_{N_j(n)+1}}{{N_j(n)+1}}\\frac{{N_j(n)+1}}{N_j(n)}. \\] <p>By Strong law of large number, we have \\(\\frac{S_n}{n}\\rightarrow \\mu_j, a.s.\\), and since \\(N_j(n)\\rightarrow \\infty\\) as \\(n\\rightarrow \\infty\\) (recurrent state),</p> \\[ \\frac{n}{N_j(n)}\\rightarrow \\mu_j\\quad a.s. \\] <p>and we have </p> \\[ \\frac{N_j(n)}{n}\\rightarrow \\frac{1}{\\mu_j}\\quad a.s. \\] <ul> <li>Prove second \\(\\lim\\limits_{n\\rightarrow \\infty} \\frac{1}{n}\\sum_{m=1}^n p_{jj}(m)=\\frac{1}{\\mu_j}\\).</li> </ul> <p>By definition of sojourn number, we have</p> \\[ N_j(n)=\\sum_{m=1}^n 1_{\\{X_m=j\\}},\\quad 1_{\\{X_m=j\\}}\\sim B(1,p_{jj}(m)) \\] <p>So</p> \\[ E\\frac{N_j(n)}{n}=\\frac{1}{n}\\sum_{m=1}^n p_{jj}(n), \\] <p>take the limit and we have the result.</p> <p></p> <p>Theorem for limit of passage probability of three states</p> <p>(i) If state \\(j\\) is a non-periodic positive recurrent state (ergodic), then </p> \\[ \\lim_{n\\rightarrow \\infty}p_{jj}(n)=\\frac{1}{\\mu_j} \\] <p>(ii) If state \\(j\\) is a positive recurrent state with period \\(d\\), then </p> \\[ \\lim_{n\\rightarrow \\infty}p_{jj}(nd)=\\frac{d}{\\mu_j}, \\] <p>(iii) If state \\(j\\) is null recurrent state or transient state, then </p> \\[ \\lim_{n\\rightarrow \\infty}p_{jj}(n)=0. \\] Proof for (i)Proof for (ii)Proof for (iii) <p>We could derive this from Ces\u00e0ro limit. We could also get the same result from subsequence limit.</p> <p>Assume \\(\\lim_{n\\rightarrow \\infty}p_{jj}(n)=\\frac{1}{\\mu_j}+\\delta\\), substitute in the result of the lemma and we have \\(\\delta=0\\).</p> <p><p>\\(\\square\\)</p></p> <p>Let \\(n=dn'\\), we could make \\(X_{dn'}=X_{n}\\) a non-periodical Markov chain. So by (i)</p> \\[ \\lim_{n\\rightarrow\\infty}p_{jj}(n)=\\lim_{n'\\rightarrow\\infty}p_{jj}(n')=\\frac{1}{\\mu'}. \\] <p>Notice that</p> \\[ \\mu'=\\sum_{n'=1}^\\infty n'f_{jj}(dn')=\\frac{1}{d}\\sum_{n'=1}^\\infty n'd f_{jj}(n'd)=\\frac{\\mu}{d}. \\] <p>and we are done.</p> <p><p>\\(\\square\\)</p></p> <ul> <li>For null recurrent state, by (ii), let \\(\\mu_j\\rightarrow \\infty\\), and we have \\(\\lim\\limits_{n\\rightarrow \\infty}p_{jj}(nd)=0\\). Since \\(p_{jj}(n)=0\\) for \\(n\\neq\\equiv 0(\\text{ mod } d)\\), so by Haine Theorem, </li> </ul> \\[ \\lim\\limits_{n\\rightarrow \\infty}p_{jj}(nd)=0. \\] <ul> <li>For transient state, just by convergence of \\(\\sum_{n=1}^\\infty p_{jj}(n)\\), we have each item \\(\\lim_{n\\rightarrow \\infty }p_{jj}(n)=0\\). <p>\\(\\square\\)</p></li> </ul> <p>Now we could give the last transitivity of communicating class, which is the most detailed.</p> <p></p> <p>Corollary: Transitivity of positive and null recurrent states</p> <p>(i) If state \\(i\\leftrightarrow j\\), and \\(j\\) is a positive recurrent state, then \\(i\\) is also a positive recurrent state.</p> <p>(ii) If state \\(i\\leftrightarrow j\\), and \\(j\\) is a null recurrent state, then \\(i\\) is also a null recurrent state.</p> Proof <p>By Transitivity of Recurrent state, we have \\(i\\leftrightarrow j\\). So \\(\\exists m,n&gt;0\\), such that \\(p_{ij}(m)&gt;0\\), and \\(P_{ji}(n)&gt;0\\), so</p> \\[ p_{ii}(m+n+kd)\\geq p_{ij}(m)p_{jj}(kd)p_{ji}(n) \\] <p>let \\(k\\rightarrow \\infty\\), we have</p> \\[ \\lim_{k\\rightarrow\\infty}p_{ii}(m+n+kd)\\geq p_{ji}(n) p_{ij}(m)\\lim_{k\\rightarrow\\infty} p_{jj}(kd) \\] <p>If \\(j\\) is a positive recurrent state, then we have \\(\\lim\\limits_{n\\rightarrow\\infty} p_{ii}(n)&gt;0\\) and we are done. <p>\\(\\square\\)</p></p> <p>Now the following theorem gives a limit situation for different initial states.</p> <p></p> <p>Limit theorem for states starting from different initial state</p> <p>(i) If \\(j\\) is a non-periodic positive recurrent state, then \\(\\forall\\) state \\(i\\),</p> \\[ \\lim_{n\\rightarrow \\infty}p_{ij}(n)=\\frac{f_{ij}}{\\mu_j}. \\] <p>(ii) If \\(j\\) is a null recurrent state or transient state, then \\(\\forall\\) state \\(i\\),</p> \\[ \\lim_{n\\rightarrow \\infty}p_{ij}(n)=0,\\quad i\\in S. \\] Proof for (i)Proof for (ii) <p>We have a lot of proof method, including using Lebesgue's Dominated convergence theorem. Here we give a squeeze method.</p> <p>By Theorem for relationship between first-passage probability and passage probability, we have</p> \\[ p_{ij}(n)=\\sum_{m=1}^n f_{ij}(m)p_{jj}(n-m) \\] <p>choose \\(M&lt;n\\), we have</p> \\[ \\begin{align*} p_{ij}(n)&amp;=\\sum_{m=1}^M f_{ij}(m)p_{jj}(n-m) + \\sum_{m=M+1}^n f_{ij}(m)p_{jj}(n-m)\\\\ &amp;\\leq \\sum_{m=1}^M f_{ij}(m)p_{jj}(n-m) +\\sum_{m=M+1}^n f_{ij}(m) \\end{align*} \\] <p>let \\(n\\rightarrow \\infty\\), and let \\(M\\rightarrow \\infty\\), we have</p> \\[ \\begin{align*} \\lim_{n\\rightarrow \\infty}p_{ij}(n)&amp;\\leq \\sum_{m=1}^M f_{ij}(m)\\frac{1}{\\mu_j} +\\sum_{m=M+1}^\\infty f_{ij}(m)\\\\ \\lim_{M\\rightarrow \\infty}\\lim_{n\\rightarrow \\infty}p_{ij}(n)&amp;\\leq \\frac{f_{ij}}{\\mu_j}+0. \\end{align*} \\] <p>For the other direction, we just leave out \\(n-M\\) number of summation items, and let \\(M, n\\rightarrow \\infty\\), we have</p> \\[ \\begin{align*} p_{ij}(n)&amp;\\geq \\sum_{m=1}^M f_{ij}(m)p_{jj}(n-m)\\\\ \\lim_{n\\rightarrow \\infty} p_{ij}(n)&amp;\\geq \\sum_{m=1}^M f_{ij}(m) \\frac{1}{\\mu_j}\\\\ \\lim_{M\\rightarrow \\infty}\\lim_{n\\rightarrow \\infty}p_{ij}(n)&amp;\\geq \\frac{f_{ij}}{\\mu_j}. \\end{align*} \\] <p>So we are done. <p>\\(\\square\\)</p></p> <ul> <li>For null recurrent state.</li> </ul> \\[ \\begin{align*} p_{ij}(n)&amp;=\\sum_{m=1}^M f_{ij}(m)p_{jj}(n-m) + \\sum_{m=M+1}^n f_{ij}(m)p_{jj}(n-m)\\\\ &amp;\\leq \\sum_{m=1}^M f_{ij}(m)p_{jj}(n-m)+ \\sum_{m=M+1}^n f_{ij}(m)\\\\ \\Rightarrow \\quad \\lim_{n\\rightarrow \\infty} p_{ij}(n)&amp;\\leq 0+ \\sum_{m=M+1}^\\infty f_{ij}(m)\\\\ \\lim_{M\\rightarrow \\infty}\\lim_{n\\rightarrow \\infty}p_{ij}(n)&amp;\\leq 0 \\end{align*} \\] <p>and we are done.</p> <ul> <li>For transient state. By Recurrent state description with passage probability, since \\(j\\) is a transient state, we have</li> </ul> \\[ \\sum_{n=1}^\\infty p_{jj}(n)&lt;\\infty. \\] <p>So by Theorem for relationship between first-passage probability and passage probability,</p> \\[ \\begin{align*} \\sum_{n=1}^\\infty p_{ij}(n) &amp;= \\sum_{n=1}^\\infty \\sum_{m=1}^n f_{ij}(m)p_{jj}(n-m)\\\\ &amp;=\\sum_{m=1}^\\infty \\sum_{n=m}^\\infty  f_{ij}(m)p_{jj}(n-m) \\quad\\text{triangle choice}\\\\ &amp;=\\sum_{m=1}^\\infty f_{ij}(m) \\sum_{n=m}^\\infty p_{jj}(n-m)\\\\ &amp;=\\sum_{m=1}^\\infty f_{ij}(m) \\sum_{n=0}^\\infty p_{jj}(n)\\quad\\text{by adjusting index}\\\\ &amp;=f_{ij} \\sum_{n=0}^\\infty p_{jj}(n)\\leq \\sum_{n=0}^\\infty p_{jj}(n)&lt;\\infty. \\end{align*} \\] <p>so \\(\\lim\\limits_{n\\rightarrow\\infty}p_{ij}(n)=0\\).</p>"},{"location":"Math/StochasticProcesses/Markov/#limit-stationary-distribution","title":"Limit &amp; Stationary Distribution","text":"<p>Actually, we hope to find no matter whether the initial state is, after enough steps, after enough number of sampling, the state has a convergent distribution. That is, whether </p> \\[ \\lim_{n\\rightarrow \\infty}P(x_n=j) \\] <p>exists? </p> <p>We have the following necessary condition.</p> <p>Existence for limit distribution</p> <p>If \\(\\forall i\\in S\\), \\(\\exists\\) a distribution \\(\\nu_{i}=\\{\\nu_{ij}\\}_{j\\in S}\\) such that \\(\\lim\\limits_{n\\rightarrow \\infty}p_{ij}(n)=\\nu_{ij}\\), then limit distribution exists.</p> Proof <p>By total probability formula</p> \\[ \\begin{align*} p_j(n)&amp;=P(X_n=j)\\\\ &amp;=\\sum_{i=1}^\\infty P(X_0=i)P(X_n=j\\mid X_0=i)\\\\ &amp;=\\sum_{i=1}^\\infty P(X_0=i) p_{ij}(n)\\\\ \\end{align*} \\] <p>So let \\(n\\rightarrow \\infty\\), and by LDCT we have</p> \\[ \\begin{align*} \\lim_{n\\rightarrow \\infty} p_j(n)&amp;=\\lim_{n\\rightarrow \\infty} \\sum_{i=1}^\\infty P(X_0=i)p_{ij}(n)\\\\ &amp;=\\sum_{i=1}^\\infty  P(X_0=i)\\lim_{n\\rightarrow \\infty}p_{ij}(n)\\\\ &amp;=\\sum_{i=1}^\\infty  P(X_0=i)\\nu_{ij} \\end{align*} \\] <p>So define \\(\\mu_j=\\lim\\limits_{n\\rightarrow \\infty} p_j(n)=\\sum\\limits_{i=1}^\\infty  P(X_0=i)\\nu_{ij}\\), which is a distribution, since</p> \\[ \\sum_{j=1}^\\infty \\mu_j=\\sum_{j=1}^\\infty \\sum_{i=1}^\\infty  P(X_0=i)\\nu_{ij}=1. \\] <p>So we have to know whether transition probability converges as steps go to infinity. In actual practice, this means calculation of \\(\\lim\\limits_{n\\rightarrow \\infty}\\pmb{P}^{(n)}=\\lim\\limits_{n\\rightarrow \\infty}\\pmb{P}^n\\).</p> <p>Limit distribution is connected close to the following stationary distribution.</p> <p>Definition of Stationary distribution</p> <p>Assume there exists a distribution \\(\\{\\pi_k\\}_{k\\geq 0}\\) such that for all state \\(k\\in S\\), we have</p> \\[ \\pmb{\\pi}=\\pmb{\\pi}\\pmb{P} \\] <p>where \\(\\pi=[\\pi_0,\\pi_1,\\cdots]\\) is a row vector. </p> <p>The above definition implies</p> \\[ \\pi_k=\\sum_{j=0}^\\infty \\pi_j p_{jk},\\quad k=0,1,\\cdots \\] <p>which also have a practical meaning</p> \\[ \\pi_k(1-p_{kk})=\\sum_{j=0\\atop j\\neq k}^\\infty \\pi_j p_{jk},\\quad k=0,1,\\cdots \\] <p>which means the probability of leaving \\(k\\) and entering \\(k\\) in one step are the same.</p> <p>A Markov chain with an initial Stationary distribution is Strictly stationary process</p> <p>A Markov chain with an initial Stationary distribution is strictly stationary process. That is, if</p> \\[ p_k(0)=\\pi_k,\\quad k\\geq 0 \\] <p>where \\(p_k(0)=P(X_0=k)\\), \\(\\{\\pi_k\\}\\) is a stationary distribution of the Markov chain, then for all \\(n\\geq 1\\), we have</p> \\[ p_k(n)=P(X_n=k)=\\pi_k. \\] Proof <p>Since \\(\\pi_k=\\sum_{j=0}^\\infty \\pi_j p_{jk}\\), then </p> \\[ p_{k}(1)=\\sum_{j=0}^\\infty p_j(0)p_{jk}=\\sum_{j=0}^\\infty \\pi_j p_{jk}=\\pi_k. \\] <p>By induction, or Markov characteristic, we are done.</p> <p><p>\\(\\square\\)</p></p> <p>Now we could talk about the relationship between limit distribution and stationary distribution.</p> <p></p> <p>Lemma: First-passage probability with transitivity</p> <p>If state \\(i\\rightarrow j\\), \\(i\\) is a recurrent state, then \\(f_{ij}=1\\) and \\(j\\rightarrow i\\).</p> Proof <p>\\(j\\rightarrow i\\) is apparent because after reaching \\(j\\), \\(\\exists n'&gt;0\\) such that \\(p_{ji}(n')&gt;0\\). It is a little hard to prove \\(f_{ij}=1\\), i.e. we could reach \\(j\\) in finite many times.</p> <p>A lot of proof could be used here. We give two versions. The first, in my opinion, is the simplest.</p> <p>Assume \\(X_0=i\\), since \\(i\\rightarrow j\\), \\(\\exists n&gt;0\\), such that \\(p_{ij}(n)&gt;0\\). Now we make use of the properties of recurrent state \\(i\\). We check the number of failure for reaching \\(j\\). </p> <p>If \\(X_n\\neq j\\), then \\(\\exists T_1=\\min\\{k\\geq n: X_k=i\\}\\) (we fail to reach \\(j\\), but afterwards we could still reach \\(i\\)). Check if \\(X_{T_1+n}\\neq j\\), then \\(\\exists T_2=\\min\\{k\\geq n+T_1: X_k=i\\}\\). Follow this routine, we define </p> \\[ T_l=\\min\\{k\\geq n+ T_{l-1} : X_k=i\\},\\quad l=1,2,\\cdots \\] <p>and \\(T_0=0\\) as default. Define \\(\\zeta=\\#\\{l\\geq 0: X_{T_l+n}\\neq j\\}\\), which is the number of failing reaching state \\(j\\) before success after \\(T_l+n\\) steps (if successfully reaching \\(j\\), then we stop the routine and return \\(l\\)). Then easy to see that </p> \\[ P(\\zeta=k)=(1-p_{ij}(n))^k p_{ij}(n) \\] <p>(fail \\(k\\) times and success at the end). This random variable is reasonable, because \\(i\\) is recurrent. So \\(\\zeta\\) follows geometric distribution and </p> \\[ E\\zeta=\\frac{1-p_{ij}(n)}{p_{ij}(n)}&lt;\\infty. \\] <p>which means in average, we fail in finite many times, and \\(\\zeta&lt;\\infty\\) a.s. or \\((P(\\zeta=\\infty)=0)\\). So \\(\\exists T&lt;\\infty\\) such that \\(X_{T}=j\\), which means \\(f_{ij}=1\\).</p> <p><p>\\(\\square\\)</p></p> <p>Theorem: Relationship between stationary and limit distribution</p> <p>Assume \\(\\{X_n\\}\\) is an irreducible Markov chain.</p> <p>(i) it has a unique stationary distribution, iff all of its state are positive recurrent, and</p> \\[ \\lim\\limits_{n\\rightarrow \\infty}\\frac{1}{n}\\sum_{k=1}^n p_{ij}(k) = \\pi_j. \\] <p>(ii) if \\(X_n\\) is ergodic, then its stationary distribution equals the limit distribution, i.e. for all state \\(i,j\\), </p> \\[ \\lim\\limits_{n\\rightarrow \\infty}p_{ij}(n)=\\pi_j \\] <p>which means it is also irrelevant with initial state.</p> Proof for (i) <ul> <li>Necessary. </li> </ul> <p>Assume it has a unique stationary distribution, then </p> \\[ \\pi_k=\\sum_{j=0}^\\infty \\pi_j p_{jk}=\\sum_{j=0}^\\infty \\pi_j p_{ik}(n),\\quad k\\geq 1. \\] <p>Let \\(n\\rightarrow \\infty\\), exchange the order of limit and summation, we have </p> \\[ \\pi_k=\\lim_{n\\rightarrow \\infty}\\sum_{j=0}^\\infty \\pi_j p_{ik}(n)=\\sum_{j=0}^\\infty  \\pi_j \\lim_{n\\rightarrow \\infty} p_{ik}(n) =\\sum_{j=0}^\\infty  \\pi_j \\cdot 0=0 \\] <p>for null recurrent state and transient state, which contradicts! Since it is irreducible, then all the states is positive recurrent.</p> <ul> <li>Sufficient. We focus on the limit of \\(p_{ij}(n)\\) in formulating stationary distribution.</li> </ul> <p>Since \\(i\\leftrightarrow j\\), by Lemma: First-passage probability with transitivity \\(f_{ij}=1\\). Since all the states are positive recurrent, by Limit theorem for states starting from different initial state</p> \\[ \\lim_{n\\rightarrow \\infty} p_{ij}(n)=\\frac{f_{ij}}{\\mu_j}=\\frac{1}{\\mu_j},  \\] <p>We want to prove that \\(\\{\\frac{1}{\\mu_j}, j\\geq 0\\}\\) is the stationary distribution.</p> <p>Firstly, since \\(\\sum\\limits_{j=0}^\\infty p_{ij}(n)=1\\), we want to take limit of \\(n\\) to get \\(\\frac{1}{\\mu_j}\\), i.e. by Fatou Lemma, </p> \\[ \\sum_{j=0}^\\infty \\frac{1}{\\mu_j} =\\sum_{j=0}^\\infty \\lim_{n\\rightarrow \\infty} p_{ij}(n)\\leq \\lim_{n\\rightarrow \\infty} \\sum_{j=0}^\\infty p_{ij}(n)=1. \\] <p>Secondly, since </p> \\[ p_{ij}(n+1)=\\sum_{k=0}^\\infty p_{ik}(n)p_{kj}(1) \\] <p>still by Fatou lemma, we have</p> \\[ \\frac{1}{\\mu_j}=\\lim_{n\\rightarrow \\infty} \\sum_{k=0}^\\infty p_{ik}(n)p_{kj}(1)\\geq \\sum_{k=0}^\\infty\\lim_{n\\rightarrow \\infty} p_{ik}(n) p_{kj}(1)=\\sum_{k=0}^\\infty \\frac{1}{\\mu_k} p_{kj}. \\] <p>Actually, for a stationary distribution, we shoule prove \\(\\frac{1}{\\mu_j}=\\sum_{k=0}^\\infty \\frac{1}{\\mu_k} p_{ik}\\). We show this by contradiction. If \\(\\exists j_0\\) such that </p> \\[ \\frac{1}{\\mu_{j_0}}&gt;\\sum_{k=0}^\\infty \\frac{1}{\\mu_k} p_{kj_0} \\] <p>then</p> \\[ \\sum_{j=0}^\\infty \\frac{1}{\\mu_j}&gt;\\sum_{j=0}^\\infty\\sum_{k=0}^\\infty \\frac{1}{\\mu_k} p_{kj}=\\sum_{k=0}^\\infty\\frac{1}{\\mu_k} \\sum_{j=0}^\\infty p_{kj}=\\sum_{k=0}^\\infty\\frac{1}{\\mu_k} \\] <p>which contradicts!</p> <p>Lastly, we prove that \\(\\{\\frac{1}{\\mu_j},j\\geq 0\\}\\) is a distribution, i.e. \\(\\sum_{j=0}^\\infty \\frac{1}{\\mu_j}=1\\). With the same result, we have \\(\\frac{1}{\\mu_j}=\\sum_{k=0}^\\infty \\frac{1}{\\mu_k} p_{kj} (m)\\) (iteratively, proved by readers). So</p> \\[ \\begin{align*} \\frac{1}{\\mu_j} &amp;= \\lim_{m\\rightarrow \\infty} \\sum_{k=0}^\\infty \\frac{1}{\\mu_k} p_{kj} (m)\\\\ &amp;= \\sum_{k=0}^\\infty  \\frac{1}{\\mu_k} \\lim_{m\\rightarrow \\infty} p_{kj} (m)\\\\ &amp;= \\sum_{k=0}^\\infty  \\frac{1}{\\mu_k} \\frac{1}{\\mu_j}\\\\ \\Rightarrow \\quad \\sum_{j=0}^\\infty \\frac{1}{\\mu_j} &amp;= \\sum_{k=0}^\\infty \\frac{1}{\\mu_k} \\sum_{j=0}^\\infty \\frac{1}{\\mu_j}\\leq \\sum_{j=0}^\\infty \\frac{1}{\\mu_j}. \\end{align*} \\] <p>From the above proof, we have the following useful theorem.</p> <p>Corollary: Theorem for mean recurrence time</p> <p>Assume \\(X_n\\) is a non-periodic irreducible Markov chain, then its stationary distribution</p> \\[ \\pi_{j}=\\frac{1}{\\tau_j},\\quad j\\in S, \\] <p>where \\(\\tau_j=ET_j\\) is the mean recurrence time of state \\(j\\).</p>"},{"location":"Math/StochasticProcesses/Markov/#decomposition-of-state-space","title":"Decomposition of State Space","text":"<p>Now we come back to the relationship between communicating class and recurrent state.</p> <p>Firstly, let us talk abuot the number of elements in the Markov chain which plays a role in determining recurrence.</p> <p>Lemma: Number of elements</p> <p>(i) If A Markov chain has finitely many number of elements, then there exists at least one positive recurrent state. Moreover, if the chain is irreducible, then all of the states are positive recurent.</p> Proof <p>We prove by contradiction.</p> <p>Assume there is no recurrent states, then all the states must be transient or null recurrent. Then by Limit theorem for states starting from different initial state, we have for all state \\(i\\), \\(j\\)</p> \\[ \\lim_{n\\rightarrow \\infty}p_{ij}(n)=0. \\] <p>Still make use of the normality of transitive probability.</p> \\[ 1=\\lim_{n\\rightarrow \\infty}\\sum_{j=1}^\\infty p_{ij}(n)=\\sum_{j=1}^\\infty \\lim_{n\\rightarrow \\infty} p_{ij}(n)=0, \\] <p>which contradicts!</p> <p>If it is irreducible, then by Transitivity of positive and null recurrent states, we have all the states are positive recurrent.</p> <p><p>\\(\\square\\)</p></p> <p></p> <p>Relationship between communicating class and recurrent state</p> <p>(i) If state \\(i\\) is recurrent, then its communicating class is closed.</p> <p>(ii) If a finite-element communicating class is closed, then all state of it is positive recurrent.</p> Proof <p>(i) By contradiction. If \\(C(i)\\) is not closed, then \\(\\exists k\\not\\in C(i)\\), such that \\(i\\rightarrow k\\), but \\(k\\not\\rightarrow i\\). So \\(\\exists n&gt;0\\)</p> \\[ P(\\mu_i=\\infty \\mid X_0=i)\\geq p_{ik}(n)P(\\mu_i=\\infty\\mid X_0=k)&gt;0. \\] <p>(ii) Limit \\(\\{X_n\\}\\) on \\(C(i)\\), which is a irreducible Markov chain. So it must be positive recurrent Markov chain.</p> <p>Decomposition of Markov Chain</p> <p>The state space \\(S\\) of Markov chain could be decomposed into denumerable mutually disjoint state sets, i.e.</p> \\[ S=\\mathcal(N)\\cup C_1\\cup C_2\\cup \\cdots\\cup C_n, \\] <p>where \\(C_i\\cap C_j=\\varnothing\\) whenever \\(i\\neq j\\), \\(\\mathcal{N}\\) is the set of all transient states.</p> Proof <p>Iterate over all elements of \\(S\\). Arbitrarily choose a state \\(j_1\\in S\\), let \\(C_1=C(j_1)\\), if \\(j_1\\) is a recurrent state; otherwise let \\(C_1=\\{j_1\\}\\), i.e. \\(j_1\\) is a transient state. Next round, Arbitrarily choose a state \\(j_2\\in S-C_1\\), let \\(C_2=C(j_2)\\), if \\(j_2\\) is a recurrent state; otherwise let \\(C_2=\\{j_2\\}\\), i.e. \\(j_2\\) is a transient state.</p> <p>Continue until \\(S-C_1-C_2-\\cdots=\\varnothing\\). We combine all the transient states together to \\(\\mathcal{N}\\). Notice \\(n\\) could be \\(\\infty\\).</p>"},{"location":"Math/StochasticProcesses/Markov/#inverse-markov-chain","title":"Inverse Markov Chain","text":"<p>Check whether the following Markov property holds?</p> \\[ P(X_{n}=j\\mid X_{n+1}=i,X_{n+2}=i_{n+2},\\cdots, X_N=i_N)=P(X_{n}=j\\mid X_{n+1}=i) \\] Proof \\[ \\begin{align*} P(X_{n}=j&amp; \\mid X_{n+1}=i,X_{n+2}=i_{n+2},\\cdots, X_N=i_N)\\\\ &amp;=\\frac{p_j(n)p_{ji} p_{i i_{n+2}}\\cdots p_{i_{N-1}i_{N}}}{p_{i}(n+1)p_{ii_{n+2}}\\cdots p_{i_{N-1}i_{N}}}\\\\ &amp;= \\frac{P(X_n=j)}{P(X_{n+1}=i)}p_{ji}\\\\ &amp;=P(X_{n}=j\\mid X_{n+1}=i). \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>Actually, we need some special condition to interpret.</p> <p>The first is to making use of stationary distribution. That is, if the initial distribution is \\(\\pi\\), then \\(p_{j}(n)=\\pi_j\\), \\(p_{i}(n+1)=\\pi_i\\), and </p> \\[ \\frac{P(X_n=j)}{P(X_{n+1}=i)}p_{ji}=\\frac{\\pi_j}{\\pi_i}p_{ji} \\] <p>which is irrelevant with time.</p> <p>Now we have a new but natural idea of a special term: Inverse distribution.</p> <p>Definition of inverse distribution</p> <p>If</p> \\[ (X_1,X_0)\\overset{d}{=}(X_0,X_1), \\] <p>i.e.</p> \\[ \\pi_i p_{ij}=\\pi_j p_{ji} \\] <p>then the whole Markov chain is inversible.</p> <p>Actually, having a stationary distribution is a necessary condition for a Markov chain to be inverse.</p> <p>Kolmogorov Criterion</p> <p>Assume \\(X_n\\) is a irreducible stationary Markov chain, then it is inversible iff for all closed path </p> \\[ i_0,i_1,\\cdots,i_{m-1}, i_m=i_0 \\] <p>we have</p> \\[ p_{i_0i_1}p_{i_1i_2}\\cdots p_{i_{m-1}i_0}=p_{i_0 i_{m-1}}\\cdots p_{i_2i_1}p_{i_1i_0}. \\] Proof <ul> <li>necessary.</li> </ul> <p>If \\(X\\) is inversible, then \\(\\forall i,j\\in S\\), \\(\\pi_ip_{ij}=\\pi_jp_{ji}\\). So</p> \\[ \\begin{align*} \\pi_{i_0} p_{i_0i_1}p_{i_1i_2}\\cdots p_{i_{m-1}i_m}&amp;=\\pi_{i_1} p_{i_1i_0}p_{i_1i_2}\\cdots p_{i_{m-1}i_m}\\\\ &amp;=p_{i_1i_0}\\pi_{i_1} p_{i_1i_2}\\cdots p_{i_{m-1}i_m}\\\\ &amp;=p_{i_1i_0} p_{i_2i_1}\\pi_{i_2}\\cdots p_{i_{m-1}i_m}\\\\ &amp;=p_{i_1i_0} p_{i_2i_1}\\cdots \\pi_{i_{m-1}} p_{i_{m-1}i_m}\\\\ &amp;=p_{i_1i_0} p_{i_2i_1}\\cdots  p_{i_m i_{m-1}} \\pi_{i_{m}}\\\\ &amp;=p_{i_1i_0} p_{i_2i_1}\\cdots  p_{i_0 i_{m-1}} \\pi_{i_{0}}\\\\ &amp;=\\pi_{i_0}p_{i_0 i_{m-1}}\\cdots p_{i_2i_1}p_{i_1i_0}. \\end{align*} \\] <ul> <li>Sufficient. This is similar to a curve integral.</li> </ul> <p>Fix a state \\(j_0\\), by assumption, for any state \\(j\\), there must be a path \\(j, j_n, j_{n-1},\\cdots, j_{1},j_0\\) such that </p> \\[ p_{jj_n}p_{j_nj_{n-1}}\\cdots p_{j_1j_0}&gt;0 \\] <p>and then we could define</p> \\[ \\pi_j=\\frac{1}{Z}\\frac{p_{j_0j_1}\\cdots p_{j_{n} j}}{p_{j_n j}\\cdots p_{j_1j_0}} \\] <p>where \\(Z\\) is made for \\(\\sum_{j}\\pi_j=1\\). Then it is easy to show that the above definition is well-defined, and for stationary distribution, we have to show for \\(j,k \\in S\\),</p> \\[ \\pi_jp_{jk}=\\pi_kp_{kj}. \\] <p>If \\(p_{jk}=p_{kj}=0\\), then the above holds naturally. If \\(p_{jk}\\neq 0\\), then by assumption, we could define </p> \\[ \\pi_k:=\\frac{1}{Z}\\frac{p_{j_0j_1}\\cdots p_{j_{n} j}p_{jk}}{p_{kj}p_{j_n j}\\cdots p_{j_1j_0}} \\] <p>and it also holds.</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Markov/#branching-process","title":"Branching Process","text":"<p>This is a special case of Markov chain.</p> <p>we have to solve the smallest positive root of \\(\\phi(s)=s\\), where </p> \\[ \\phi(s)=E(s^{\\xi}) \\] <p>\\(\\xi\\) is the product.</p>"},{"location":"Math/StochasticProcesses/Martingale/","title":"Martingale","text":"<p>Reference</p> <ul> <li>Probability Theory and Examples, Rick Durrett</li> </ul> <p>If you choose to stop playing at some bounded time \\(N\\), then your expected winnings \\(EX_N\\) are equal to your initial fortune \\(X_0\\).</p>"},{"location":"Math/StochasticProcesses/Martingale/#radon-nikodym-theorem","title":"Radon-Nikodym Theorem","text":"<p>We shall talk about signed measure, which differs from measure in that it takes values on \\((-\\infty,\\infty]\\) or \\([-\\infty,\\infty)\\). It cannot be both \\(-\\infty\\) or \\(\\infty\\) since it would cause \\(\\alpha(A)+\\alpha(B)\\) to be ambiguous. We ignore the second situation for simplicity. </p> <p>definition of signed measure</p> <p>\\(\\alpha\\) is said to be a signed measure if </p> <p>(i) it takes values on \\((-\\infty, +\\infty]\\), </p> <p>(ii) \\(\\alpha(\\varnothing)=0\\),</p> <p>(iii) if \\(E=\\bigcup_n E_n\\) is a disjoint union then \\(\\alpha(E)=\\sum_n \\alpha(E_n)\\).</p> <p>Here gives some examples about signed measure.</p> <p></p> <p>Example. Assume \\(\\mu\\) is a measure, \\(f\\) is a function with \\(\\int_A f^- d\\mu&lt;\\infty\\), define </p> \\[ \\alpha(A):=\\int_A f d\\mu. \\] <p>Show that \\(\\alpha\\) is a signed measure.</p> <p>Example. Assume \\(\\mu_1\\) and \\(\\mu_2\\) are measures with \\(\\mu_2(\\Omega)&lt;\\infty\\), define </p> \\[ \\alpha(A):=\\mu_1(A)-\\mu_2(A). \\] <p>Show that \\(\\alpha\\) is a signed measure.</p> Proof <p>The above conclusion holds by sigma-additivity of integral and measure. The only work is left to show that disjoint additivity holds at \\(\\alpha(E)=\\infty\\).</p> <p><p>\\(\\square\\)</p></p> <p>A set \\(A\\) is positive if for all its subset \\(E\\), \\(\\alpha(E)\\geq 0\\). A set \\(A\\) is positive if for all its subset \\(E\\), \\(\\alpha(E)\\leq 0\\). </p> <p>Example. Show that In Example for integral-signed-measure, \\(A\\) is positive iff </p> \\[ \\mu(A\\cap \\{f(x)&lt;0\\})=0. \\] Proof <ul> <li> <p>Necessary. Choose subset \\(A\\cap \\{f(x)&lt;0\\}\\) yields the result.</p> </li> <li> <p>Sufficient. Decompose any subset \\(E\\) with \\(E\\cap \\{f(x)&lt;0\\}\\) and \\(E\\cap \\{f(x)\\geq 0\\}\\) and complete the proof.</p> </li> </ul> <p><p>\\(\\square\\)</p></p> <p></p> <p>Hahn decomposition</p> <p>Assume \\(\\alpha\\) is a signed measure. There exists a positive set \\(A\\) and a negative set \\(B\\) such that \\(\\Omega=A\\cup B\\) and \\(A\\cap B=\\varnothing\\).</p> Proof <p>Let \\(c=\\inf\\{\\alpha(B): B \\text{ is negative}\\}\\leq 0\\). Choose \\(\\{B_n\\}\\) such that \\(\\alpha(B_n)\\searrow c\\). Thus \\(B=\\bigcup_n B_n\\) is a negative set by Lemma 1: Property of positive/negative set. We claim \\(A:=\\Omega-B\\) is a positive set. We show by contradiction. If not, then there exists a subset \\(E\\) of \\(A\\), with \\(\\alpha(E)&lt;0\\). Then by Lemma 2: Non-positive set must have a pure-negative subset, there exsits a negative set \\(F\\subset E\\subset A\\) with \\(\\alpha(F)&lt;0\\). So \\(B\\cup F\\) is also a negative set by Lemma 1. Thus</p> \\[ \\alpha(B\\cup F)=\\alpha(B)+\\alpha(F) &lt; c, \\] <p>which contradicts!</p> <p><p>\\(\\square\\)</p></p> <p></p> Lemma 1: Property of positive/negative set <p>The following result is the same for negative sets.</p> <p>(i) Every subset of a positive set is positive.</p> <p>(ii) If \\(\\{A_n\\}_{n\\geq 1}\\subset \\Omega\\) are positive sets, then \\(A:=\\bigcup_{n} A_n\\) is also a postive set.</p> <ul> <li>Proof. (i) is trivial. For (ii), We have to use disjoint decomposition of \\(A\\) to decompose any subset \\(E\\), thus gives the result using sigma-additivity. Indeed, let </li> </ul> \\[ B_n=A_n-\\left(\\bigcup_{i=1}^{n-1}A_n\\right)\\subset A_n \\] <p>so we have \\(B_n\\) is still a positive set by (i) and mutually disjoint. We have \\(\\bigcup_n A_n =\\bigcup_n B_n\\). For any subset \\(E\\) of \\(A\\), let \\(E_n = E\\cap B_n\\) is also a positive set, and \\(E=\\bigcup_n E_n\\). By sigma-additivity, we have</p> \\[ \\alpha(E)=\\bigcup_n \\alpha(E_n)\\geq 0. \\] <p>Notice that subadditivity gives an opposite direction, which fails here.</p> <p><p>\\(\\square\\)</p> </p> <p></p> Lemma 2: Non-positive set must have a pure-negative subset <p>Assume \\(E\\) is a measurable set with \\(\\alpha(E) &lt; 0\\), then there exists a subset of \\(E\\), denoted by \\(F\\), which satisfies \\(F\\) is a negative set with $\\alpha(F)&lt; 0 $.</p> <p>Remark here that \\(\\alpha(F)\\) is strictly less than \\(0\\), which is given by \\(\\alpha(E)&lt;0\\).</p> <ul> <li>Proof. If \\(E\\) is nagative, we are done. Suppose \\(E\\) is not a negative set, which means \\(\\exists E_1\\subset E\\) s.t. \\(\\alpha(F_1)&gt; 0\\). Choose a smallest integer \\(n_1\\) such that \\(\\alpha(E_1)&gt;1/n_1\\) holds for a plus-sign set \\(E_1\\subset E\\). Let \\(F_1=E-E_1\\). If \\(F_1\\) is a negative set, we are done. Otherwise, choose a smallest integer \\(n_2\\) such that \\(\\alpha(E_2)&gt;1/n_2\\) holds for a plus-sign \\(E_2\\subset F_1\\). Continue the procedure, we have</li> </ul> \\[ F_n=E-\\bigcup_{i=1}^{n} E_i \\] <p>with \\(n_1&gt;n_2&gt;\\cdots\\). Let \\(n\\rightarrow \\infty\\), we have a limit set \\(F=E-\\bigcup_n E_n\\). By construction, we have \\(E_i\\cap E_j=\\varepsilon\\) for \\(i\\neq j\\). So by sigma-additivity (definition of signed measure),</p> \\[ \\alpha(E)=\\alpha(F)+\\sum_{n} \\alpha(E_n). \\] <p>Thus \\(\\alpha(F)\\leq \\alpha(E)&lt;0\\). Since \\(E\\) is finite-measure, so \\(\\sum_{n} \\alpha(E_n)\\) converges, therefore \\(\\alpha(E_n) \\rightarrow 0\\) and \\(n_i \\rightarrow \\infty\\) as \\(i\\rightarrow \\infty\\). Now we claim \\(F\\) is a negative set, for if not, we choose an integer \\(N\\) for a plus-sign subset \\(F'\\), which contradicts!</p> <p><p>\\(\\square\\)</p></p> <p>Example. Show that in example for integral-signed-measure, \\(A\\) could be</p> \\[ \\{f(x)&gt;0\\}\\subset A \\subset \\{f(x)\\geq 0\\}. \\quad a.e. \\] <p>Two measures \\(\\mu_1\\), \\(\\mu_2\\) are said to be singular with each other, if there exsits a set \\(A\\) with \\(\\mu_1(A)=0\\) and \\(\\mu_2(A^c)=0\\). Now the following theorem gives the uniqueness for a signed measure, which is a subtraction of two mutually singular measures.</p> <p>Jordan decomposition</p> <p>Assume \\(\\alpha\\) is a signed measure, then there exist unique measures \\(\\alpha_+\\) and \\(\\alpha_-\\) such that \\(\\alpha=\\alpha_+-\\alpha_-\\) with \\(\\alpha_+\\) and \\(\\alpha_-\\) mutually singular. </p> Proof <ul> <li>Existence. According to a signed measure \\(\\alpha\\), we have \\(\\Omega=A\\cup B\\) by Hahn decomposition. Define </li> </ul> \\[ \\alpha_+(E) = \\alpha(E\\cap A),\\quad \\alpha_- = -\\alpha(E\\cap B), \\] <p>which turn out to be measures. Choose \\(A\\) and we have \\(\\alpha_+(A^c) = 0\\) and \\(\\alpha_-(A)=0\\), therefore \\(\\alpha_+\\) and \\(\\alpha_-\\) are mutually singular.</p> <ul> <li>Uniqueness. Here the uniqueness is given by equivalence of measure, disregarding zero-measure situation. Assume \\(\\alpha=\\nu_1-\\nu_2\\), and one corresponding set pair \\(C, D\\) for its Hahn decomposition \\(\\Omega=C\\cup D\\). Let \\(\\nu_1(C)=0\\). So we have \\(A\\cap D\\), \\(B\\cap C\\) are null set, meaning \\(\\alpha_+(A\\cap D)=\\alpha(A\\cap D)=0\\). Thus </li> </ul> \\[ \\alpha_+(E\\cap A)=\\alpha_+(E\\cap (A\\cup C))=\\alpha_+(E\\cap C), \\] <p>which gives \\(\\alpha_+=\\nu_1\\). Similar logic gives \\(\\alpha_-=\\nu_2\\) and we complete the proof.</p> <p><p>\\(\\square\\)</p></p> <p>Now the following construction of \\(g\\) and decomposition of \\(\\nu\\) plays an important role in the proof of the final theorem.</p> <p></p> <p>Lebesgue decomposition</p> <p>Assume \\(\\nu\\), \\(\\mu\\) are \\(\\sigma\\)-finite measure, then there exsits \\(\\nu_r\\) and \\(\\nu_s\\) such that \\(\\nu=\\nu_r+\\nu_s\\), where \\(\\nu_s\\) is singular with respect to \\(\\mu\\), and </p> \\[ \\begin{align} \\nu_r(E)=\\int_E g d\\mu.\\label{nu-r-mu} \\end{align} \\] Proof <p>Since \\(\\Omega=\\bigcup_n \\Omega_n\\) and by condition, we assume without loss of generality, \\(\\nu\\) and \\(\\mu\\) are finite measure on \\(\\Omega\\). Let </p> \\[ \\mathcal{G}=\\{g\\geq 0: \\int_E g d\\mu \\leq \\nu(E),\\forall E\\subset \\Omega\\}. \\] <ul> <li>We first claim that if \\(g,h\\in \\mathcal{G}\\), then \\(g\\lor h\\in \\mathcal{G}\\). Indeed, let \\(A=\\{g&lt;h\\}\\), then for each \\(E\\subset \\Omega\\),</li> </ul> \\[ \\int_E g \\lor h d\\mu = \\int_{E\\cap A} h d\\mu + \\int_{E\\cap A^c} g d\\mu\\leq \\nu(E\\cap A)+\\nu(E\\cap A^c)=\\nu(E). \\] <ul> <li>Then we formulate a function \\(h\\) and define the correponding measure \\(\\nu_r\\) like in \\(\\ref{nu-r-mu}\\). Actually, for each \\(E\\), define \\(\\mathbb{k}=\\sup\\{\\int_E g d\\mu: g\\in \\mathcal{G}\\}\\). Choose \\(\\{g_n\\}\\) such that \\(\\int_E gd\\mu \\geq \\mathbb{k}-\\frac{1}{n}\\). Define \\(h_n=g_1\\lor\\cdots\\lor g_n\\). Then \\(h_n\\geq g_n\\) and \\(h_n\\nearrow h\\). By Levy's Theorem, we have</li> </ul> \\[ \\mathbb{k}=\\int_E h d\\mu=\\lim_{n\\rightarrow \\infty} \\int_E h_n d\\mu\\geq  \\lim_{n\\rightarrow \\infty} \\int_E g_n d\\mu=\\mathbb{k}. \\] <p>Now we let \\(\\nu_r=\\int_E h d\\mu\\) and \\(\\nu_s=\\nu-\\nu_r\\).</p> <ul> <li>We finally have to show that \\(\\nu_s\\) is singular with respect to \\(\\mu\\). This is a little tricky. For all \\(\\varepsilon\\), by Hahn decomposition, we have \\(\\Omega=A_\\varepsilon \\cup B_\\varepsilon\\) with respect to signed measure \\(\\nu_s-\\varepsilon \\mu\\) (Let \\(\\nu_s-\\varepsilon \\mu(A_\\varepsilon)\\geq 0\\)). We contend that \\(\\mu(A_\\varepsilon)=0\\). Indeed, let \\(k=h +\\varepsilon 1_{A_\\varepsilon}\\), then </li> </ul> \\[ \\int_E k d\\mu = \\int_E h d\\mu + \\varepsilon \\mu(A_\\varepsilon)\\leq \\nu_r(E) + \\nu_s(E\\cap A_\\varepsilon)\\leq \\nu(E) \\] <p>which means \\(k\\in \\mathcal{G}\\). However, \\(h\\) has reached the superium, so \\(\\mu(A_\\varepsilon)=0\\). </p> <p>Choose \\(\\varepsilon_n=1/n\\), and define \\(A=\\bigcup_n A_n\\). We show that \\(A\\) makes the singularity of \\(\\nu_s\\) and \\(\\mu\\). Since \\(\\mu(A)\\leq \\sum_n \\mu(A_{1/n})\\) we have \\(\\mu(A)=0\\). We claim that \\(\\nu_s(A^c)=0\\) for if not, \\(\\nu_s(A^c)&gt;0\\), then \\((\\nu_s-\\varepsilon \\mu)(A^c)&gt;0\\) for some \\(\\epsilon\\), which contradicts, since \\(A^c\\subset B_\\varepsilon\\).</p> <p><p>\\(\\square\\)</p></p> <p>We call measure \\(\\nu\\) is absolutely continuous with measure \\(\\mu\\), denoted by \\(\\nu \\ll \\mu\\), if \\(\\mu(E)=0\\) implies \\(\\nu(E)=0\\). Example for integral-signed-measure gives a method to define a measure which is absolutely continuous with respect to another measure, which we would use in existence of conditional expectation.</p> <p>Radon-Nikodym Theorem</p> <p>Assume \\(\\nu\\), \\(\\mu\\) are \\(\\sigma\\)-finite measure, and \\(\\nu\\) is absolutely continuous with respect to \\(\\mu\\), then there exists a \\(g\\geq 0\\), such that \\(\\nu(E)=\\int_E g d\\mu\\). And for uniqueness, it is a equivalent category that \\(g=h\\), \\(\\mu, a.e.\\)</p> <p>The function \\(g\\) is usually denoted by \\(d\\nu/d\\mu\\).</p> Proof <p>By Lebesgue decomposition, we have two measure \\(\\nu_r\\) and \\(\\nu_s\\), with \\(\\nu_s\\) singular with respect to \\(\\mu\\), \\(\\nu_r(E)=\\int_E g d\\mu\\). </p> <p>We first show that \\(\\mu_s\\equiv 0\\). Indeed, since \\(\\nu\\) is absolutely continuous with \\(\\mu\\), then for \\(A\\) which follows that \\(\\nu_s(A^c)=0\\) and \\(\\mu(A)=0\\), it follows that \\(\\nu(A)=0\\), so \\(\\nu_s(A)=0\\), meaning \\(\\nu_s\\equiv 0\\). So \\(\\nu=\\nu_r\\). </p> <p>For uniqueness, suppose \\(g\\) and \\(h\\) both follows that \\(\\nu(E)=\\int_E g d\\mu=\\int_E h d\\mu\\), then \\(\\int_E g-h d\\mu\\). Choose finite-measure set \\(E\\subset \\{g&gt;h, g\\leq n\\}\\) for each \\(n\\), then we have \\(\\mu(g&gt;h, g\\leq n)=0\\) for each \\(n\\). So \\(\\mu(g&gt;h)=0\\). Similar logic gives \\(\\mu(g&lt;h)=0\\) and complete the proof.</p> <p><p>\\(\\square\\)</p></p> <p>Example. Show that the jacobi function \\(d\\nu/d\\mu\\) has the following properties.</p> <p>(i) Linearity. If \\(\\nu_1 \\ll \\mu\\) and \\(\\nu_2\\ll \\mu\\), then \\(\\nu_1+\\nu_2\\ll \\mu\\) and \\(d{(\\nu_1 +\\nu_2)}/d\\mu = d\\nu_1/d\\mu + d\\nu_2/d\\mu\\).</p> <p>(ii) If \\(\\nu \\ll \\mu\\) and \\(f\\geq 0\\), then \\(\\int_E f d\\nu=\\int_E f \\frac{d\\nu}{d\\mu}d\\mu\\).</p> <p>(iii) Transitivity. If \\(\\pi\\ll \\nu \\ll \\mu\\), then \\(d\\pi/d\\mu=d\\pi/d\\nu \\cdot d\\nu/d\\mu\\).</p> <p>(iv) Dual property. If \\(\\nu\\ll \\mu\\) and \\(\\mu\\ll\\\\nu\\), then \\(d\\nu/d\\mu = (d\\mu/d\\nu)^{-1}\\).</p> Proof <p>For (ii), use characteristic function, simple function and then for non-negative measurable function.</p> <p>For (iii), use (ii). For (iv), use (iii).</p> <p><p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Martingale/#conditional-expectation","title":"Conditional Expectation","text":"<p>Definition</p> <p>Given are a probability space \\((\\Omega,\\mathcal{F}_o,P )\\), a \\(\\sigma\\)-field \\(\\mathcal{F}\\in \\mathcal{F}_o\\), and a random variable \\(X \\in \\mathcal{F}_o\\) with \\(E|X|&lt;\\infty\\). We define the conditional expectation of \\(X\\) given \\(\\mathcal{F}\\), denoted by \\(E(X|\\mathcal{F})\\), to be any random variable \\(Y\\) that satisfies</p> <p>(i) \\(Y\\in \\mathcal{F}\\), i.e. is \\(\\mathcal{F}\\) measurable.</p> <p>(ii) for all \\(A\\in \\mathcal{F}\\), \\(\\int_A XdP=\\int_A Y dP\\). We could write this as</p> \\[ E(Y;A)=E(X;A),\\quad \\forall A\\in\\mathcal{F}. \\] <p>Choose \\(A=\\Omega\\), then we have \\(EX=E(E(X|\\mathcal{F}))\\).</p> <p>The first thing to be settled is that the conditional expectation exists and is unique.</p> <p>Existence &amp; Uniqueness</p> <p>\\(E(X|\\mathcal{F})\\) exists and is unique.</p> <p></p> Proof for existenceProof for uniqueness <ul> <li>We first show that CE \\(Y\\) is integrable. That is, if \\(Y=E(X|\\mathcal{F})\\) is \\(\\mathcal{F}\\) integrable. Let \\(A=\\{Y&gt;0\\}\\in \\mathcal{F}\\), then</li> </ul> \\[ \\int_A YdP = \\int_A X dP \\leq \\int_A |X| dP \\] <p>and</p> \\[ \\int_{A^c} -Y dP = \\int_{A^c} -XdP\\leq \\int_{A^c} |X|dP \\] <p>adding both together, we have \\(E|Y|\\leq E|X|&lt;\\infty\\).</p> <ul> <li>We assume \\(X\\geq 0\\), then define</li> </ul> \\[ \\nu(E):=\\int_E X dP,\\quad E\\in \\mathcal{F} \\] <p>which is a measure by Corollary 3: Theorem of denumberable additivity of LDCT. Naturally \\(\\nu\\ll P\\). Then by Radon Nikodym Theorem, there exsits a measurable function \\(d\\nu/dP\\) such that</p> \\[ \\int_E X dP = \\int_E \\frac{d\\nu}{dP} dP. \\] <p>Let \\(E=\\Omega\\) and we have \\(d\\nu/dP\\) is integrable, which is a version of \\(E(X|\\mathcal{F})\\).</p> <ul> <li>General case. That is, \\(X=X^+ - X^-\\) with \\(Y^+=E(X^+|\\mathcal{F})\\) and \\(Y^-=E(X^-|\\mathcal{F})\\). Then \\(Y^+ - Y^-\\) is integrable by the former discussion. Then for all \\(E\\in \\mathcal{F}\\),</li> </ul> \\[ \\begin{align*} \\int_E X dP &amp;= \\int_E X^+ - X^- dP\\\\  &amp;= \\int_E Y^+ - \\int_E Y^- dP\\\\ &amp;= \\int_E (Y^+ - Y^-) dP \\end{align*} \\] <p>thus \\(Y^+-Y^-\\) could be one version of \\(E(X|\\mathcal{F})\\) and complete the proof.</p> <p><p>\\(\\square\\)</p></p> <p>Assume \\(Y'\\) also satisfies the above definition. Let \\(A=\\{Y-Y'\\geq \\varepsilon&gt;0\\}\\in \\mathcal{F}\\), then </p> \\[ 0=\\int_A X-XdP=\\int_A Y-Y'dP\\geq \\varepsilon P(A) \\] <p>which gives \\(P(A)=0\\), meaning \\(Y\\leq Y'\\), a.s., and another direction gives \\(Y=Y'\\), a.s.. So the uniqueness is an equivalent category in a sense of zero-measure. </p> <p>Corollary: equivalence fixed on a subset</p> <p>If \\(X_1=X_2\\) on \\(B\\in \\mathcal{F}\\), then \\(E(X_1|\\mathcal{F})=E(X_2|\\mathcal{F})\\) a.e. on \\(B\\).</p>"},{"location":"Math/StochasticProcesses/Martingale/#properties","title":"Properties","text":"<p>Intuitively, we think of \\(\\mathcal{F}\\) as information we have at disposal (for each \\(A\\in \\mathcal{F}\\), we know whether or not \\(A\\) has occured), and \\(E(X|\\mathcal{F})\\) is our best guess of \\(X\\) based on the information.</p> <p>Properties of CE</p> <p>(i) If \\(X\\in \\mathcal{F}\\), then \\(E(X|\\mathcal{F})=X\\).</p> <p>(ii) If \\(X\\) is independent with \\(\\mathcal{F}\\), then \\(E(X|\\mathcal{F})=EX\\). </p> Proof <p>(i) is trivial.</p> <p>(ii) We have an inspiration. Notice that if \\(X\\) is independent with \\(\\mathcal{F}\\), then for all \\(B\\in \\mathcal{R}\\), \\(A\\in \\mathcal{F}\\), </p> \\[ P(\\{X\\in B\\}\\cap A)=P(\\{X\\in B\\})P(A). \\] <p>So we claim \\(E(X|\\mathcal{F})=EX\\). Condition (i) follows apparently. For condition (ii), we use \\(EXY=EXEY\\), then \\(E(X;A)=E(X1_A)=EX E1_A=E(EX;A)\\), and this completes the proof.</p> <p>From the proof we extract a method for calculating CE, i.e. \"guess and verify\".</p> <p>We list two examples regarding notions of CE in undergraduate probability courses. The proof lies in showing that formula satisfies the condition in the definition of CE. </p> <p>Example. Suppose \\(\\{\\Omega_n\\}\\) is a finite or infinite partition of \\(\\Omega\\) into disjoint sets, each of which has positive probability. Let \\(\\mathcal{F}=\\sigma(\\Omega_1,\\cdots)\\) be the sigma-field generated by these sets. </p> <p>(i) Show that</p> \\[ E(X | \\mathcal{F})=\\sum_i \\frac{E(X;\\Omega_i)}{P(\\Omega_i)} \\cdot 1_{\\Omega_i}. \\] <p>Note that \\(E(X;\\Omega_n)\\) is a value and \\(1_{\\Omega_i}\\) is a measurable function. </p> <p>(ii) A degenerate but important special case is \\(\\mathcal{F}=\\{\\varnothing, \\Omega\\}\\), then \\(E(X|\\mathcal{F})=EX\\). Moreover, recall that \\(P(A|\\mathcal{G})=E(1_A|\\mathcal{G})\\), \\(P(A|B)=P(A\\cap B)/P(B)\\), then </p> \\[ P(A|\\mathcal{F})=\\sum_{i}P(A|\\Omega_i)\\cdot 1_{\\Omega_i}. \\] <p>A last but common usage is the following definition</p> \\[ E(X|Y)=E(X|\\sigma(Y)), \\] <p>where \\(X,Y\\) are random variables, \\(\\sigma(Y)\\) is the sigma-field generated by \\(Y\\).</p> Proof <p>(i) For \\(\\mathcal{F}\\) measurability it is easy to check. For second condition, we only need to consider \\(A=\\Omega_i\\in \\mathcal{F}\\), since linearity gives the general case. Notice</p> \\[ \\int_{\\Omega_i} \\frac{E(X;\\Omega_i)}{P(\\Omega_i)} dP = E(X;\\Omega_i)=\\int_{\\Omega_i} XdP. \\] <p>(ii)</p> <p>Example. Suppose \\(X\\), \\(Y\\) has joint density \\(f(x,y)\\), i.e. distribution </p> \\[ P((X,Y)\\in B)=\\int_B f(x,y)dxdy,\\quad B\\in \\mathcal{R}^2. \\] <p>Suppose \\(\\int f(x,y)dx&gt;0\\) for all \\(y\\). If \\(E|g(X)|&lt;\\infty\\), then \\(E(g(X)| Y)=h(Y)\\), where</p> \\[ h(y)=\\int g(x)f(x,y)dx \\bigg/\\int f(x,y)dx \\] <p>Now we give some properties as common expectation has.</p> <p>Theorem for CE</p> <p>(i) Linearity. \\(E(aX+bY | \\mathcal{F})=aE(X|\\mathcal{F})+bE(Y|\\mathcal{F})\\).</p> <p>(ii) If \\(X&lt;Y\\), then \\(E(X|\\mathcal{F})&lt;E(Y|\\mathcal{F})\\).</p> <p>(iii) If \\(X_n\\geq 0\\) and \\(X_n\\nearrow X\\), with \\(EX&lt;\\infty\\), then</p> \\[ E(X_n|\\mathcal{F})\\nearrow E(X|\\mathcal{F}). \\] <p>Now we give a Jensen's Inequality in CE. </p> <p>Jensen's Inequality</p> <p>If \\(\\varphi\\) is convex, and \\(E|X|, E|\\varphi(X)|&lt;\\infty\\), then</p> \\[ \\varphi(E(X|\\mathcal{F}))\\leq E(\\varphi(X)|\\mathcal{F}). \\] Proof <p>Contraction in \\(L^p\\)</p> <p>Conditional expectation is a contraction in \\(L^p\\), \\(p \\geq 1\\). That is,</p> \\[ |E(X|\\mathcal{F})|^p\\leq E(|X|^p|\\mathcal{F}). \\] <p>The following example shows that the smaller \\(\\sigma\\)-field always wins.</p> <p>Example. If \\(\\mathcal{F}\\subset \\mathcal{G}\\), and \\(E(X|\\mathcal{G})\\in \\mathcal{F}\\), then show that</p> \\[ E(X|\\mathcal{F})=E(X|\\mathcal{G}). \\] <p>Moreover, we have the following property. If \\(\\mathcal{F}_1\\subset \\mathcal{F}_2\\), then</p> <p>(i) \\(E(E(X|\\mathcal{F}_1)|\\mathcal{F}_2) = E(X|\\mathcal{F}_1)\\).</p> <p>(ii) \\(E(E(X|\\mathcal{F}_2)|\\mathcal{F}_1) = E(X|\\mathcal{F}_1)\\).</p> Proof <p>(i) it trivial. For (ii), we prove by definition, for \\(A\\in\\mathcal{F}_1\\subset \\mathcal{F}_2\\),</p> \\[ \\int_A E(X|\\mathcal{F}_2)dP=\\int_A XdP=\\int_A E(X|\\mathcal{F}_1)dP. \\] <p><p>\\(\\square\\)</p></p> <p>The next theorem tells shows that conditional expectation with respect to \\(\\mathcal{F}\\), random variables \\(X\\in\\mathcal{F}\\) is like a constant.</p> <p></p> <p>Theorem for rearranging CE</p> <p>If \\(X\\in\\mathcal{F}\\), and \\(E|Y|, E|XY|&lt;\\infty\\), then</p> \\[ E(XY|\\mathcal{F})=XE(Y|\\mathcal{F}). \\] Proof <p>use the usual four-step procedure. If \\(X=1_B\\) where \\(B\\in \\mathcal{F}\\) and for all \\(A\\in \\mathcal{F}\\), </p> \\[ \\int_A 1_B E(Y|\\mathcal{F})dP= \\int_{A\\cap B}E(Y|\\mathcal{F})dP=\\int_{A\\cap B}YdP=\\int_A 1_B YdP. \\] <p>So for indicator function the proposition holds. It also holds for simple random variables, for its linearity. Then for \\(X\\geq 0\\), choose a sequence of simple random variables \\(X_n\\nearrow X\\), and by Levy's Theorem, we have</p> \\[ \\int_A XE(Y|\\mathcal{F}) dP=\\lim_{n\\rightarrow \\infty}\\int_A X_n E(Y|\\mathcal{F})dP=\\lim_{n\\rightarrow \\infty}\\int_A X_n Y dP=\\int_A XYdP. \\] <p>To prove the general case, choose its positive and negative part and complete the proof.</p> <p><p>\\(\\square\\)</p></p> <p>Now we give a geometric interpretation of \\(E(X|\\mathcal{F})\\).</p> <p>Geometric intepretation of CE</p> <p>Suppose \\(EX^2&lt;\\infty\\). \\(E(X|\\mathcal{F})\\) is the random variable \\(Y\\in\\mathcal{F}\\) that minimizes the \"mean square error\" \\(E(X-Y)^2\\).</p> Proof <p>Define \\(L^2(\\mathcal{F})=\\{Y\\in \\mathcal{F}: EY^2&lt;\\infty\\}\\).</p> <p>If \\(Z\\in L^2(\\mathcal{F})\\), then by Theorem for rearranging CE, we have \\(E(XZ|\\mathcal{F})=ZE(X|\\mathcal{F})\\), then taking ME we have</p> \\[ E(ZE(X|\\mathcal{F}))=E (E(XZ|\\mathcal{F})) = E(XZ). \\] <p>where \\(EXZ&lt;\\infty\\) is guaranteed by Cauchy-Schwarz inequation. So \\(E(Z(E(X|\\mathcal{F})-X))=0\\) for \\(Z\\in \\mathcal{F}\\). Now for \\(Y\\in\\mathcal{F}\\), let \\(Z = E(X|\\mathcal{F}) - Y\\in\\mathcal{F}\\)</p> \\[ E(X-Y)^2=E(X - E(X|\\mathcal{F}) + E(X|\\mathcal{F}) -Y)^2=E(X - E(X|\\mathcal{F}))^2 + EZ^2. \\] <p>Taking \\(Z=0\\) gives the minimum value.</p> <p><p>\\(\\square\\)</p></p> <p>Example. Bayes's formula. Assume \\(G\\in\\mathcal{G}\\) and show that</p> \\[ P(G|A)=\\int_G P(A|\\mathcal{G})dP \\bigg/ \\int_\\Omega P(A|\\mathcal{G})dP. \\] <p>When \\(\\mathcal{G}\\) is a sigma-field generated by a partition, this reduces to </p> \\[ P(G_i|A)=P(A|G_i)P(G_i)\\bigg/ \\sum_j P(A|G_j)P(G_j). \\] <p>Example. Prove Chebyshev\u2019s inequality. If \\(a&gt;0\\), then</p> \\[ P(|X|\\geq a|\\mathcal{F})\\leq \\frac{E(X^2|\\mathcal{F})}{a^2}. \\]"},{"location":"Math/StochasticProcesses/Martingale/#martingales","title":"Martingales","text":"<p>Let \\(\\mathcal{F}_n\\) be a filtration, i.e an increasing sequence of sigma-field. A sequence \\(X_n\\) is said to be adapted to \\(\\mathcal{F}_n\\) if \\(X_n\\in\\mathcal{F}_n\\) for all \\(n\\).</p> <p>Definition of discrete-time Martingales</p> <p>A sequence \\(X_n\\) is called a martingale with respect to a filtration \\(\\mathcal{F_n}\\), if</p> <p>(i) \\(E|X_n|&lt;\\infty\\),</p> <p>(ii) \\(X_n\\) is adapted to \\(\\mathcal{F}_n\\).</p> <p>(iii) \\(E(X_{n+1}|\\mathcal{F}_n)=X_n\\) for all \\(n\\).</p> <p>Furthermore, if in (iii) \\(E(X_{n+1}|\\mathcal{F}_n)\\geq (\\leq)X_n\\), then \\(X_n\\) is a submartingale(supermartingale) with respect to \\(\\mathcal{F}_n\\). </p> <p>Example. Let \\(\\xi_1,\\xi_2,\\cdots\\) be independent and identically distributed. Let \\(S_n=S_0+\\xi_1+\\cdots+\\xi_n\\), where \\(S_0\\) is a constant. Let \\(\\mathcal{F}_n=\\sigma(\\xi_1,\\cdots,\\xi_n)\\) for \\(n\\geq 1\\) and take \\(\\mathcal{F}_0=\\{\\varnothing, \\Omega\\}\\).</p> <p>(i) Linear Martingale. If \\(\\mu=E\\xi_i=0\\), then \\(\\{S_n\\}_{n\\geq 0}\\) is a martingale with respect to \\(\\mathcal{F}_n\\).</p> <p>(ii) Quadratic martingale. If \\(\\mu=E\\xi_i=0\\), \\(\\sigma^2=var(\\xi_i)&lt;\\infty\\), then \\(\\{S_n-n\\sigma^2\\}\\) is a martingale.</p> <p>(iii) Exponential martingale. Let \\(Y_1,Y_2,\\cdots\\) be nonnegative i.i.d random variables with \\(EY_i=1\\). Still \\(\\mathcal{F}=\\sigma(Y_1,\\cdots,Y_n)\\), then \\(M_n=\\prod_{i\\leq n}Y_i\\) is a martingale.</p>"},{"location":"Math/StochasticProcesses/Martingale/#properties_1","title":"Properties","text":"<p>The following property comes immediately from definition of supermartingale.</p> <p>Property of Supermartingale</p> <p>(i) If \\(X_n\\) is a supermartingale, then \\(E(X_n|\\mathcal{F}_m)\\leq X_m\\) for all \\(n&gt;m\\).</p> <p>(ii) Similarly, if \\(X_n\\) is a submartingale, then \\(E(X_n|\\mathcal{F}_m)\\geq X_m\\) for all \\(n&gt; m\\).</p> <p>(iii) \\(X_n\\) is a martingale, then \\(E(X_n|\\mathcal{F}_m)= X_m\\) for all \\(n&gt; m\\).</p> Proof <p>(i) We prove by induction.</p> <p>(ii) use \\(-X_n\\) is a supermartingale.</p> <p>Let \\(\\mathcal{F}_n, n \\geq 0\\) be a filtration. \\(\\{H_n\\}_{n\\geq 1}\\), is said to be a predictable sequence if \\(H_n \\in\\mathcal{F}_{n\u22121}\\) for all \\(n\\geq 1\\). In words, the value of \\(H_n\\) may be predicted (with certainty) from the information available at time \\(n\u22121\\). </p> <p>Gambling Game</p> <p>We see \\(H_n\\) as the amount of money a gambler will bet at time \\(n\\). (This can be based on the outcomes at times \\(1, \\cdots, n\u22121\\) but not on the outcome at time \\(n\\))</p> <p>Let \\(X_n\\) be the net amount of money you would have won at time \\(n\\) if you had bet one dollar each time. Then your winnings at time \\(n\\) would be</p> \\[ (H\\cdot X)_n = \\sum_{m=1}^n H_m (X_m-X_{m-1}). \\] <p>(i) Suppose now that \\(\\xi_m = X_m \u2212 X_{m\u22121}\\), and we have \\(P(\\xi_m = 1) = p\\) and \\(P(\\xi_m = \u22121) = 1 \u2212 p\\). One famous gambling system called the martingale is defined by \\(H_1=1\\), and for \\(n\\geq 2\\),</p> \\[ H_n=\\begin{cases} 2H_{n-1},\\quad &amp;\\xi_{n-1}=-1,\\\\ 1,\\quad &amp;\\xi_{n-1}=1. \\end{cases} \\] <p>However, the next result says there is no system for beating an unfavorable game.</p> <p></p> <p>Transitivity of supermartingale</p> <p>Let \\(X_n, n \\geq 0\\), be a supermartingale. If \\(H_n \\geq  0\\) is predictable and each \\(H_n\\) is bounded, then \\((H \u00b7 X)_n\\) is a supermartingale.</p> Proof <p>The same result is also true for submartingale and martingale( do not need \\(H_n\\geq 0\\)).</p> <p>From Jensen's Inequality, we could easily get the following theorem, which is useful inthe following proof, if we replace \\(\\varphi\\) with \\(()^+\\).</p> <p>Corollary</p> <p>If \\(X_n\\) is a martingale w.r.t. \\(F_n\\) and \\(\\varphi\\) is a convex function with \\(E|\\varphi(X_n)|&lt;\u221e\\) for all \\(n\\), then \\(\\varphi(X_n)\\) is a submartingale w.r.t. \\(F_n\\).</p> <p>We will now consider a very special gambling system: bet \\(1\\) each time \\(n \\leq N\\) then stop playing. A random variable \\(N\\) is said to be a stopping time if \\(\\{N = n\\} \\in \\mathcal{F}_n\\) for all \\(n &lt; \\infty\\), i.e., the decision to stop at time \\(n\\) must be measurable with respect to the information known at that time.</p> <p>If we let \\(H_n=1_{\\{n\\leq N\\}}\\), then \\(\\{n\\leq N\\}=\\{N\\leq n-1\\}^c\\in \\mathcal{F}_{n-1}\\), so here \\(H_n\\) is predictable. Then by Transitivity of supermartingale, we have \\((H\\cdot X)_n\\) is a supermartingale. Notice</p> \\[ (H\\cdot X)_n = \\sum_{m=1}^n 1_{\\{m\\leq N\\}}(X_{m}-X_{m-1})=X_{n\\land N}-X_0. \\] <p>which gives \\(H_{n\\land N}\\) is also a supermartingale.</p> <p>Gambling game again</p> <p>Now we give an important definition of gambling strategy. Assume \\(\\{X_n\\}_{n\\geq 0}\\) is a supermartingale, let \\(a&lt;b\\), and \\(N_0=0\\), firstly we define iteratively (here we use \\(m\\) as time index)</p> \\[ \\begin{cases} N_{2k-1}&amp;=\\inf \\{m&gt; N_{2k-2}: X_m &lt;a\\},\\\\ N_{2k}&amp;=\\inf \\{m&gt; N_{2k-1}: X_m &gt;b\\}. \\end{cases} \\] <p>then For each \\(j=0,1\\cdots\\), \\(N_j\\) is a stopping time (We prove this by induction). Then since \\(\\{N_{2k-1}&lt; m \\leq N_{2k}\\}=\\{N_{2k-1}\\leq m-1\\}\\cap \\{N_{2k}\\leq m-1\\}^c\\in \\mathcal{F}_{m-1}\\), so </p> \\[ H_n = \\begin{cases} 1,\\quad &amp; N_{2k-1}&lt; m \\leq N_{2k}\\\\ 0,\\quad &amp;\\text{otherwise}. \\end{cases} \\] <p>is predictable w.r.t \\(\\mathcal{F}_{n-1}\\). As we show in the following image, we take advantage of the upcrossings.</p> <p><p> </p></p> <p>In stock market terms, we buy when \\(X_m \\leq a\\) and sell when \\(Xm \\geq b\\), so every time an upcrossing is completed, we make a profit of \\(\\geq (b \u2212 a)\\). </p> <p>Define \\(U_n=\\sup\\{k: N_{2k}\\leq n\\}\\) to be the total numnber of upcrossings by time \\(n\\). Now we give the important upcrossing inequailty.</p> <p>Upcrossing inequality</p> <p>If \\(X_m,n\\geq 0\\) is a submartingale, then </p> \\[ (b-a)EU_n\\leq E(X_n-a)^+ - E(X_0-a)^+ \\] Proof <p>From the upcrossing inequality, we easily get</p> <p>Martingale convergence theorem</p> <p>If \\(X_n\\) is a submartingale, with \\(\\sup X^+_n&lt;\\infty\\), then as \\(n\\rightarrow \\infty\\), \\(X_n\\) converges a.s. to a limit \\(X\\) with \\(E|X|&lt;\\infty\\).</p>"},{"location":"Math/StochasticProcesses/Poisson_Pro/","title":"Poisson Process","text":"<p>Definition of Poisson Process</p> <p>Assume Stachostic process \\(\\{N_t: t\\geq 0\\}\\) has state space \\(S=\\mathbb{N}^+\\). If it satisfies</p> <p>(i) \\(N(0)=0\\).</p> <p>(ii) independent increment (additive).</p> <p>(iii) \\(\\forall s,t\\geq 0\\), \\(N(s+t)-N(s) \\sim \\pi(\\lambda t),\\lambda&gt;0\\),</p> <p>then we call \\(\\{N_t\\}\\) is a Poisson Process.</p> <p>Equivalent Definition of Poisson Process</p> <p>Assume Stachostic process \\(\\{N_t: t\\geq 0\\}\\) is a Poisson process, iff it satisfies </p> <p>(i) \\(N(0)=0\\).</p> <p>(ii) independent increment (additive).</p> <p>(iii) \\(\\mathbb{P}(N(h+t)-N(t)=1)=\\lambda h + o(h),\\quad h&gt;0\\).</p> <p>(iv) \\(\\mathbb{P}(N(h+t)-N(t)\\geq 2)=o(h), \\quad h&gt;0\\).</p> Proof for \\(\\Rightarrow\\)Proof for \\(\\Leftarrow\\) <p>Easy to see. Use \\(e^{-\\lambda h}=1-\\lambda h+o(h)\\), we have</p> \\[ \\mathbb{P}(N(h+t)-N(t)=1)=e^{-\\lambda h} \\frac{(\\lambda h)^1}{1}=\\lambda h (1-\\lambda h + o(h))=\\lambda h +o(h), \\] <p>and </p> \\[ \\begin{align*} \\mathbb{P}(N(h+t)-N(t)\\geq 1)&amp;= 1-\\mathbb{P}(N(h+t)-N(t)\\leq 1)\\\\ &amp;=1-e^{-\\lambda h}(1+\\lambda h)\\\\ &amp;=1-(1-\\lambda h + o(h))(1+\\lambda h)=o(h). \\end{align*} \\] <p>It is a little hard to prove this direction. For we have to show that \\(N(t+h)-N(t)\\) is a Poisson distribution.</p> <p>Notice the following theorem for continuous random variables.</p> <p>Probability at a point equals zero</p> \\[ \\begin{align*} P(N_{(t,t+h]}=0)&amp;=P(N_{[0,t+h]}-N_{[0,t]}=0)\\\\ &amp;=1-\\lambda h +o(h)\\rightarrow 1 (h\\rightarrow 0). \\end{align*} \\] <p>which means \\(P(N_{[0,t)}=N_{[0,t]})=1\\).</p> <p>Later, we focus on the number on intervals without considering carefully about the boundary.</p>"},{"location":"Math/StochasticProcesses/Poisson_Pro/#mathematical-characteristic","title":"Mathematical Characteristic","text":"<p>Here we use generating function to deduce its mathematical characteristics. Recall that for random variable \\(\\xi\\) which follows Poisson distribution with para \\(\\lambda\\), we have </p> \\[ \\psi_\\xi(s)=e^{\\lambda (s-1)}. \\] <p>Note that for ME, we have</p> \\[ \\begin{align*} E(s^\\xi)&amp;=\\psi_\\xi(s)=e^{\\lambda (s-1)},\\\\ E(\\xi s^{\\xi-1})&amp;=\\psi'_\\xi(s)=\\lambda e^{\\lambda (s-1)}\\\\ E[\\xi (\\xi-1) s^{\\xi-2}]&amp;=\\psi''_\\xi(s)=\\lambda^2 e^{\\lambda (s-1)}\\\\ \\end{align*} \\] <p>Using Levy Theorem, we let \\(s\\nearrow 1\\), then \\(E(\\xi)=\\lambda\\), \\(E(\\xi^2-\\xi)=\\lambda^2\\), so \\(D(\\xi)=\\lambda\\). As for Poisson process, we have</p> <p>Mathematical characteristic of Poisson Process</p> <p>(i) \\(EN_t=\\lambda t\\), \\(DN_t=\\lambda t\\).</p> <p>(ii) for \\(s&lt;t\\), </p> \\[ \\begin{align*} r_N(t,s)&amp;=E(N_t N_s)\\\\ &amp;=E(N_s^2)+E(N_{t-s})E(N_s)\\\\ &amp;=\\lambda^2 s^2+\\lambda s+\\lambda^2 s(t-s)\\\\ &amp;=\\lambda^2 ts+\\lambda s. \\end{align*} \\] <p>so for arbitrary \\(s,t\\), \\(r_N(t,s)=\\lambda \\min\\{t,s\\}+\\lambda^2 ts\\).</p> <p>(iii) for \\(s,t\\)</p> \\[ \\begin{align*} C_N(t,s)&amp;=E(N_tN_s)-E(N_t)E(N_s)\\\\ &amp;=\\min\\{t,s\\}+\\lambda^2 ts - \\lambda^2 ts\\\\ &amp;=\\min\\{t,s\\}. \\end{align*} \\] <p>Assume \\(S_n\\) to denote the arriving time of the \\(n\\)th \"customer\", and \\(T_n\\) to denote the time interval of the arriving time of \\(n\\)th and \\(n-1\\)th customer.</p>"},{"location":"Math/StochasticProcesses/Poisson_Pro/#process-of-interval-time","title":"Process of Interval Time","text":"<p>For \\(T_n\\), we have the following theorem.</p> <p>Theorem of interval time</p> <p>The sequence of interval time \\(\\{T_n\\}\\) in Poisson process is an independent sequence with the same distribution, which in particular, follows an exponential distribution with a mean of \\(1/\\lambda\\).</p> Proof <ul> <li>For \\(T_1\\), we have a simple solution. For \\(t\\leq 0\\), \\(P(T_1&lt;t)=0\\); for \\(t&gt;0\\), </li> </ul> \\[ P(T_1\\leq t)=P(N_{[0,t)}\\geq 1)=P(N_{[0,t]}\\geq 1)=1-P(N_{[0,t]}=0)=1-e^{-\\lambda t}, \\] <p>so \\(f(T_1\\leq t)=\\lambda e^{-\\lambda t}\\), then \\(T_1\\) is an exponential distribution.</p> <ul> <li>For \\(T_n\\), we have two methods. The first one, is using the conditional probability to show that </li> </ul> \\[ P(T_n\\leq t \\mid T_1=\\tau_1, \\cdots, T_{n-1}=\\tau_{n-1}) \\] <p>is irrelevant to \\(\\tau_i\\), \\(i=1,\\cdots,n-1\\), so that they are mutually independent. Actually, For \\(t, s&gt;0\\),</p> \\[ \\begin{align*} P(T_n&gt;t \\mid S_{n-1}=s)&amp;=P(N_{[0,s+t]}\\leq n-1\\mid N_{[0,s]}=n-1, N_{[0,s)}\\leq n-2)\\\\ &amp;=P(N_{(s,s+t]}=0)\\\\ &amp;=P(N_{[s,s+t]}=0)=e^{-\\lambda t}. \\end{align*} \\] <ul> <li>The second method is to use induction. The logic is similar.</li> </ul> <p><p>\\(\\square\\)</p></p> <p>Actually, the reverse of the above theorem also holds.</p> <p>Reverse of theorem of interval time</p> <p>Assume \\(\\{T_n: n\\geq 1\\}\\) is a sequence of the arriving time, which are independent mutually and follow an exponential distribution of a mean of \\(1/\\lambda\\), and let \\(N_t\\) to denote the number of customers arriving during time \\([0,t]\\), i.e.</p> \\[ N_t:=\\max\\{n\\geq 1: \\sum_{i=1}^n T_i\\leq t\\}. \\] <p>then \\(N_t\\) is a Poisson process with a parameter \\(\\lambda\\).</p> Proof <p>We would use Theorem of arriving time. \\(S_n=\\sum_{i=1}^n T_i\\).</p> <p>For \\(t&gt;0\\), we use total probability formula in continuous form,</p> \\[ \\begin{align*} P(N_t=k)&amp;=P(S_k\\leq t&lt;S_{k+1})\\\\ &amp;=P(S_{k}\\leq t, t&lt; T_{k+1}+S_k)\\\\ &amp;=\\int_{0}^t P(t&lt;T_{k+1}+s \\mid S_{k}=s)f_{S_k}(s)ds\\quad \\text{tpf for } S_k\\in[0,t]\\\\ &amp;=\\int_{0}^tP(t-s&lt;T_{k+1})f_{S_k}(s)ds \\quad \\text{independence}\\\\ &amp;=\\int_{0}^t e^{-\\lambda (t-s)}\\frac{\\lambda^n s^{k-1}}{(k-1)!}e^{-\\lambda s}ds\\\\ &amp;=e^{-\\lambda t}\\frac{\\lambda^k t^{k}}{k!} \\end{align*} \\] <p>which is exactly Poisson distribution with para \\(\\lambda t\\).</p> <p>Then we prove that \\(N_t\\) satisfies independent increment and stationary increment. For \\(t,s\\),</p> \\[ \\begin{align*} P(N_t-N_s=n)&amp;=\\sum_{k=1}^\\infty P(N_t-N_s=n, N_s=k)\\\\ &amp;=\\sum_{k=1}^\\infty P(N_t=n+k, N_s=k) \\end{align*} \\] <p>still follows Poisson distribution. <p>\\(\\square\\)</p></p>"},{"location":"Math/StochasticProcesses/Poisson_Pro/#process-of-arriving-time","title":"Process of Arriving Time","text":"<p>Theorem of arriving time</p> <p>The sequence of arriving time \\(\\{S_n\\}\\) in Poisson process satisfies \\(S_n\\sim \\Gamma (n,\\lambda)\\), i.e.</p> \\[ f_{S_n}(t)=\\begin{cases} \\displaystyle\\frac{\\lambda^n t^{n-1}}{\\Gamma(n)}e^{-\\lambda t}=\\frac{\\lambda^n t^{n-1}}{(n-1)!}e^{-\\lambda t},\\quad &amp;t&gt;0,\\\\ 0,\\quad &amp;t\\leq 0. \\end{cases} \\] Proof using characteristic funcProof using distribution func <p>By \\(S_n=\\sum_{i=1}^n T_i\\), and \\(\\{T_i\\}\\) are independent nutually and follow exponential distribution of a mean \\(1/\\lambda\\), then using characteristic function</p> \\[ \\phi_{S_n}(u)=E(e^{juS_n})=E(e^{ju\\sum_i^n T_i})=\\prod_{i=1}^n E(e^{ju T_i})=\\prod_{i=1}^n \\left(1-\\frac{ju}{\\lambda}\\right)^{-1}=\\left(1-\\frac{ju}{\\lambda}\\right)^{-n} \\] <p>which is the characteristic function of a \\(\\Gamma\\) distribution.</p> <p><p>\\(\\square\\)</p></p> <p>For \\(t&gt;0\\), we consider </p> \\[ \\begin{align*} P(S_n\\leq t)&amp;=1-P(S_n&gt;t)\\\\ &amp;=1-P(N_t\\leq n-1)\\\\ &amp;=1-\\sum_{i=0}^{n-1}P(N_t=i)\\\\ &amp;=1-\\sum_{i=0}^{n-1}\\frac{e^{-\\lambda t}(\\lambda t)^{i}}{i!} \\end{align*} \\] <p>taking derivative, and we have</p> \\[ \\begin{align*} f_{S_n}(t)&amp;=-e^{-\\lambda t}\\sum_{i=0}^{n-1}\\left(\\frac{-\\lambda^{i+1}t^i}{i!}+\\frac{\\lambda^it^{i-1}}{(i-1)!}\\right)\\quad\\text{denote }t^{0-1}=0 \\\\ &amp;=e^{-\\lambda t}\\sum_{i=0}^{n-1}\\left(\\frac{\\lambda^{i+1}t^i}{i!}-\\frac{\\lambda^it^{i-1}}{(i-1)!}\\right)\\\\ &amp;=e^{-\\lambda t}\\frac{\\lambda^n t^{n-1}}{(n-1)!}. \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>We usually use the following conditional form of \\(S_n\\).</p> <p>Properties of conditional probability of arriving time</p> <p>Assume \\(N_t\\) is a Poisson process of a parameter \\(\\lambda\\), \\(\\{S_n\\}\\) is its arriving sequence, then</p> <p>(i) Looking forward. \\(\\forall n\\), \\(0&lt;t_1&lt;\\cdots&lt;t_n\\), and integers \\(0\\leq k_1\\leq \\cdots \\leq k_n\\), we have</p> \\[ \\begin{align*} P&amp;(N_{t_1}=k_1,\\cdots, N_{t_n}=k_n)\\\\ &amp;=P(N_{t_1}=k_1,N_{t_2-t_1}=k_2-k_1,\\cdots, N_{t_n-t_{n-1}}=k_n-k_{n-1})\\\\ &amp;=P(N_{t_1}=k_1)P(N_{t_2-t_1}=k_2-k_1)\\cdots P(N_{t_n-t_{n-1}}=k_n-k_{n-1})\\\\ &amp;=e^{\\lambda t_1}\\frac{(\\lambda t_1)^k_1}{k_1 !}\\cdot \\prod_{i=2}^n e^{-\\lambda(t_i-t_{i-1})}\\frac{[\\lambda (t_i-t_{i-1})]^{k_i-k{i-1}}}{(k_i-k_{i-1})!}. \\end{align*} \\] <p>(ii) Looking backward.  \\(\\forall 0&lt;x&lt;t\\) and \\(1\\leq j\\leq n\\), we have</p> \\[ P(S_j&lt;x \\mid N_{t}=n)=\\sum_{k=j}^nC_n^k \\left(\\frac{x}{t}\\right)^k\\left(1-\\frac{x}{t}\\right)^{n-k}. \\] Proof for (ii) <p>Use the formula of conditional probability and apply the result of (i).</p> \\[ \\begin{align*} P(S_j&lt;x\\mid N(t)=n)&amp;=\\frac{P(S_j\\leq x, N(t)=n)}{P(N(t)=n)}\\\\ &amp;=P(N(x)\\geq j, N(t)=n)/P(N(t)=n)\\\\ &amp;=\\sum_{k=j}^n \\frac{(\\lambda x)^k}{k!} \\frac{[\\lambda(t-x)]^{n-k}}{(n-k)!}/\\frac{(\\lambda t)^n}{n!}\\\\ &amp;=\\sum_{k=j}^n C_n^k \\left(\\frac{x}{t}\\right)^k\\left(1-\\frac{x}{t}\\right)^k. \\end{align*} \\] <p>Order Statistics</p> <p>Assume random variables \\(X_1,\\cdot,X_n\\) are mutually independent and follow the same distribution, with a density function \\(f\\). If we sort them from small to large in an order, we have</p> \\[ X_{(1)}\\leq\\cdots\\leq X_{(n)}, \\] <p>then random vector \\((X_{(1)},\\cdots,X_{(n)})\\) has a density function </p> \\[ g(x_1,\\cdots,x_n)=\\begin{cases} n! f(x_1)\\cdots f(x_2),\\quad &amp;x_1&lt;x_2&lt;\\cdots&lt;x_n\\\\ 0,\\quad &amp; \\text{others}. \\end{cases} \\] Proof <p>By definition of density function.</p> <p>\\(\\forall x_1&lt;\\cdots&lt;x_n\\), \\(\\exists \\varepsilon_i&gt;0 (i=1,c\\dots,n-1)\\), such that \\(x_i+\\varepsilon_i&lt;x_{i+1}\\) and \\(\\exists \\varepsilon_n&gt;0\\). Then</p> \\[ \\begin{align*} P&amp;(x_i&lt;X_{(i)}\\leq x_i+\\varepsilon_i, \\forall i\\in [1,n])\\\\ &amp;=\\sum_{\\tau \\text{ is a permutation} \\atop\\text{of }\\{1,\\cdots,n\\}}P(x_i&lt;X_{\\tau_i}\\leq x_i+\\varepsilon_i, \\forall i\\in [1,n])\\\\ &amp;=n! P(x_i&lt;X_{i}\\leq x_i+\\varepsilon_i, \\forall i\\in [1,n])\\\\ &amp;=n!\\prod_{i=1}^n P(x_i&lt;X_{i}\\leq x_i+\\varepsilon_i)\\quad \\text{by independence}\\\\ \\Rightarrow \\quad g(x_1,\\cdots,x_n)&amp;=\\lim_{\\max\\limits_{1\\leq i\\leq n}\\{\\varepsilon_i\\}\\rightarrow 0} \\frac{n!\\prod\\limits_{i=1}^n P(x_i&lt;X_{i}\\leq x_i+\\varepsilon_i)}{\\varepsilon_1\\cdots\\varepsilon_n}\\\\ &amp;=n! \\prod_{i=1}^n f(x_i). \\end{align*} \\] <p>Distribution of arriving vector in conditional probability</p> <p>Assume \\(N_t\\) is a Poisson process, with arriving time \\(S_n\\). \\(\\forall t&gt;0\\)m \\(n\\in \\mathbb{N}^+\\), </p> \\[ (S_1,\\cdots,S_n \\mid N_t=n)\\overset{d}{=}(U_{(1)},\\cdots,U_{(n)}) \\] <p>where \\(U_{(1)},\\cdots,U_{(n)}\\) are order statistics which are mutually independent and follow the same uniform distribution \\(U(0,t)\\).</p> Proof <p>\\(\\forall n\\in \\mathbb{N}^+\\), \\(t_1&lt;s_1&lt;t_2&lt;s_2&lt;\\cdots&lt;t_n&lt;s_n\\), we have</p> \\[ \\begin{align*} P&amp;(S_1\\in (t_1,s_1],\\cdots,S_n\\in (t_n,s_n] \\mid N_t=n)\\\\ &amp;=\\frac{P(S_i\\in (t_i,s_i] (\\forall i\\in[1,n]), N_t=n)}{P(N_t=n)}\\\\ &amp;=\\frac{P[N_{(t_i,s_i]}=1(\\forall i\\in[1,n]), N_{t_1}=0, N_{(t_i,s_{i+1}]}=0(\\forall i\\in [1,n-1]), N_{(s_{i}, t]}=0]}{P(N_t=n)}\\\\ &amp;=\\frac{\\left\\{\\prod\\limits_{i=1}^n P(N_{(t_i,s_i]}=1)\\right\\} P(N_{t_1}=0) \\left\\{ \\prod\\limits_{i=1}^{n-1}P(N_{(s_i,t_{i+1}]}=0)\\right\\} P(N_{(s_{i}, t]}=0) }{P(N_t=n)}\\\\ &amp;=\\frac{e^{-\\lambda t} \\prod\\limits_{i=1}^n(\\lambda (s_i-t_i))}{e^{-\\lambda t} \\frac{(\\lambda t)^n}{n!}}\\\\ &amp;=\\frac{n!\\prod\\limits_{i=1}^n(s_i-t_i)}{t^n} \\end{align*} \\] <p>So we have density function</p> \\[ g(x_1,\\cdots,x_n\\mid N_t=n)=\\lim_{\\max\\limits_{1\\leq i\\leq n}\\{t_i-s_i\\}\\rightarrow 0}\\frac{\\frac{n!\\prod\\limits_{i=1}^n(s_i-t_i)}{t^n}}{\\prod\\limits_{i=1}^n(s_i-t_i)}=\\frac{n!}{t^n} \\] <p>which is exactly the distribution of order statistics of uniform distribution.</p> <p>Actually, if we do not distinguish \\(S_1,\\cdots,S_n\\), then the \\(n\\) customers arrive randomly with a distribution of \\(U(0,t)\\), which are mutually independent. </p>"},{"location":"Math/StochasticProcesses/Poisson_Pro/#repair-model","title":"Repair Model","text":"<p>\\(N_t\\) could be interpreted as a process of number of the update of components. That is, \\(N_t\\) could be the number of the components updated during time \\([0,t]\\), and \\(T_n\\) could be the lifespan of the \\(n\\)th component, \\(\\lambda\\) means the average number of the components updated in a unit time.</p> <p>Definition of the ages of components</p> <p>For all \\(t&gt;0\\), \\(t\\neq S_n\\), \\(n=0,1,\\cdots\\), we denote \\(S_0=0\\). Define </p> \\[ \\alpha_t=t-S_{N_t}, \\quad \\beta_t=S_{N_{t}+1}-t \\] <p>to be the age of the component at time \\(t\\), and the rest age of that at time \\(t\\), respectively.</p> <p>Properties of age</p> <p>(i) \\(\\alpha_t\\) follows</p> \\[ F_{\\alpha_t}(x)=\\begin{cases} 0,\\quad &amp;x\\leq 0,\\\\ 1-e^{-\\lambda x},\\quad &amp; 0&lt;x\\leq t,\\\\ 1,\\quad &amp; x&gt;t. \\end{cases} \\] <p>(ii) \\(\\beta_t\\) is independent of \\(t\\), and follows an exponential distribution of a mean of \\(1/\\lambda\\).</p> <p>(iii) \\(\\alpha_t\\) and \\(\\beta_t\\) are independent.</p> Proof <p>Notice the following proof is based on a given \\(t\\).</p> <p>(i) Whenever \\(x\\in [0,t]\\), we have</p> \\[ P(\\alpha_t&lt;x)=P(t-S_{N_{t}}&lt;x)=P(S_{N_t}&gt;t-x)=P(N_t-X_{t-x}&gt;0)=1-e^{-\\lambda x}. \\] <p>(ii) By definition. </p> \\[ P(\\beta_t&lt;x)=P(S_{N_{t}+1}&lt;x)=P(N_{t+x}-N_t\\geq 1)=1-e^{-\\lambda x}. \\]"},{"location":"Math/StochasticProcesses/Poisson_Pro/#synthesis-decomposition","title":"Synthesis &amp; Decomposition","text":"<p>Synthesis of Poisson Process</p> <p>Assume \\(N_1(t)\\) and \\(N_2(t)\\) are mutually independent Poisson processes, with parameter \\(\\lambda_1\\) and \\(\\lambda_2\\) respectively. Then process \\(N(t)=N_1(t)+N_2(t)\\) is also a Poisson process with parameter \\(\\lambda_1+\\lambda_2\\).</p> Proof <p>(i) \\(N(0)=0\\).</p> <p>(ii) Independent increment property. Here \\(\\forall n\\),</p> \\[ \\begin{cases} \\text{Group 1: }N_1(t_2)-N_1(t_1),\\cdots,N_1(t_n)-N_1(t_{n-1}),\\\\ \\text{Group 2: }N_2(t_2)-N_2(t_1),\\cdots,N_2(t_n)-N_2(t_{n-1}),\\\\ \\end{cases} \\] <p>the independence in group 1 and 2 is apparent. Since \\(N_1(t)\\) and \\(N_2(t)\\) are mutually independent Poisson processes, Group 1 and 2 are also independent with each other. So we have the above random variables independent mutually.</p> <p>(iii) Follows Poisson distribution. We have two methods. The first one is to use properties of Poisson distribution. That is, since \\(N_1(t)-N_1(s)\\), \\(N_2(t)-N_2(s)\\) follow Poisson distribution of \\(\\pi (\\lambda_1 (t-s))\\) and \\(\\pi(\\lambda_2(t-s))\\), respectively, and they are mutually independent, so by the additivity of Poisson distribution, we have</p> \\[ N(t)-N(s)=[N_1(t)-N_1(s)]+[N_2(t)-N_2(s)]\\sim \\pi((\\lambda_1+\\lambda_2)(t-s)) \\] <p>also follows Poisson distribution.</p> <p>The second method is to directly use distribution function to show that \\(N(t)-N(s)\\) follows Poisson distribution. </p> <p>Decomposition of Poisson Process</p> <p>Assume \\(\\{N(t)\\}\\) is a Poisson process with a parameter \\(\\lambda\\), every customer arrives in different type independently, with each type happening in a probability \\(p_i(i=1,\\cdots,n, \\sum\\limits_{i=1}^n p_i=1)\\), denote \\(N_i(t)\\) to be the number of arriving customer of the certain type \\(i\\), then \\(N_i(t)\\) is also a Poisson process, with a parameter \\(p_i\\lambda\\), respectively and they are mutually independent.</p>"},{"location":"Math/StochasticProcesses/Poisson_Pro/#nonhomogeneous-poisson-process","title":"Nonhomogeneous Poisson Process","text":"<p>Definition of Nonhomogeneous Poisson Process</p> <p>A counting process is called a nonhomogeneous Poisson process, with an intensity of \\(\\lambda(t)\\), if</p> <p>(i) \\(N(0)=0\\).</p> <p>(ii) \\(N(t)\\) is an independent increment process.</p> <p>(iii) \\(P(N(t+h)-N(t)=1)=\\lambda(t) h+o(h)\\).</p> <p>(iv) \\(P(N(t+h)-N(t)\\geq 2)=o(h)\\).</p> <p>Theorem of Nonhomogeneous Poisson Process</p> <p>A counting process is called a nonhomogeneous Poisson process, with an intensity of \\(\\lambda(t)\\), iff</p> <p>(i) (ii) the same with definition.</p> <p>(iii) \\(\\forall 0\\leq s&lt;t\\), </p> \\[ N_t-N_s\\sim \\pi\\left(\\int_s^t \\lambda(\\tau)d\\tau\\right). \\] <p>Apparently, nonhomogeneous Poisson process does not follow stationary increment, but still follow independent increment.</p> <p>Time transfer</p> <p>If \\(\\{Y(t)\\}\\) is a homogeneous Poisson process with para \\(\\lambda=1\\), \\(\\Lambda(t)=\\int_0^t \\lambda(\\tau)d\\tau\\), then </p> \\[ X(t)=Y(\\Lambda(t)),\\quad t\\geq 0, \\] <p>is a nonhomogeneous Poisson process, with intensity function \\(\\lambda(t)\\).</p> Proof <p>Note for \\(t,s\\),</p> \\[ \\begin{align*} X(t)-X(s)&amp;=Y(\\Lambda(t))-Y(\\Lambda(s))\\\\ &amp;\\sim \\pi \\left[1\\cdot(\\Lambda(t)-\\Lambda(s))\\right]\\\\ &amp;=\\pi\\left(\\int_s^t \\lambda(\\tau)d\\tau\\right). \\end{align*} \\]"},{"location":"Math/StochasticProcesses/Poisson_Pro/#compound-poisson-process","title":"Compound Poisson Process","text":"<p>Definition of Compound Poisson process</p> <p>Assume \\(N_t\\) is a Poisson process with para \\(\\lambda\\). We call the following process \\(Y(t)\\) compound Poisson Process, if </p> \\[ Y(t):=\\sum_{n=1}^{N(t)}X_n \\] <p>where \\(\\{X_n\\}\\) following the same distribution, are independent mutually and with \\(N(t)\\), which is usually called Jump Sizes. </p> <p></p> <p>Distribution of compound Poisson process</p> <p>For \\(t,s\\geq 0\\), the characteristic function of \\(Y(t)-Y(s)\\) is </p> \\[ \\phi_{Y(t)-Y(s)}(u)=e^{-\\lambda(t-s)}e^{\\lambda (t-s)\\phi_{X_1}(u)}. \\] Proof \\[ \\begin{align*} \\phi_{Y(t)-Y(s)}&amp;=E(e^{iu[Y(t)-Y(s)]})\\\\ \\displaystyle &amp;=E\\left(e^{iu\\left(\\sum\\limits_{i=N(s)}^{N(t)} X_i\\right)}\\right)\\\\ &amp;=\\sum_{n=1}^\\infty E\\left(e^{iu\\left(\\sum\\limits_{i=1}^{n} X_i\\right)}\\right) P(N(t)-N(s)=n)\\\\ &amp;=\\sum_{n=1}^\\infty [\\phi_{X_1}(u)]^n \\frac{e^{-\\lambda(t-s)}[\\lambda(t-s)]^n}{n!}\\\\ &amp;=e^{\\lambda(t-s)(\\phi_{X_1}(u)-1)} \\end{align*} \\] <p><p>\\(\\square\\)</p></p> <p>Properties of Compound Poisson Process</p> <p>\\(Y(t)\\) is stationary increment and independent incerment.</p> Proof <ul> <li>Independent increment. Easy to see, by definition. \\(\\forall n\\), \\(0&lt;t_1&lt;t_2&lt;\\cdots&lt;t_n\\), </li> </ul> \\[ Y(t_2)-Y(t_1),\\quad \\cdots, \\quad Y(t_n)-Y(t_{n-1}) \\] <p>i.e.</p> \\[ \\sum_{i=N(t_1)}^{N(t_2)}X_i,\\quad \\cdots, \\quad\\sum_{i=N(t_{n-1})}^{N(t_n)}X_i \\] <p>are mutually independent because none of the above random variable has a common \\(X_n\\).</p> <ul> <li>Stationary increment. Use Characteristic function. From Distribution of compound Poisson process, we could see that its characteristic function is only dependent on \\(t-s\\).</li> </ul> <p>ME of Compound Poisson Process</p> <p>If \\(E(X_1^2)&lt;\\infty\\), denote \\(\\mu=E(X_1)\\), \\(\\sigma^2=D(X_1)\\), then </p> \\[ E(Y(t))=\\lambda t E(X_1)=\\mu \\lambda t, \\quad D(Y(t))=\\lambda t E(X_1^2)=(\\mu^2+\\sigma^2)\\lambda t, \\] <p>and for \\(t,s\\),</p> \\[ R_Y(t,s)=E(X_1^2)[\\lambda \\min(t,s)+\\lambda^2ts],\\quad C_Y(t,s)=E(X_1^2)\\lambda \\min(t,s). \\] Proof \\[ \\begin{align*} E(Y(t))&amp;=E\\left(\\sum_{n=0}^{N(t)}X_n\\right)\\\\ &amp;=E\\left(E\\left(\\sum_{n=0}^{N(t)}X_n \\mid N(t)\\right)\\right)\\\\ &amp;=\\sum_{n=0}^\\infty E\\left(\\sum_{i=1}^{n}X_i \\mid N(t)=n\\right)P(N(t)=n)\\\\ &amp;=\\sum_{n=0}^\\infty nE(X_1)\\frac{e^{-\\lambda t}(\\lambda t)^n}{n!}\\\\ &amp;=E(X_1)e^{-\\lambda t} \\lambda t \\sum_{n=1}^\\infty \\frac{(\\lambda t)^{n-1}}{(n-1)!}\\\\ &amp;=E(X_1)\\lambda t. \\end{align*} \\] <p>Similarly, </p> \\[ \\begin{align*} E(Y^2(t)) \\end{align*} \\] <p>or use conditional variance</p> \\[ \\begin{align*} D(Y(t))&amp;=E(D(Y(t)\\mid N(t)))+D(E(Y(t)\\mid N(t)))\\\\ &amp;=\\sum_{n=0}^\\infty D\\left(\\sum_{i=1}^{n}X_i \\mid N(t)=n\\right)P(N(t)=n)+D(N(t)E(X_1))\\\\ &amp;=\\sum_{n=0}^\\infty n\\sigma^2P(N(t)=n) + E^2(X_1)\\lambda t\\\\ &amp;=\\sigma^2\\lambda t+\\mu^2 \\lambda t. \\end{align*} \\] <p>So </p> \\[ \\begin{align*} R_Y(t,s)&amp;=E(Y(t)Y(s))\\\\ &amp;=E\\left( \\right) \\end{align*} \\]"},{"location":"Math/StochasticProcesses/Poisson_Pro/#filtered-poisson-process","title":"Filtered Poisson Process","text":"<p>Definition of filtered Poisson process</p> <p>Assume stochastic process \\(\\{Y(t): t\\geq 0\\}\\) is a filtered Poisson process, if </p> \\[ Y(t)=\\sum_{n=1}^{N(t)}W(t, S_n, X_n), \\quad t\\geq 0. \\] <p>where \\(\\{N(t): t\\geq 0\\}\\) is a Poisson process with parameter \\(\\lambda\\), \\(S_n\\) are arriving time of \\(n\\)th event, \\(\\{X_n: n\\geq 1\\}\\) are mutually independent random variables, and independent with \\(N(t)\\), \\(W(t, S_n, X_n)\\) are three-variable function, called response function.</p> <p>Usually \\(X_n\\) is the magnitude of a signal associated with \\(n\\)th event, and \\(W\\) represents the response of signal with magnitude \\(X_n\\) at time \\(t\\), which starts from \\(S_n\\).</p> <p>Common form of response function</p> <p>We denote \\(W(t,\\tau,x)=W_0(t-\\tau,x)\\), then</p> <p>(i)</p> \\[ W_0(s,x)=\\begin{cases} 1,\\quad &amp;0&lt;s&lt;x,\\\\ 0,\\quad &amp;\\text{others}. \\end{cases} \\] <p>(ii)</p> \\[ W_0(s,x)=\\begin{cases} x-s,\\quad &amp;0&lt;s&lt;x,\\\\ 0,\\quad &amp;\\text{others}. \\end{cases} \\] <p>(iii)</p> \\[ W_0(s,x)=\\begin{cases} xW_1(x),\\quad &amp;0\\leq s,\\\\ 0,\\quad &amp;s&lt;0. \\end{cases} \\]"},{"location":"Math/StochasticProcesses/Stationary_Pro/","title":"Stationary Process","text":"<p>Recall the definition of Weak Stationary Process. If \\(X_t\\) satisfies \\(\\mu_X(t)\\equiv C\\), and \\(r_X(t,t+\\tau)=r_X(\\tau)\\), then we call \\(X_t\\) weak stationary process, or briefly stationary process. </p>"},{"location":"Math/StochasticProcesses/Stationary_Pro/#properties-of-self-relevant-coefficient","title":"Properties of self-relevant coefficient","text":"<p>Properties of self-relevant coefficient</p> <p>(i) \\(r_X(0)=\\sigma^2\\), and \\(|r_X(\\tau)|\\leq r_X(0)\\).</p> <p>(ii) symmetry. \\(r_X(-\\tau)=\\overline{r_X(\\tau)}\\).</p> <p>(iii) \\(r_X(\\tau)\\) is not negative definitive, i.e. \\(\\forall n\\), \\(a_1,\\cdots,a_n\\) and \\(t_1,\\cdots,t_n\\), </p> \\[ \\sum_{k=1}^n \\sum_{m=1}^n r_X(t_k-t_m) a_k\\overline{a_m}\\geq 0. \\] <p>Here we dig into its mean value of time.</p>"},{"location":"Math/mpm/","title":"Material Point Method","text":"<p>Reference</p> <ul> <li>The Material Point Method for Simulating Continuum Materials, Chenfanfu Jiang, SIGGRAPH 2016 Course Notes Version 1 (May 2016).</li> </ul>"},{"location":"Math/mpm/#preliminary","title":"Preliminary","text":""},{"location":"Math/mpm/#contimuum-motion","title":"Contimuum Motion","text":"<p>Here \\(d\\) denotes dimension of the space, usually \\(d=2,3\\).</p> <p>We denote \\(\\pmb{x}\\in \\Omega^t\\subset \\mathbb{R}^d\\) world (deformed) space(coordinates), current position. Physically, we focus on a fixed position in the space and measure the velocity of whichever particle that is passing by the position.</p> <p>Then we denote \\(\\pmb{X}\\in \\Omega^0\\subset \\mathbb{R}^d\\) material (undeformed) space(coordinates), or initial position. Physically, we measure velocity from a fixed particle, which has its mass and occupies some volume since the beginning.</p> <p>Define </p> \\[ \\pmb{x}=\\phi(\\pmb{X},t) \\] <p>which is usually a bijection. This is associated with the assumption that no two different particles of material ever occupy the same space at the same time. So </p> \\[ \\forall \\pmb{x}\\in \\Omega^t, \\exists ! \\pmb{X}\\in \\Omega^0, \\text{ s.t. } \\phi(\\pmb{X},t)=\\pmb{x}. \\] <p></p> <p>Example</p> <p>For a given initial position \\(\\pmb{X}\\), we have some common function of \\(\\pmb{x}\\).</p> <p>(i) \\(\\pmb{x}=\\pmb{X}+tv(t)\\vec{N}\\)</p> <p>discrete form at time \\(n\\):</p> \\[ \\pmb{x}^{(n)}=\\pmb{x}^{(n-1)}+\\Delta t v^{(n)} \\vec{N}^{(n)} \\] <p>(ii) \\(\\pmb{x}=R\\pmb{X}+\\vec{b}\\)</p> <p>discrete form at time \\(n\\):</p> \\[ \\pmb{x}^{(n)}=R^{(n-1)}\\pmb{x}^{(n-1)}+\\vec{b}^{(n)} \\] <p>The velocity of a given material point \\(\\pmb{X}\\) at time \\(t\\) is a mapping \\(V(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^d\\) defined by</p> \\[ \\pmb{V}(\\pmb{X},t)\\overset{\\Delta}{=}\\frac{\\partial \\phi(\\pmb{X},t)}{\\partial t} \\] <p>and acceleration \\(A(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^d\\)</p> \\[ \\begin{align*} \\pmb{A}(\\pmb{X},t)&amp;\\overset{\\Delta}{=}\\frac{\\partial \\pmb{V}(\\pmb{X},t)}{\\partial t}\\\\ &amp;=\\frac{\\partial^2 \\phi(\\pmb{X},t)}{\\partial t^2} \\end{align*} \\]"},{"location":"Math/mpm/#deformation","title":"Deformation","text":"<p>For every small unit in a material, it has a deformation mapping :</p> \\[ \\pmb{F}(\\pmb{X},t)\\overset{\\Delta}{=}\\frac{\\partial \\pmb{x}}{\\partial \\pmb{X}} \\in \\mathbb{R}^{d\\times d} \\] <p>Note that \\(F\\) is also related with both \\(\\pmb{X}\\) and \\(t\\). And \\(J=\\det{(F)}\\) characterizes infinitesimal volume change.</p> <p>In Example we have \\(\\pmb{F}=I\\) (i) and \\(\\pmb{F}=R\\) (ii).</p> <p>The above is rigid transformation, and local.</p>"},{"location":"Math/mpm/#push-forward-and-push-back","title":"Push forward and Push back","text":"<p>Push forward of a function, is often referred to as Eulerian (a function of \\(\\pmb{x}\\)). That is, given \\(G: \\Omega^0\\rightarrow \\mathbb{R}\\), the push forward \\(g(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}\\) is defined</p> \\[ g(\\pmb{x})=G(\\phi^{-1}(\\pmb{x}),t) \\] <p>Push back function is often referred to as Lagrangian (a function of \\(\\pmb{X}\\)). That is, given \\(g:\\Omega^t\\rightarrow \\mathbb{R}\\), the push back \\(G(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}\\) is defined </p> \\[ G(\\pmb{X})=g(\\phi(\\pmb{x}), t) \\] <p>But we usually do not use push back but push forward more frequently since we can always have access to the grid velocity and acceleration based on the particles around it.</p> <p>So it is useful to define Eulerian counterparts. The velocity</p> \\[ \\pmb{v}(\\pmb{x},t)=\\pmb{V}(\\phi^{-1}(\\pmb{x},t),t) \\] <p>and the acceleration</p> \\[ \\pmb{a}(\\pmb{x},t)=\\pmb{A}(\\phi^{-1}(\\pmb{x},t),t) \\] <p>The following result is really important:</p> \\[ \\begin{align*} a_i(\\pmb{x},t)&amp;=A_i(\\phi^{-1}(\\pmb{x},t), t)\\\\ &amp;=\\frac{\\partial V_i}{\\partial t}(\\phi^{-1}(\\pmb{X}, t),t)\\\\ &amp;=\\frac{\\partial v_i}{\\partial t}(\\pmb{x}, t)+\\sum_{j=1}^d\\frac{\\partial v_i}{\\partial x_j}(\\pmb{x},t)v_j(\\pmb{x},t)\\\\ &amp;\\overset{\\Delta}{=}\\frac{D}{Dt}v_i(\\pmb{x},t) \\end{align*} \\] <p>So </p> \\[ \\pmb{a}=\\frac{D}{Dt}\\pmb{v} \\] <p>More generally, for a general Eulerian function \\(f(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}\\), we can use this same notation to mean</p> \\[ \\frac{D}{Dt}f(\\pmb{x},t)=\\frac{\\partial f}{\\partial t}(\\pmb{x}, t)+\\sum_{j=1}^d\\frac{\\partial f}{\\partial x_j}(\\pmb{x},t)v_j(\\pmb{x},t) \\] <p>which is called material derivative, and the first item of derivative is called local rate of change. The above one is also the push forward of \\(\\partial F/\\partial t\\) where \\(F\\) is a Lagrangian function with \\(F(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}\\).</p>"},{"location":"Math/mpm/#relationship-between-two-deformation-gradients","title":"Relationship between two Deformation Gradients","text":"<p>For deformation gradient, most of the time in the physics of a material, the Lagrangian view is the dominant one. There is however a useful evolution of the Eulerian (push forward) of \\(\\pmb{F}(\\cdot,t):\\Omega^0\\rightarrow \\mathbb{R}^{d\\times d}\\). If we denote \\(\\pmb{f}(\\cdot, t):\\Omega^t\\rightarrow \\mathbb{R}^{d\\times d}\\) be the push forward of \\(\\pmb{F}\\), then</p> \\[ \\frac{D}{Dt}\\pmb{f}(\\pmb{x},t) = \\frac{\\partial \\pmb{v}}{\\partial \\pmb{x}}\\pmb{f} \\] <p>or </p> \\[ \\dot{\\pmb{F}}=(\\nabla \\pmb{v})\\pmb{F} \\] <p>cause we have</p> \\[ \\begin{align*} \\frac{\\partial}{\\partial t}F_{ij}(\\pmb{X}, t)&amp;=\\frac{1}{\\partial t }\\frac{\\partial \\phi_i}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\frac{1}{\\partial X_j}\\frac{\\partial \\phi_i}{ \\partial t }(\\pmb{X},t)\\\\ &amp;=\\frac{\\partial V_i}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\frac{\\partial v_i}{\\partial X_j}(\\phi(\\pmb{X},t),t)\\\\ &amp;=\\sum_{k=1}^d\\frac{\\partial v_i}{\\partial x_k}(\\phi(\\pmb{x},t),t)\\cdot \\frac{\\partial \\phi_k}{\\partial X_j}(\\pmb{X},t)\\\\ &amp;=\\sum_{k=1}^d\\frac{\\partial v_i}{\\partial x_k}(\\phi(\\pmb{x},t),t)\\cdot F_{kj}(\\pmb{X},t) \\end{align*} \\] <p>The above equation will play an important role in deriving the discretized deformation gradient \\(F\\) update on each MPM particle.</p>"},{"location":"Math/mpm/#volume-and-area-change","title":"Volume and Area Change","text":"<ul> <li>Volume</li> </ul> <p>Consider \\(dV\\) being defined over the standard basis vectors \\(\\vec{e}_i\\), \\(i=1.2.3\\), with \\(dV=dL_1\\vec{e}_1\\cdot (dL_2\\vec{e}_2 \\times dL_3\\vec{e}_3)\\). If we denote \\(d\\pmb{L}_i=dL_i\\vec{e}\\), then </p> \\[ dV=dL_1dL_2dL_3, \\quad  \\] <p>cause we have \\(d\\pmb{l}_i=\\pmb{F}d\\pmb{L}_i\\) (like \\((\\pmb{x}_2-\\pmb{x}_1)=\\pmb{F}(\\pmb{X_2}-\\pmb{X}_1)\\)), so it can be shown that \\(dl_1dl_2dl_3=JdL_1dL_2dL_3\\) or </p> \\[ dv=JdV. \\] <p>Based on the above property we can have a great weapon in proof. Given function \\(G(\\pmb{X})\\) or \\(g(\\pmb{x},t)\\) we have </p> \\[ \\int_{B^t}g(\\pmb{x})d\\pmb{x}=\\int_{B^0}G(\\pmb{X})J(\\pmb{X},t)d\\pmb{X} \\] <p>Actually it is the variable substitution formula of multiple integral.</p> <ul> <li>Areas</li> </ul> <p>And similar analysis can be done for areas.</p> \\[ \\begin{align*} dv&amp;=JdV\\\\ \\pmb{n}ds\\cdot d\\pmb{l}&amp;=J\\pmb{N}dS\\cdot d\\pmb{L}\\\\ \\pmb{n}ds\\cdot \\pmb{F}d\\pmb{L}&amp;=J\\pmb{N}dS\\cdot d\\pmb{L}\\quad \\text{using $d\\pmb{l}=\\pmb{F}d\\pmb{L}$}\\\\ \\pmb{n}ds\\cdot \\pmb{F}&amp;=J\\pmb{N}dS \\end{align*} \\] <p>So </p> \\[ \\begin{equation} \\pmb{n}ds=J\\pmb{F}^{-T}\\pmb{N}dS.\\label{eq-area} \\end{equation} \\] <p>which is also called Nansom's formula.</p>"},{"location":"Math/mpm/#piola-kirchhoff-stress","title":"Piola-Kirchhoff Stress","text":"<p>Consider a vector element of surface in the material world(Lagrangian), \\(\\pmb{N}dS\\) and after deformation, the material particles making up this area now occupy the element defined by \\(\\pmb{n}ds\\), where \\(ds\\) is the area and \\(\\pmb{n}\\) is the normal vector in the current configuration(Eulerian).</p> <p>Then by definition of the Cauchy stress</p> \\[ d\\pmb{f}=\\pmb{\\sigma}\\pmb{n}ds \\] <p>The first Piola-Kirchhoff stress tensor \\(\\pmb{P}\\) (PK1 stress, Nominal Stress tensor) is defined by</p> \\[ d\\pmb{f}=\\pmb{P}\\pmb{N}dS \\] <p>which relates the force acting in the current configuration to the surface element in the reference configuration.</p> <p>So similarly with (Cauchy) traction vector was defined by</p> \\[ \\pmb{t}=\\frac{d\\pmb{f}}{ds} \\] <p>we can introduce a PK1 traction vector</p> \\[ \\pmb{T}=\\frac{d\\pmb{f}}{dS}, \\quad \\pmb{T}=\\pmb{P}\\pmb{N} \\] <p>which is a fictitious quantity.</p> <p>Note that \\(d\\pmb{f}=\\pmb{t}ds=\\pmb{T}dS\\), so \\(\\pmb{t}\\) and \\(\\pmb{T}\\) at the same area has the same direction but different magnitudes.</p> <p> </p> <ul> <li>Relation between the Cauchy and PK1 Stresses</li> </ul> <p>From the above definition,</p> \\[ \\begin{align*} \\pmb{P}\\pmb{N}dS&amp;=\\pmb{\\sigma}\\pmb{n}\\\\ \\pmb{P}\\pmb{N}dS&amp;=\\pmb{\\sigma}J\\pmb{F}^{-T}\\pmb{N}dS\\quad\\text{using equation $\\ref{eq-area}$}\\\\ \\pmb{P}&amp;=J\\pmb{\\sigma}\\pmb{F}^{-T}\\\\ \\pmb{\\sigma}&amp;=J^{-1}\\pmb{P}\\pmb{F}^T \\end{align*} \\]"},{"location":"Math/mpm/#hyperelasticity","title":"Hyperelasticity","text":"<p>Stress is related to strain(Deformation gradient \\(F\\)) through \"Constitutive Relationship\". </p> <p>For perfect hyperelastic materials, the relation is defined through the potential energy, which increases with non-rigid deformation from the initial state. That is, The elastic solids whose first Piola-Kirchoff stress \\(\\pmb{P}\\) can be derived from an strain energy density function \\(\\Psi(\\pmb{F})\\) (a scalar function) via</p> \\[ \\pmb{P}=\\frac{\\partial \\Psi}{\\partial \\pmb{F}} \\] <p>and Cauchy stress with respect to \\(\\Psi(\\pmb{F})\\)</p> \\[ \\sigma=\\frac{1}{J}\\pmb{P}\\pmb{F}^T=\\frac{1}{J}\\frac{\\partial \\Psi}{\\partial \\pmb{F}}\\pmb{F}^T \\]"},{"location":"Math/mpm/#energy-density-function","title":"Energy density function","text":"<ul> <li>Neo-Hookean</li> </ul> \\[ \\Psi(\\pmb{F})=\\frac{\\mu}{2}(\\text{tr}{(\\pmb{F}^T\\pmb{F})}-d)-\\mu\\log(J)+\\frac{\\lambda}{2}\\log^2(J) \\] <p>where \\(d\\) denoted dimension, \\(2\\) or \\(3\\) in practice and </p> \\[ \\mu=\\frac{E}{2(1+v)},\\quad \\lambda=\\frac{Ev}{(1+v)(1-2v)} \\] <p>in which \\(E\\) is Young's modulus and \\(v\\) is Poisson's ratio.</p> <ul> <li>Fixed corotated model</li> </ul> <p>This is derived from the Singular Value Decomposition (SVD). Assuming the polar SVD \\(\\pmb{F}=U\\Sigma V^T\\), then the energy for fixed corotated model is </p> \\[ \\Psi(\\pmb{F})=\\hat{\\Psi}(\\Sigma(\\pmb{F}))=\\mu\\sum_{i=1}^d(\\sigma_i-1)^2+\\frac{\\lambda}{2}(J-1)^2 \\] <p>where \\(J=\\prod\\limits_{i=1}^d\\sigma_i\\) and </p> \\[ \\pmb{P}(\\pmb{F})=\\frac{\\partial\\Psi}{\\partial \\pmb{F}}(\\pmb{F})=2\\mu(\\pmb{F}-\\pmb{R})+\\lambda(J-1)J\\pmb{F}^{-T}. \\]"},{"location":"Math/mpm/#governing-equations","title":"Governing Equations","text":"<p>Let </p> \\[ \\pmb{V}(\\pmb{X},t)=\\frac{\\partial \\phi(\\pmb{X},t)}{\\partial t}=\\frac{\\partial{\\pmb{x}}}{\\partial t} \\] <p>be the velocity defined over \\(X\\). Then from Lagrangian view of point, the equations are</p> \\[ \\begin{cases} \\displaystyle R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0) \\quad &amp;\\text{Conservation of mass},\\\\ \\displaystyle R(\\pmb{X},t)\\frac{\\partial \\pmb{V}}{\\partial t}=\\nabla^{\\pmb{X}}\\cdot \\pmb{P}+R(\\pmb{X},0)g \\quad &amp;\\text{Conservation of momentum}. \\end{cases} \\] <p>where \\(R\\) is the Lagrangian mass density which is related to the more commonly used Eulerian mass density \\(\\rho\\). Note that mass conservation can also be written as </p> \\[ \\frac{\\partial }{\\partial t}[R(\\pmb{X},t)J(\\pmb{X},t)]=0 \\] <p>In Eulerian view, the governing equations are</p> \\[ \\begin{cases} \\displaystyle \\frac{D}{Dt}\\rho(\\pmb{x},t)+\\rho(\\pmb{x},t)\\nabla^{\\pmb{x}}\\cdot \\pmb{v}(\\pmb{x},t)=0,\\quad &amp;\\text{Conservation of mass},\\\\ \\displaystyle \\rho(\\pmb{x},t)\\frac{D\\pmb{v}}{Dt}=\\nabla^{\\pmb{x}}\\cdot \\sigma +\\rho(\\pmb{x},t)g, \\quad &amp;\\text{Conservation of momentum}.\\\\ \\end{cases} \\] <p>where </p> \\[ \\frac{D}{Dt}=\\frac{\\partial}{\\partial t}+\\pmb{v}\\cdot \\nabla^{\\pmb{x}} \\]"},{"location":"Math/mpm/#conservation-of-mass","title":"Conservation of Mass","text":"<p>To be more specific, we have two ways to get the Continuity Equation.</p> HintsFrom global viewFrom material view <p>By using multiple integral for density.</p> <p>Notice that </p> \\[ \\rho(\\pmb{x},t)=\\lim_{\\varepsilon\\rightarrow 0^+}\\frac{\\text{mass}(B_{\\varepsilon}^t)}{\\int_{B_{\\varepsilon}^t}d\\pmb{x}} \\] <p>We choose a fixed volume, in which the rate of increase of mass must equal the rate at which mass is flowing into the volume through its bounding surface.</p> <p>The rate of increase mass in a fixed volume \\(v\\) is</p> \\[ \\frac{\\partial m}{\\partial t}=\\frac{\\partial}{\\partial t}\\int_{v}\\rho(\\pmb{x},t)dv=\\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv \\] <p>while the mass flux out through the surface is given by</p> \\[ \\int_s \\rho \\pmb{v}\\cdot \\pmb{n} ds \\] <p>here \\(s\\) can denote the overall outer surface of the material(we consider in and out).</p> <p>so combine the above two we get</p> \\[ \\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv + \\int_s \\rho \\pmb{v}\\cdot \\pmb{n} ds=0 \\] <p>using divergence theorem, we get</p> \\[ \\begin{align*} \\int_v \\frac{\\rho(\\pmb{x},t)}{\\partial t}dv + \\int_v \\nabla^{\\pmb{x}}\\cdot(\\rho \\pmb{v}) dv&amp;=0\\\\ \\frac{\\rho(\\pmb{x},t)}{\\partial t}+ \\nabla^{\\pmb{x}}\\cdot(\\rho \\pmb{v}) &amp;=0\\\\ \\frac{\\partial \\rho(\\pmb{x},t)}{\\partial t}+ (\\nabla^{\\pmb{x}}\\cdot\\rho)\\pmb{v} +(\\nabla^{\\pmb{x}}\\cdot\\pmb{v})\\rho &amp;=0\\\\ \\frac{D\\rho}{Dt} +\\rho \\nabla^{\\pmb{x}}\\cdot(\\pmb{v})&amp;=0 \\end{align*} \\] <p>We check the fixed volume in terms of material point. Note \\(R(\\pmb{X},t)=\\rho(\\phi(\\pmb{X},t),t)\\), then we see the mass from initial time to time \\(t\\) must be the same:</p> \\[ \\begin{align*} \\text{mass}(v)&amp;=\\text{mass}V_0\\\\ \\int_{v}\\rho(\\pmb{x},t)d\\pmb{x}&amp;=\\int_{V}R(\\pmb{X},0)d\\pmb{X}\\\\ \\Rightarrow \\int_{V}R(\\pmb{X},t)J(\\pmb{X},t)d\\pmb{X}&amp;=\\int_{V}R(\\pmb{X},0)d\\pmb{X} \\end{align*} \\] <p>So </p> \\[ R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0), \\quad \\forall \\pmb{X}\\in \\Omega^0, t\\geq 0 \\] <p>Note that \\(J(\\pmb{X},0)=1\\), so \\(R(\\pmb{X},t)J(\\pmb{X},t)=R(\\pmb{X},0)J(\\pmb{X},0)\\), that is, </p> \\[ \\frac{\\partial }{\\partial t}[R(\\pmb{X},t)J(\\pmb{X},t)]=0. \\]"},{"location":"Math/mpm/#conservation-of-momentum","title":"Conservation of Momentum","text":"<p>Also, we have two ways to consider.</p> HintsFrom glabal viewFrom material view <p>Use the similar logic in proof of mass conservation.</p> <p>This is also the spatial form.</p> <p>We know that</p> \\[ \\pmb{t}(\\pmb{x},\\pmb{n},t)=\\pmb{\\sigma}(\\pmb{x},t)\\pmb{n} \\] <p>We consider a fixed mass, so the space occupied by this matter may change over time.</p> <p>If \\(\\pmb{v}\\) denotes the Eulerian velocity, then the linear momentum of Euler can be denoted as</p> \\[ \\pmb{L}(t)=\\int_{B_{\\varepsilon}^t} \\rho\\pmb{v}d\\pmb{x}, \\] <p>where </p> <p>then by \\(\\pmb{f}^{ext}=\\frac{d(m\\pmb{v})}{dt}\\) formulated by Euler,</p> \\[ \\begin{align*} \\int_{\\partial B_{\\varepsilon}^t}\\pmb{\\sigma}\\pmb{n}ds+\\int_{B_{\\varepsilon}^t}\\pmb{f}^{ext}d\\pmb{x}&amp;=\\frac{d}{dt}\\int_{B_{\\varepsilon}^t}\\rho \\pmb{v}d\\pmb{x}\\\\ \\int_{B_{\\varepsilon}^t}\\nabla^{\\pmb{x}}\\cdot \\sigma+\\pmb{f}^{ext} d\\pmb{x}&amp;=\\frac{d}{dt}\\int_{B_{\\varepsilon}^0}R\\pmb{V}Jd\\pmb{X}\\\\ &amp;=\\int_{B_{\\varepsilon}^0}R\\pmb{A}Jd\\pmb{X}\\\\ &amp;=\\int_{B_{\\varepsilon}^t}\\rho\\pmb{a}d\\pmb{x} \\end{align*} \\] <p>Note \\(\\pmb{V}(\\pmb{X},t)=\\pmb{v}(\\phi(\\pmb{X},t),t)\\), so we have a fixed volume \\(V\\) with momentum </p> \\[ \\pmb{L}(t) = \\int_V R(\\pmb{X},t)V(\\pmb{X},t)dV \\] <p>So </p> \\[ \\begin{align*} \\frac{\\partial}{\\partial t}\\int_V R(\\pmb{X},t)V(\\pmb{X},t)dV &amp;=\\int_S\\pmb{T}dS +\\int_V \\pmb{F}dV\\\\ \\int_V R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t)dV&amp;= \\int_S \\pmb{P}\\cdot\\pmb{N}dS+\\int_V\\pmb{F} dV\\\\ \\int_V R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t)dV&amp;= \\int_V \\nabla^{\\pmb{X}}\\cdot \\pmb{P}+\\pmb{F} dV\\\\ \\Rightarrow R(\\pmb{X},t)\\frac{\\partial}{\\partial t}V(\\pmb{X},t) &amp;=\\nabla^{\\pmb{X}}\\cdot \\pmb{P}+\\pmb{F} \\end{align*} \\]"},{"location":"Math/mpm/#material-particles","title":"Material particles","text":"<p>Recall that the material point method is Lagrangian in the sense that we track actual particles of material. That is we keep track of mass (\\(m_p\\)), velocity (\\(v_p\\)) and position (\\(x_p\\)) for a collection of material particles \\(p\\).</p> <p>However, all stress based forces are computed on the Eulerian grid, so we have to transfer the material state to the Eulerian configuration to incorporate the effects of material forces. </p> <p>Then, we transfer these effects back to the material particles and move them in the normal Lagrangian way. The Lagrangian nature makes advection very trivial compared to pure Eulerian methods (such as grid-based fluid simulation).</p>"},{"location":"Math/mpm/#eulerian-interpolating-functions","title":"Eulerian Interpolating Functions","text":"<p>We can denote the interpolation function at grid node \\(\\pmb{i}=(i,j,k)\\) evaluated at a particle location \\(\\pmb{x}_p\\) with </p> \\[ N_{\\pmb{i}}(\\pmb{x}_p)=N\\left(\\frac{1}{h}(\\pmb{x}_p-\\pmb{x}_{\\pmb{i}})\\right)N\\left(\\frac{1}{h}(\\pmb{y}_p-\\pmb{y}_{\\pmb{i}})\\right)N\\left(\\frac{1}{h}(\\pmb{z}_p-\\pmb{z}_{\\pmb{i}})\\right) \\] <p>where \\(h\\) is the grid spacing. We can define diffenrent kernel \\(N:\\mathbb{R}\\rightarrow \\mathbb{R}\\).</p> <p>Common Kernel \\(N\\)</p> <ul> <li>cubic kernel. It is more expensive but provide wider coverage, thus less sensitive to numerical errors.</li> </ul> \\[ N(x)=\\begin{cases} \\displaystyle \\frac{1}{2}|x|^3-|x|^2+\\frac{2}{3}, \\quad &amp;0\\leq |x| &lt; 1\\\\ \\displaystyle \\frac{1}{6}(2-|x|)^3,\\quad &amp;1\\leq |x|&lt; 2\\\\ \\displaystyle 0,\\quad &amp;2\\leq|x| \\end{cases} \\] <ul> <li>quadratic kernel. It is more computational efficient and memory saving.</li> </ul> \\[ N(x)=\\begin{cases} \\displaystyle \\frac{3}{4}-|x|^2,\\quad &amp; \\displaystyle 0\\leq |x| &lt; \\frac{1}{2}\\\\ \\displaystyle \\frac{1}{2}\\left(\\frac{3}{2}-|x|\\right)^2,\\quad &amp;\\displaystyle \\frac{1}{2}\\leq |x|&lt;\\frac{3}{2}\\\\ \\displaystyle 0,\\quad &amp;\\displaystyle \\frac{3}{2}\\leq |x| \\end{cases} \\] <p>Then the gradient of function \\(N_{\\pmb{i}}(\\pmb{x}_p)\\) is </p> \\[ \\nabla N_{\\pmb{i}}(\\pmb{x}_p) =\\sum_{k=1}^d\\left[N'\\left(\\frac{1}{h}(x_k-x_{\\pmb{i}})\\right)\\prod_{j=1\\atop j\\neq k}^d N\\left(\\frac{1}{h}(x_{j}-x_{\\pmb{i}})\\right)\\right] \\] <p>where \\(x_k\\), \\(x_j\\) denote the component index of \\(\\pmb{x}_p\\), i.e. \\(x_k, x_j\\in\\{x_p,y_p,z_p\\}\\).</p>"},{"location":"Math/mpm/#eulerianlagrangian-mass","title":"Eulerian/Lagrangian Mass","text":"<p>Mass of the particle</p> \\[ m_p^n=\\int_{B_{\\Delta x,p}^{t^n}}\\rho(\\pmb{x},t^n)d\\pmb{x} \\] <p>and define the mass from Eulerian perspective</p> \\[ m_{\\pmb{i}}=\\sum_pm_p\\cdot N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Easy to see that </p> \\[ \\sum_{\\pmb{i}}m_{\\pmb{i}}=\\sum_p m_p \\] <p>since the weight function \\(N_{\\pmb{i}}(\\pmb{x}_p)\\) is normalized to \\(1\\).</p>"},{"location":"Math/mpm/#eulerian-momentum","title":"Eulerian Momentum","text":"<p>Similarly, we transfer particle monentum \\(m_p\\pmb{v}_p\\) to the grid</p> \\[ (m\\pmb{v})_{\\pmb{i}}=\\sum_p m_p \\pmb{v}_p N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Also easy to that </p> \\[ \\sum_{\\pmb{i}}(m\\pmb{v})_{\\pmb{i}}=\\sum_{p}m_p\\pmb{v}_p \\] <p>and the Eulerian velocity \\(\\pmb{v}_{\\pmb{i}}\\) is defined as </p> \\[ \\pmb{v}_{\\pmb{i}}=\\frac{(m\\pmb{v})_{\\pmb{i}}}{m_{\\pmb{i}}} \\]"},{"location":"Math/mpm/#eulerian-to-lagrangian-transfer","title":"Eulerian to Lagrangian Transfer","text":"<p>We do not need to transfer mass from the grid to the particles since Lagrangian particle mass never changes. But the Velocity is simply interpolated as</p> \\[ \\pmb{v}_p =\\sum_{\\pmb{i}}\\pmb{v}_{\\pmb{i}}N_{\\pmb{i}}(\\pmb{x}_p) \\] <p>Easy to see that</p> \\[ \\sum_p m_p \\pmb{v}_p=\\sum_{\\pmb{i}}m_i\\pmb{v}_{\\pmb{i}} \\]"},{"location":"Math/mpm/#discretization","title":"Discretization","text":""},{"location":"Math/mpm/#explicite-time-integration","title":"Explicite time Integration","text":""},{"location":"Math/mpm/#apic-transfers","title":"APIC Transfers","text":"<p>Transfer from particle to grid</p> \\[ \\begin{cases} m_i=\\sum_p w_{ip}m_p\\\\ m_i\\pmb{v}_i=\\sum_p w_{ip}m_p(\\pmb{v}_p+\\pmb{B}_p(\\pmb{D}_p)^{-1}(\\pmb{x}_i-\\pmb{x}_p)) \\end{cases} \\] <p>where \\(\\pmb{B}_p\\) is a matrix quantity stored at each particle(like mass, position and velocity), \\(\\pmb{D}_p\\) is given by </p> \\[ \\pmb{D}_p=\\sum_i w_{ip}(\\pmb{x}_i-\\pmb{x}_p)(\\pmb{x}_i-\\pmb{x}_p)^T \\] <p>which has a simple form \\(\\frac{1}{4}\\Delta x^2\\pmb{I}\\) for quadratic and \\(\\frac{1}{3}\\Delta x^2\\pmb{I}\\) for cubic interpolation stencils.</p> <p>Then from grid to particle</p> \\[ \\begin{cases} \\pmb{v}_p=\\sum_i w_{ip}\\pmb{v}_i\\\\ \\pmb{B}_p=\\sum_i w_{ip}\\pmb{v}_i (\\pmb{x}_i-\\pmb{x}_p)^T \\end{cases} \\]"},{"location":"Math/mpm/#deformation-gradient-update","title":"Deformation Gradient Update","text":"<p>Given \\(f_p\\), we can update the position and velocity of the grid</p> \\[ \\begin{cases} \\pmb{v}_i^{n+1}=\\pmb{v}_i^n+\\Delta t f_i(\\pmb{x}_i^n)/m_i\\\\ \\pmb{x}_{i}^{n+1}=\\pmb{x}_i^n+\\Delta t \\pmb{v}_i^{n+1} \\end{cases} \\] <p>Given a grid velocity field \\(\\pmb{v}_i^{n+1}\\), we can update \\(F\\) as</p> \\[ F_p^{n+1}=\\left(I+\\Delta t \\sum_i \\pmb{v}_i^{n+1}(\\nabla w_{ip}^n)^T\\right) F^n_p \\]"},{"location":"Math/mpm/#forces","title":"Forces","text":"<p>MPM Forces are defined on grid nodes. If we assume a deformation gradient based hyperelastic energy density, then the total elastic potential energy is then </p> \\[ e=\\sum_p V_p^0\\Psi_p(F_p) \\] <p>where \\(V_p^0\\) is the material space volume of particle.</p> <p>Nodal elastic force is the negative gradient of the total potential energy evaluated at nodal positions. So the MPM spatial discretization of the stress-based forces is given as </p> \\[ f_i(x_i^n)=-\\frac{\\partial e}{\\partial x_i}(x)=-\\sum_p V_p^0\\left(\\frac{\\partial \\Psi_p}{\\partial F}(F_p(x_i^n))\\right)(F^n_p)^T\\nabla w_{ip}^n \\] <p>which fully depends on the existing particle/grid weights and particle attributes.</p>"},{"location":"Math/mpm/preliminary/","title":"Preliminary","text":"<p>Reference</p> <ul> <li>Mechanics Lecture Notes Part III: Foundations of Continuum Mechanics, pa.kelly@auckland.ac.nz.</li> </ul> <p>Website</p>"},{"location":"Math/mpm/preliminary/#tensors","title":"Tensors","text":"<p>A tensor of order zero is simply another name for a scalar \\(\\alpha\\).</p> <p>A first-order tensor is simply another name for a vector \\(\\pmb{u}\\). </p> <p>We use uppercase bold-face Latin letters to denote second order tensor.</p> <p>A second-order tensor \\(\\pmb{T}\\) may be defined as an operator that acts on a vector \\(\\pmb{u}\\) generating another vector \\(\\pmb{v}\\), such that </p> \\[ \\pmb{T}(\\pmb{u})=\\pmb{v} \\] <p>which is a linear operator. </p> <ul> <li>dyad(tensor product)</li> </ul> <p>the tensor product of two vectors \\(\\pmb{u}\\) and \\(\\pmb{v}\\)</p> \\[ \\pmb{u}\\otimes \\pmb{v} \\] <p>is defined by</p> \\[ (\\pmb{u}\\otimes \\pmb{v} ) \\pmb{w} = \\pmb{u}(\\pmb{v}\\cdot \\pmb{w}) \\] <p>Properties</p> <p>(i)</p> \\[ (\\pmb{u}\\otimes \\pmb{v})(\\pmb{w}\\otimes \\pmb{x})=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{u}\\otimes \\pmb{x}) \\] <p>cause</p> \\[ \\begin{align*} (\\pmb{u}\\otimes \\pmb{v})(\\pmb{w}\\otimes \\pmb{x})\\pmb{y}&amp;=(\\pmb{u}\\otimes \\pmb{v})(\\pmb{x}\\cdot\\pmb{y})\\pmb{w}\\\\ &amp;=(\\pmb{x}\\cdot\\pmb{y})(\\pmb{u}\\otimes \\pmb{v})\\pmb{w}\\\\ &amp;=(\\pmb{x}\\cdot\\pmb{y})(\\pmb{v}\\cdot\\pmb{w})\\pmb{u}\\\\ &amp;=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{x}\\cdot\\pmb{y})\\pmb{u}\\\\ &amp;=(\\pmb{v}\\cdot\\pmb{w})(\\pmb{u}\\otimes \\pmb{x})\\pmb{y} \\end{align*} \\] <p>(ii)</p> \\[ \\pmb{u}(\\pmb{v}\\otimes \\pmb{w})=(\\pmb{u}\\cdot \\pmb{v})\\pmb{w} \\] <p>cause</p> \\[ \\begin{align*} (\\pmb{y}\\otimes \\pmb{u})(\\pmb{v}\\otimes \\pmb{w})&amp;=(\\pmb{u}\\cdot \\pmb{v})(\\pmb{y}\\otimes \\pmb{w})\\\\ &amp;=\\pmb{y} \\otimes [(\\pmb{u}\\cdot \\pmb{v})\\pmb{w}] \\end{align*} \\] <p>Some example</p> <ul> <li>Projection Tensor \\((\\pmb{e}\\otimes \\pmb{e})\\) </li> </ul> <p>So </p> \\[ (\\pmb{e}\\otimes \\pmb{e})\\pmb{u} = (\\pmb{e}\\cdot \\pmb{u})\\pmb{e} \\] <p>is the vector projection of \\(\\pmb{u}\\) on \\(\\pmb{e}\\), denoted by \\(\\pmb{P}\\).</p> <p>A dyadic is a linear combination of dyads (with scalar coefficients).</p> <p>In the following discussion, we can treat \\(\\pmb{T}\\) as a matrix.    </p>"},{"location":"Math/mpm/preliminary/#cartesian-tensors","title":"Cartesian Tensors","text":"<p>A second order tensor and the the vector it operates on can be described in terms of Cartesian components.</p> <p>Example</p> <ul> <li>Identity tensor/(or unit tensor).</li> </ul> \\[ \\pmb{I}=\\sum_{i=1}^d\\pmb{e}_i\\otimes \\pmb{e}_i \\] <p>cause it follows</p> \\[ \\begin{align*} \\pmb{I} \\pmb{u}&amp;=\\sum_{i=1}^d(\\pmb{e}_i\\otimes \\pmb{e}_i) \\pmb{u}\\\\ &amp;=\\sum_{i=1}^d(\\pmb{e}_i \\cdot \\pmb{u})\\pmb{e}_i \\\\ &amp;=\\sum_{i=1}^d u_i\\pmb{e}_i \\\\ &amp;=\\pmb{u} \\end{align*} \\] <p>Or identity tensor can be written as</p> \\[ \\pmb{I} = \\sum_{i,j=1}^d\\delta_{ij}(\\pmb{e}_i\\otimes \\pmb{e}_j) \\] <p>Second order tensor as a Dyadic</p> <p>Every second order tensor can always be written as a dyadic involving the Cartesian base vectors \\(\\pmb{e}_i\\), that is, if we denote </p> \\[ \\pmb{E}_i=\\pmb{T}\\left(\\pmb{e}_i\\right) \\] <p>then </p> \\[ \\pmb{T}= \\sum_{i=1}^d \\left(\\pmb{E}_i\\otimes \\pmb{e}_i\\right) \\] Proof \\[ \\begin{align*} \\pmb{b}&amp;=\\pmb{T}(\\pmb{a})\\\\ &amp;=\\pmb{T}\\left(\\sum_{i=1}^d a_i\\pmb{e}_i\\right)\\\\ &amp;=\\sum_{i=1}^d a_i \\pmb{T}(\\pmb{e}_i)\\\\ \\end{align*} \\] <p>Denote \\(\\pmb{T}(\\pmb{e}_i)\\) to be \\(\\pmb{E}_i\\), then</p> \\[ \\begin{align*} \\pmb{b}&amp;=\\sum_{i=1}^d a_i \\pmb{E}_i\\\\ &amp;=\\sum_{i=1}^d (\\pmb{a}\\cdot \\pmb{e}_i )\\pmb{E}_i\\\\ &amp;=\\sum_{i=1}^d (\\pmb{E}_i\\otimes \\pmb{e}_i) \\pmb{a} \\end{align*} \\] <p>So </p> \\[ \\pmb{T}= \\sum_{i=1}^d (\\pmb{E}_i\\otimes \\pmb{e}_i) \\] <p>If we write \\(\\pmb{E}_i\\) with base vectors like</p> \\[ \\pmb{E}_i=\\sum_{j=1}^d E_{ij}\\pmb{e}_j, \\quad i=1,\\cdots, d \\] <p>Then </p> \\[ \\begin{align*} (\\pmb{E}_i\\otimes \\pmb{e}_i)&amp;=\\left(\\sum_{j=1}^d E_{ij}\\pmb{e}_j\\right)\\otimes \\pmb{e}_i\\\\ &amp;=\\sum_{j=1}^d E_{ij} (\\pmb{e}_j\\otimes \\pmb{e}_i), \\quad i=1,\\cdots, d \\end{align*} \\] <p>Thus</p> \\[ \\pmb{T}=\\sum_{i=1}^d\\sum_{j=1}^d E_{ij} (\\pmb{e}_j\\otimes \\pmb{e}_i) \\] <p>Introduce 9 scalars \\(T_{ij}=E_{ji}\\), then </p> \\[ \\begin{align*} \\pmb{T}&amp;=\\sum_{i=1}^d\\sum_{j=1}^d T_{ji} (\\pmb{e}_j\\otimes \\pmb{e}_i)\\\\ &amp;=\\sum_{j=1}^d\\sum_{i=1}^d T_{ji} (\\pmb{e}_j\\otimes \\pmb{e}_i) \\quad \\text{switch summation turn}\\\\ &amp;=\\sum_{i=1}^d\\sum_{j=1}^d T_{ij} (\\pmb{e}_i\\otimes \\pmb{e}_j)\\quad \\text{switch $i$ and $j$} \\end{align*} \\] <p>We can see that 9 dyads \\(\\{\\pmb{e}_i\\otimes \\pmb{e}_j\\}_{i,j=1}^3\\) forms a basis for the space of second order tensors.</p> <p>Recall that </p> \\[ \\begin{align*} T_{ij}&amp;=E_{ji}\\\\ &amp;=\\pmb{E}_j \\cdot \\pmb{e}_i \\\\ &amp;=T(\\pmb{e}_j) \\cdot (\\pmb{e}_i)\\\\ &amp;=(\\pmb{e}_i) \\cdot T(\\pmb{e}_j)\\\\ \\end{align*} \\] <p>So we can get the component of a tensor by the above way.</p>"},{"location":"Math/mpm/preliminary/#cauchy-stress-tensor","title":"Cauchy Stress Tensor","text":"<p>The traction vector, the limiting value of the ratio of force over area, that is,</p> \\[ \\pmb{t}^{\\pmb{n}}=\\lim_{\\Delta_s\\rightarrow 0}\\frac{\\Delta F}{\\Delta S} \\] <p>where \\(\\pmb{n}\\) denotes normal vector to the surface.</p> <p>The stress \\(\\pmb{\\sigma}\\), a second order tensor which maps \\(\\pmb{n}\\) onto \\(\\pmb{t}\\)</p> \\[ \\pmb{t}=\\pmb{\\sigma}\\pmb{n} \\] <p>If we consider a coordinate system with base vectors \\(\\pmb{e}_i\\), then \\(\\pmb{\\sigma}=\\sum\\limits_{i,j=1}^d(\\sigma_{ij}\\pmb{e}_i \\otimes \\pmb{e}_j)\\) </p> \\[ \\pmb{\\sigma}=(\\sigma_{ij}) \\] <p>So </p> \\[ t_i \\pmb{e_i} = \\sum_{j=1}^3\\sigma_{ij}n_{j} \\pmb{e}_i \\] <p> </p> <p>For example, </p> \\[ \\pmb{\\sigma}\\pmb{e}_j=\\sum_{i=1}^3\\sigma_{ij}\\pmb{e}_i  \\] <p>which denotes the summation of the \\(j\\)th column of matrix \\(\\pmb{\\sigma}\\).</p> <p>So the components \\(\\sigma_{11}, \\sigma_{21}, \\sigma_{31}\\) of the stress tensor are the three components of the traction vector which acts on the plane with normal \\(\\pmb{e}_1\\).</p>"},{"location":"Math/mpm/preliminary/#hamilton-operator","title":"Hamilton Operator","text":"<p>First we want to introduce the operator </p> \\[ \\nabla=\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z} \\] <p>then </p> \\[ \\nabla f= \\pmb{i}\\frac{\\partial f}{\\partial x}+\\pmb{j}\\frac{\\partial f}{\\partial y}+\\pmb{k}\\frac{\\partial f}{\\partial z}=\\text{grad} f \\] <p>we also have inner product </p> \\[ \\begin{align*} \\nabla\\cdot \\pmb{a}&amp;=\\left(\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z}\\right)\\cdot (P\\pmb{i}+Q\\pmb{j}+R\\pmb{k})\\\\ &amp;=\\frac{\\partial P}{\\partial x}+\\frac{\\partial Q} {\\partial y}+\\frac{\\partial R}{\\partial z}\\\\ &amp;=\\text{div} \\pmb{a} \\end{align*} \\] <p>and cross product</p> \\[ \\begin{align*} \\nabla\\times \\pmb{a}&amp;=\\left(\\pmb{i}\\frac{\\partial }{\\partial x}+\\pmb{j}\\frac{\\partial }{\\partial y}+\\pmb{k}\\frac{\\partial }{\\partial z}\\right)\\times (P\\pmb{i}+Q\\pmb{j}+R\\pmb{k})\\\\ &amp;=\\left|\\begin{array}{ccc} \\pmb{i}&amp;\\pmb{j}&amp;\\pmb{k}\\\\ \\displaystyle \\frac{\\partial }{\\partial x}&amp;\\displaystyle \\frac{\\partial }{\\partial y} &amp; \\displaystyle \\frac{\\partial }{\\partial z}\\\\ P &amp; Q &amp; R\\\\ \\end{array} \\right|\\\\ &amp;=\\left( \\frac{\\partial R}{\\partial y}- \\frac{\\partial Q}{\\partial z}\\right)\\pmb{i}+\\left( \\frac{\\partial R}{\\partial x}- \\frac{\\partial P}{\\partial z}\\right)\\pmb{j}+\\left( \\frac{\\partial Q}{\\partial x}- \\frac{\\partial P}{\\partial y}\\right)\\pmb{k}\\\\ &amp;=\\text{rot} \\pmb{a} \\end{align*} \\] <p>Then Gauss Formula can be expressed by</p> \\[ \\iint_{\\partial\\Omega}\\pmb{a}d\\pmb{S}=\\iiint_{\\Omega}\\nabla\\cdot \\pmb{a}dV \\] <p>Stokes Formula can be expressed by</p> \\[ \\int_{\\partial \\Sigma}\\pmb{a}d\\pmb{s}=\\iint_{\\Sigma}(\\nabla\\times \\pmb{a})\\cdot d\\pmb{S} \\]"},{"location":"Readings/","title":"Readings","text":"<p>Here I put some notes about readings regarding my researches.</p>"},{"location":"about/","title":"About me","text":"<p> <p></p> <p>Hi! I'm Xuancheng Tu, a junior student from Zhejiang University, Chu Kochen Honors College. I major in Automation, in College of Control Science &amp; Engineering. </p> <p>My research interests lie in control theory, particularly mathematical modelling and its control strategies in applications for aerial robots and subaqueous robots. I'm very fortunate to be advised by Prof. Tiefeng Li at Zhejiang University to do research on control of bouyancy for underwater robots using phase transition. I'm also under the guidance of Prof. Hehe Fan, focusing on a project to model avatar and control facial expressions in avatars using 3D Gaussian Splatting (3DGS) and continuum mechanics.</p> <p>During my spare time, I usually listen to music, watch movies and play table tennis or badminton. If you find similar or common interest with me, feel free to reach out to me at XcTu291@gmail.com!</p>"},{"location":"blog/","title":"Blog","text":"<p>In this part, I hope you can stand my childish thoughts and points.</p>"},{"location":"blog/MiddleSchool/","title":"\u7ecf\u9a8c\u5206\u4eab","text":"<p>2025.02.11</p>"},{"location":"blog/MiddleSchool/#_2","title":"\u5173\u4e8e\u6211","text":"<ul> <li> <p>\u6d77\u57ce\u4e2d\u5b662016\u7ea7</p> </li> <li> <p>\u9f99\u6e7e\u4e2d\u5b662019\u7ea7(\u63d0\u524d\u6279)</p> </li> <li> <p>\u6d59\u6c5f\u5927\u5b662022\u7ea7\u672c\u79d1\u751f</p> </li> </ul>"},{"location":"blog/MiddleSchool/#_3","title":"\u6211\u7684\u521d\u4e2d\u751f\u6d3b","text":""},{"location":"blog/MiddleSchool/#tips","title":"\u5b66\u4e60Tips","text":"<ul> <li> <p>\u5b66\u4e60\u4e0d\u9700\u8981\u592a\u591a\u7684\u89c4\u5212\uff0c\u91cd\u5728\u201c\u5b9e\u5e72\u201d\uff1b\u65e0\u610f\u4e49\u7684\u601d\u8003\u5e26\u6765\u601d\u7ef4\u8d1f\u62c5</p> </li> <li> <p>\u4e0d\u540c\u7684\u9636\u6bb5\u6709\u4e0d\u540c\u7684\u4efb\u52a1\uff1a</p> </li> </ul> \u65f6\u95f4\u6bb5 \u7ec6\u8282 \u8003\u8bd5\u524d\u3001\u65e5\u5e38\u5b66\u4e60 \u97ec\u5149\u517b\u6666\u3002\u4e0d\u6025\u4e0d\u6162\uff0c\u628a\u6bcf\u4e00\u4e2a\u77e5\u8bc6\u70b9\u5b66\u624e\u5b9e\u3001\u5f04\u6e05\u695a\u3002 \u8003\u8bd5\u9636\u6bb5 \u8d81\u70ed\u6253\u94c1\u3002\u53ca\u65f6\u603b\u7ed3\u5b66\u4e60\u6210\u679c\uff0c\u6446\u597d\u81ea\u5df1\u5b66\u8bc6\u3001\u601d\u7ef4\u7684\u6c34\u679c\u62fc\u76d8\u3002 \u5047\u671f \u4fdd\u7559\u81ea\u5df1\u7684\u661f\u8fb0\uff0c\u63a2\u7d22\u81ea\u5df1\u7684\u5174\u8da3\u3002\uff08\u521d\u4e2d\u7684\u6211\u662f\u5404\u79cd\u51fa\u53bb\u5b66\u4e60\u7684\uff0c\u4e5f\u82b1\u4e86\u7238\u5988\u4e0d\u5c11\u94b1\uff0c\u4f3c\u4e4e\u73b0\u5728\u66f4\u5377\uff1f\uff09 <ul> <li> <p>\u65f6\u95f4\u7ba1\u7406\uff1a\u5b66\u4f1a\u5404\u7c7b\u72b6\u6001\u4e4b\u95f4\u7684\u8f6c\u79fb\uff0c\u4e0d\u626d\u634f\uff0c\u4e0d\u5de6\u53f3\u6447\u6446</p> </li> <li> <p>\u610f\u5fd7\u529b\uff1a\u201c\u5012\u4e5f\u8981\u5012\u5728\u79bb\u68a6\u60f3\u6700\u8fd1\u7684\u5730\u65b9\u3002\u201d</p> </li> </ul> <p>\u672c\u6b21\u6574\u7406\u7684\u7ecf\u9a8c\u6559\u8bad</p> <p>\u201c\u4e00\u4e2a\u6c11\u65cf\u7684\u5386\u53f2\u662f\u4e00\u4e2a\u6c11\u65cf\u5b89\u8eab\u7acb\u547d\u7684\u57fa\u7840\u3002\u201d</p> <p>\u5927\u5bb6\u4e00\u5b9a\u8981\u4fdd\u7559\u597d\u81ea\u5df1\u7684\u6587\u4ef6\uff0c\u73b0\u5728\u56de\u5934\u770b\u53d1\u73b0\u6ca1\u6709\u591a\u5c11\u6587\u4ef6\u9057\u7559\u3002</p>"},{"location":"blog/MiddleSchool/#_4","title":"\u5b66\u751f\u5e72\u90e8\u5de5\u4f5c","text":""},{"location":"blog/MiddleSchool/#_5","title":"\u521d\u4e2d\u804c\u52a1","text":"<ul> <li> <p>\u73ed\u957f(\u521d\u4e00-\u521d\u4e09)</p> </li> <li> <p>\u5b66\u751f\u4f1a\u4e3b\u5e2d\uff08\u521d\u4e09\uff09</p> </li> </ul>"},{"location":"blog/MiddleSchool/#_6","title":"\u5de5\u4f5c\u7ecf\u9a8c","text":"<ul> <li> <p>\u66f4\u591a\u7684\u662f\u601d\u7ef4\u5b9e\u9a8c\uff1a\u6a21\u62df\u73ed\u7ea7\u6cd5\u5ead\u3001\u5236\u5b9a\u73ed\u89c4</p> </li> <li> <p>\u5c3d\u53ef\u80fd\u5b8c\u6210\u81ea\u5df1\u80fd\u591f\u5b8c\u6210\u7684\uff1a</p> </li> </ul> <p> </p> <p>(\u4e0a\u9762\u7684\u6587\u5b57\u751f\u6210\u4e8e\u516b\u5e74\u7ea7\u4e0b\uff0c\u6807\u53f7\u4e3a\u8bed\u6587\u8003\u8bd5\u7c7b\u578b\u6807\u53f7)</p>"},{"location":"blog/MiddleSchool/#1","title":"\u96441\uff1a\u4f18\u79c0\u5b66\u751f\u9648\u8ff0","text":"<p>\u9605\u8bfb\u63d0\u9192</p> <p>\u4e0b\u9762\u6587\u5b57\u751f\u6210\u4e8e\u516b\u5e74\u7ea7\u4e0b\u3002</p> <p>\u7b14\u8005\u6ce8\uff1a\u4f3c\u4e4e\u6bd4\u8f83\u65e9\u7684\u65f6\u5019\u5c31\u6709\u7ec8\u8eab\u5b66\u4e60\u7684\u89c2\u5ff5\u3002</p> <p>\u601d\u60f3\u662f\u4e00\u5207\u884c\u52a8\u7684\u6e90\u5934\u3002\u6211\u79ef\u6781\u4e50\u89c2\uff0c\u660e\u8fa8\u662f\u975e\uff0c\u5168\u9762\u601d\u8003\u3002\u6211\u8ba4\u4e3a\uff1a\u201c\u751f\u547d\u4f1a\u968f\u5149\u9634\u4e00\u8d77\u6d41\u901d\uff0c\u6211\u4eec\u8981\u5b8c\u6210\u751f\u547d\u7684\u4f7f\u547d\u3002\u201d\u8fd9\uff0c\u5c31\u9700\u8981\u6211\u4eec\u7ed9\u4eba\u7c7b\u3001\u4e16\u754c\u505a\u51fa\u6709\u610f\u4e49\u7684\u4e8b\u60c5\u3002\u6211\u61c2\u5f97\u77e5\u6653\u56fd\u5bb6\u5927\u4e8b\uff0c\u5b66\u4e60\u7406\u89e3\u5386\u53f2\uff0c\u660e\u786e\u7231\u56fd\u60c5\u6000\u3002\u6211\u9488\u5bf9\u4e8e\u751f\u6d3b\u5404\u4e2a\u65b9\u9762\uff0c\u50cf\u5982\u4f55\u505a\u4eba\u3001\u5982\u4f55\u5f85\u4eba\uff0c\u5982\u4f55\u52b3\u9038\u7ed3\u5408\uff0c\u90fd\u6709\u542c\u53d6\u4ed6\u4eba\u7684\u6559\u8bf2\u3001\u8bc4\u8bba\u7b49\uff0c\u5f62\u6210\u81ea\u5df1\u7684\u601d\u8003\u3002</p> <p>\u6211\u7684\u601d\u60f3\u544a\u8bc9\u6211\uff0c\u5f97\u4e0d\u5fd8\u5b66\u4e60\u3002\u53ef\u4ee5\u80af\u5b9a\uff1a\u201c\u4e0d\u540c\u5b66\u79d1\uff0c\u4e0d\u540c\u65b9\u6cd5\u201d\u3002\u4f46\u662f\u77e5\u8bc6\u603b\u662f\u6709\u89c4\u5f8b\u7684\uff0c\u5b83\u4eec\u6709\u5171\u540c\u7684\u7279\u70b9\uff0c\u6211\u5f97\u51fa\u6211\u7684\u601d\u8003\u65b9\u5f0f\uff1a\u7406\u89e3\u77e5\u8bc6\u610f\u4e49\uff0c\u6df1\u7a76\u77e5\u8bc6\u5185\u6db5\uff0c\u5584\u4e8e\u603b\u7ed3\u3002</p> <p>\u5b66\u4e60\u6570\u5b66\u65b0\u77e5\u8bc6\u5e73\u884c\u56db\u8fb9\u5f62\u4e4b\u524d\uff0c\u6211\u5148\u4e86\u89e3\u8fd9\u4e00\u77e5\u8bc6\u7684\u5e94\u7528\uff1a\u751f\u6d3b\u4e2d\u7684\u8bb8\u591a\u4e1c\u897f\u90fd\u53ef\u4ee5\u5229\u7528\u8fd9\u4e00\u77e5\u8bc6\uff0c\u4e14\u5728\u5b66\u4e60\u4e2d\u53ef\u4ee5\u953b\u70bc\u6211\u7684\u591a\u89d2\u5ea6\u601d\u8003\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u6211\u5c31\u4f1a\u4ea7\u751f\u66f4\u52a0\u5f3a\u70c8\u5b66\u4e60\u7684\u52a8\u529b\u3002</p> <p>\u5b66\u4e60\u65b0\u77e5\u8bc6\u65f6\uff0c\u6211\u4e0a\u8bfe\u8ba4\u771f\u542c\u8bb2\uff0c\u601d\u7ef4\u6d3b\u8dc3\uff1b\u8bfe\u540e\u591a\u505a\u9898\u76ee\u5e76\u5206\u6790\uff0c\u501f\u4e1a\u4f59\u65f6\u95f4\u67e5\u9605\u4e66\u672c\u548c\u7f51\u7edc\u3001\u8be2\u95ee\u8001\u5e08\uff0c\u9010\u6e10\u53d1\u73b0\u8fd9\u4e2a\u5355\u5143\u7684\u5b9e\u8d28\uff1a\u7ebf\u6bb5\u5e73\u884c\u4e0e\u76f8\u7b49\u7684\u5e94\u7528\u3002\u6240\u6709\u7684\u9898\u76ee\u90fd\u662f\u7531\u8fd9\u4e9b\u57fa\u672c\u7684\u5185\u5bb9\u5c42\u5c42\u94fa\u57ab\u800c\u6210\u7684\uff0c\u638c\u63e1\u8fd9\u4e00\u5185\u5bb9\uff0c\u4fbf\u80fd\u8f83\u4e3a\u8f7b\u677e\u5730\u77e5\u9053\u4e86\u89e3\u9898\u601d\u8def\uff0c\u66f4\u597d\u5730\u5b66\u4e60\u3002\u6bd4\u5982\uff0c\u5728\u9047\u5230\u4e2d\u4f4d\u7ebf\u4e0e\u7b49\u8fb9\u4e09\u89d2\u5f62\u76f8\u7ed3\u5408\u7684\u9898\u76ee\u65f6\uff0c\u6211\u8fd0\u7528\u6bcf\u4e00\u4e2a\u56fe\u5f62\u6240\u5177\u6709\u7684\u6027\u8d28\uff0c\u5c06\u5b83\u4eec\u4e32\u8054\u8d77\u6765\u5206\u6790\u5b83\u7684\u89e3\u9898\u5173\u952e\u6240\u5728\u3002</p> <p>\u5b66\u4e60\u5b8c\u8fd9\u4e00\u5355\u5143\u6216\u5176\u4e2d\u7684\u67d0\u4e00\u77e5\u8bc6\u540e\uff0c\u6709\u5fc5\u8981\u65f6\uff0c\u6211\u4f1a\u82b1\u70b9\u65f6\u95f4\u603b\u7ed3\uff1a\u5f52\u7eb3\u9898\u76ee\u7684\u5957\u8def\u548c\u5bf9\u9898\u76ee\u7c7b\u578b\u7684\u8bb0\u5fc6\u548c\u7406\u89e3\u3002\u6211\u4f1a\u8bf7\u6559\u8001\u5e08\uff0c\u53c2\u8003\u5b66\u4e60\u63d0\u7eb2\u548c\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u6709\u6240\u9009\u62e9\uff0c\u4ece\u800c\u5f62\u6210\u81ea\u5df1\u7684\u601d\u8def\u548c\u63d0\u7eb2\u3002\u8fd9\u5c31\u662f\u6211\u6240\u4e00\u76f4\u4fdd\u6301\u7740\u7684\u4f18\u79c0\u6210\u7ee9\u7684\u91cd\u8981\u56e0\u7d20\u3002</p> <p>\u6211\u7684\u4f53\u80b2\u8fd0\u52a8\u4e5f\u53d7\u5230\u6211\u7684\u601d\u60f3\u5f71\u54cd\uff0c\u5728\u4f53\u8d28\u65b9\u9762\uff0c\u6211\u5728\u4f53\u8d28\u68c0\u6d4b\u4e2d\u662f\u4f18\u79c0\u7684\u3002\u540c\u65f6\uff0c\u6211\u77e5\u9053\u8fd0\u52a8\u53ef\u4ee5\u653e\u677e\u8eab\u5fc3\uff0c\u4f53\u9a8c\u751f\u547d\u7684\u751f\u673a\u3002\u6240\u4ee5\uff0c\u548c\u540c\u5b66\u4eec\u4e00\u8d77\u6253\u7403\uff0c\u51fa\u53bb\u4e00\u540c\u6e38\u73a9\uff0c\u662f\u6211\u7684\u751f\u6d3b\u5b66\u4e60\u5e38\u4e8b\u3002</p> <p>\u6211\u65f6\u523b\u63d0\u9192\u81ea\u5df1\uff1a\u540d\u5229\u53ea\u662f\u751f\u6d3b\u5b66\u4e60\u4e2d\u7684\u6700\u5fae\u5c0f\u7684\u4e00\u90e8\u5206\u3002\u6de1\u6cca\u540d\u5229\uff0c\u4ece\u5bb9\u6de1\u5b9a\uff0c\u771f\u8bda\u771f\u60c5\u505a\u5b9e\u4e8b\uff0c\u76f8\u4fe1\u6211\u7684\u672a\u6765\u4f1a\u8d8a\u6765\u8d8a\u7cbe\u5f69\u3002</p>"},{"location":"blog/MiddleSchool/#2","title":"\u96442\uff1a\u8fd4\u6821\u5ba3\u8bb2\u7b14\u8bb0\uff08\u672a\u8bb2\u8fc7\uff09","text":"<p>\u9605\u8bfb\u63d0\u9192</p> <p>\u4e0b\u9762\u6587\u5b57\u751f\u6210\u4e8e\u9ad8\u4e00\u5e74\u7ea7\u3002</p> <p>\u7b14\u8005\u6ce8\uff1a\u53ef\u80fd\u4f1a\u6709\u81ea\u521d\u4e2d\u4ee5\u6765\u7ee7\u627f\u7684\u4e00\u4e9b\u51e1\u5c14\u8d5b\u3002</p> <p>\u6211\u77e5\u9053\uff0c\u4e0d\u5230\u8003\u524d\u51e0\u5929\uff0c\u4f60\u4eec\u7684\u5fc3\u60c5\u4ece\u4e0d\u7d27\u5f20\u3002\u56e0\u4e3a\u6211\u4e5f\u4e00\u6837\uff0c\u6bcf\u6b21\u8003\u8bd5\uff0c\u90fd\u53ea\u6709\u5230\u4e86\u8003\u524d\u51e0\u5929\u624d\u5fc3\u7406\u7d27\u5f20\u8d77\u6765\uff0c\u77e5\u9053\u8981\u66f4\u4e25\u8083\u4e86\u3002\u5176\u5b9e\uff0c\u6211\u4eec\u4e3a\u4ec0\u4e48\u8981\u8fd9\u4e48\u65e9\u7d27\u5f20\u8d77\u6765\u5462\uff1f\u9664\u4e86\u4f24\u8eab\u8017\u529b\u4e4b\u5916\uff0c\u5c31\u8fd8\u6709\u4e00\u4e2a\u597d\u5904\uff1a\u4f60\u80fd\u591f\u5728\u6709\u9650\u7684\u65f6\u95f4\u91cc\u9762\uff0c\u505a\u51fa\u66f4\u591a\u8d81\u70ed\u6253\u94c1\u7684\u4e8b\u60c5\u3002 \u8fd9\u7c7b\u4e8b\u60c5\uff0c\u5f80\u5f80\u80fd\u591f\u5de9\u56fa\u4f60\u7684\u73b0\u6709\u7684\u77e5\u8bc6\uff0c\u953b\u70bc\u4f60\u7684\u5fc3\u7406\u7d20\u8d28\u548c\u8003\u8bd5\u7d20\u8d28\uff0c\u8fd9\u5fc5\u5c06\u80fd\u4e3a\u4f60\u7684\u8003\u8bd5\u505a\u51c6\u5907\uff0c\u800c\u4e14\u4e5f\u80fd\u4e30\u5bcc\u4f60\u6574\u4e2a\u4eba\u751f\u7684\u9605\u5386\u3002\u505a\u8fd9\u7c7b\u4e8b\u60c5\uff0c\u6211\u5fc5\u987b\u8981\u70b9\u660e\u5408\u7406\u7684\u65b9\u6cd5\uff0c\u5373\u80fd\u591f\u8ba9\u4f60\u6210\u529f\u8d81\u70ed\u6253\u94c1\u3002</p> <p>\u7b2c\u4e00\u70b9\uff0c\u8981\u591a\u505a\u9898\u76ee\u3002\u4e0e\u6211\u4eec\u6240\u8c13\u7684\u201c\u5237\u9898\u201d\u662f\u4e00\u6837\u7684\u9053\u7406\u3002\u9898\u76ee\u4f60\u6709\u65f6\u89c9\u5f97\u592a\u5c11\uff0c\u90a3\u4f60\u5c31\u81ea\u5df1\u627e\u9898\u76ee\u53bb\u505a\u3002</p> <p>\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u6211\u8981\u5f3a\u8c03\u51e0\u70b9\uff1a</p> <p>\u7b2c\u4e00\uff0c\u4f60\u8981\u660e\u786e\u81ea\u5df1\u5237\u9898\u7684\u610f\u56fe\u2014\u2014\u5c31\u662f\u8981\u4f7f\u81ea\u5df1\u7684\u9898\u5e93\u66f4\u4e30\u5bcc\uff0c\u4ee5\u4f7f\u4f60\u8003\u8bd5\u7684\u65f6\u5019\u80fd\u591f\u66f4\u5a07\u8f7b\u8f66\u719f\u8def\u2014\u2014\u56e0\u6b64\uff0c\u4f60\u8981\u5fc3\u91cc\u77e5\u9053\uff0c\u5bf9\u4e8e\u6709\u96be\u5ea6\u3001\u6709\u610f\u601d\u7684\u9898\u76ee\uff0c\u4e00\u5b9a\u8981\u8bb0\u5f55\u4e0b\u6765\uff1a\u73b0\u5728\u4f60\u8ba4\u4e3a\u4f60\u7684\u8111\u5b50\u53ef\u4ee5\uff0c\u90a3\u4f60\u5c31\u8bb0\u5728\u8111\u91cc\u7edd\u5bf9\u6ca1\u6709\u95ee\u9898\uff1b\u5982\u679c\u4f60\u771f\u7684\u8ba4\u4e3a\u81ea\u5df1\u9700\u8981\u6574\u7406\u5f52\u7eb3\u4e0e\u63d0\u70bc\uff0c\u90a3\u5c31\u4e70\u4e2a\u7b14\u8bb0\u672c\u6309\u4f60\u81ea\u5df1\u7684\u683c\u5f0f\u53bb\u8d70\u2014\u2014\u6ca1\u5fc5\u8981\u90a3\u4e48\u597d\u770b\uff0c\u5c31\u50cf\u6211\u4eec\u8001\u5e08\u8bf4\u7684\uff1b\u4e5f\u6ca1\u5fc5\u8981\u5728\u4e2d\u8003\u524d\u53cd\u590d\u5730\u8bfb\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u65f6\u5019\u4f60\u5df2\u7ecf\u7262\u7262\u5f97\u5c06\u5b83\u5370\u5728\u8111\u5b50\u91cc\u4e86\u3002</p> <p>\u7b2c\u4e8c\uff0c\u5728\u8fd9\u6837\u7684\u610f\u56fe\u4e4b\u4e0b\uff0c\u4f60\u8981\u81ea\u5df1\u4e3b\u52a8\u5730\u53bb\u505a\u8fd1\u51e0\u5e74\u7684\u4e2d\u8003\u8bd5\u5377\uff08\u4e5f\u8bb8\u8001\u5e08\u5df2\u7ecf\u7ed9\u4f60\u505a\u4e86\uff09\uff0c\u53bb\u5237\u597d\u7684\u9898\u76ee\u3002\u867d\u7136\u4e2d\u8003\u8bd5\u5377\u662f\u4eba\u7eaf\u8111\u5b50\u51fa\u7684\uff0c\u800c\u4e0d\u662f\u4ece\u522b\u7684\u5730\u65b9\u6252\u6765\u7684\uff0c\u4f46\u662f\u8fd9\u4e2a\u4eba\u4e5f\u662f\u4eba\uff0c\u4ed6\u7b26\u5408\u4e00\u4e2a\u4eba\u7684\u7279\u8d28\uff0c\u53ea\u662f\u9605\u5386\u6bd4\u4f60\u4e30\u5bcc\uff0c\u6240\u4ee5\u4f60\u8981\u53bb\u8fce\u5408\u51fa\u9898\u4eba\uff0c\u53bb\u8003\u8651\u51fa\u9898\u4eba\u7684\u89d2\u5ea6\u3002\u4f60\u4eec\u77e5\u9053\u6211\u8fd9\u51e0\u4e2a\u5b66\u671f\u7684\u8bed\u6587\u671f\u672b\u5206\u6570\u5427\uff0c\u4e3a\u4ec0\u4e48\u4ece\u516b\u5e74\u7ea7\u4ee5\u6765\u6211\u603b\u662f\u80fd\u5728\u6e29\u5dde\u5e02\u671f\u672b\u8bed\u6587\u8bd5\u5377\u4e2d\u83b7\u5f97\u9ad8\u5206\uff1f\u5982\u679c\u8bf4\u81ea\u5df1\u5728\u591a\u6b21\u8bad\u7ec3\u540e\u7684\u611f\u89c9\u662f\u91cd\u8981\u7684\u4e00\u5757\uff0c\u90a3\u4e48\u731c\u900f\u4ed6\u7684\u51fa\u9898\u5957\u8def\u4fbf\u662f\u53e6\u4e00\u4e2a\u5236\u80dc\u6cd5\u5b9d\u3002\u56e0\u4e3a\u6211\u5728\u9002\u5e94\u8fd9\u7c7b\u7684\u8003\u8bd5\uff0c\u6240\u4ee5\u6211\u80fd\u591f\u8003\u51fa\u597d\u7684\u5206\u6570\u3002</p> <p>\u7b2c\u4e09\uff0c\u522b\u5bb3\u6015\u9519\u9898\u3002\u8fd8\u662f\u90a3\u53e5\u8001\u8bdd\uff1a\u522b\u4eba\u7b11\u4f60\u90a3\u5c31\u7b11\u4f60\u5457\uff0c\u53cd\u6b63\u4f60\u6536\u83b7\u7684\u662f\u7ed9\u4e88\u81ea\u5df1\u5185\u5fc3\u7684\u3002\u9047\u5230\u9519\u9898\uff0c\u614c\u5f20\u80af\u5b9a\u662f\u6709\u7684\uff0c\u8fd9\u4e2a\u4eba\u561b\uff0c\u65e0\u6cd5\u907f\u514d\uff1b\u4f46\u662f\u66f4\u591a\u7684\u5e94\u8be5\u662f\u865a\u5fc3\u5730\u6c42\u6559\uff0c\u8fd9\u4e2a\u9898\u76ee\u6211\u9519\u5728\u54ea\u91cc\uff0c\u5982\u679c\u662f\u4e0b\u6b21\uff0c\u6211\u96be\u9053\u8fd8\u4f1a\u8fd9\u6837\u9519\u5417\uff1f\u627e\u5230\u9519\u9898\u7684\u539f\u56e0\uff0c\u5e76\u6574\u7406\u5f52\u7eb3\u2014\u2014\u8fd9\u91cc\u6211\u8ba4\u4e3a\u4f60\u4e00\u5b9a\u8981\u8bb0\u5f55\u5728\u672c\u5b50\u4e0a\uff0c\u4e13\u95e8\u7684\u672c\u5b50\u4e0a\u2014\u2014\u56e0\u4e3a\u6211\u4eec\u8eab\u4e3a\u4e00\u4e2a\u4f1a\u5f97\u610f\u5fd8\u5f62\u7684\u4eba\uff0c\u72af\u4e86\u9519\u4e4b\u540e\u603b\u662f\u5f88\u5feb\u5c31\u5fd8\u4e86\u75bc\u2014\u2014\u6240\u4ee5\u5e38\u5e38\u62ff\u51fa\u6765\u770b\u770b\uff0c\u6709\u52a9\u4e8e\u4f60\u66f4\u597d\u5730\u5de9\u56fa\u3002\u6bd4\u5982\u8bf4\uff0c\u5728\u6570\u5b66\u7684\u7edd\u5bf9\u503c\u95ee\u9898\u91cc\uff0c\u6211\u7528\u4e86\u4e00\u4e2a\u5f88\u9ebb\u70e6\u7684\u5206\u7c7b\u8ba8\u8bba\u53bb\u505a\uff0c\u7ed3\u679c\u8001\u5e08\u8bb2\u89e3\u7684\u65f6\u5019\u5c31\u8fd0\u7528\u4e86\u5b83\u7684\u51e0\u4f55\u610f\u4e49\u2014\u2014\u70b9\u5230\u70b9\u7684\u8ddd\u79bb\u2014\u2014\u9a6c\u4e0a\u5c31\u89e3\u51b3\u4e86\u3002\u8fd9\u65f6\uff0c\u6211\u5c31\u8981\u9a6c\u4e0a\u8d81\u70ed\u6253\u94c1\u8bb0\u5f55\u4e0b\u6240\u611f\u609f\u7684\u8fd9\u4e00\u5207\uff0c\u5e76\u5728\u6bcf\u6b21\u505a\u7ec3\u4e60\u65f6\u60f3\u8fc7\u4e00\u904d\u53c8\u4e00\u904d\uff0c\u90a3\u4e48\u8fd9\u4e00\u6570\u5b66\u601d\u60f3\u5c31\u6210\u4e3a\u4f60\u81ea\u5df1\u7684\u5e38\u5e38\u60f3\u5230\u7684\u4e1c\u897f\u4e86\u3002</p> <p>\u8bb2\u4e86\u8fd9\u4e48\u591a\uff0c\u4e5f\u8bb8\u4f60\u4f1a\u6cc4\u4e86\u70b9\u6c14\u3002\u4e3a\u4ec0\u4e48\u5237\u4e2a\u9898\u8981\u8017\u8d39\u8fd9\u4e48\u591a\u7cbe\u529b\uff1f\u4f46\u662f\u4f60\u60f3\uff0c\u5982\u679c\u6211\u4e0d\u8017\u8d39\u8fd9\u4e48\u591a\u7cbe\u529b\uff0c\u90a3\u4e48\u6211\u7684\u9898\u4e0d\u5c31\u767d\u767d\u5730\u88ab\u5237\u4e86\u4e00\u904d\u4e4b\u540e\u88ab\u6211\u7684\u90a3\u4e2a\u5bb9\u6613\u9057\u5fd8\u7684\u5927\u8111\u629b\u5728\u4e86\u65f6\u95f4\u7684\u5c18\u57c3\u91cc\u800c\u518d\u4e5f\u65e0\u6cd5\u60f3\u8d77\u4e86\u5417\uff1f\u591a\u7528\u70b9\u65f6\u95f4\u7cbe\u529b\u5728\u5de9\u56fa\u4e0a\uff0c\u6bd4\u4e00\u5473\u7684\u5237\u9898\u8981\u9ad8\u6548\u591a\u4e86\u3002\u6211\u8fd8\u8bb0\u5f97\u6211\u4eec\u8bed\u6587\u8001\u5e08\u66fe\u7ecf\u8bf4\u8fc7\uff1a\u201c\u53ea\u6709\u4f60\u4ed8\u51fa\u4e86\uff0c\u540e\u6765\u624d\u4f1a\u6709\u6b63\u5f53\u7684\u56de\u62a5\u3002\u201d\u73b0\u5728\u8fd9\u53e5\u8bdd\u89e3\u91ca\u5728\u8fd9\u91cc\u5c31\u5c24\u6709\u5b83\u7684\u610f\u4e49\u4e86\u3002</p> <p>\u7b2c\u4e8c\u70b9\uff0c\u6bcf\u5468\u6765\u4e00\u6b21\u704c\u9e21\u8840\u6d3b\u52a8\u3002\u6240\u8c13\u704c\u9e21\u8840\uff0c\u8868\u9762\u4e0a\uff0c\u5c31\u662f\u597d\u597d\u5730\u8ddf\u4f60\u8bb2\u8bb2\u522b\u4eba\u7684\u6545\u4e8b\uff0c\u4ee5\u6b64\u6fc0\u52b1\u4f60\uff1b\u6697\u5730\u91cc\uff0c\u5c31\u662f\u628a\u73b0\u5728\u7684\u597d\u7aef\u7aef\u7684\u4f60\u653e\u5230\u548c\u4f60\u6781\u4e0d\u613f\u610f\u76f8\u6bd4\u7684\u90a3\u4e2a\u4eba\u65c1\u8fb9\uff0c\u72e0\u72e0\u5730\u628a\u4f60\u7684\u7f3a\u70b9\u548c\u522b\u4eba\u7684\u4f18\u70b9\u6765\u4e2a\u6bd4\u8f83\u3002\u5c31\u662f\u8bf4\uff0c\u522b\u4eba\u591a\u4e48\u7684\u60b2\u58ee\uff0c\u800c\u4f60\u73b0\u5728\u5374\u5982\u6b64\u5e73\u6de1\uff1b\u4e0d\u884c\uff0c\u6211\u4e00\u5b9a\u8981\u8ba9\u4f60\u7d27\u5f20\u8d77\u6765\u3002</p> <p>\u4e3a\u4ec0\u4e48\u6211\u4eec\u9700\u8981\u6253\u9e21\u8840\uff1f\u4eba\u7684\u52a8\u529b\u5f80\u5f80\u662f\u4e09\u5206\u949f\u70ed\u5ea6\uff0c\u5728\u65f6\u95f4\u7684\u6d17\u5237\u548c\u65c1\u8fb9\u4eba\u7684\u51b7\u6de1\u4e4b\u4e0b\uff0c\u6211\u4eec\u7684\u52a8\u529b\u5f88\u5feb\u5c31\u51e0\u5929\u65f6\u95f4\uff0c\u5c31\u4f1a\u4e27\u5931\u4e00\u5927\u90e8\u5206\u3002\u8fd9\u6837\uff0c\u4e5f\u6b63\u5e38\uff0c\u56e0\u4e3a\u6211\u4eec\u65e0\u6cd5\u957f\u65f6\u95f4\u5730\u9ad8\u529f\u7387\u5de5\u4f5c\u2014\u2014\u4f46\u662f\u6211\u5e76\u6ca1\u6709\u6392\u9664\u6211\u4eec\u53ef\u4ee5\u5e38\u5e38\u5730\u7206\u53d1\u4e00\u6b21\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4e5f\u53ea\u80fd\u9694\u4e2a\u51e0\u5468\u548c\u4f60\u4eec\u6765\u8bb2\u51e0\u53e5\u8bdd\u3002\u8fd9\u5468\uff0c\u6211\u5c31\u9001\u7ed9\u4f60\u4eec\u8fd9\u53e5\u8bdd\uff1a\u201c\u5012\u4e5f\u8981\u5012\u5728\u79bb\u68a6\u60f3\u6700\u8fd1\u7684\u5730\u65b9\u3002\u201d</p> <p>\u8fd9\u53e5\u8bdd\u662f\u6211\u5728\u590f\u4ee4\u8425\u7684\u65f6\u5019\uff0c\u4ece\u90a3\u4e2a\u8001\u5e08\u90a3\u91cc\u542c\u8fc7\u6765\u7684\u3002\u5f53\u65f6\u7684\u6211\u5c31\u6709\u7740\u4e09\u5206\u949f\u70ed\u5ea6\uff0c\u4eff\u4f5b\u81ea\u5df1\u4ec0\u4e48\u90fd\u80fd\u5e72\u597d\u3002\u6211\u73b0\u5728\u8fd8\u4f9d\u7a00\u8bb0\u5f97\u4e00\u70b9\u3002\u8fd9\u662f\u74ef\u6d77\u4e2d\u5b66\u7684\u4e00\u4e2a\u4e24\u4e2a\u540c\u5b66\u3002\u4ed6\u4eec\u66fe\u7ecf\u5728\u5168\u73ed\u540c\u5b66\u9762\u524d\u653e\u8a00\u72e0\u8bdd\uff0c\u5982\u679c\u9ad8\u8003\u8003\u4e0d\u4e86\u5168\u6821\u7b2c\u4e00\uff0c\u90a3\u4ed6\u4eec\u5c31\u8df3\u5230\u6821\u65c1\u8fb9\u7684\u6cb3\u91cc\u53bb\u3002\u540e\u6765\uff0c\u4ed6\u4eec\u6bcf\u5929\u5c31\u6b7b\u5750\u5728\u6559\u5ba4\u91cc\uff0c\u51cc\u6668\u534a\u591c\u8d77\u6765\u8bfb\u4e66\u3002</p> <p>\u4f60\u4eec\u77e5\u9053\u540e\u6765\u7ed3\u679c\u600e\u4e48\u4e86\uff1f</p> <p>\u786e\u5b9e\uff0c\u9ad8\u8003\u8003\u7b2c\u4e00\u662f\u6709\u96be\u5ea6\u7684\uff0c\u6ca1\u6709\u4e00\u4e2a\u4eba\u4e0a\u4e86\u7b2c\u4e00\uff1b\u4f46\u662f\u4ed6\u4eec\u4e24\u4e2a\u90fd\u4e0a\u4e86\u6bb5\u91cc\u524d\u5341\uff0c\u6709\u4e2a\u662f\u7b2c\u4e8c\u540d\u3002\u4f46\u662f\u4ed6\u4eec\u6ca1\u6709\u529e\u6cd5\uff0c\u5fc5\u987b\u4fe1\u5b88\u627f\u8bfa\u3002\u5168\u73ed\u540c\u5b66\u627e\u4e86\u4e2a\u65f6\u95f4\uff0c\u628a\u4ed6\u4eec\u5171\u540c\u805a\u5728\u6cb3\u8fb9\uff0c\u8131\u4e86\u8863\u670d\u5c31\u8df3\u4e86\u4e0b\u53bb\u3002\u653e\u5fc3\uff0c\u4ed6\u4eec\u4f1a\u6e38\u6cf3\uff1b\u5f53\u7136\uff0c\u5168\u73ed\u540c\u5b66\u4e5f\u90fd\u7b11\u4e86\u4e00\u5802\u3002</p> <p>\u8fd9\u4e2a\u6545\u4e8b\u5c31\u8fd9\u6837\uff0c\u4f46\u662f\u6211\u60f3\u5c31\u6b64\u544a\u8bc9\u4f60\u4eec\uff0c\u6211\u4eec\u9700\u8981\u6709\u68a6\u60f3\u3002\u8fd9\u4e2a\u68a6\u60f3\u8fd8\u5fc5\u987b\u9700\u8981\u4f60\u4ed8\u51fa\u76f8\u5e94\u7684\u4ee3\u4ef7\u3002\u6211\u5e76\u4e0d\u662f\u60f3\u8981\u8ba9\u4f60\u4eec\u90fd\u53bb\u8df3\u6cb3\uff0c\u4e07\u4e00\u4f60\u4eec\u4e0d\u4f1a\u6e38\u6cf3\u5462\uff1f\u4f46\u662f\u8fd9\u4e2a\u8981\u6c42\u5fc5\u987b\u5bf9\u4e8e\u4f60\u81ea\u5df1\u8981\u6709\u9ad8\u5ea6\uff0c\u5b83\u4f1a\u903c\u8feb\u7740\u4f60\u53bb\u594b\u6597\uff0c\u53bb\u8ffd\u9010\u68a6\u60f3\u3002\u8fd9\u4e9b\u5199\u4e0b\u6765\u5f53\u7136\u53ea\u662f\u767d\u7eb8\u9ed1\u5b57\uff0c\u6211\u5e0c\u671b\u6211\u4eec\u7684\u73ed\u4e3b\u4efb\u8001\u5e08\u80fd\u591f\u6bcf\u5468\u4e00\u6b21\u5ba3\u8bfb\u4e00\u4e0b\u8fd9\u6837\u7684\u68a6\u60f3\uff0c\u6bcf\u4e2a\u540c\u5b66\u6bcf\u5468\u90fd\u8981\u91cd\u65b0\u518d\u53d7\u5230\u4e00\u6b21\u523a\u6fc0\u3002</p> <p>\u6211\u5728\u5f53\u65f6\u5c31\u5199\u4e0b\u4e86\u4e00\u5b9a\u8981\u6fc0\u626c\u9752\u6625\uff0c\u8003\u4e0a\u6e29\u4e00\u4e2d\u3002\u4f46\u662f\uff0c\u6211\u4e5f\u662f\u9a8c\u8bc1\u4e86\u8fd9\u53e5\u8bdd\uff1a\u201c\u5012\u4e5f\u8981\u5012\u5728\u79bb\u68a6\u60f3\u6700\u8fd1\u7684\u5730\u65b9\u3002\u201d\u867d\u7136\u68a6\u60f3\u6ca1\u6709\u5b9e\u73b0\uff0c\u4f46\u662f\u6211\u5012\u5728\u4e86\u79bb\u5b83\u6700\u8fd1\u7684\u5730\u65b9\uff1a\u9f99\u6e7e\u4e2d\u5b66\u3002\u8fd9\u53ea\u662f\u5728\u540d\u5206\u4e0a\uff1b\u5728\u771f\u6b63\u7684\u5b66\u8bc6\u7684\u79ef\u7d2f\u4e0a\uff0c\u90a3\u4e00\u4e2a\u521d\u4e09\u7684\u5b66\u671f\u65f6\u95f4\u662f\u6211\u4e00\u751f\u521d\u6b21\u7ecf\u5386\u7684\u6700\u7e41\u5fd9\u7684\u4e00\u6bb5\u65f6\u5149\uff1b\u6240\u6709\u7684\u8fd9\u4e9b\uff0c\u5bf9\u4e8e\u6211\u5728\u672a\u6765\u9ad8\u4e2d\u7684\u5b66\u4e60\uff0c\u5fc5\u7136\u662f\u975e\u5e38\u6709\u5e2e\u52a9\u7684\u3002\u521a\u624d\u8bf4\u7684\u90a3\u4e24\u4e2a\u540c\u5b66\uff0c\u4e5f\u5012\u5728\u4e86\u79bb\u68a6\u60f3\u6700\u8fd1\u7684\u5730\u65b9\uff0c\u6709\u4e00\u4e2a\u8003\u4e86\u5168\u6bb5\u7b2c\u4e8c\uff0c\u5fc5\u7136\u4e5f\u662f\u4e00\u4e2a\u91cd\u70b9\u5927\u5b66\u3002\u8fd9\u4e2a\u5bf9\u4e8e\u4ed6\u7684\u4eba\u751f\u4e5f\u5fc5\u7136\u6ca1\u6709\u8fc7\u591a\u7684\u9057\u61be\u3002</p> <p>\u56e0\u6b64\uff0c\u6211\u60f3\u8bf4\uff1a\u201c\u4e2d\u8003\u7684\u5206\u6570\u5c31\u662f\u4f60\u7684\u6700\u8fd1\u7684\u68a6\u60f3\u3002\u54ea\u6015\u4f60\u5012\u5728\u4e86\u5b83\u7684\u524d\u9762\uff0c\u8fd9\u5bf9\u4e8e\u4f60\u73b0\u5728\u7684\u81ea\u5df1\u4e5f\u5fc5\u7136\u6709\u5f88\u5927\u7684\u6539\u89c2\u3002\u201d</p>"},{"location":"blog/Travel/","title":"Preface","text":"<p>Write down the life before it passes by...</p>"},{"location":"blog/Travel/5th_05_24/","title":"\u5c0f\u957f\u5047\u51fa\u6e38","text":"<p>\u5047\u671f\u76ee\u6807\uff1a\u4ece\u5404\u7c7b\u4e8b\u9879\u4e2d\u89e3\u653e\u51fa\u6765\uff0c\u601d\u8003\u603b\u7ed3\u524d\u534a\u4e2a\u5b66\u671f\u7684\u751f\u5b58\u72b6\u6001\u3002</p> <p>\u884c\u52a8\u539f\u5219\uff1a\u6162\u4e0b\u6765\uff0c\u6162\u4e0b\u6765\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430-52","title":"4.30-5.2","text":"<p>\u9996\u5148\u611f\u8c22\u591c\u5bb5\u7ec4\u4f19\u4f34\u7684\u5b89\u6392\uff0c\u7279\u522b\u611f\u8c22\u5c0f\ud83d\udc1f\u7684\u7edf\u7b79\uff0c\u5c0f\ud83c\udf43\u7684\u666f\u5fb7\u9547\u8ba1\u5212\uff0c\u8ba9\u6211\u4eec\u7684\u6574\u4e2a\u884c\u7a0b\u90fd\u975e\u5e38\u987a\u5229\u3002</p> <p>\u4e00\u5927\u65e9\u5c31\u5f00\u59cb\u7fd8\u8bfe\u51fa\u884c\uff0c\u6211\u4eec\u5148\u5750\u9ad8\u94c1\u5230\u8fbe\u5a7a\u6e90\uff0c\u542f\u52a8\u81ea\u9a7e\u6a21\u5f0f\uff0c\u4e09\u5929\u53bb\u4e86\u7bc1\u5cad\u3001\u4e09\u6e05\u5c71\u548c\u666f\u5fb7\u9547\u3002</p>"},{"location":"blog/Travel/5th_05_24/#430","title":"4.30","text":"<p>\u7b2c\u4e00\u5929\u7684\u7bc1\u5cad\u7740\u5b9e\u8ba9\u4eba\u773c\u524d\u4e00\u4eae\uff0c\u5f88\u591a\u7167\u7247\uff0c\u65e0\u8bba\u662f\u98ce\u666f\u8fd8\u662f\u4eba\u7269\uff0c\u90fd\u5f88\u4e0a\u955c\u3002\u4ece\u73bb\u7483\u6808\u9053\uff08\u5792\u5fc3\u6865\uff09\u5230\u508d\u5c71\u5c0f\u9547\uff08\u82b1\u6eaa\u6c34\u8857\uff09\uff0c\u4ece\u675c\u9e43\u56ed\u5230\u98d8\u96ea\u6c11\u56fd\u9986\uff0c\u7edd\u7f8e\u7684\u5c71\u95f4\u98ce\u5149\u3001\u7eaf\u51c0\u7684\u5c71\u95f4\u7a7a\u6c14\uff0c\u8ba9\u4e00\u4e2a\u957f\u671f\u751f\u6d3b\u5728\u95ed\u585e\u6821\u56ed\u91cc\u7684\u4eba\u611f\u5230\u65e2\u964c\u751f\u53c8\u4eb2\u5207\u3002</p>"},{"location":"blog/Travel/5th_05_24/#51","title":"5.1","text":"<p>\u65e9\u6668\u9192\u6765\uff0c\u521a\u53d1\u751f\u7684\u5e7f\u4e1c\u6885\u5927\u9ad8\u901f\u584c\u9677\u8ba9\u6211\u5403\u4e86\u4e00\u5927\u60ca\u3002\u540e\u9762\u4ece\u4e09\u6e05\u5c71\u56de\u6765\u5f00\u9ad8\u901f\u7684\u65f6\u5019\uff0c\u5176\u5b9e\u6709\u4e00\u70b9\u5bd2\u98a4\u3002</p> <p>\u4e09\u6e05\u5c71\u6211\u66fe\u7ecf\u53bb\u8fc7\u4e00\u6b21\uff0c\u8bb0\u5fc6\u4e2d\u5370\u8c61\u975e\u5e38\u4e0d\u9519\uff0c\u4e91\u96fe\u7f2d\u7ed5\uff0c\u4eba\u4eec\u508d\u5c71\u800c\u884c\uff0c\u5982\u4e34\u4ed9\u5883\u3002\u7136\u800c\uff0c\u8fd9\u6b21\u975e\u5e38\u7684\u4e0d\u884c\uff0c\u4e91\u96fe\u8fc7\u4e8e\u6d53\u91cd\uff0c\u5927\u90e8\u5206\u7684\u5c71\u95f4\u7f8e\u666f\u90fd\u65e0\u6cd5\u6e05\u65b0\u7528\u773c\u770b\u89c1\uff08\u6216\u8bb8\u53ef\u4ee5\u8bd5\u8bd5\u5176\u4ed6\u6ce2\u6bb5\u7684\u5149hh\uff09\u3002</p> <p>\u5728\u80fd\u89c1\u5ea6\u4e0d\u523010m\u7684\u5c71\u95f4\uff0c\u62cd\u7167\u51e0\u4e4e\u662f\u4e00\u4ef6\u5403\u529b\u4e0d\u8ba8\u597d\u7684\u4e8b\u60c5\u3002\u597d\u5728\u6211\u4eec\u6700\u540e\u4e5f\u627e\u5230\u4e86\u8bb8\u591a\u7684\u62cd\u7167\u70b9\uff0c\u8d4b\u4e88\u6d53\u96fe\u91cc\u7684\u5c71\u548c\u6211\u4eec\u4e00\u70b9\u7279\u522b\u7684\u610f\u4e49\u3002</p> <p>\u53e6\u5916\uff0c\u4e94\u4e00\u4eba\u6570\u8fc7\u591a\uff0c\u4e0b\u5c71\u6392\u961f\u7b49\u5019\u8d85\u8fc7\u4e00\u4e2a\u534a\u5c0f\u65f6\uff0c\u4e5f\u662f\u4e00\u4e2a\u51cf\u5206\u9879\u3002</p>"},{"location":"blog/Travel/5th_05_24/#52","title":"5.2","text":"<p>\u6211\u4eec\u53bb\u5230\u4e86\u666f\u5fb7\u9547\uff0c\u4f53\u4f1a\u4e86\u4e00\u4e0b\u4e2d\u56fd\u74f7\u90fd\u7684\u97f5\u5473\u3002\u6211\u672c\u4eba\u5bf9\u4e8e\u74f7\u5668\u5e76\u65e0\u592a\u5927\u611f\u53d7\uff0c\u74f7\u5668\u53ca\u5176\u4ecb\u7ecd\u4e5f\u53ea\u662f\u8d70\u9a6c\u89c2\u82b1\u5306\u5306\u800c\u8fc7\u3002\u4f46\u662f\uff0c\u7ec6\u7ec6\u8d70\u6765\uff0c\u53d1\u73b0\u5176\u5236\u4f5c\u6d41\u7a0b\u7684\u7cbe\u7ec6\u3001\u7e41\u6742\uff0c\u5f97\u5230\u7684\u74f7\u5668\u8d28\u91cf\u4e0a\u4e58\u3001\u4ef7\u503c\u9ad8\u6602\uff0c\u5185\u542b\u5927\u5bb6\u5bf9\u5320\u5fc3\u7684\u8ffd\u6c42\u3002</p> <p>\u665a\u4e0a\u56de\u6765\uff0c\u53c2\u4e0e\u4e86\u5927\u8868\u54e5\u7684\u5a5a\u793c\u3002\u4e0d\u77e5\u4e3a\u4f55\uff0c\u6211\u4e00\u8fb9\u4e3a\u4ed6\u4eec\u611f\u5230\u5f88\u5f00\u5fc3\uff0c\u4e00\u8fb9\u611f\u53d7\u5230\u65f6\u5149\u98de\u901d\uff0c\u6bcf\u4e2a\u4eba\u90fd\u5728\u5f80\u524d\u8d70\u3002\u6211\u5bf9\u672a\u6765\u6709\u671f\u5f85\uff0c\u4e5f\u6709\u7126\u8651\u3002</p>"},{"location":"blog/Travel/5th_05_24/#53-55","title":"5.3-5.5","text":"<p>\u56de\u5230\u6e29\u5dde\uff0c\u9664\u4e86\u4e45\u8fdd\u7684\u6d77\u9c9c\u4e0d\u65ad\u6ee1\u8db3\u6211\u81ea\u5df1\u4e4b\u5916\uff0c\u6211\u8fd8\u5c06\u81ea\u5df1\u7684\u751f\u6d3b\u6162\u4e86\u4e0b\u6765\u3002\u6211\u548c\u5bb6\u91cc\u4eba\u90fd\u76f8\u5904\u4e86\u4e00\u4e0b\uff0c\u6211\u660e\u767d\u4e4b\u540e\u53ef\u80fd\u6ca1\u6709\u5f88\u591a\u7684\u65f6\u95f4\u966a\u4f34\u5bb6\u4eba\u3002</p> <p>\u6211\u5237\u4e86\u4e00\u4e0b\u8001\u53cb\u8bb0\uff0c\u5176\u5b9e\u8fd9\u4e2a\u662f\u53ef\u4ee5\u4e0a\u763e\u7684\uff01\u770b\u5267\u4e0a\u763e\u7684\u673a\u5236\uff0c\u6e90\u81ea\u4e8e\u5267\u60c5\u5bf9\u4eba\u7684\u5174\u8da3\u7684\u4e0d\u65ad\u6311\u9017\uff0c\u4eba\u770b\u5267\u65f6\u80fd\u591f\u6709\u7684\u677e\u5f1b\u611f\u548c\u5fd8\u6211\u611f\u3002</p> <p>\u5b9e\u9645\u4e0a\uff0c\u8fd9\u4e2a\u4e1c\u897f\u662f\u53ef\u4ee5\u8c03\u63a7\u7684\uff0c\u56e0\u4e3a\u5f88\u591a\u79ef\u6781\u7684\u4e8b\u60c5\uff0c\u90fd\u5728\u52aa\u529b\u4e2d\uff0c\u53d8\u5f97\u4e0a\u763e\u3002</p> <p>\u9ebb\u5c06\u5f53\u7136\u4e5f\u662f\u4e00\u6b21\u805a\u4f1a\u7684\u597d\u65f6\u673a\u3002\u5927\u5bb6\u90fd\u80fd\u591f\u804a\u804a\u5404\u81ea\u7684\u751f\u6d3b\uff0c\u4e0d\u540c\u7684\u4eba\u4e0d\u540c\u7684\u5730\u65b9\uff0c\u6709\u7740\u76f8\u4f3c\u7684\u60c5\u611f\u3002</p> <p>\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u80fd\u591f\u4e3a\u6211\u7684\u5bb6\u5ead\u628a\u628a\u8109\uff0c\u770b\u770b\u6709\u6ca1\u6709\u4ec0\u4e48\u6bdb\u75c5\uff0c\u63a5\u4e0b\u6765\u79bb\u5bb6\u540e\uff0c\u6211\u662f\u4e0d\u662f\u8981\u505a\u4e00\u4e9b\u8c03\u6574\u3002\u6700\u91cd\u8981\u7684\u662f\u6211\u548c\u7238\u5988\u7684\u8ddd\u79bb\u53d8\u5f97\u66f4\u52a0\u9065\u8fdc\uff0c\u6211\u53d1\u73b0\u4e4b\u524d\u65e5\u5e38\u7684\u4e2d\u9910\u5e76\u4e0d\u80fd\u5f88\u597d\u5730\u89e3\u51b3\u95ee\u9898\u3002\u6211\u9700\u8981\u591a\u53d1\u4e00\u4e9b\u7167\u7247\uff0c\u5173\u4e8e\u6211\u7684\u751f\u6d3b\uff0c\u6211\u7684\u5468\u672b\uff0c\u8ba9\u4ed6\u4eec\u591a\u770b\u770b\u6211\u5230\u5e95\u662f\u600e\u4e48\u505a\u7684\u3002</p> <p>\u5047\u671f\u7ed3\u5c3e\u6211\u7adf\u7136\u80fd\u591f\u8bfb\u5230\u4e00\u53e5\u597d\u6587\uff1a \u4e09\u8054\u751f\u6d3b\u5468\u520a<pre><code>\u4e0d\u8981\u8bd5\u56fe\u5bfb\u627e\u4e00\u79cd\u201c\u4e0d\u7126\u8651\u201d\u7684\u72b6\u6001\uff0c\u56e0\u4e3a\u7126\u8651\u4e0e\u4eba\u7684\u9009\u62e9\u548c\u751f\u5b58\u76f8\u4f34\u968f\uff0c\u4e0d\u5982\u5b66\u4f1a\u4e00\u4e9b\u5c0f\u7684\u6280\u5de7\uff0c\u8ba9\u81ea\u5df1\u80fd\u548c\u7126\u8651\u5171\u5904\u3002\n</code></pre></p> <p>\u5b89\u6170\u4e86\u66fe\u7ecf\u52aa\u529b\u7684\u81ea\u5df1\uff0c\u6e05\u6670\u4e86\u5bf9\u672a\u6765\u7684\u52aa\u529b\u3002</p> <p>\u4ee5\u524d\u63a2\u7d22\u7684\u5404\u79cd\u6709\u76ca\u7684\u65b9\u5f0f\uff0c\u90fd\u80fd\u591f\u548c\u7126\u8651\u4e32\u8054\u8d77\u6765\uff0c\u9a7e\u9a6d\u7126\u8651\uff0c\u4e3a\u6211\u6240\u7528\u3002</p> <p>\u611f\u89c9\u81ea\u5df1\u7684\u5f88\u591a\u53d1\u8a00\u90fd\u6709\u529f\u5229\u5b9e\u7528\u4e3b\u4e49\u7684\u8272\u5f69\uff0c\u65e5\u6e10\u957f\u5927\u7684\u81ea\u5df1\u5f80\u5f80\u90fd\u662f\u5bf9\u7406\u60f3\u5931\u53bb\u8010\u5fc3\u548c\u4fe1\u5fc3\uff0c\u8d70\u4e00\u6b65\u770b\u4e00\u6b65\uff0c\u800c\u603b\u662f\u8981\u56de\u5934\u770b\u770b\uff0c\u81ea\u5df1\u4e3a\u4ec0\u4e48\u51fa\u53d1\u3002</p> <p>\u6211\u89c9\u5f97\u5bf9\u6211\u6765\u8bf4\uff0c\u6211\u9700\u8981\u4e0d\u65ad\u6253\u5f00\u81ea\u5df1\u3002\u6211\u80fd\u591f\u653e\u4e0b\u5f88\u591a\u4ee5\u524d\u6240\u8ba4\u4e3a\u7684\u7981\u5fcc\uff0c\u53bb\u505a\u5f88\u591a\u5b9e\u9645\u4e0a\u90fd\u80fd\u591f\u505a\u7684\u4e8b\u60c5\u3002</p> <p>\u5176\u5b9e\u5f88\u591a\u4e1c\u897f\u90fd\u662f\u4f1a\u4e0a\u763e\u7684\u3002</p> <p>\u5b66\u4f1a\u628a\u4e0d\u65ad\u7684\u4e0a\u763e\u53d8\u6210\u81ea\u5df1\u6700\u559c\u6b22\u7684\u6a21\u6837\u3002</p>"},{"location":"blog/Travel/Mount_Lu/","title":"Travel 2 Mount Lu!","text":""},{"location":"blog/Travel/Mount_Lu/#schedule","title":"Schedule","text":""},{"location":"blog/Travel/Mount_Lu/Schedule/","title":"Schedule","text":""},{"location":"blog/Travel/Mount_Lu/Schedule/#time","title":"Time","text":"<p>Jan 12<sup>th</sup>, 2025 to Jan 17<sup>th</sup>, 2025, 5 days in total</p>"},{"location":"blog/Travel/Mount_Lu/Schedule/#destination","title":"Destination","text":"<p>Mount Lu (\u5e90\u5c71) &amp; City of Jiujiang (\u4e5d\u6c5f)</p>"},{"location":"blog/Travel/Mount_Lu/Schedule/#accommodation","title":"Accommodation","text":"Time(night) Places Details Jan 12 Luqi Hotel (\u5e90\u6816\u9152\u5e97) check in around 21:30, It is a good time to make plans for the following days, buy some necessary goods and materials, which might be cheap at the foot of mountain Jan 13 - 15 Lushan Hotel (\u5e90\u5c71\u9152\u5e97) check in on the morning of Jan 13, and check out on the morning of Jan 16, three days in total Jan 16 Yuanqi Hotel (\u6e90\u542f\u9152\u5e97) check in on the evening of Jan 16 and check out on Jan 17, then go back"},{"location":"blog/Travel/Mount_Lu/Schedule/#time-table","title":"Time Table","text":"Time 1 Time 2 Places Details 1.12 Evening Assemble at Lushan Railway Station Zzh 18:49, Lj 20:47, Lwq &amp; Txc 21:03 1.12 Evening Have a good night, maybe we can wander around 1.13 Morning Hotel get up at 07:30  Caiji Jianbao for breakfast  Take taxis to cable way [30 min]  Take the Cable way up [15 min]   check in on Gulin Avenue (\u726f\u5cad\u8857).  Have lunch before 12:00 1.13 Afternoon West Route Begin before 13:00  -&gt; Ruqin Lake (\u5982\u7434\u6e56) [30 min, 1.5km]  -&gt; Huajing (\u82b1\u5f84) [10 min, 0.5km]  -&gt; Xianren Hole (\u4ed9\u4eba\u6d1e) [30 min, 1.2km]  -&gt; Datianchi (\u5927\u5929\u6c60) [30 min, 1.2km]  -&gt; Elec-Dam (\u7535\u7ad9\u5927\u575d) [40 min, 2km]  return by tour bus 1.13 Evening Hotel Have a good dinner at Gulin Avenue. 1.14 Morning East Route \u2160 get up at 04:30  -&gt; Hanpokou (\u542b\u9131\u53e3) [4.9 km, 1.5h]  -&gt; See Sun rises at spot [07:12]  -&gt; Sandie Spring Stop (\u4e09\u53e0\u6cc9) [20 min, TB] -&gt; Checking Spot (\u9a8c\u7968\u5904) [30 min]  -&gt; Sandie Spring (\u4e09\u53e0\u6cc9) [2 h]  -&gt; Zhengjie (\u6b63\u8857) [30 min]  -&gt; free time for snow play 1.14 Afternoon East Route \u2161 Take Tour Bus [13:00]  -&gt; Wulaofeng (\u4e94\u8001\u5cf0) [20 min, TB]  -&gt; botanical garden (\u690d\u7269\u56ed) [10 min, TB]  -&gt; Zhengjie (\u6b63\u8857) [20 min, TB] 1.14 Evening 1.15 Morning sleep till noon 1.15 Afternoon Middle route -&gt; Meilu (\u7f8e\u5e90)  -&gt; \u5e90\u5c71\u5730\u8d28\u535a\u7269\u9986  -&gt; \u5e90\u5c71\u4f1a\u8bae\u65e7\u5740  -&gt; Lulin Lake (\u82a6\u6797\u6e56)  -&gt; \u5e90\u5c71\u535a\u7269\u9986  -&gt; See sun set at Xiaotianchi (\u5c0f\u5929\u6c60). 1.15 Evening 1.16 Morning check in at Jiujiang check out and go down the mountain by tour car [11:00] 1.16 Afternoon Yuliangnan street (\u5ebe\u4eae\u7537\u8def\u5386\u53f2\u6587\u5316\u8857) Walk around Yuliangnan street 1.16 Evening Yangtze River (\u957f\u6c5f\u8fb9) Pipa Booth (\u7435\u7436\u4ead)  -&gt; Xunyang Lou (\u6d54\u9633\u697c)  -&gt; \u957f\u6c5f\u5927\u6865 1.17 Morning check out, have a good breakfast at Jiujiang 1.17 Afternoon Assemble at Jiujiang Railway Station return, lj, zzw, lwq to Wenzhounan, txc to hangzhouxi"},{"location":"blog/Travel/Mount_Lu/Schedule/#warm-reminder","title":"Warm Reminder","text":"<ul> <li>Map</li> </ul> <ul> <li> <p>\u4e09\u53e0\u6cc9\u5c3d\u91cf\u4e0a\u5348\u53bb\uff0c\u4e0b\u53481-3\u70b9\u6216\u4eba\u6d41\u91cf\u592a\u5927</p> </li> <li> <p>Tour Bus</p> </li> </ul> <p>Places where Tour Bus could go</p> <ul> <li> <p>\u6838\u5fc3\u666f\u533a</p> </li> <li> <p>\u4e09\u53e0\u6cc9\u3001\u767d\u9e7f\u6d1e\u4e66\u9662\u3001\u79c0\u5cf0\uff08Southeast\uff09\u3001\u89c2\u97f3\u6865</p> </li> </ul> <p>Time when Tour Bus could be taken</p> <ul> <li>07:30-18:00</li> </ul>"},{"location":"blog/Travel/Xi%27an/","title":"Xi'an","text":"<p>\u8fd9\u662f\u4e00\u6bb5\u8bf4\u8d70\u5c31\u8d70\u7684\u7684\u9519\u5cf0\u51fa\u884c\u3002\u6211\u4eec\u60f3\u8981\u7684\uff0c\u662f\u80fd\u591f\u907f\u6691\u7684\u3001\u884c\u7a0b\u5bc6\u5ea6\u4e0d\u4f4e\u7684\u3001\u4f4e\u6210\u672c\u7684\u65c5\u884c\u3002</p> <p>\u4ece\u673a\u7968\u5165\u624b\uff0c\u5927\u57ce\u5e02\u66f4\u53d7\u9752\u7750\u3002\u540c\u65f6\uff0c\u5927\u57ce\u5e02\u4ea4\u901a\u65b9\u4fbf\uff0c\u9002\u5408\u5c0f\u56e2\u4f19\uff082\u4eba\uff09\u51fa\u884c\uff08\u8fd9\u91cc\u5bf9\u6bd4\u81ea\u9a7e\u6e38\uff09\u3002\u57ce\u5e02\u7684\u7ecf\u6d4e\u53d1\u5c55\u7a0b\u5ea6\uff0c\u51b3\u5b9a\u4e86\u57ce\u5e02\u7684\u5c45\u4f4f\u548c\u51fa\u884c\u5f00\u9500\uff08\u65c5\u6e38\u8d39\u7528\u5728\u65c5\u6e38\u666f\u533a\u90fd\u662f\u4e0d\u4fbf\u5b9c\u7684\uff09\u3002</p> <p>\u5176\u6b21\u662f\u907f\u6691\u5f62\u52bf\u3002\u5317\u65b9\u3001\u9ad8\u5c71\u7b49\u90fd\u662f\u826f\u597d\u7684\u9009\u62e9\u3002\u5176\u5b9e0906\u4e4b\u540e\u7684\u5929\u6c14\u5f62\u52bf\u5f88\u6e05\u695a\uff0c\u5c31\u662f\u526f\u9ad8\u4f9d\u65e7\u63a7\u5236\u957f\u6c5f\u4e2d\u4e0b\u6e38\uff0c\u4f46\u662f\u534e\u897f\u79cb\u96e8\u9010\u6b21\u5c55\u5f00\u3002\u6070\u597d\u5317\u65b9\u6709\u4e00\u4e2a\u51b7\u6da1\uff0c\u6c14\u6e29\u5f88\u9002\u5b9c\u3002</p> <p>\u6700\u540e\u7684\u9009\u62e9\u4fbf\u662f\u53e4\u57ceXi'an\uff0c\u8e0f\u4e0a\u8fd9\u5757\u201c\u65bd\u5de5\u5730\u4e0b\u5bb9\u6613\u4ea7\u51fa\u6587\u7269\u201d\u7684\u571f\u5730\u3002</p> 0909\u65e9\u6668\u6e2d\u6cb3\u5357"},{"location":"blog/Travel/Xi%27an/#mount-hua","title":"Mount Hua","text":"<p>\u8fd9\u771f\u7684\u662f\u6211\u76ee\u524d\u722c\u8fc7\u7684\u6700\u9669\u7684\u5c71\uff0c\u4e5f\u662f\u767b\u9876\u540e\u98ce\u666f\u6700\u7f8e\u7684\u5c71\u3002</p> <p>\u8bf7\u63a5\u53d7\u7f8e\u666f\u7684\u8f70\u70b8\u3002</p>"},{"location":"blog/Travel/Xi%27an/#xjtu","title":"XJTU","text":"<p>\u6765\u4e00\u5ea7\u57ce\u5e02\uff0c\u5c31\u8981\u770b\u770b\u5979\u7684\u5b66\u5e9c\u3002</p>"},{"location":"blog/Travel/Xi%27an/#city-wall","title":"City wall","text":"<p>\u591c\u722c\u57ce\u5899\u3002\u671f\u671b\u7684\u5355\u8f66\u867d\u7136\u6ca1\u6709\uff0c\u4f46\u662f\u8def\u4e0a\u6709\u70b9\u5c0f\u786e\u5e78\u3002</p>"},{"location":"blog/Travel/Xi%27an/#_1","title":"\u79e6\u59cb\u7687\u9675","text":"<p>\u5175\u9a6c\u4fd1\u4e2d\u6709\u5f88\u591a\u6709\u8da3\u7684\u5386\u53f2\u6545\u4e8b\uff0c\u636e\u8bf4\u90fd\u662f\u8003\u53e4\u6587\u7269\u540e\uff0c\u7ed3\u5408\u6587\u732e\u5206\u6790\u800c\u6765\u3002\u6bd4\u5982\uff0c\u809a\u5b50\u7684\u5927\u5c0f\u4ee3\u8868\u5b98\u5175\u7684\u4f4d\u52bf\uff0c\u6545\u5f53\u65f6\u201c\u4ee5\u8179\u5927\u4e3a\u7f8e\u201d\u3002</p> <p>\u6709\u4e2a\u5bfc\u6e38\u5206\u4eab\u4e86\u98ce\u6c34\u3002\u4ed6\u8bf4\uff0c\u79e6\u59cb\u7687\u5c06\u9675\u5893\u9009\u5740\u4e8e\u73b0\u5728\u7684\u4e34\u6f7c\u533a\uff0c\u662f\u60f3\u8981\uff1a\u5934\u6795\u5c71\uff0c\u811a\u6dcc\u6c34\u3002</p> <p>\u540e\u9762\u6211\u4eec\u53bb\u4e86\u534e\u6e05\u5bab\u3002\u4e91\u96e8\u5929\u53cd\u800c\u522b\u6709\u4e00\u756a\u6c14\u8d28\u3002\u9a8a\u5c71\u76f8\u5bf9\u6765\u8bf4\u6bd4\u8f83\u5e73\u7f13\uff0c\u4f46\u662f\u722c\u5347\u4e0d\u9519\u3002\u8fd9\u5bf9\u4e8e\u7ecf\u5386\u8fc7\u534e\u5c71\u7684\u6211\u4eec\u6765\u8bf4\u786e\u5b9e\u662f\u4e00\u9053\u5c0f\u83dc\u3002</p> <p>\u552f\u4e00\u8ba9\u4eba\u610f\u5916\u7684\uff0c\u662f\u6211\u4eec\u6e9c\u51fa\u666f\u533a\u4f46\u4e0d\u81ea\u77e5\uff0c\u786c\u7740\u5934\u76ae\u4ece\u6b63\u786e\u7684\u6ce5\u6cde\u8def\u4e0b\u5c71\u3002\u978b\u5b50\u727a\u7272\u60e8\u91cd\u3002</p> <p>\u8fd8\u53d1\u751f\u4e86\u5f88\u591a\u4e8b\u60c5\uff0c\u5c31\u4e0d\u4e00\u4e00\u8d58\u8ff0\u3002\u53ea\u662f\u60f3\u8d81\u5e74\u8f7b\uff0c\u6709\u7cbe\u529b\uff0c\u6709\u95f2\u4f59\uff0c\u5c31\u591a\u51fa\u53bb\u8d70\u8d70\u3002</p>"},{"location":"blog/Travel/Yunnan/","title":"Yunnan","text":""},{"location":"blog/Travel/Yunnan/#_1","title":"\u5b89\u6392\u8868","text":"\u65f6\u95f4 \u5730\u70b9 \u4e8b\u9879 09.06 \u6606\u660e \u3010\u822a\u73ed\u301107\uff1a35 - 10\uff1a35 \u676d\u5dde -&gt; \u6606\u660e  \u4e0b\u5348\u6e38\u73a9  \u665a\u4e0a\u4f4f\u5728\u6606\u660e 09.07 \u5927\u7406 \u65e9\u4e0a\u53bb\u5403\u4e2a\u65e9\u9910\uff0c\u6606\u660e\u519c\u8d38  \u4e1c\u98ce\u5e7f\u573a\u51fa\u53d115-20\u5206\u949f\u5230\u94c1\u8def\u7ad9 \u3010\u9ad8\u94c1\u3011\u6606\u660e -&gt;\u5927\u7406\uff0c\u4e24\u5c0f\u65f6\uff0c\u8f66\u6b21\u591a  \u4e0b\u53482\u70b9\u524d\u5165\u4f4f\u9152\u5e97\uff0c\u5927\u7406\u53e4\u57ce\u9644\u8fd1  \u4e0b\u5348\u6574\u7406\u4e00\u4e0b\uff0c\u79df\u7535\u74f6\u8f66\u73af\u6d31\u6d77\u6e38\uff0c\u6e38\u4e1c\u7ebf\uff0c\u770b\u53cc\u5eca\u53e4\u9547\uff0c\u665a\u4e0a\u6e38\u5927\u7406\u53e4\u57ce 09.08 \u5927\u7406 \u65e9\u6668\u770b\u65e5\u51fa  \u6e38\u897f\u7ebf\u3010\u94c1\u8def\u301117:35-19:32 \u5927\u7406 -&gt; \u4e3d\u6c5f  \u665a\u4e0a\u4f4f\u5728\u4e3d\u6c5f 09.09 \u4e3d\u6c5f \u7389\u9f99\u96ea\u5c71 \u84dd\u6708\u8c37-&gt;\u4e91\u6749\u576a-&gt;\u7266\u725b\u576a  \u4e3d\u6c5f\u53e4\u57ce\u9644\u8fd1 09.10 \u9999\u683c\u91cc\u62c9 \u3010\u94c1\u8def\u301120:53-05:02 \u4e3d\u6c5f -&gt; \u9999\u683c\u91cc\u62c9  \u4e3d\u6c5f\u53e4\u57ce\u9644\u8fd1 09.11 \u4e3d\u6c5f \u3010\u94c1\u8def\u301120:53-05:02 \u9999\u683c\u91cc\u62c9 -&gt; \u4e3d\u6c5f   \u3010\u822a\u73ed\u301118\uff1a10-23\uff1a05 \u4e3d\u6c5f-&gt;\u676d\u5dde"},{"location":"blog/Travel/Yunnan/#_2","title":"\u6606\u660e","text":"<ul> <li>\u73a9</li> </ul> \u65f6\u95f4 \u5730\u70b9 \u4e8b\u9879 12\uff1a00\u524d \u9152\u5e97 \u6d77\u57c2\u5927\u575d\uff0810\u70b9\u524d\u5230\u8fbe\u5582\u6d77\u9e25\uff09\u6216\u6d77\u6d2a\u6e7f\u5730 \u6597\u5357\u82b1\u5e02\uff0820.30\u540e\u665a\u5e02\u5f00\u95e8\uff09 <ul> <li>\u5403</li> </ul> \u98df \u5730\u70b9 \u4e8b\u9879 \u8fc7\u6865\u7c73\u7ebf \u65e9\u4e0a\u5403 \u91ce\u751f\u83cc\u706b\u9505 \u9c9c\u82b1\u997c \u73b0\u70e4 <ul> <li>\u4f4f</li> </ul> <p>\u4e91\u5357\u868a\u5b50\u591a</p>"},{"location":"blog/Travel/Yunnan/#_3","title":"\u5927\u7406","text":"<ul> <li>\u73a9</li> </ul> \u65f6\u95f4 \u5730\u70b9 \u4e8b\u9879 12\uff1a00\u524d \u6d31\u6d77\u751f\u6001\u5eca\u9053 \u4e0b\u96e8\u5219\u79df\u6c7d\u8f66 \u6597\u5357\u82b1\u5e02\uff0820.30\u540e\u665a\u5e02\u5f00\u95e8\uff09 <ul> <li>\u5403</li> </ul> \u98df \u5730\u70b9 \u4e8b\u9879 \u90ed\u6c0f\u6c99\u575d\u9c7c\u5e84\u3001"},{"location":"courses/","title":"Content","text":"<p>The following contains some notes of courses at ZJU.</p>"},{"location":"courses/#artificial-intelligence-machine-learning","title":"Artificial Intelligence &amp; Machine Learning","text":"<p>Completed.</p>"},{"location":"courses/#c-plus-plus","title":"C Plus Plus","text":"<p>Completed.</p>"},{"location":"courses/#data-structure-analysis","title":"Data Structure &amp; Analysis","text":"<p>To be continued...</p>"},{"location":"courses/#introduction-to-visualization","title":"Introduction to Visualization","text":"<p>Completed.</p>"},{"location":"courses/AIML/","title":"Artificial Intelligence &amp; Machine Learning","text":""},{"location":"courses/AIML/#artificial-intelligence","title":"Artificial Intelligence","text":""},{"location":"courses/AIML/#introduction","title":"Introduction","text":""},{"location":"courses/AIML/#agent","title":"Agent","text":""},{"location":"courses/AIML/#problem-solving","title":"Problem Solving","text":""},{"location":"courses/AIML/#knowledge-logic","title":"Knowledge &amp; Logic","text":""},{"location":"courses/AIML/#machine-learning","title":"Machine Learning","text":""},{"location":"courses/AIML/#introduction_1","title":"Introduction","text":""},{"location":"courses/AIML/#concept-learning","title":"Concept Learning","text":""},{"location":"courses/AIML/AI/","title":"Artificial Intelligence","text":""},{"location":"courses/AIML/AI/#introduction","title":"Introduction","text":""},{"location":"courses/AIML/AI/#agent","title":"Agent","text":""},{"location":"courses/AIML/AI/#problem-solving","title":"Problem Solving","text":""},{"location":"courses/AIML/AI/#knowledge-logic","title":"Knowledge &amp; Logic","text":""},{"location":"courses/AIML/AI/Agent/","title":"Agent","text":"<p>\u57fa\u672c\u5c5e\u6027</p> <ul> <li> <p>\u611f\u77e5\uff1a\u4efb\u4f55\u7ed9\u5b9a\u65f6\u523bAgent\u7684\u611f\u77e5\u8f93\u5165</p> </li> <li> <p>\u611f\u77e5\u5e8f\u5217\uff1a\u6240\u6709\u8f93\u5165\u6570\u636e\u7684\u5b8c\u6574\u5386\u53f2</p> </li> <li> <p>Agent\u51fd\u6570\uff1a\u5c06\u4efb\u610f\u7ed9\u5b9a\u611f\u77e5\u5e8f\u5217\u6620\u5c04\u4e3a\u884c\u52a8\uff0c\u53ef\u4ee5\u7531Agent\u7a0b\u5e8f\u5b9e\u73b0</p> </li> </ul> <p>\u80fd\u591f\u611f\u77e5\u548c\u52a8\u4f5c\u7684\u5b9e\u4f53\uff0c\u5177\u4f53\u6765\u8bf4\uff0c\u662f\u4e00\u4e2a\u4ece\u611f\u77e5\u5e8f\u5217\u5230\u52a8\u4f5c\u7684\u4e00\u4e2a\u51fd\u6570</p> \\[ f:P^{*}\\mapsto A \\]"},{"location":"courses/AIML/AI/Agent/#rational-agent","title":"\u7406\u6027\u667a\u80fd\u4f53 | Rational Agent","text":"<p>\u662f\u505a\u4e8b\u6b63\u786e\u7684\u667a\u80fd\u4f53\u3002</p> <ul> <li>\u6027\u80fd\u5ea6\u91cf</li> </ul> <p>\u7531\u8bbe\u8ba1\u8005\u7ed9\u51fa\uff0c\u6839\u636e\u5b9e\u9645\u5728\u6240\u5904\u7684\u73af\u5883\u4e2d\u5e0c\u671b\u5f97\u5230\u7684\u7ed3\u679c\u6765\u8bbe\u8ba1\u5ea6\u91cf\uff0c\u800c\u4e0d\u6839\u636e\u667a\u80fd\u4f53\u5e94\u8be5\u8868\u73b0\u7684\u884c\u4e3a\u3002</p> <ul> <li>\u7406\u6027\u7684\u5224\u65ad</li> </ul> <p>\u7406\u6027\u7684\u5224\u65ad</p> <ul> <li> <p>\u5b9a\u4e49\u6210\u529f\u6807\u51c6\u7684\u6027\u80fd\u5ea6\u91cf\uff08Performance measure\uff09</p> </li> <li> <p>Agent\u5bf9\u73af\u5883\u7684\u5148\u9a8c\u77e5\u8bc6\uff08Environment\uff09</p> </li> <li> <p>Agent\u53ef\u4ee5\u6267\u884c\u7684\u52a8\u4f5c\uff08Actuators\uff09</p> </li> <li> <p>Agent\u7684\u611f\u77e5\u5e8f\u5217\uff08Sensors\uff09</p> </li> </ul> <p>\u4e00\u4e2a\u7406\u6027\u7684Agent\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u53ef\u80fd\u7684\u611f\u77e5\u5e8f\u5217\uff0c\u80fd\u6839\u636e\u5df2\u77e5\u7684\u611f\u77e5\u5e8f\u5217\u548c\u5185\u5efa\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9009\u62e9\u80fd\u4f7f\u6027\u80fd\u6307\u6807\u7684\u671f\u671b\u503c\u6700\u5927\u5316\u7684\u52a8\u4f5c\u3002</p> <p>\u7406\u6027 vs \u5168\u77e5</p> <p>\u7406\u6027\u4e0d\u8981\u6c42\u5168\u77e5\u3002</p> <ul> <li> <p>\u5168\u77e5\u662f\u77e5\u9053\u5b83\u7684\u52a8\u4f5c\u4ea7\u751f\u7684\u5b9e\u9645\u6548\u679c</p> </li> <li> <p>\u7406\u6027\u5e94\u8be5\u7ecf\u5e38\u89c2\u5bdf\uff0c\u6709\u52a9\u4e8e\u6700\u5927\u5316\u671f\u671b\u6027\u80fd\u3002</p> </li> </ul> \u7406\u6027 \u5b8c\u7f8e \u4f7f\u671f\u671b\u7684\u6027\u80fd\u6700\u5927\u5316 \u4f7f\u5b9e\u9645\u7684\u6027\u80fd\u6700\u5927\u5316 <ul> <li>\u81ea\u4e3b\u6027</li> </ul> <p>\u4e0d\u5e94\u8be5\u4f9d\u8d56\u8bbe\u8ba1\u8005\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5c3d\u53ef\u80fd\u5730\u5b66\u4e60</p>"},{"location":"courses/AIML/AI/Agent/#_1","title":"\u4efb\u52a1\u73af\u5883","text":"<p>\u5206\u7c7b</p> <ul> <li>\u5b8c\u5168\u53ef\u89c2\u5bdf\u548c\u90e8\u5206\u53ef\u89c2\u5bdf</li> </ul> <p>\u667a\u80fd\u4f53\u80fd\u591f\u83b7\u53d6\u73af\u5883\u7684\u5b8c\u6574\u72b6\u6001\uff0c\u4e0e\u4f20\u611f\u5668\u6709\u5173</p> <ul> <li>\u786e\u5b9a\u6027\uff1a\u73af\u5883\u7684\u4e0b\u4e00\u4e2a\u72b6\u6001\u5b8c\u5168\u53d6\u51b3\u4e8e\u5f53\u524d\u72b6\u6001\u548c\u667a\u80fd\u4f53\u7684\u884c\u52a8</li> </ul> <p>\u5982\u6b64\uff0c\u82e5\u65e0\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u6d3b\u52a8\uff0c\u90a3\u4e48\u73af\u5883\u662f\u7b56\u7565\u7684</p> <ul> <li> <p>\u7247\u6bb5\u5f0f\u4e0e\u5ef6\u7eed\u5f0f\uff1a\u884c\u52a8\u7684\u9009\u62e9\u662f\u5426\u53d6\u51b3\u4e8e\u5f53\u524d\u7247\u6bb5\uff0c\u662f\u5426\u4f1a\u5bf9\u672a\u6765\u6709\u5f71\u54cd\u3002</p> </li> <li> <p>\u9759\u6001\u548c\u52a8\u6001\uff1a\u73af\u5883\u5728\u667a\u80fd\u4f53\u601d\u8003\u65f6\u662f\u5426\u53d8\u5316</p> </li> <li> <p>\u79bb\u6563\u4e0e\u8fde\u7eed</p> </li> </ul> <p>\u771f\u5b9e\u7684\u4e16\u754c\u662f\u90e8\u5206\u5ba2\u89c2\u3001\u968f\u673a\u7684\u3001\u5ef6\u7eed\u5f0f\u7684\u3001\u52a8\u6001\u7684\u3001\u8fde\u7eed\u7684\u3001\u591a\u667a\u80fd\u4f53\u7684\u3002</p>"},{"location":"courses/AIML/AI/Agent/#_2","title":"\u667a\u80fd\u4f53\u7684\u7ed3\u6784","text":"<p>AI\u7684\u76ee\u7684\u662f\u8bbe\u8ba1Agent\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u628a\u611f\u77e5\u4fe1\u606f\u6620\u5c04\u5230\u884c\u52a8\u7684Agent\u51fd\u6570\u3002\u6ce8\u610f\uff0c\u6709\u4e9b\u51fd\u6570\u4e0d\u80fd\u88ab\u4efb\u4f55agent\u7a0b\u5e8f\u6784\u6210\uff0c\u5982\u56fe\u7075\u673a\u3002</p> <ul> <li>\u4f53\u7cfb\u7ed3\u6784\uff1a\u628a\u7a0b\u5e8f\u5728\u67d0\u4e2a\u5177\u5907\u7269\u7406\u4f20\u611f\u5668\u548c\u6267\u884c\u5668\u7684\u8ba1\u7b97\u88c5\u7f6e\u4e0a\u8fd0\u884c\u7684\u62bd\u8c61\u6574\u4f53</li> </ul> <p>\u5b83\u4e3a\u7a0b\u5e8f\u63d0\u4f9b\uff1a\u6765\u81ea\u4f20\u611f\u5668\u7684\u611f\u77e5\u4fe1\u606f\u3001\u8fd0\u884c\u7a0b\u5e8f\u3001\u628a\u7a0b\u5e8f\u4ea7\u751f\u7684\u884c\u52a8\u9001\u5230\u6267\u884c\u5668</p> \\[ \\text{Agent}=\\text{\u4f53\u7cfb\u7ed3\u6784}+\\text{\u7a0b\u5e8f} \\] <p>\u56db\u79cd\u57fa\u672c\u7684\u667a\u80fd\u4f53\u7ed3\u6784</p> <ul> <li>\u7b80\u5355\u53cd\u5c04\u578b: if-then \u7ed3\u6784(\u6761\u4ef6-\u884c\u4e3a\u89c4\u5219)</li> </ul> <p><p> </p></p> <p>\u7b80\u5355\u53cd\u5c04\u578b<pre><code>function Simple-Reflex-Agent\n    persistent: a set of condition-action rules.\n\n    state = interpret-input(percept)\n    rule = Rule-match(state, rules)\n    action = rule.ACTION\n    return action\n</code></pre> \u73af\u5883\u8981\u5b8c\u5168\u53ef\u89c2\u3002</p> <ul> <li>\u57fa\u4e8e\u6a21\u578b\u7684\u7b80\u5355\u53cd\u5c04\u578b</li> </ul> <p><p> </p></p> \u57fa\u4e8e\u6a21\u578b\u7684\u7b80\u5355\u53cd\u5c04\u578b<pre><code>function Model_Based-Agent\n    persistent: a set of condition-action rules,\n                state, the agent's current conception of the world state,\n                model, a description of how the next state depends on current state and action,\n                action, the most recent action\n\n\n    state = Undate-State(percept, state, action, model)\n    rule = Rule-match(state, rules)\n    action = rule.ACTION\n    return action\n</code></pre> <ul> <li>\u57fa\u4e8e\u76ee\u6807\u578b</li> </ul> <p><p> </p></p> <p>\u6548\u7387\u964d\u4f4e\uff0c\u7075\u6d3b\u5ea6\u589e\u52a0\uff0c\u529f\u80fd\u589e\u5f3a</p> <ul> <li>\u57fa\u4e8e\u6548\u7528\u578b(utility)</li> </ul> <p><p> </p></p> <ul> <li>\u5b66\u4e60\u578b</li> </ul> <p><p> </p></p> <p>\u5b66\u4e60\u5143\u4ef6(Learning element): \u6839\u636e\u53cd\u9988\u5bf9agent\u505a\u8bc4\u4ef7\uff0c\u4fee\u6539\u6267\u884c\u5143\u4ef6</p> <p>\u6267\u884c\u5143\u4ef6(Performance element)\uff1a\u4e4b\u524d\u7684\u6240\u6709agent</p> <p>\u8bc4\u8bba\u5143\u4ef6(Critic)\uff1a\u6839\u636e\u6027\u80fd\u6807\u51c6\u6765\u505a\u53cd\u9988</p> <p>\u95ee\u9898\u751f\u6210\u5668(Problem generator)\uff1a\u5efa\u8bae\u63a2\u7d22\u6027\u884c\u52a8\uff0c\u77ed\u671f\u6b21\u4f18\uff0c\u957f\u671f\u66f4\u597d</p>"},{"location":"courses/AIML/AI/Intro/","title":"Introduction","text":""},{"location":"courses/AIML/AI/ProblemSolving/","title":"Problem Solving","text":""},{"location":"courses/AIML/AI/ProblemSolving/#basic-search-method","title":"Basic Search Method","text":""},{"location":"courses/AIML/AI/ProblemSolving/#a-well-defined-problem","title":"\u826f\u5b9a\u4e49 | A well-defined problem","text":"<p>\u5047\u8bbe\u73af\u5883\u662f\u53ef\u89c2\u5bdf\u7684\uff08\u4e07\u80fd\u4f20\u611f\u5668\uff09\u3001\u79bb\u6563\u7684\u3001\u5df2\u77e5\u7684\uff08\u884c\u52a8\u9020\u6210\u786e\u5b9a\u7ed3\u679c\uff09\u3001\u786e\u5b9a\u7684\uff08\u884c\u52a8\u7ed3\u679c\u4e00\u4e00\u5bf9\u5e94\uff09</p> <p>\u8981\u7d20</p> <ul> <li> <p>\u72b6\u6001\u7a7a\u95f4\uff1a\u521d\u59cb\u72b6\u6001\u5230\u53ef\u4ee5\u8fbe\u5230\u7684\u6240\u6709\u72b6\u6001\u7684\u96c6\u5408</p> <ul> <li> <p>\u72b6\u6001\uff1astate \\(s\\), a node in search tree</p> </li> <li> <p>\u521d\u59cb\u72b6\u6001\uff1ainitial state \\(s_0\\)</p> </li> <li> <p>\u884c\u52a8\uff1aaction \\(a\\)</p> </li> <li> <p>\u8f6c\u79fb\u6a21\u578b(\u540e\u7ee7\u72b6\u6001)\uff1afunction(\\(s\\),\\(a\\))</p> </li> </ul> </li> <li> <p>\u76ee\u6807\u6d4b\u8bd5\uff1a\u786e\u5b9a\u5f53\u524d\u72b6\u6001\\(s\\)\u662f\u5426\u662f\u76ee\u6807\u72b6\u6001</p> </li> <li> <p>\u8def\u5f84\u8017\u6563\uff1a\u8003\u8651\u5355\u6b65\u8017\u6563</p> </li> </ul> <p>\u6027\u80fd\u6307\u6807</p> <ul> <li> <p>\u5b8c\u5907\u6027</p> </li> <li> <p>\u6700\u4f18\u6027</p> </li> <li> <p>\u65f6\u95f4\u590d\u6742\u5ea6</p> </li> <li> <p>\u7a7a\u95f4\u590d\u6742\u5ea6</p> </li> </ul>"},{"location":"courses/AIML/AI/ProblemSolving/#_1","title":"\u65e0\u4fe1\u606f\uff08\u76f2\u76ee\u5f0f\uff09\u641c\u7d22","text":"<p>\u533a\u522b\uff1a\u5982\u4f55\u62d3\u5c55\u5b50\u8282\u70b9\u3002</p> \u641c\u7d22\u7b97\u6cd5 \u7b97\u6cd5\u63cf\u8ff0 \u5b8c\u5907\u6027 \u6700\u4f18\u6027 time cost space cost \u5bbd\u5ea6\u4f18\u5148\u641c\u7d22 \u5148\u62d3\u5c55\u6839\u8282\u70b9\uff0c\u518d\u62d3\u5c55\u540e\u7ee7\u8282\u70b9\uff0c\u603b\u662f\u62d3\u5c55\u6df1\u5ea6\u6700\u6d45\u7684\u672a\u62d3\u5c55\u8282\u70b9\u3002\u7528\u961f\u5b9e\u73b0\u3002 Y Y \\(O(b^d)\\) \\(O(b^d)\\) \u4e00\u81f4\u4ee3\u4ef7\u641c\u7d22 \u62d3\u5c55\u7684\u662f\u8def\u5f84\u635f\u8017\\(g(n)\\)\u6700\u5c0f\u7684\u8282\u70b9\\(n\\).\u6700\u4f18\u6027\u6210\u7acb\uff0c\u5982\u679c\u6bcf\u4e00\u6b65\u4ee3\u4ef7\u5927\u4e8e\u67d0\u4e2a\u5c0f\u6b63\u6570\\(\\varepsilon\\). Y Y \\(O(b^{1+C/\\varepsilon})\\) \\(O(b^{1+C/\\varepsilon})\\) \u6df1\u5ea6\u4f18\u5148\u641c\u7d22 \u603b\u662f\u62d3\u5c55\u6811\u7684\u5f53\u524d\u8fb9\u7f18\u8282\u70b9\u96c6\u4e2d\u6700\u6df1\u7684\u8282\u70b9\u3002\u662f\u5b8c\u5907\u7684\uff0c\u4f46\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u56e0\u4e3a\u9047\u5230\u89e3\u5c31\u4f1a\u8fd4\u56de\u3002 N N \\(O(bd)\\) \\(O(bd)\\) \u6df1\u5ea6\u53d7\u9650\u641c\u7d22 \u8bbe\u7f6e\u4e00\u4e2a\u6df1\u5ea6\u754c\u9650\\(l\\), \u82e5 \\(l&lt;d\\), \u4e0d\u5b8c\u5907\uff1b\u82e5 \\(l&gt;d\\), \u5219\u4e0d\u6700\u4f18 N/Y N \\(O(b^l)\\) \\(O(bl)\\) \u8fed\u4ee3\u52a0\u6df1\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22 \u4e0d\u65ad\u5730\u589e\u5927\u6df1\u5ea6\u9650\u5236\uff0c\u8fbe\u5230\u6700\u6d45\u7684\u76ee\u6807\u8282\u70b9\u6240\u5728\u6df1\u5ea6\\(d\\)\u65f6\uff0c\u5c31\u80fd\u627e\u5230\u76ee\u6807\u8282\u70b9\u3002 Y N/Y \\(O(b^d)\\) \\(O(bd)\\)"},{"location":"courses/AIML/AI/ProblemSolving/#_2","title":"\u6709\u4fe1\u606f\uff08\u542f\u53d1\u5f0f\uff09\u641c\u7d22","text":"<p>\u8282\u70b9\u662f\u57fa\u4e8e\u8bc4\u4ef7\u51fd\u6570\\(f(n)\\)\u88ab\u9009\u62e9\u62d3\u5c55\u7684\u3002</p> \u641c\u7d22\u7b97\u6cd5 \u7b97\u6cd5\u63cf\u8ff0 time cost space cost \u8d2a\u5a6a\u6700\u4f73\u4f18\u5148\u641c\u7d22 \u62d3\u5c55\u79bb\u76ee\u6807\u6700\u8fd1\u7684\u8282\u70b9\uff0c\u53ea\u662f\u7528\u542f\u53d1\u5f0f\u4fe1\u606f\uff0c\\(f(n)=h(n)\\)\uff0c\u4f46\u4e0d\u5b8c\u5907 \u6700\u574f\u60c5\u51b5\\(O(b^d)\\) A*\u641c\u7d22 \u5bf9\u8282\u70b9\u7684\u8bc4\u4f30\u5305\u542b\u4e86\u5230\u6b64\u8282\u70b9\u5df2\u6709\u7684\u4ee3\u4ef7\uff0c\u5373\\(f(n)=g(n)+h(n)\\)\uff0c\\(h(n)\\)\u6ee1\u8db3\u53ef\u91c7\u7eb3\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5c31\u80fd\u4fdd\u8bc1\u95ee\u9898\u6c42\u89e3\u7684\u5b8c\u5907\u548c\u6700\u4f18\u6027 \u9012\u5f52\u6700\u4f73\u4f18\u5148\u641c\u7d22\uff08RBFS\uff09 \u4f7f\u7528f_limit\u6765\u8ddf\u8e2a\u4ece\u5f53\u524d\u8282\u70b9\u7684\u7956\u5148\u5f97\u5230\u7684\u6700\u4f73\\(f\\)\uff0c\u5982\u679c\u67d0\u8282\u70b9\u8d85\u8fc7\u6b64\u503c\uff0c\u5c31\u9012\u5f52\u56de\u6eaf\uff0c\u5bf9\u5f53\u524d\u8def\u5f84\u4e0a\u7684\u6bcf\u4e2a\u8282\u70b9\uff0c\u7528\u8be5\u8282\u70b9\u5b50\u8282\u70b9\u7684\u6700\u4f73\\(f\\)\u66f4\u65b0\u5176\\(f\\)\u503c <p>A*\u641c\u7d22\u7684\u5b8c\u5907\u548c\u6700\u4f18\u6027\u6761\u4ef6</p> <ul> <li>A*\u641c\u7d22\u7684\u53ef\u91c7\u7eb3\u6027</li> </ul> <p>\u4e13\u6ce8\u4e8e\u5bf9\u6811\u641c\u7d22\u8fdb\u884cA*\u641c\u7d22\uff0c\u8981\u6c42\\(h(n)\\)\u4e0d\u4f1a\u8fc7\u9ad8\u4f30\u8ba1\u5230\u8fbe\u76ee\u6807\u7684\u4ee3\u4ef7\uff0c\u4ece\u800c\\(f(n)\\)\u4e0d\u4f1a\u8d85\u8fc7\u7ecf\u8fc7\u8282\u70b9\\(n\\)\u7684\u89e3\u7684\u5b9e\u9645\u4ee3\u4ef7\u3002\u5728\u641c\u7d22\u4e2d\uff0c\u76f4\u7ebf\u8ddd\u79bb\u662f\u53ef\u91c7\u7eb3\u7684\u542f\u53d1\u5f0f\u3002</p> <ul> <li>A*\u641c\u7d22\u7684\u4e00\u81f4\u6027(\u5355\u8c03\u6027)</li> </ul> <p>\u4e13\u6ce8\u4e8e\u5bf9\u56fe\u641c\u7d22\u8fdb\u884cA*\u641c\u7d22, \u8981\u6c42\u6ee1\u8db3\u4e09\u89d2\u4e0d\u7b49\u5f0f\u3002\u5982\u679c\u5bf9\u4e8e\u6bcf\u4e2a\u8282\u70b9\\(n\\), \u901a\u8fc7\u4eba\u4ee5\u884c\u52a8\\(a\\)\u4ea7\u751f\u7684\\(n\\)\u7684\u6bcf\u4e2a\u540e\u7ee7\u8282\u70b9\\(n'\\), \u4ece\u8282\u70b9\\(n\\)\u5230\u8fbe\u76ee\u6807\u7684\u4f30\u8ba1\u4ee3\u4ef7\u4e0d\u5927\u4e8e\u4ece\\(n\\)\u5230\\(n'\\)\u7684\u5355\u6b65\u4ee3\u4ef7\u4e0e\u4ece\\(n'\\)\u5230\u8fbe\u76ee\u6807\u7684\u4f30\u8ba1\u4ee3\u4ef7\u4e4b\u548c</p> \\[ h(n)\\leq c(n,a,n')+h(n') \\] <p>\u53ef\u4ee5\u7406\u89e3\u6210\\(h(n)\\)\u662f\u5728\u56fe\u641c\u7d22\u4e2d\u4ece\\(n\\)\u5230\u76ee\u6807\u7684\u8def\u5f84\u635f\u8017\u7684\u4e0b\u754c\u3002\u4f30\u8ba1\u7684\u4e0b\u754c\u8d8a\u9760\u8fd1\u4e0b\u786e\u754c\uff0c\u641c\u7d22\u8d8a\u5feb\u3002</p> <p>\u6709\u4e0a\u5f0f\u53ef\u4ee5\u63a8\u5f97\\(f(n)\\)\u662f\u975e\u9012\u51cf\u7684\uff1a</p> \\[ f(n')=g(n')+h(n')=g(n)+c(n.a.n')+h(n')\\geq g(n)+h(n)=f(n) \\]"},{"location":"courses/AIML/AI/ProblemSolving/#_3","title":"\u5bf9\u6297\u641c\u7d22\uff08\u535a\u5f08\uff09","text":"<ul> <li>\u6781\u5927\u6781\u5c0f\u641c\u7d22</li> </ul> <p>\u4f7f\u7528\u4e86\u7b80\u5355\u7684\u9012\u5f52\u7b97\u6cd5\u3002\u7528\u6808\u5b9e\u73b0\uff0c\u6545\u662f\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u3002\u6545time cost \\(O(b^d)\\), space cost \\(O(bd)\\). \u65f6\u95f4\u5f00\u9500\u592a\u5927\u3002</p> Minimax Search<pre><code>action MiniMax_Decision(state)\n    return arg_max(Min_Value(Result(state,a)));\n\nvalue Max_Value(state)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=-INFTY; \n    for(auto&amp; a:actions(state))\n        v=max(v,Min_Value(result(state,a)));\n    return v;\n\nvalue Min_Value(state)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=INFTY; \n    for(auto&amp; a:actions(state))\n        v=min(v,Max_Value(result(state,a)));\n    return v;\n</code></pre> <ul> <li>\\(\\alpha-\\beta\\) \u526a\u679d</li> </ul> <p>\\(\\alpha\\) \u4e3a\u76ee\u524d\u4e3a\u6b62\u8def\u5f84\u4e0a\u53d1\u73b0\u7684MAX\u7684\u6700\u4f73\u9009\u62e9\uff08\u6781\u5927\u503c\uff09\uff0c\\(\\beta\\) \u4e3a\u76ee\u524d\u4e3a\u6b62\u8def\u5f84\u4e0a\u53d1\u73b0\u7684MIN\u7684\u6700\u4f73\u9009\u62e9\uff08\u6700\u5c0f\u503c\uff09\uff0c\u5e76\u4e0d\u65ad\u66f4\u65b0\u3002\u5f53\u67d0\u4e2a\u8282\u70b9\u7684\u503c\u6bd4\u76ee\u524d\u7684MAX\u7684\\(\\alpha\\)/MIN\u7684\\(\\beta\\)\u8981\u67e5\u65f6\u4fbf\u505a\u88c1\u526a\u3002</p> Alpha-Beta Search<pre><code>action Alpha_Beta_Search(state)\n    double v = max(state, -INFTY, +INFTY);\n    return actions.get(v)\n\nvalue Max_Value(state, alpha, beta)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=-INFTY; \n    for(auto&amp; a:actions(state))\n        v=max(v,Min_Value(result(state,a), alpha, beta));\n        if(v&gt;=beta) return v;\n        alpha = max(alpha, v)\n    return v;\n\nvalue Min_Value(state)\n    if(terminal_test(state))return utility(state);//leaf node\n    double v=INFTY; \n    for(auto&amp; a:actions(state))\n        v=min(v,Min_Value(result(state,a), alpha, beta));\n        if(v&lt;=alpha) return v;\n        beta = min(alpha, v)\n    return v;\n</code></pre>"},{"location":"courses/AIML/AI/logic/","title":"Knowledge &amp; Logic","text":""},{"location":"courses/AIML/AI/logic/#logic","title":"Logic","text":"<ul> <li>\u8bed\u4e49\uff1a\u5b9a\u4e49\u4e86\u8bed\u53e5\u5728\u6bcf\u4e2a\u53ef\u80fd\u4e16\u754c\uff08\u6a21\u578b\uff09\u7684\u771f\u503c</li> </ul> <p>\u5982\\(m\\)\u662f\\(\\alpha\\)\u7684\u4e00\u4e2a\u6a21\u578b\uff0c\u8868\u793a\u8bed\u53e5\\(\\alpha\\)\u5728\u6a21\u578b\\(m\\)\u4e2d\u4e3a\u771f\u3002\u7528\\(M(\\alpha)\\)\u8868\u793a\u6240\u6709\u7684\u6a21\u578b\u3002</p> <ul> <li>\u903b\u8f91\u8574\u6db5(entailment): \u67d0\u8bed\u53e5\u5728\u903b\u8f91\u4e0a\u8ddf\u968f\u53e6\u4e00\u4e2a\u8bed\u53e5. \u5982</li> </ul> \\[ \\alpha |=\\beta \\] <p>\u8868\u793a\u8bed\u53e5\\(\\alpha\\)\u8574\u6db5\u8bed\u53e5\\(\\beta\\)\u3002\u4e0a\u53e5\u5f53\u4e14\u4ec5\u5f53\uff0c\u5728\u4f7f\\(\\alpha\\)\u4e3a\u771f\u7684\u6bcf\u4e2a\u6a21\u578b\u4e2d\uff0c\\(\\beta\\)\u4e5f\u4e3a\u771f\u3002\u5373</p> \\[ \\alpha|=\\beta \\Leftrightarrow M(\\alpha)\\subset M(\\beta) \\] <p>\u6ce8\u610f\uff0c\u8fd9\u8868\u793a\\(\\alpha\\)\u662f\u6bd4\\(\\beta\\)\u66f4\u5f3a\u7684\u65ad\u8a00\uff0c\u6392\u9664\u4e86\u66f4\u591a\u53ef\u80fd\u6027\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8574\u6db5\u6765\u5b9e\u73b0\u903b\u8f91\u63a8\u7406\u3002\u901a\u8fc7\u679a\u4e3e\u6240\u6709\u53ef\u80fd\u7684\u6a21\u578b\u6765\u68c0\u9a8cKB\u4e3a\u771f\u65f6\\(\\alpha\\)\u5747\u4e3a\u771f\u3002\u8fd9\u6837\u7684\u8fc7\u7a0b\u79f0\u4e3a\u63a8\u7406\u7b97\u6cd5\uff0c\u662f\u53ef\u9760\u7684\u3002</p>"},{"location":"courses/AIML/AI/logic/#_1","title":"\u547d\u9898\u903b\u8f91","text":"<p>\u7528\u5927\u5199\u5b57\u6bcd\u8868\u793a\u547d\u9898\u3002\u590d\u5408\u53e5\u7531\u7b80\u5355\u8bed\u53e5\u7528\u62ec\u53f7\u548c\u903b\u8f91\u8fde\u63a5\u8bcd\u6784\u9020\u800c\u6210\u3002</p> <p>Basic notation</p> <p>\\(\\land\\) \u8868\u793a\"\u4e0e\"\uff0c\u770b\u8d77\u6765\u548c\"And\"\u4e2d\u7684\"A\"\u5f88\u50cf\uff0c\\(W_{1,3}\\land P_{3,1}\\)\u88ab\u79f0\u4e3a\u5408\u53d6\u5f0f\u3002</p> <p>\\(\\lor\\) \u8868\u793a\"\u6216\"\uff0c\u6765\u6e90\u4e8e\u62c9\u4e01\u6587\"vel\"\uff0c\\(W_{1,3}\\lor P_{3,1}\\)\u88ab\u79f0\u4e3a\u6790\u53d6\u5f0f\u3002</p> <p>\\(\\Rightarrow\\) \u8868\u793a\u8574\u6db5(implication)\u3002\\(P\\Rightarrow Q\\) \u8868\u793a\uff0c\u5982\u679c\\(P\\)\u4e3a\u771f\uff0c\u90a3\u6211\u4e3b\u5f20\\(Q\\)\u4e3a\u771f\uff0c\u5426\u5219\u65e0\u53ef\u5949\u544a\u3002</p>"},{"location":"courses/AIML/AI/logic/#_2","title":"\u5b9a\u7406\u8bc1\u660e","text":"<ul> <li>\u903b\u8f91\u7b49\u4ef7: \\(\\equiv\\), \u5982 \\(P\\land Q\\equiv Q\\land P\\). \u4e5f\u53ef\u4ee5\u8868\u793a\u6210\u76f8\u4e92\u8574\u6db5</li> </ul> \\[ \\alpha\\equiv \\beta \\quad\\text{iff}\\quad (\\alpha|=\\beta) \\land (\\beta|=\\alpha) \\] <ul> <li> <p>\u6709\u6548\u6027\uff1a\u4e00\u4e2a\u8bed\u53e5\u662f\u6709\u6548\u7684\uff0c\u5982\u679c\u5728\u6240\u6709\u7684\u6a21\u578b\u4e2d\u90fd\u4e3a\u771f\u3002</p> </li> <li> <p>\u53ef\u6ee1\u8db3\u6027\uff1a\u4e00\u4e2a\u8bed\u53e5\u662f\u53ef\u6ee1\u8db3\u7684\uff0c\u5982\u679c\u5728\u67d0\u4e9b\u6a21\u578b\u4e2d\u4e3a\u771f\u3002\u53ef\u4ee5\u901a\u8fc7\u679a\u4e3e\u8fdb\u884c\uff0c\u662f\u4e00\u4e2aSAT\u95ee\u9898\uff0cNP\u5b8c\u5168\u95ee\u9898\u3002</p> </li> </ul> <p>\u4e24\u8005\u7684\u5173\u7cfb\u53ef\u4ee5\u63a8\u51fa\u53cd\u8bc1\u6cd5\u7684\u601d\u8def\u3002</p> \\[ \\alpha|=\\beta \\quad \\text{iff} \\quad (\\alpha\\land \\neg  \\beta)\\text{\u4e0d\u53ef\u6ee1\u8db3} \\] <ul> <li>\u63a8\u5bfc\u4e0e\u8bc1\u660e</li> </ul> <p>\u63a8\u7406\u89c4\u5219</p> <ul> <li>\u5047\u8a00\u63a8\u7406\u89c4\u5219(Modus Ponens)</li> </ul> <p>Given \\(\\alpha\\Rightarrow \\beta\\) and \\(\\alpha\\), we can deduce \\(\\beta\\).</p> \\[ \\frac{\\alpha\\Rightarrow \\beta,\\quad \\alpha}{\\beta} \\] <ul> <li>\u6d88\u53bb\u5408\u53d6\u8bcd</li> </ul> <p>extract subsentence from \u5408\u53d6\u5f0f</p> \\[ \\frac{\\alpha \\land \\beta}{\\alpha} \\] <ul> <li>\u5f52\u7ed3(resolution)</li> </ul> <p>types of Resolution</p> <ul> <li>\u5355\u5143\u5f52\u7ed3(unit resolution)</li> </ul> <p>if \\(m=\\neg l_i\\), then</p> \\[ \\frac{l_1\\lor \\cdots \\lor l_k, \\quad m}{l_1\\lor\\cdots \\lor l_{i-1}\\lor l_{i+1}\\lor \\cdots\\lor l_k} \\] <ul> <li>\u5168\u5f52\u7ed3(full reosolution)</li> </ul> <p>if \\(m_j=\\neg l_i\\), then</p> \\[ \\frac{l_1\\lor \\cdots \\lor l_k, \\quad m1\\lor\\cdots\\lor m_n}{l_1\\lor\\cdots \\lor l_{i-1}\\lor l_{i+1}\\lor \\cdots\\lor l_k\\lor m_1\\lor \\cdots\\lor m_{j-1}\\lor m_{j+1}\\lor\\cdots\\lor m_n} \\] <p>Q1. Given </p> \\[ \\begin{align*} &amp;R_1: \\quad\\neg P_{1,1}\\\\ &amp;R_2: \\quad B_{1,1}\\Leftrightarrow (P_{1,2}\\lor P_{2,1})\\\\ &amp;R_3: \\quad B_{2,1}\\Leftrightarrow (P_{1,1}\\lor P_{2,2} \\lor P_{3,1})\\\\ &amp;R_4: \\quad \\neg B_{1,1}\\\\ &amp;R_5: \\quad B_{2,1} \\end{align*} \\] <p>Simplify \\(R_1\\land R_2\\land R_3\\land R_4\\land R_5\\)</p> Answer <p>Notice that...</p> <p>\u4efb\u4f55\u57fa\u4e8e\u5f52\u7ed3\u7684\u5b9a\u7406\u8bc1\u660e\u5668\uff0c\u90fd\u80fd\u786e\u5b9a\\(\\alpha |=\\beta\\) \u662f\u5426\u6210\u7acb\u3002\u5176\u672c\u8d28\u5c31\u662f\u53cd\u8bc1\u6cd5\u3002\u82e5\u76ee\u6807\u547d\u9898\u4e3a\\(\\alpha\\), \u5df2\u77e5\u77e5\u8bc6\u5e93\u4e3a\\(KB\\), \u7136\u540e\u4f7f\u7528\\(KB\\land \\neg \\alpha\\)\uff0c\u82e5\u4e0d\u53ef\u6ee1\u8db3(\u5bf9\u6240\u6709\u6a21\u578b\u90fd\u4e3a\u5047)\uff0c\u5219\\(KB |=\\alpha\\).</p> <ul> <li>\u5408\u53d6\u8303\u5f0f: \u5b50\u53e5\u7684\u8fde\u63a5\u7b26\u662f\\(\\land\\)</li> </ul> <p>\u5bf9\u6240\u6709\u5b50\u53e5\u7684\u53ef\u80fd\u7684\u4e24\u4e24\u7ec4\u5408\uff08\u542b\u6709\u4e92\u8865\u8bed\u53e5\uff09\u8fdb\u884c\u5f52\u7ed3\uff0c\u4ea7\u751f\u65b0\u5b50\u53e5\u3002\u5982\u679c\u8be5\u5b50\u53e5\u6ca1\u6709\u51fa\u73b0\u8fc7\uff0c\u5219\u5c06\u4ed6\u88c5\u5165\u5b50\u53e5\u96c6\u3002\u76f4\u5230\uff1a</p> <p>\uff08i\uff09 \u6ca1\u6709\u65b0\u8bed\u53e5(new \\(\\subset\\) clauses), \u5219\\(KB \\land \\neg\\alpha\\) \u53ef\u6ee1\u8db3\uff0c</p> <p>\uff08ii\uff09 \u4ea7\u751f\u7a7a\u8bed\u53e5(like \\(P\\land \\neg P\\)), \u5219\\(KB \\land \\neg\\alpha\\) \u4e0d\u53ef\u6ee1\u8db3\uff0c\u539f\u547d\u9898\u53ef\u63a8\u51fa\\(\\alpha\\).</p> <p>This is the following theorem.</p> <p>Basic Resolution Theorem</p> <p>If the set \\(S\\) of sub clauses is not satisfiable, then the resolution closure \\(RC(S)\\) must contain empty clause.</p> Proof <p>By prove its converse-negative proposition. That is, if \\(RC(S)\\) does not contain empty clause, then \\(S\\) is satisfiable. ?</p>"},{"location":"courses/AIML/ML/","title":"Machine Learning","text":""},{"location":"courses/AIML/ML/#introduction","title":"Introduction","text":""},{"location":"courses/AIML/ML/#concept-learning","title":"Concept Learning","text":""},{"location":"courses/AIML/ML/#decision-tree","title":"Decision Tree","text":""},{"location":"courses/AIML/ML/Concept_Learning/","title":"Concept Learning","text":"<p>\u6982\u5ff5\u5b66\u4e60\u7684\u5b9a\u4e49</p> <p>\u7ed9\u5b9a\u7684\u67d0\u4e00\u7c7b\u522b\u7684\u82e5\u5e72\u6b63\u4f8b\u548c\u53cd\u4f8b\uff0c\u4ece\u4e2d\u83b7\u5f97\u8be5\u7c7b\u522b\u7684\u4e00\u822c\u5b9a\u4e49\u3002</p> <p>\u6982\u5ff5\u5b66\u4e60\u662f\u6307\u4ece\u6709\u5173\u67d0\u4e2a\u5e03\u5c14\u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u8bad\u7ec3\u6837\u4f8b\u4e2d\u63a8\u65ad\u51fa\u8be5\u5e03\u5c14\u51fd\u6570\u3002</p>"},{"location":"courses/AIML/ML/Concept_Learning/#_1","title":"\u6982\u5ff5\u5b66\u4e60\u4efb\u52a1","text":"<p>\u5f62\u5f0f\u5316\u8868\u8fbe</p> <p>\u6bcf\u4e2a\u5c5e\u6027\u53ef\u6709\u4e09\u79cd\u53d6\u503c</p> <ul> <li> <p>\"?\" \u8868\u793a\u4efb\u610f\u672c\u5c5e\u6027\u53ef\u63a5\u53d7\u7684\u503c\u3002</p> </li> <li> <p>\u660e\u786e\u7684\u5c5e\u6027</p> </li> <li> <p>\"\\(\\varnothing\\)\" \u8868\u793a\u4e0d\u63a5\u53d7\u4efb\u4f55\u503c\u3002</p> </li> </ul> <p>\u5982\u6700\u4e00\u822c\u7684\u5047\u8bbe\uff1a</p> \\[ &lt;?, ?, ?, ?, ?, ?&gt; \\] <p>\u6700\u7279\u6b8a\u7684\u5047\u8bbe\uff1a</p> \\[ &lt;\\varnothing, \\varnothing, \\varnothing, \\varnothing, \\varnothing, \\varnothing&gt; \\]"},{"location":"courses/AIML/ML/Concept_Learning/#_2","title":"\u672f\u8bed\u5b9a\u4e49","text":"<p>\u672f\u8bed\u5b9a\u4e49</p> <ul> <li> <p>\u6982\u5ff5\u5b9a\u4e49\u5728 \u5b9e\u4f8b\u96c6 \\(X\\) \u4e0a. </p> </li> <li> <p>\u5f85\u5b66\u4e60\u7684\u6982\u5ff5\u6216\u76ee\u6807\u51fd\u6570\u88ab\u79f0\u4e3a \u76ee\u6807\u6982\u5ff5(target concept, \\(c\\)). \u53ef\u4ee5\u662f\u5b9a\u4e49\u5728\u5b9e\u4f8b\u96c6\\(X\\)\u4e0a\u7684\u4efb\u610f\u5e03\u5c14\u51fd\u6570\uff0c\u5373</p> </li> </ul> \\[ c: X \\mapsto \\{0,1\\} \\] <ul> <li>\u8bad\u7ec3\u6837\u4f8b(training examples): \u6bcf\u4e2a\u6837\u4f8b\u4e3a \\(X\\) \u4e2d\u7684\u4e00\u4e2a\u5143\u7d20 \\(x\\) \u4ee5\u53ca\u5bf9\u5e94\u7684\u76ee\u6807\u6982\u5ff5 \\(c(x)\\)\uff0c\u8bb0\u4e3a \\(&lt;x, c(x)&gt;\\). \u82e5 \\(c(x)=1\\) \u8be5\u6837\u4f8b\u88ab\u79f0\u4e3a\u6b63\u4f8b\uff0c\u662f\u76ee\u6807\u6982\u5ff5\u7684\u6210\u5458\u3002</li> </ul> <p>\u7ed9\u5b9a\u8bad\u7ec3\u6837\u4f8b\u96c6\uff0c\u6211\u4eec\u53ef\u4ee5\u5b66\u4e60\u6216\u4f30\u8ba1\\(c\\)\u3002</p> <p>\u672f\u8bed\u5b9a\u4e49</p> <ul> <li> <p>\u6240\u6709\u53ef\u80fd\u5047\u8bbe\u96c6(all possible hypotheses) \\(H\\).</p> </li> <li> <p>\\(h\\) \u662f \\(H\\) \u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\uff0c\u8868\u793a \\(X\\) \u4e0a\u5b9a\u4e49\u7684\u5e03\u5c14\u51fd\u6570 \\(h:X\\mapsto \\{0,1\\}\\). </p> </li> </ul> <p>\u5b66\u4e60\u7684\u76ee\u6807\u5c31\u662f\uff0c\u5bfb\u627e\u4e00\u4e2a \\(h\\)\uff0c\u4f7f\u5f97 \\(\\forall x\\in X\\), \u6709 \\(h(x)=c(x)\\).</p> <ul> <li>\u5f52\u7eb3\u5b66\u4e60\u5047\u8bbe: \u5bf9\u4e8e\u672a\u89c1\u5b9e\u4f8b\u6700\u597d\u7684\u5047\u8bbe\u5c31\u662f\u4e0e\u8bad\u7ec3\u6570\u636e\u6700\u4f73\u62df\u5408\u7684\u5047\u8bbe\u3002\u4efb\u4e00\u5047\u8bbe\u5982\u679c\u5728\u8db3\u591f\u5927\u7684\u8bad\u7ec3\u6837\u4f8b\u96c6\u4e2d\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570\uff0c\u5b83\u4e5f\u80fd\u5728\u672a\u89c1\u7684\u5b9e\u4f8b\u4e2d\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570\u3002</li> </ul>"},{"location":"courses/AIML/ML/Concept_Learning/#find-s","title":"\u7528\u641c\u7d22\u8fdb\u884c\u6982\u5ff5\u5b66\u4e60 | Find-S\u7b97\u6cd5","text":"<p>\u4ece \\(H\\) \u4e2d\u6700\u7279\u6b8a\u5047\u8bbe\u5f00\u59cb\uff0c\u7136\u540e\u5728\u5047\u8bbe\u8986\u76d6\u6b63\u4f8b\u65f6\u5c06\u5176\u4e00\u822c\u5316\u3002</p> <p>FIND-S\u7b97\u6cd5</p> <ol> <li> <p>\u5c06 \\(h\\) \u521d\u59cb\u5316\u4e3a \\(H\\) \u4e2d\u6700\u7279\u6b8a\u5047\u8bbe (\\(&lt;\\varnothing &gt;\\) \u7ec4\u6210)</p> </li> <li> <p>\u5bf9\u6bcf\u4e2a\u6b63\u4f8b \\(x\\) in \\(X\\)</p> <p>\u5bf9 \\(h\\) \u7684\u6bcf\u4e00\u4e2a\u5c5e\u6027\u7ea6\u675f \\(a_i\\)</p> Text Only<pre><code>if $x$ \u6ee1\u8db3\n\n    $a_i$ continue;\n\nelse\n\n    \u5c06 $h$ \u4e2d $a_i$ \u66ff\u6362\u4e3a $x$ \u6ee1\u8db3\u7684\u53e6\u4e00\u4e2a\u66f4\u4e00\u822c\u7ea6\u675f\n</code></pre> </li> <li> <p>\u8f93\u51fa \\(h\\).</p> </li> </ol> <p>\u5176\u5b9e\u662f\u5229\u7528 more_general_than \u7684\u504f\u5e8f\u6765\u641c\u7d22\u3002\u6bcf\u4e00\u6b65\u5f97\u5230\u7684\u5047\u8bbe\u90fd\u662f\u90a3\u4e00\u70b9\u4e0a\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6700\u7279\u6b8a\u5047\u8bbe\u3002</p> <ul> <li>\u5b58\u5728\u95ee\u9898</li> </ul> <p>\u6536\u655b\u6027\uff1a \u65e0\u6cd5\u786e\u5b9a\u662f\u5426\u62df\u5408\u5230\u76ee\u6807\u6982\u5ff5 \\(c\\). \u5f97\u5230\u7684\u53ea\u662f\u80fd\u591f\u62df\u5408\u8bad\u7ec3\u6837\u4f8b\u7684\u591a\u4e2a\u5047\u8bbe\u7684\u4e00\u4e2a\u3002</p> <p>\u6ca1\u6709\u8003\u8651\u6ee1\u8db3\u6761\u4ef6\u7684\u6700\u4e00\u822c\u5047\u8bbe\u548c\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u7684\u5176\u4ed6\u5047\u8bbe\u3002</p> <p>\u7b97\u6cd5\u4e0d\u591frobust\uff0c\u5f53\u6837\u4f8b\u4e2d\u51fa\u73b0\u9519\u8bef\u65f6\uff0c\u4f1a\u4e25\u91cd\u7834\u574f\u3002</p>"},{"location":"courses/AIML/ML/Concept_Learning/#candidate-elimination","title":"\u53d8\u5f62\u7a7a\u95f4\u548c\u5019\u9009\u6d88\u9664\u7b97\u6cd5(Candidate-Elimination)","text":"<p>\u8be5\u7b97\u6cd5\u5f97\u5230\u7684\u662f\u548c\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe\u7684\u96c6\u5408\u3002</p> <p>\u4f18\u70b9: \u4f9d\u8d56\u4e8e more_general_than \u504f\u5e8f\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u63cf\u8ff0\u65f6\u4e0d\u7528\u5217\u4e3e\u6240\u6709\u6210\u5458\u3002</p> <ul> <li>\u53d8\u5f62\u7a7a\u95f4: \u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe\u7684\u96c6\u5408\uff0c\u5373\u53d8\u5f62\u7a7a\u95f4\u4e2d\u4efb\u610f\u4e00\u4e2a\u5047\u8bbe\u90fd\u80fd\u8986\u76d6\u6240\u6709\u7684\u6b63\u4f8b\u5e76\u6392\u65a5\u6240\u6709\u7684\u53cd\u4f8b\u3002\u4e00\u822c\u4f7f\u7528\u4e00\u4e2a \u6781\u5927\u4e00\u822c\u5047\u8bbe(G) \u548c \u6781\u5927\u7279\u6b8a\u5047\u8bbe(S)\uff0c\u6765\u5f62\u6210\u4e00\u822c\u548c\u7279\u6b8a\u8fb9\u754c\uff0c\u5728\u6574\u4e2a\u504f\u5e8f\u7ed3\u6784\u4e2d\u5212\u5206\u51fa\u53d8\u5f62\u7a7a\u95f4\u3002</li> </ul> <p>\u5019\u9009\u6d88\u9664\u7b97\u6cd5</p> <ol> <li> <p>\u521d\u59cb\u5316 G \u4e3a \"?\", S \u4e3a \"\\(\\varnothing\\)\".</p> </li> <li> <p>\u904d\u5386\u8bad\u7ec3\u6837\u4f8b x in X:</p> <p>\u82e5\u4e3a\u6b63\u4f8b\uff0c\u5219\u4e00\u822c\u5316 S;  </p> <p>\u5426\u5219\u4e3a\u8d1f\u4f8b\uff0c\u7279\u6b8a\u5316 G</p> </li> </ol> <p>\u6ce8\u610f\u70b9</p> <ul> <li> <p>\u53ef\u4ee5\u7b5b\u9009\u6837\u4f8b\u4e2d\u7684\u9519\u8bef\uff1a \u7ed9\u5b9a\u8db3\u591f\u6837\u4f8b\uff0c\u4f1a\u6536\u655b\u5230\u7a7a\u7684\u53d8\u5f62\u7a7a\u95f4\u3002</p> </li> <li> <p>\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u80fd\u591f\u6536\u655b\u5230\u6b63\u786e\u5047\u8bbe\u7684\u524d\u63d0: \u6837\u4f8b\u6ca1\u6709\u9519\u8bef\uff0c H \u4e2d\u786e\u5b9e\u5305\u542b\u63cf\u8ff0\u76ee\u6807\u6982\u5ff5\u7684\u6b63\u786e\u5047\u8bbe(\u76ee\u6807\u6982\u5ff5\u4e0d\u518d\u5047\u8bbe\u7a7a\u95f4\u5185\uff0c\u5373\u4e0d\u80fd\u591f\u7531\u51e0\u4e2a\u5c5e\u6027\u7684\u5408\u53d6\uff0c\u800c\u53ea\u80fd\u662f\u6790\u53d6)\u3002</p> </li> </ul>"},{"location":"courses/AIML/ML/Concept_Learning/#_3","title":"\u5f52\u7eb3\u504f\u7f6e","text":"<p>\u5982\u679c\u76ee\u6807\u6982\u5ff5\u4e0d\u5728\u5047\u8bbe\u7a7a\u95f4\uff0c\u90a3\u8be5\u5982\u4f55\u89e3\u51b3? \u5982\u4e24\u4e2a\u6b63\u4f8b\u5305\u542b\u5c5e\u6027<code>sky=sunny</code> \u548c <code>sky=cloudy</code>, \u8fd9\u65f6\u5019\u8bad\u7ec3\u5f97\u5230\u7684\u6781\u5927\u7279\u6b8a\u5047\u8bbe\u662f <code>sky=?</code>\uff0c\u8fd9\u6837\u5c31\u4f1a\u5c06 \u53cd\u4f8b<code>sky=rainy</code> \u5224\u6210\u6b63\u4f8b\u3002\u8fd9\u4e2a\u95ee\u9898\u51fa\u73b0\u7684\u539f\u56e0\u5728\u4e8e\u6211\u4eec\u7684\u5047\u8bbe\u7a7a\u95f4\u4e0d\u591f\u5927\uff0c\u4e0a\u8ff0\u7684\u6781\u5927\u7279\u6b8a\u5047\u8bbe\u8fd8\u662f\u592a\u4e00\u822c\u4e86\u3002</p> <p>\u663e\u7136\uff0c\u9700\u8981\u4e00\u4e2a\u5047\u8bbe\u7a7a\u95f4\uff0c\u80fd\u591f\u8868\u8fbe\u6240\u6709\u7684 \u53ef\u6559\u6388\u6982\u5ff5, \u5373\u80fd\u591f\u8868\u8fbe\u5b9e\u4f8b \\(X\\) \u6240\u6709\u53ef\u80fd\u7684\u5b50\u96c6\uff0c\u628a\u8fd9\u4e9b\u5b50\u96c6\u7ec4\u6210\u7684\u96c6\u5408\u79f0\u4e3a \\(X\\) \u7684\u5e42\u96c6(power set)\u3002</p> <p>\u8fd9\u5c31\u7528\u5230\u4e86\u4e0b\u9762\u7684 \u5f52\u7eb3\u504f\u7f6e\u3002</p> <p>\u5f53\u5b66\u4e60\u5668\u53bb\u9884\u5176\u672a\u9047\u5230\u8fc7\u7684\u8f93\u5165\uff0c\u4f1a\u505a\u51fa\u5047\u8bbe\uff0c\u5b66\u4e60\u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e\u662f\u8fd9\u4e9b\u5047\u8bbe\u7684\u96c6\u5408\u3002</p> <p>\u5f52\u7eb3\u504f\u7f6e</p> <ul> <li>FIND-S \u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e</li> </ul> <p>\u76ee\u6807\u6982\u5ff5\u5728\u5047\u8bbe\u7a7a\u95f4\u4e2d\uff1b\u5bf9\u4e8e\u4efb\u4f55\u5b9e\u4f8b\uff0c\u82e5\u4e0d\u7b26\u5408\u6700\u7279\u6b8a\u5047\u8bbe\uff0c\u9664\u975e\u5176\u4e3a\u6b63\u4f8b\uff1b</p> <ul> <li>\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e</li> </ul> <p>\u76ee\u6807\u6982\u5ff5\u5305\u542b\u5728\u7ed9\u5b9a\u7684\u5047\u8bbe\u7a7a\u95f4\u5185\u3002</p> <p>\u5bf9\u4e8e\u65b0\u5b9e\u4f8b\u505a\u5206\u7c7b\uff0c\u6216\u65e0\u6cd5\u5206\u7c7b\u3002</p> <p>\u4e00\u4e2a\u7b97\u6cd5\u6709\u504f\u6027\u8d8a\u5f3a\uff0c\u5219\u5f52\u7eb3\u80fd\u529b\u8d8a\u5f3a\uff0c\u53ef\u4ee5\u5206\u7c7b\u66f4\u591a\u7684\u672a\u89c1\u5b9e\u4f8b\u3002\u65e0\u504f\u7684\u5b66\u4e60\u5668\u65e0\u6cd5\u5bf9\u672a\u89c1\u6837\u4f8b\u8fdb\u884c\u5f52\u7eb3\u3002\u5982\u679c\u5047\u8bbe\u7a7a\u95f4\u88ab\u62d3\u5c55\uff0c\u5b9e\u4f8b\u96c6\u7684\u5e42\u96c6\u90fd\u6709\u4e00\u4e2a\u5047\u8bbe\uff0c\u5219\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u7684\u5f52\u7eb3\u504f\u7f6e\u6d88\u5931\u3002</p>"},{"location":"courses/AIML/ML/Decision_Tree/","title":"Decision Tree","text":"<p>\u5f52\u7eb3\u504f\u7f6e: \u4f18\u5148\u9009\u62e9\u8f83\u5c0f\u7684\u6811</p> <p>\u51b3\u7b56\u6811</p> <p>\u6839\u5f00\u59cb\uff0c\u5230\u53f6\u5b50\u8282\u70b9\uff0c\u5bf9\u5b9e\u4f8b\u8fdb\u884c\u5206\u7c7b\u3002\u6811\u4e0a\u7684\u8282\u70b9\u5bf9\u5e94\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u5c5e\u6027\u7684\u6d4b\u8bd5\uff0c\u540e\u7ee7\u5206\u652f\u4ee3\u8868\u8be5\u5c5e\u6027\u7684\u4e00\u4e2a\u53ef\u80fd\u503c\u3002</p> <p>\u672c\u8d28\u4e0a\uff0c\u5b83\u4ee3\u8868\u5b9e\u4f8b\u5c5e\u6027\u503c\u7684\u5408\u53d6(conjuction, \u4ece\u6839\u5230\u53f6\u5b50\u8282\u70b9)\u7684\u6790\u53d6\u5f0f(disjunction, \u4e00\u4e2a\u6839\u8282\u70b9\u7684\u4e0d\u540c\u7684\u53ef\u80fd\u6027)</p> <p>\u9002\u7528\u95ee\u9898\u7279\u5f81</p> <p>\u4e0b\u9762\u4e09\u70b9\u6bd4\u8f83\u663e\u7136\u3002</p> <ul> <li> <p>\u5b9e\u4f8b\u7531 \"\u5c5e\u6027-\u5bf9\" \u7ec4\u6210\uff0c\u5c5e\u6027\u5bf9\u5e94\u7684\u503c\u53ef\u4ee5\u5f0f\u79bb\u6563\u7684\uff0c\u4e5f\u53ef\u4ee5\u662f\u5206\u533a\u95f4\u8fde\u7eed\u7684\u3002</p> </li> <li> <p>\u76ee\u6807\u51fd\u6570\u6709\u79bb\u6563\u7684\u8f93\u51fa\u503c\uff0c\u5982\u5e03\u5c14\u578b\u5206\u7c7b\uff0c\u6216\u4e24\u4e2a\u4ee5\u4e0a\u8f93\u51fa\u503c\u7684\u5206\u7c7b\u5668\u3002</p> </li> <li> <p>\u9700\u8981\u6790\u53d6\u7684\u63cf\u8ff0</p> </li> </ul> <p>\u4e0b\u9762\u4e24\u70b9\u503c\u5f97\u63a2\u8ba8\u3002</p> <ul> <li> <p>\u5b9e\u4f8b\u96c6\u53ef\u4ee5\u5305\u542b\u9519\u8bef\uff1f\uff1f</p> </li> <li> <p>\u5b9e\u4f8b\u96c6\u53ef\u4ee5\u5305\u542b\u7f3a\u5c11\u5c5e\u6027\u503c\u7684\u5b9e\u4f8b\uff1f\uff1f</p> </li> </ul>"},{"location":"courses/AIML/ML/Decision_Tree/#id3","title":"ID3 \u7b97\u6cd5","text":"<p>\u4f7f\u7528\u8d2a\u5a6a\u7b97\u6cd5\u3002</p> <p>\u6838\u5fc3\u95ee\u9898\u662f\u9009\u62e9\u6bcf\u4e2a\u8282\u70b9\u8981\u6d4b\u8bd5\u7684\u5c5e\u6027\u3002\u5982\u4f55\u8861\u91cf\u4e00\u4e2a\u5c5e\u6027\u5177\u6709\u6700\u597d\u7684\u9009\u62e9\u80fd\u529b\uff1f\u8fd9\u91cc\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u3002</p>"},{"location":"courses/AIML/ML/Decision_Tree/#_1","title":"\u4fe1\u606f\u589e\u76ca","text":"<p>For a specific characteristic, we could categorize samples into different kinds. We take an inspiration from entropy.</p> <p>Entropy of a random variable</p> <p>Assume \\(\\xi\\) is a discrete random variable, with its distribution </p> \\[ P(\\xi=x_k)=p_k, (i=1,2,\\cdots, K). \\] <p>Then its entropy is defined by</p> \\[ H(\\xi)=-\\sum_{k=1}^K p_k\\log_2 p_k. \\] <p>Note that entropy does not depend direcly on the value of \\(x_k\\), but its probability \\(p_k\\). </p> <p>In practive, usually given a sample set \\(S\\), we let \\( K \\) denotes the number of states, and \\( p_k \\) represents the proportion of instances for a certain classification value (estimated using maximum likelihood), then the above calculation gives empirical entropy. </p> <p>The range of entropy is</p> \\[ 0\\leq H(S)\\leq \\log_2 K. \\] <p>The maximum value of entropy is estimated using Jensen's inequality. The minimum value is \\( 0 \\), achieved when all instances belong to a single state, while the maximum is attained when instances are equally distributed among all states.</p> <p>Conditional Entropy</p> <p>Assume two random variables \\(X,Y\\) has a unified distribution </p> \\[ P(X=x_i, Y=y_k)=p_{ik},\\quad i=1,\\cdots, n; \\quad k=1,\\cdots, K. \\] <p>Given \\(X\\), the entropy of random variable \\(Y\\) is</p> \\[ \\begin{align} H(Y\\mid X)&amp;=\\sum_{i=1}^n P(X=x_i) H(Y\\mid X=x_i)\\\\ &amp;=\\sum_{i=1}^n p_i \\left(-\\sum_{k=1}^K p_{ik}\\log_2 p_{ik}\\right) \\end{align} \\] <p>In practice, given \\(X\\) means given a characteristic \\(A\\), and it has \\(n\\) different options. The data set has still \\(K\\) categories. The conditional entropy means the entropy under a certain characteristic.</p> <p>Gain of info</p> <p>The gain of infomation of characteristic \\(A\\) on data set \\(S\\), is defined by</p> \\[ g(D,A)=H(S)-H(S\\mid A) \\] <p>here \\(H(S\\mid A)\\) is the empirical conditional entropy. We could calculate it as follows. </p> <p>Data set \\(S\\) has \\(|S|\\) number of samples with \\(|S_k|\\) for category \\(k\\), \\((k=1,\\cdots,K)\\). </p> <p>For a given characteristic \\(A\\), with \\(n\\) values or options, namely \\(\\{x_i\\}_{1\\leq i\\leq n}\\). \\(A\\) partitions \\(S\\) into another \\(n\\) parts \\(S_{Ai}\\), with \\(|S_{Ai}|\\) number of samples, \\((i=1,\\cdots, n)\\). So let \\(X\\) to be values of \\(A\\) for a sample in \\(S\\), then we have an estimation </p> \\[ P(X=x_i)\\approx \\frac{|S_{Ai}|}{|S|}, \\quad i=1,\\cdots,n. \\] <p>Let \\(Y\\) to be the categories of a sample in \\(S\\), which has \\(K\\) values, then </p> \\[ H(Y)\\approx H(S)=-\\sum_{k=1}\\frac{|S_k|}{|S|}\\log_2 \\frac{|S_k|}{|S|}. \\] <p>For \\(X=x_i\\), let \\(S_{ik}=S_{A_i}\\cap S_k\\), then </p> \\[ H(Y \\mid X)\\approx H(S\\mid A)=\\sum_{i=1}^n \\frac{|S_{Ai}|}{|S|} \\left(-\\sum_{k=1}^K \\frac{|S_{ik}|}{|S_{Ai}|}\\log_2 \\frac{|S_{ik}|}{|S_{Ai}|}\\right). \\] In Chinese <p>\u8fd9\u91cc\u7528\u671f\u671b\uff0c\u8868\u660e\u4e00\u4e2a\u5c5e\u6027\u7684\u4e0d\u540c\u503c\u5bf9\u505a\u51fa\u6837\u4f8b\u96c6\u5212\u5206\u540e\uff0c\u8981\u52a0\u6743\u6c42\u5e73\u5747\u6765\u8868\u8fbe\u603b\u4f53\u7684\u71b5\u51cf\u60c5\u51b5\u3002</p> \\[ g(S,A) = H(S) - \\sum_{v\\in value(A)}\\frac{|S_v|}{|S|}H(S_v) \\] <p>\u6ce8\u610f\u540e\u9762\u7684\u71b5\\(H(S_v)\\)\u662f\u6761\u4ef6\u71b5\uff0c\u57fa\u4e8e\\(S_v\\)\u6765\u8ba1\u7b97\u540e\u7eed\u522b\u7684\u5c5e\u6027\u7684\u71b5.</p> <p>Usually \\(\\frac{|S_{Ai}|}{|S|}\\) causes the algorithm to prefer the characteristic which has more samples in \\(S\\).</p> <p>Information gain ratio</p> <p>Define the entropy with respect to characteristic \\(A\\) </p> \\[ H_A(S)=-\\sum_{i=1}^n \\frac{|S_{Ai}|}{|S|}\\log_2 \\frac{|S_{Ai}|}{|S|}, \\] <p>then information gain ratio is </p> \\[ g_R(D,A)=\\frac{g(D, A)}{H_A(D)}. \\] <p>We currently do not have theoretical analysis for this.</p> <p>\u9012\u5f52\u8c03\u7528\u3002\u5f53\u6837\u4f8b\u4e2a\u6570\u4e3a0\u6216\u71b5\u4e3a0\u65f6\uff0c\u786e\u5b9a\u53f6\u5b50\u8282\u70b9\u7684\u5c5e\u6027\u3002\u5df2\u88ab\u6811\u7684\u8f83\u9ad8\u8282\u70b9\u6d4b\u8bd5\u7684\u5c5e\u6027\u88ab\u6392\u9664\u5728\u5916\u3002</p> <p>ID3 \u7b97\u6cd5\u7684\u4f18\u52a3</p> \u4f18\u52bf \u52a3\u52bf \u5047\u8bbe\u7a7a\u95f4\u5305\u542b\u6240\u6709\u7684\u51b3\u7b56\u6811\uff0c\u65f6\u5173\u4e8e\u73b0\u6709\u5c5e\u6027\u7684\u6709\u9650\u79bb\u6563\u503c\u51fd\u6570\u7684\u4e00\u4e2a\u5b8c\u6574\u7a7a\u95f4\u3002 \u8bad\u7ec3\u7ed3\u675f\u540e\uff0cID3\u53ea\u7ef4\u62a4\u5355\u4e00\u7684\u5f53\u524d\u5047\u8bbe\u3002\u4e0d\u50cf\u53d8\u5f62\u7a7a\u95f4\u7684\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u80fd\u591f\u7ef4\u62a4\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe \u6bcf\u4e00\u6b65\u90fd\u4f7f\u7528\u5f53\u524d\u7684\u6240\u6709\u8bad\u7ec3\u6837\u4f8b\uff0c\u57fa\u4e8e\u7edf\u8ba1\uff0c\u80fd\u591f\u964d\u4f4e\u5bf9\u4e2a\u522b\u8bad\u7ec3\u6837\u4f8b\u9519\u8bef\u7684\u654f\u611f\u6027\uff0c\u4e0d\u50cfFind_S\u7b97\u6cd5/\u5019\u9009\u6d88\u9664\u7b97\u6cd5\u5bf9\u4e2a\u522b\u6570\u636e\u7684\u9519\u8bef\u975e\u5e38\u654f\u611f \u641c\u7d22\u4e0d\u8fdb\u884c\u56de\u6eaf\uff0c\u5bb9\u6613\u6536\u655b\u5230\u5c40\u90e8\u6700\u4f18\u7b54\u6848\uff1f\uff1f"},{"location":"courses/AIML/ML/Decision_Tree/#_2","title":"\u5f52\u7eb3\u504f\u7f6e","text":"<p>\u8fd1\u4f3c\u7684\u5f52\u7eb3\u504f\u7f6e: \u8f83\u77ed\u7684\u6811\u6bd4\u8f83\u957f\u7684\u6811\u4f18\u5148\u3002\u4fe1\u606f\u589e\u76ca\u9ad8\u7684\u5c5e\u6027\u66f4\u9760\u8fd1\u6839\u8282\u70b9\u7684\u6811\u4f18\u5148\u3002</p> ID3\u7b97\u6cd5 \u5019\u9009\u6d88\u9664\u7b97\u6cd5 \u4f18\u9009\u504f\u7f6e\uff0c\u641c\u7d22\u504f\u7f6e\uff0c\u5bf9\u4e8e\u67d0\u79cd\u5047\u8bbe\u80dc\u8fc7\u5176\u4ed6\u5047\u8bbe\u7684\u4f18\u9009\uff0c\u5e76\u4e0d\u8003\u8651\u6700\u7ec8\u53ef\u5217\u4e3e\u7684\u5047\u8bbe\u79cd\u7c7b \u9650\u5b9a\u504f\u7f6e\uff0c\u8bed\u8a00\u504f\u7f6e\uff0c\u9650\u5b9a\u5047\u8bbe\u7684\u7a7a\u95f4"},{"location":"courses/AIML/ML/Decision_Tree/#occams-razor","title":"\u5965\u574e\u59c6\u5243\u5200(Occam's razor)","text":"<p>\u4f18\u5148\u9009\u62e9\u62df\u5408\u6570\u636e\u7684\u6700\u7b80\u5355\u7684\u5047\u8bbe\u3002</p> <p>ID3\u7b97\u6cd5\u4f18\u9009\u8f83\u77ed\u51b3\u7b56\u6811\u7684\u5f52\u7eb3\u504f\u7f6e\u662f\u6cdb\u5316\u7684\u4e00\u79cd\u4f9d\u636e\u3002</p>"},{"location":"courses/AIML/ML/Decision_Tree/#_3","title":"\u5e38\u89c1\u95ee\u9898: \u8fc7\u5ea6\u62df\u5408","text":"<ul> <li>\u566a\u58f0\u3001\u5de7\u5408\u4f1a\u5bfc\u81f4\u51b3\u7b56\u6811\u66f4\u52a0\u590d\u6742\u3002</li> </ul> <p>\u89e3\u51b3\u65b9\u6848</p> <ul> <li> <p>\u53ca\u65e9\u505c\u6b62\u6811\u589e\u957f: \u8f83\u4e3a\u56f0\u96be</p> </li> <li> <p>\u540e\u4fee\u526a\u6cd5: \u91c7\u7528\u4e00\u5b9a\u7684\u51c6\u5219\u8fdb\u884c\u4fee\u526a\u3002</p> </li> </ul> <p>||| |:---:|</p> <ul> <li>\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6\u6cd5</li> </ul> <p>\u8bad\u7ec3\u96c6\u53ef\u80fd\u56e0\u566a\u58f0\u3001\u968f\u5373\u9519\u8bef\u548c\u5de7\u5408\u89c4\u5f8b\u6027\u8bef\u5bfc\uff0c\u4f46\u9a8c\u8bc1\u96c6\u4e0d\u592a\u53ef\u80fd\u8868\u73b0\u51fa\u540c\u6837\u7684\u6ce2\u52a8\u3002\u8fd9\u6837\u6c42\u9a8c\u8bc1\u96c6\u8981\u8db3\u591f\u5927\uff0c\u4e00\u822c\u2153\u7684\u6570\u636e\u7528\u4e8e\u9a8c\u8bc1\u3002</p> <p>\u540e\u4fee\u526a\u6cd5</p> <ul> <li> <p>\u9519\u8bef\u7387\u964d\u4f4e\u4fee\u526a</p> </li> <li> <p>\u89c4\u5219\u540e\u4fee\u526a</p> </li> </ul>"},{"location":"courses/AIML/ML/Intro/","title":"Introduction","text":""},{"location":"courses/AIML/ML/Intro/#_1","title":"\u795e\u7ecf\u7f51\u7edc","text":"<p>ouline</p> <p>\u795e\u7ecf\u7f51\u7edc\u5b9a\u4e49\u4e0e\u6982\u5ff5</p> <p>\u611f\u77e5\u5668\uff08perceptron\uff09</p> <p>\u7ebf\u6027\u5355\u5143\uff08linear unit\uff09</p> <p>\u8bad\u7ec3\u591a\u5c42\u7f51\u7edc\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff08Delta\u6cd5\u5219\uff09</p> <p>ANN\u7684\u8868\u5f81\u80fd\u529b</p> <p>\u5047\u8bbe\u7a7a\u95f4\u641c\u7d22\u7684\u672c\u8d28\u7279\u5f81</p> <p>\u795e\u7ecf\u7f51\u7edc\u8fc7\u5ea6\u62df\u5408\u95ee\u9898</p> <ul> <li>\u795e\u7ecf\u7f51\u7edc\u5b9a\u4e49</li> </ul> <p>\u795e\u7ecf\u7f51\u7edc\u662f\u7531\u5177\u6709\u9002\u5e94\u6027\u7684\u7b80\u5355\u5355\u5143\u7ec4\u6210\u7684\u5e7f\u6cdb\u5e76\u884c\u4e92\u8054\u7684\u7f51\u7edc\uff0c\u5b83\u7684\u7ec4\u6210\u80fd\u591f\u6a21\u62df\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u6240\u4f5c\u51fa\u7684\u53cd\u5e94</p> <ul> <li>\u611f\u77e5\u673a</li> </ul> <p>\u7531\u4e24\u5c42\u795e\u7ecf\u5143\u7ec4\u6210\u8f93\u5165\u5c42\u63a5\u53d7\u5916\u754c\u4fe1\u53f7\u4f20\u9012\u7ed9\u8f93\u51fa\u5c42\uff0c\u8f93\u51fa\u5c42\u662fM-P\u795e\u7ecf\u5143\uff08\u9608\u503c\u903b\u8f91\u5355\u5143\uff09</p> <p>\u5b66\u4e60\u80fd\u529b</p> <ul> <li> <p>\u7ebf\u6027\u53ef\u5206 (\u4e0e\u6216\u975e)\uff0c\u5219\u4e00\u5b9a\u6536\u655b</p> </li> <li> <p>\u975e\u7ebf\u6027\u53ef\u5206\u95ee\u9898 (\u591a\u5c42\u611f\u77e5\u673a)</p> </li> </ul> <ul> <li>\u591a\u5c42\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> </ul> <p>\u5b66\u4e60\uff1a\u6839\u636e\u8bad\u7ec3\u6570\u636e\u6765\u8c03\u6574\u795e\u7ecf\u5143\u4e4b\u95f4\u7684\"\u8fde\u63a5\u6743\"\uff0c\u4ee5\u53ca\u6bcf\u4e2a\u529f\u80fd\u795e\u7ecf\u7684\"\u9608\u503c\"</p> <p>\u8868\u793a\u80fd\u529b: \u53ea\u8981\u5305\u542b\u8db3\u591f\u591a\u795e\u7ecf\u5143\u7684\u9690\u5c42\uff0c\u5c31\u80fd\u4ee5\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\u4efb\u610f\u590d\u6742\u5ea6\u7684\u8fde\u7eed\u51fd\u6570</p> <ul> <li> <p>\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5</p> </li> <li> <p>\u7f13\u89e3\u8fc7\u62df\u5408\u7684\u7b56\u7565</p> </li> </ul> <p>\u7f13\u89e3\u8fc7\u62df\u5408\u7684\u7b56\u7565</p> <p>\u65e9\u505c: \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u82e5\u8bad\u7ec3\u8bef\u5dee\u964d\u4f4e\uff0c\u9a8c\u8bc1\u8bef\u5dee\u589e\u52a0\uff0c\u5c31\u505c\u6b62\u8bad\u7ec3</p> <p>\u6b63\u5219\u5316: \u8bef\u5dee\u76ee\u6807\u51fd\u6570\u589e\u52a0\u4e00\u9879\u63cf\u8ff0\u7f51\u7edc\u590d\u6742\u7a0b\u5ea6\u7684\u90e8\u5206\uff0c\u5982\u8fde\u63a5\u6743\u503c\u548c\u9608\u503c\u7684\u5e73\u65b9\u548c</p> <p>\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\uff1aK-fold \u8f83\u5dee\u9a8c\u8bc1\u3002\u628a\u8bad\u7ec3\u6837\u4f8b\u5206\u6210k\u4efd\uff0c\u5176\u4e2d\u4e00\u4efd\u5206\u522b\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u53e6\u5916\u7684\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6\u3002\u9010\u6b21\u8f6e\u6362\uff0c\u8ba9\u6bcf\u4e00\u4efd\u6570\u636e\u90fd\u6210\u4e3a\u9a8c\u8bc1\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bb0\u5f55\u6bcf\u6b21\u8bad\u7ec3\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230\u6700\u4f73\u8bad\u7ec3\u6548\u679c(\u8bef\u5dee\u6700\u4f4e)\u7684\u8bad\u7ec3\u6b21\u6570i\uff0c\u5c06k\u4e2a\u4e0d\u540c\u7684i\u53d6\u5e73\u5747\uff0c\u4f5c\u4e3a \\(\\overline{i}\\)\u3002\u6700\u540e\u6240\u6709\u7684\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6\uff0c\u8bad\u7ec3\\(\\overline{i}\\)\u540e\u505c\u6b62</p> <p>\u53ef\u4ee5\u5e94\u7528\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\u786e\u5b9a\u9690\u5c42\u5355\u5143\u6570</p>"},{"location":"courses/AIML/ML/Intro/#_2","title":"\u6982\u5ff5\u5b66\u4e60","text":"<p>Outline</p> <p>\u6982\u5ff5\u5b66\u4e60\u5b9a\u4e49</p> <p>\u6982\u5ff5\u5b66\u4e60\u4efb\u52a1</p> <p>\u5f52\u7eb3\u5b66\u4e60\u4e0e\u5f52\u7eb3\u5b66\u4e60\u5047\u8bbe</p> <p>Find-S: \u5bfb\u627e\u6781\u5927\u7279\u6b8a\u5047\u8bbe\uff0c\u504f\u5e8f</p> <p>\u53d8\u5f62\u7a7a\u95f4\u548c\u5019\u9009\u6d88\u9664\u7b97\u6cd5</p> <p>\u5f52\u7eb3\u504f\u7f6e</p>"},{"location":"courses/AIML/ML/Intro/#_3","title":"\u6982\u5ff5\u5b66\u4e60\u5b9a\u4e49","text":"<p>\u662f\u4ece\u6709\u5173\u67d0\u4e2a\u5e03\u5c14\u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u8bad\u7ec3\u6837\u4f8b\u4e2d\u63a8\u65ad\u51fa\u8be5\u5e03\u5c14\u51fd\u6570</p>"},{"location":"courses/AIML/ML/Intro/#_4","title":"\u6982\u5ff5\u5b66\u4e60\u7684\u4efb\u52a1","text":"<p>\u5bfb\u627e\u4e00\u4e2a\u5047\u8bbeh, \u4f7f\u5f97\u5bf9\u6240\u6709\u7684h\uff0c\u90fd\u6709h(x)=c(x)</p>"},{"location":"courses/AIML/ML/Intro/#_5","title":"\u5f52\u7eb3\u5b66\u4e60\u5b9a\u4e49","text":"<p>\u4ece\u7279\u6b8a\u7684\u6837\u4f8b\u5f97\u5230\u666e\u904d\u7684\u89c4\u5f8b</p>"},{"location":"courses/AIML/ML/Intro/#_6","title":"\u5f52\u7eb3\u5b66\u4e60\u5047\u8bbe","text":"<p>\u4efb\u4e00\u5047\u8bbe\uff0c\u5982\u679c\u5728\u8db3\u591f\u5927\u7684\u8bad\u7ec3\u6837\u4f8b\u4e0a\u80fd\u591f\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570(\u4e0e\u8bad\u7ec3\u6570\u636e\u6700\u4f73\u62df\u5408)\uff0c\u5b83\u4e5f\u80fd\u5728\u672a\u89c1\u5b9e\u4f8b\u4e2d\u5f88\u597d\u5730\u903c\u8fd1\u76ee\u6807\u51fd\u6570</p>"},{"location":"courses/AIML/ML/Intro/#find-s","title":"Find-S","text":""},{"location":"courses/AIML/ML/Intro/#_7","title":"\u5019\u9009\u6d88\u9664\u7b97\u6cd5","text":"<p>\u8f93\u51fa\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u5730\u6240\u6709\u5047\u8bbe\u7684\u96c6\u5408</p> <p>\u6b63\u4f8b-\u6781\u5c0f\u4e00\u822c\u5316</p> <p>\u5e94\u7528\uff1a\u5316\u5b66\u8d28\u8c31\u5206\u6790\u3001\u542f\u53d1\u5f0f\u641c\u7d22\u7684\u63a7\u5236\u89c4\u5219</p>"},{"location":"courses/AIML/ML/Intro/#_8","title":"\u53d8\u5f62\u7a7a\u95f4","text":"<p>\u4e0e\u8bad\u7ec3\u6837\u4f8b\u4e00\u81f4\u7684\u6240\u6709\u5047\u8bbe\u7ec4\u6210\u7684\u96c6\u5408\uff0c\u8868\u793a\u4e86\u76ee\u6807\u6982\u5ff5\u7684\u6240\u6709\u5408\u7406\u7684\u53d8\u578b</p> <p>\u88ab\u8868\u793a\u4e3a\u6781\u5927\u4e00\u822c(G)\u548c\u6781\u5927\u7279\u6b8a(S)\u7684\u6210\u5458</p> <p>\u6982\u5ff5\u5b66\u4e60\u7684\u6700\u4f18\u67e5\u8be2\u7b56\u7565\uff1a\u662f\u4ea7\u751f\u5b9e\u4f8b\u4ee5\u6ee1\u8db3\u5f53\u524d\u53d8\u578b\u7a7a\u95f4\u4e2d\u5927\u7ea6\u534a\u6570\u7684\u5047\u8bbe\u3002\u8fd9\u6837\uff0c\u53d8\u578b\u7a7a\u95f4\u7684\u5927\u5c0f\u53ef\u4ee5\u5728\u9047\u5230\u6bcf\u4e2a\u65b0\u6837\u4f8b\u65f6\u51cf\u534a</p>"},{"location":"courses/AIML/ML/Intro/#_9","title":"\u5f52\u7eb3\u504f\u7f6e","text":"<p>\u5f52\u7eb3\u5b66\u4e60\u9700\u8981\u7684\u9884\u5148\u5047\u5b9a</p>"},{"location":"courses/AIML/ML/Intro/#_10","title":"\u7b97\u6cd5\u8bc4\u4f30","text":"<p>outline</p> <p>\u5047\u8bbe\u7684\u8bc4\u4f30\u3001\u4e24\u4e2a\u5047\u8bbe\u7cbe\u5ea6\u7684\u6bd4\u8f83\u3001\u4e24\u4e2a\u5b66\u4e60\u7b97\u6cd5\u7cbe\u5ea6\u6bd4\u8f83</p> <p>\u6837\u672c\u9519\u8bef\u7387\u3001\u771f\u5b9e\u9519\u8bef\u7387</p> <p>\u4ea4\u53c9\u9a8c\u8bc1\u4e0et\u914d\u5bf9\u6d4b\u8bd5</p>"},{"location":"courses/AIML/ML/Intro/#_11","title":"\u8d1d\u53f6\u65af\u5b66\u4e60","text":"<p>success</p> <p>\u8d1d\u53f6\u65af\u7406\u8bba\u6982\u5ff5\u5b9a\u4e49</p> <p>\u6781\u5927\u4f3c\u7136\u5047\u8bbe\uff08ML\uff09\u548c\u6781\u5927\u540e\u9a8c\u6982\u7387\u5047\u8bbe\uff08MAP\uff09</p> <p>\u8d1d\u53f6\u65af\u7406\u8bba\u5bf9\u5176\u4ed6\u5b66\u4e60\u7b97\u6cd5\u7684\u89e3\u91ca</p> <p>\u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668</p> <p>Gibbs\u7b97\u6cd5 </p> <p>\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668</p>"},{"location":"courses/AIML/ML/Intro/#_12","title":"\u8d1d\u53f6\u65af\u7406\u8bba","text":"<p>\u8d1d\u53f6\u65af\u7406\u8bba\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u5047\u8bbe\u6982\u7387\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5047\u8bbe\u7684\u5148\u9a8c\u6982\u7387\u3001\u7ed9\u5b9a\u5047\u8bbe\u4e0b\u89c2\u5bdf\u5230\u4e0d\u540c\u6570\u636e\u7684\u6982\u7387\u4ee5\u53ca\u89c2\u5bdf\u5230\u7684\u6570\u636e\u672c\u8eab</p>"},{"location":"courses/Data_Structure%26Algorithm/","title":"Index","text":""},{"location":"courses/Data_Structure%26Algorithm/#data-structure-and-algorithm","title":"Data Structure and Algorithm","text":"<p>Reference</p> <ul> <li>Data structures and algorithm analysis in C++, Mark Allen Weiss</li> </ul>"},{"location":"courses/Data_Structure%26Algorithm/#outline","title":"Outline","text":""},{"location":"courses/Data_Structure%26Algorithm/#algorithm-analysis","title":"Algorithm Analysis","text":""},{"location":"courses/Data_Structure%26Algorithm/#lists-stacks-and-queues","title":"Lists, Stacks and Queues","text":""},{"location":"courses/Data_Structure%26Algorithm/#trees","title":"Trees","text":"<ul> <li>BST</li> </ul> <p>insert time cost: \\(O(\\log n)=O(h)\\)</p> <p>remove time cost: \\(O(h)\\)</p> <ul> <li>AVL tree</li> </ul>"},{"location":"courses/Data_Structure%26Algorithm/#hashing","title":"Hashing","text":""},{"location":"courses/Data_Structure%26Algorithm/#priority-queuesheaps","title":"Priority Queues(Heaps)","text":"<ul> <li>Heap-Order Properties</li> </ul>"},{"location":"courses/Data_Structure%26Algorithm/#sorting","title":"Sorting","text":""},{"location":"courses/Data_Structure%26Algorithm/#the-disjoint-sets-class","title":"The disjoint Sets Class","text":""},{"location":"courses/Data_Structure%26Algorithm/#graph-algorithm","title":"Graph Algorithm","text":""},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/","title":"Algorithm Analysis","text":""},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/#basic-notations","title":"Basic Notations","text":"<p>We must have some notation to better describe our algorithm.</p> <p>Definitions</p> <p>(i) We call \\(T(N)=O(f(N))\\), if there exists positive constants \\(c\\), \\(n_0\\), such that </p> \\[ T(N)\\leq cf(N), \\quad  \\forall N\\geq n_0 \\] <p>(ii) We call \\(T(N)=\\Omega(g(N))\\), if there exists positive constants \\(c\\), \\(n_0\\), such that</p> \\[ T(N)\\geq cg(N), \\quad  \\forall N\\geq n_0 \\] <p>(iii) we call \\(T(N)=\\Theta(h(N))\\), if and only if \\(T(N)=O(h(N))\\) and \\(T(N)=\\Omega(h(N))\\).</p> <p>(iv) we call \\(T(N)=o(p(N))\\), if forall constant \\(c&gt;0\\), there exists \\(n_0\\) such that </p> \\[ T(N)&lt;cp(N),\\quad \\forall N&gt;n_0 \\] <p>Or less formally, \\(T(N)=o(p(N))\\) if \\(T(N)=O(p(N))\\) and \\(T(N)\\neq \\Theta(p(N))\\). </p> <p>The following rules are usually really useful for algorithm analysis.</p> <p>Rules</p> <p>(i) If \\(T_1(N)=O(f(N))\\) and \\(T_2(N)=O(g(N))\\), then </p> \\[ \\begin{align*} T_1(N)+T_2(N)&amp;=O(f(N)+g(N))\\quad \\text{(steps summation)}\\\\ T_1(N)\\times T_2(N)&amp;=O(f(N)\\times g(N)) \\quad \\text{(steps multiplication)}. \\end{align*} \\] <p>(ii) If \\(T(N)\\) is a polynomial of degree \\(k\\), then </p> \\[ T(N)=\\Theta(N^k). \\] <p>(Proved by find \\(c_1\\), \\(c_2\\) to be its upper and lower bound)</p> <p>(iii) \\(\\log^kN=O(N)\\), \\(\\forall k\\).</p>"},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/#recursions","title":"Recursions","text":"<p>We take Merge Sort as an example.</p> Merge<pre><code>Merge(A, p, q, r)\n    n1 = q - p + 1\n    n2 = r - q\n    L[1...(n1 + 1)]\n    R[1...(n2 + 1)]\n    for i = 1 to n1\n        L[i] = A[p + i - 1]\n    for j = 1 to n2\n        R[j] = A[q + j]\n    L(n1 + 1) = R(n2 + 1) = +inf\n    for k = p to r\n        if L[i] &lt;= R[j]\n            A[k] = L[i]\n            i = i + 1\n        else\n            A[k] = R[j]\n            j = j + 1\n</code></pre> <p>whose time cost is \\(O(N)\\), where \\(N=r+p-1\\).</p> MergeSort<pre><code>MergeSort(A, p, r)\n    if p &lt; r\n        q = (p + r) / 2\n        MergeSort(A, p, q)\n        MergeSort(A, q + 1, r)\n        Merge(A, p, q, r)\n</code></pre> <p>Assume the time cost of MergeSort is \\(T(N)\\), where \\(N=r+p-1\\), then we have</p> \\[ T(N)=\\begin{cases}O(1),\\quad N=1\\\\ 2T(N/2)+O(N),\\quad N&gt;1\\end{cases} \\] <p>It is apparent to draw a graph of tree for this recursion, get its time cost and then prove it using Induction.</p> <p>Generalize the above thoughts and we have the following theorem.</p> <p></p> <p>Master Theorem</p> <p>Assume \\(T(N)\\) is the time cost for a recursive algorithm, and satisfies</p> \\[ T(N)=\\begin{cases}O(1),\\quad N=1\\\\ aT(N/b)+f(N),\\quad N&gt;1\\end{cases} \\] <p>where \\(a\\geq 1\\), \\(b&gt;1\\) and \\(f(N)&gt;0\\) for sufficient large \\(N\\). </p> <p>(i) If there exists \\(\\varepsilon&gt;0\\) such that \\(f(N)=O(N^{\\log_b{a-\\varepsilon}})\\), then \\(T(N)=\\Theta(N^{\\log_ba})\\).</p> <p>(ii) If \\(f(N)=\\Theta(N^{\\log_ba})\\), then \\(T(N)=\\Theta(N^{\\log_b a}\\log N)\\).</p> <p>(iii) If there exists \\(\\varepsilon&gt;0\\), such that \\(f(N)=\\Omega(N^{\\log_b a+\\varepsilon})\\), and for sufficient large \\(N\\), \\(af(N/b)\\leq cf(N)\\), \\(c&lt;1\\), then \\(T(N)=\\Theta(f(N))\\).</p> <p>The above \\(\\varepsilon\\) in (i) and (iii) denotes the relationship on polynomial. And the latter condition in (iii) of the above theorem is called regularity condition.</p> <p>The focus is to compare the relative size between \\(N^{\\log_b a}\\) and \\(f(N)\\). If the former is larger, then time cost is determined by \\(T(N)=aT(N/b)\\), which is \\(O(N^{\\log_b a})\\), meaning the bifurcation of the tree is large enough such that the time cost of each layer can be ignored. However, on the other hand, if the latter is larger, then time cost is determined by \\(f(N)\\),  which is \\(O(f(N))\\).</p> <p>Q1. Solve for time cost \\(T(n)\\) in the following situation</p> <p>(i) \\(T(n)=9T(n/3)+n\\)</p> <p>(ii) \\(T(n)=T(2n/3)+1\\)</p> <p>(iii) \\(T(n)=3T(n/4)+n\\lg n\\)</p> Answer for (i)Answer for (ii)Answer for (iii) \\[ n^{\\log_b a}=n^{\\log_3 9}=n^2&gt;n \\] <p>so </p> \\[ f(n)=n=O(n^{\\log_b a}) \\] <p>which is (i) of Master Theorem. So \\(T(n)=\\Theta(n^2)\\).</p> \\[ n^{\\log_b a}=n^{\\log_{3/2} 1}=n^0=1 \\] <p>so</p> \\[ f(n)=1=\\Theta(1)=\\Theta(n^{\\log_b a}) \\] <p>which is (ii) of Master Theorem. So \\(T(n)=\\Theta(\\log n)\\).</p> \\[ n^{\\log_b a}=n^{\\log_4 3}&lt;n^1&lt;n\\lg n \\] <p>which does not satisfy (i), (ii) of Master Theorem, so we need to confirm if it satisfies the regularity condition of (iii) in Master Theorem. Notice that</p> \\[ af(n/b)=3{\\frac{n}{4}}\\lg \\left({\\frac{n}{4}}\\right) \\] <p>So if we choose \\(c=3/4\\), then </p> \\[ af(n/b)\\leq cf(n), \\forall n\\geq 1 \\] <p>So it satisfies (iii) of Master Theorem. Thus time cost \\(T(n)=\\Theta(n\\log n)\\)</p> <p>Notice that there exists gap between (i) and (ii) in Master Theorem, cause some function may not satisfy \\(\\varepsilon\\). There also exsits a gap  between (i) and (ii) in Master Theorem, cause the regularity condition.</p> <p> Q2. Solve for time cost \\(T(n)\\) in the following situation <p>(i) \\(T(n)=2T(n/2)+n\\lg n\\)</p> <p>(ii) \\(T(n)=T(n/2)+n(2-\\cos n\\pi)\\)</p> Answer <p>The above two could not be solved by Master Theorem. We have to use recursive tree.</p>"},{"location":"courses/Data_Structure%26Algorithm/AlgoAna/#divide-conquer","title":"Divide &amp; Conquer","text":"<ul> <li>\u4e8c\u5206\u641c\u7d22</li> </ul> <p>C++<pre><code>BinarySearch(A, x)\n    left, right = 0, len(A)-1\n\n    while left&lt;=right:\n        mid = (left+right)/2\n        if(x==A[mid]):\n            return mid;\n        else if (A[mid]&lt;x):\n            left = mid+1;\n        else if (A[mid]&gt;x):\n            right = mid-1;\n\n    return -1;\n</code></pre> time cost \\(O(log n)\\), space cost \\(O(1)\\).</p> C++<pre><code>BinarySearch(A, x, left, right)\n    if left&gt; right:\n        return -1;\n\n    mid = (left+right)/2;\n\n    if(A[mid]==x):\n        return mid;\n    else if (A[mid]&lt;x):\n        return BinarySearch(A,x, mid+1,right);\n    else if (A[mid]&gt;x):\n        return BinarySearch(A,x, left, mid-1);\n</code></pre> <p>time cost \\(O(log n)\\), space cost \\(O(log n)\\).</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/","title":"The disjoint Sets Class","text":""},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-size","title":"Union-by-Size","text":"<p>change size when union</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-height","title":"Union-by-Height","text":"<p>does not need to change height when union, tree height only changes when two trees are of the same size,</p> <p>find is dynamic, let the found node points directly to the root node. However, we need to change the height.</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#union-by-rank","title":"Union-by-Rank","text":"<p>After find(), we do not change the height. So the height bacomes the rank.</p>"},{"location":"courses/Data_Structure%26Algorithm/Dis_Sets_Class/#generate-maze","title":"Generate Maze","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/","title":"Graph Algorithm","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/#preliminary-dynamic-programming","title":"Preliminary: Dynamic Programming","text":"<p>(Note: content of this part is different from dynamic programming in combination optimization)</p> <ul> <li>Longest common subsequence, LCS</li> </ul> <p>input: \\(x=\\{x_i\\}_{i=1}^m\\), \\(y=\\{y_i\\}_{i=1}^n\\),</p> <p>output: their LCS.</p> <p>For example, if we have two sequences </p> \\[ \\begin{cases} x=\\{A,B,C,B,D,A,B\\}\\\\ y=\\{B,D,C,A,B,A\\} \\end{cases} \\] <p>then LCS\\((x,y)=\\{BDAB,BCAB,BCBA\\}\\).</p> <ul> <li>solve the length of LCS.</li> </ul> Proof <ul> <li>\\(x[i]=y[j]\\), if \\(C[i,j]=k\\), denote one element \\(z[1,\\cdots,k]\\in\\) LCS\\([i,j]\\). then \\(z[k]=x[i]=y[j]\\). So \\(z[1,\\cdots,k-1]\\in LCS[i-1,j-1]\\).</li> </ul> <p>Assume \\(w\\in\\) LCS\\([i-1.j-1]\\) and \\(|w|&gt;k-1\\), then connect \\(w\\) and \\(z[k]\\), \\(w+z[k]\\) satisfies</p> \\[ \\begin{cases} w+z[k]\\in \\text{LCS}[i,j]\\\\ \\left|w+z[k]\\right|&gt;k \\end{cases} \\] <p>contradicts!</p> <ul> <li>optimal substructure</li> </ul> <p>\\(\\forall z[1,\\cdots,k]\\in\\) LCS\\([m,n]\\), then \\(z[1,\\cdots,s]\\), \\(s&lt; k\\) must be LCS of their prefix.</p> Natural Version for LCS<pre><code>LCS(x,y,i,j)\n    if(i==-1||j==-1)\n        return 0\n    else if (x[i]==y[j])\n        c[i,j]=LCS(x,y,i-1.j-1)+1\n    else \n        c[i,j]=max(LCS(x,y,i,j-1),LCS(x,y,i-1.j))\n    return c[i,j]\n</code></pre> <ul> <li>cut off </li> </ul> <p>write a note of LCS that has been calculated. So next time we encounter a same item, directly use it. In fact we use a 2 dimension array.</p> Better Version for LCS<pre><code>LCS(x,y,i,j)\n    if (c[i,j]==nil)\n        if (x[i]==y[j])\n            c[i,j]=LCS(x,y,i-1.j-1)+1\n        else \n            c[i,j]=max(LCS(x,y,i,j-1),LCS(x,y,i-1.j))\n    return c[i,j]\n</code></pre> <p>time cost \\(O(mn)\\).</p> <p>\u81ea\u9876\u5411\u4e0b top down.</p> <p>\u81ea\u5e95\u5411\u4e0a bottom up.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#gragh-theory","title":"Gragh theory","text":""},{"location":"courses/Data_Structure%26Algorithm/Graph/#definitions","title":"Definitions","text":"<p>A graph \\(G=(V,E)\\) consists a set of vertices, \\(V\\) and a set of edges \\(E\\). for \\(u,w\\in V\\), \\((u,w)\\in E\\).</p> <p>If \\((u,w)\\neq(w,u)\\), then we call the graph is directed, or digraph. Otherwise we call the graph undirected.</p> <p>We call \\(u,w\\in V\\) are adjacent iff \\((u,w)\\in E\\). A map from \\(E\\) to \\(\\mathbb{R}\\) is called weight, or cost.</p> <p>A path is a sequence of vertices \\(\\{w_1,w_2,\\cdots,w_N\\}\\), such that \\((w_i,w_{i+1})\\in E\\), \\(\\forall i\\in [1,N-1]\\). When there is not weight defined, the length of the path is \\(N-1\\).</p> <p>\u81ea\u73af\u8def\u5f84(Loop). </p> <p>A simple path is a path such that all vertices are distinct, except that the first and the last could be the same.</p> <p>A cycle path(\u5faa\u73af\u8def\u5f84) in a directed graph is a path of length at least 1 such that \\(w_1=w_N\\).  </p> <p>A directed graph is acyclic if it has no cycles.</p> <p>Directed Acyclic Graph(\u6709\u5411\u65e0\u73af\u56fe). </p> <p>An undirected graph is connected, if \\(\\forall u,w\\in V\\), \\((u,w)\\in E\\). A directed graph with this property is called strongly connected. </p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#representation-of-graph","title":"Representation of Graph","text":"<ul> <li>Use 2 dimension array. Adjacency matrix.</li> </ul> <p>If \\(|E|= \\Theta(|V|^2)\\), then the graph is dense.</p> <p>If \\(|E|= \\Theta(|V|)\\), then the graph is sparse.</p> <ul> <li>Use linked-list. Adjacency list.</li> </ul> <p>For each vertex, we keep a list of all adjacent vertices. </p> <p>Space cost \\(O(|V|+|E|)\\).</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#topological-sort","title":"Topological Sort","text":"<p>Definition of Topological Sort</p> <ul> <li>\u62d3\u6251\u5e8f | Topological Sort</li> </ul> <p>\\(\\forall v_i,v_j\\in V\\), we say \\(v_i\\leq v_j\\), if there exists </p> <p>(i) a path from \\(v_i\\) to \\(v_j\\)</p> <p>(ii) \\(v_i\\) is before \\(v_j\\)</p> <ul> <li>\u62d3\u6251\u6392\u5e8f | Topological Sorting</li> </ul> <p>list \\(\\{v_i\\}_{i-1}^n\\) into a queue</p> \\[ v'_1, v'_2,\\cdots, v'_n \\] <p>such that, \\(\\forall v'_i\\neq v'_j\\), we have</p> <p>(i) \\(v_i\\leq v_j\\)</p> <p>(ii) there exists no path from \\(v_i\\) to \\(v_j\\).</p> <ul> <li>\u5165\u5ea6 | indegree</li> </ul> <p>there must exist a node with indegree = 0.</p> Version 1 for topological sort<pre><code>topsort()\n{\n    for(i=0;i&lt;n_vertex;i++)\n        v = FindNextVertexIndegreeZero() // actually we could only do this for the first time.\n        v.tp=i;\n        for each w adjacent to v\n            w.indegree --;\n}\n</code></pre> Better version for topological sort<pre><code>void Graph::topsort()\n{\n    Queue&lt;vertex&gt; q;\n    int counter = 0;\n    q.makeEmpty();\n    for each vertex in V:\n        if v.indegree==0:\n            q.enqueue(v); // O(|V|)\n\n    while(!q.empty()){\n        vertex v= q.dequeue();\n        v.tp=++counter;\n        for each vertex w adjacent to v:\n            if(--w.indegree==0):\n                q.enqueue(w); \n    } // O(|E|+|V|)\n\n    if(counter != NUM_VERTICES)\n        throw CYclwFoundException();\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#shortest-path-algorithm","title":"Shortest-Path Algorithm","text":"<p>Input is a weighted Graph.</p> <p>Assume weight is positive.</p> <p>Theorem</p> <p>If \\(P_{(u,v)}\\) is the shortest path between u and v, then \\(\\forall x,y\\in P_{(u,v)}\\), \\(P_{(x,y)}\\subset P_{(u,v)}\\) is the shortest path bwtween x and y.</p> <p>Or triangle inequality. forall \\(x,y,z \\in V\\)</p> \\[ \\delta(x,y)\\leq \\delta(x,z)+\\delta(z,y) \\] Proof <p>By contradiction. If there exists another path \\(P'_{(x,y)}\\neq P_{(x,y)}\\) and </p> \\[ w(P'_{(x,y)})&lt; w(P_{(x,y)}) \\] <p>then </p> \\[ \\begin{align*} w{(P'_{(u,v)})} &amp;= w{(P_{(u,x)})} + w{(P'_{(x,y)})} + w{(P_{(y,v)})}\\\\ &amp;&lt; w{(P_{(u,x)})} + w{(P_{(x,y)})} + w{(P_{(y,v)})}\\\\ &amp;=w{(P_{(u,v)})} \\end{align*} \\] <p>that is, we find a path that is smaller than the shortest path, which contradicts!</p> <p>We also call the above problem Single-Source shortest path.</p> <p>The method is known as Dijlstra's Algorithm.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#unweighted-graph","title":"Unweighted graph","text":"<p>This is actually BFS. Unweighted graph<pre><code>void Graph::unweighted(Vertex s)\n{   \n    Queue&lt;Vertex&gt; q;\n    for each Vertex v\n    {\n        v.dist = INFTY;\n        v.known = false;\n    }\n\n    s.dist=0;\n    q.enqueue(s);\n\n    while(!q.isEmpty())\n    {\n        Vertex v = q.dequeue();\n\n        for each Vertex w adjacent to v:\n            if(w.dist == INFTY):\n            {\n                w.dist = v.dist + 1;\n                w.path = v;\n                q.enqueue(w);\n            }\n    }\n}\n</code></pre></p> <p>Cost: \\(O(|E|+|V|)\\)</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#weighted-graph","title":"Weighted Graph","text":"<p>This time, we cannot say that the neighberhood is the shortest, which may be longer than some path with more vertices but less weight cost.</p> Weighted Graph<pre><code>s.d=0;\nfor each v in V-{s}\n    v.d=INFTY;\nQueue=V;\nwhile(!q.isEmpty()){\n    u = q.pop_min(); // smallest heap\n    for each v adjacent to u:\n        if v.d&gt;u.d+w(u,v):\n            v.d = u.d+w(u,v);\n            v.p = u;\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#all-pair-shortest-path","title":"All-pair shortest path","text":"<p>use adjacent matrix \\(A=(a_{ij})_{n\\times n}\\), with \\(a_{ij}\\) denotes the weight from \\(v_i\\) to \\(v_j\\) and \\(a_{ii}=0\\).</p> <p>use \\(d_{ij}^{(m)}\\) to denote the shortest path weight from \\(v_i\\) to \\(v_j\\) with passing \\(m\\) edges.</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#graph-with-negative-costs","title":"Graph with negative costs","text":"C++<pre><code>Bellman-Fond Algorithm\n</code></pre> <p>Theorem of Bellman-Fond Algorithm</p> <p>v.d is the shortest path from s to v.</p> Proof <p>v.d is homotonically decreasing and has lower bound.</p> \\[ v.d \\geq \\delta(s,v) \\]"},{"location":"courses/Data_Structure%26Algorithm/Graph/#bounded-graph","title":"bounded graph","text":"\\[ x_j-x_i\\leq w_{ij} \\leftrightarrow v_i\\overset{w_{ij}}{\\rightarrow}v_j \\]"},{"location":"courses/Data_Structure%26Algorithm/Graph/#solve-shortest-path-between-all-vertices","title":"solve shortest path between all vertices","text":"<p>Assume G is a dense graph.</p> <p>\\(a_{ij}\\) denotes the weight form i to j.</p> <p>\\(d_{ij}^{(m)}\\) denotes the shortest path with length \\(m\\) from i to j.</p> <p>(i) \\(d_{ij}^{(0)}=\\begin{cases}0,\\quad i=j\\\\ \\infty,\\quad \\text{else}\\end{cases}\\)</p> <p>(ii) \\(d_{ij}^{(k)}\\)</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#floyd-worshall-algorithm","title":"FLoyd-Worshall Algorithm","text":"<p>define \\(c_{ij}^{(n)}\\) the shortest path with length at most \\(n-1\\) from i to j, and only passes vertices \\(\\{1,2,\\cdots, k\\}\\)</p> \\[ c_{ij}^{(k)} = \\min\\{c_{ij}^{(k-1)}, c_{ik}^{(k-1)} + c_{kj}^{(k-1)}\\} \\]"},{"location":"courses/Data_Structure%26Algorithm/Graph/#remapping","title":"Remapping","text":"\\[ h:V\\mapsto \\mathbb{R} \\] <p>then </p> \\[ w_h(u,v):=w(u,v) + h(u) - h(v),\\quad \\forall (u,v)\\in E \\] <p>Theorem</p> <p>\\(\\forall u,v \\in V\\), \\(P_{uv}\\) is the path, then </p> \\[ w_h(u,v)-w(u,v)=Const = h(u)-h(v) \\] <p>Corollary</p> <p>\\delta_h(u,v) = \\delta(u,v)+h(u)-h(v)</p> <p>How to get \\(h\\)?</p> <p>notice that \\(w_h(u,v)=w(u,v)+ h(u)-h(v)\\geq 0 \\Rightarrow\\)</p> \\[ h(v)-h(u)\\leq w(u,v) \\] <p>which is a difference bounded system. So we can use Bellman-Fond Algorithm. Then the cost of weight is all positive, so we can use Dijkstra Algorithm. So here comes Johnsen Algorithm.</p> <ul> <li>Johnsen Algorithm</li> </ul> <p>(i) find \\(h:V\\rightarrow \\mathb{R}\\), such that </p> \\[ w_h(u,v)=w(u,v)+ h(u)-h(v)\\geq 0 ,\\quad \\forall (u,v)\\in E \\] <p>use Bellman-Fond Algorithm to solve \\(h\\).</p> <p>(ii) use w_h to run Dijkstra Algorithm, get \\(\\delta_h(u,v)\\).</p> <p>(iii) </p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#minimum-spanning-trees","title":"Minimum spanning trees","text":"<p>Theorem</p> <p>delete one edge from a MST, denoted by T1, T2, and the subgraph of G denoted by G1, G2. Then T1, T2 is the MST of G1, G2 respectively.</p> Proof <p>Assume we delete (u,v), then</p> <p>w(T) = w(u,v) + w(T1)+w(T2)</p> <p>If we find another tree T1', s.t. w(T1')&lt;w(T1)</p> <p>then T'=T1'\\cup (u,v)\\cup T2</p> <p>which satisfies w(T')&lt;w(T), contracdicts!</p> <p>Convex!</p> <p>Theorem</p> <p>Assume T is the MST of G(V,E), A\\subset V, assume (u,v)\\in E is the smallest edge which can connect A and V\\ A, then </p> <p>(u,v)\\in T</p> Proof <p>Assume (u,v)\\nin T</p>"},{"location":"courses/Data_Structure%26Algorithm/Graph/#prims-algorithm","title":"Prim's Algorithm","text":"Text Only<pre><code>Q=V\nfor each v in V:\n    v-&gt;key = \\infty // initialization\n\ns\\in V, s-&gt;key = 0  root\nwhile(Q\\neq empty)\n    u = \\min_v Q key\n    for v\\in Adj[u]: // update neigborhood\n        if v\\in Q (not \u5f39\u51fa) and w(u,v)&lt;v-&gt;key\n            v-&gt;key = w(u,v) // update\n            v-&gt;parent = u\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/Heaps/","title":"Heaps","text":""},{"location":"courses/Data_Structure%26Algorithm/outline/","title":"\u8003\u8bd5\u7eb2\u8981","text":"<p>\u6ca1\u6709 C++ \u4ee3\u7801\uff0c\u53ea\u6709\u7b97\u6cd5\u5206\u6790\u548c\u6570\u636e\u7ed3\u6784\u7684\u7406\u8bba\u77e5\u8bc6\uff0c\u6709\u4f2a\u4ee3\u7801\u3002</p> <ol> <li>\u7b97\u6cd5\u5206\u6790\uff0c\\(O\\) \u548c \\(\\varTheta\\) \u7b49\u6e10\u8fd1\u5206\u6790\u8bb0\u53f7\u7684\u4f7f\u7528\u3002\u5206\u6cbb\u7b56\u7565\uff0c\u4e3b\u5b9a\u7406\u548c\u5bf9\u5e94\u7684\u5f52\u7eb3\u6cd5\u8bc1\u660e\u3002</li> <li>\u57fa\u672c\u6570\u636e\u7ed3\u6784\uff0c\u6570\u7ec4\u3001\u94fe\u8868\u3001\u6808\u3001\u961f\u5217\u3001\u6811\u3001\u56fe\u7b49\u7684\u5b9a\u4e49\u548c\u5b9e\u73b0\u3002</li> <li>\u4e8c\u53c9\u641c\u7d22\u6811\u548c AVL \u6811\u7684\u5b9a\u4e49\uff0c\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\u7684\u590d\u6742\u5ea6\u5206\u6790\u3002(\u4e0d\u8981\u6c42\u4f38\u5c55\u6811\u7b49\u9ad8\u7ea7\u7ed3\u6784)</li> <li>\u6700\u5927\u5806\u3001\u6700\u5c0f\u5806\u548c\u5806\u6392\u5e8f\u7b97\u6cd5\u7684\u5b9a\u4e49\u548c\u5b9e\u73b0\u3002(\u4e0d\u8981\u6c42\u5de6\u5806\u3001\u659c\u5806\u7b49\u9ad8\u7ea7\u7ed3\u6784)</li> <li>\u6392\u5e8f\u7b97\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u590d\u6742\u5ea6\u5206\u6790\uff0c\u5305\u62ec\u63d2\u5165\u6392\u5e8f\u3001\u5f52\u5e76\u6392\u5e8f\u3001\u5feb\u901f\u6392\u5e8f\u3001\u5806\u6392\u5e8f\u7b49\u3002</li> <li>\u5e76\u67e5\u96c6 (union-find) \u7684\u5b9a\u4e49\u548c\u5b9e\u73b0\u3002\u6ce8\u610f\u5e76\u67e5\u662f\u540c\u6b65\u53d1\u751f\u7684\u3002\u8981\u6c42\u5e76\u67e5\u96c6\u7684\u6548\u7387\u63d0\u5347\u6280\u5de7\u3002</li> <li>\u56fe\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u90bb\u63a5\u77e9\u9635\u548c\u90bb\u63a5\u8868\u7684\u4f18\u7f3a\u70b9\u3002\u56fe\u7684\u904d\u5386\u7b97\u6cd5\uff0c\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u3002</li> <li>\u6700\u5c0f\u751f\u6210\u6811\u7b97\u6cd5\uff0c\u5bf9\u7b97\u6cd5\u7684\u7406\u89e3\uff0c\u5b9e\u73b0\u548c\u8bc1\u660e\u3002</li> <li>\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\uff0c\u5bf9\u7b97\u6cd5\u7684\u7406\u89e3\u3002\u8981\u6c42\u5355\u6e90\u6700\u77ed\u8def\u5f84\uff0c\u6b63\u8fb9\u6743\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u3002</li> <li>\u52a8\u6001\u89c4\u5212\u7b97\u6cd5\uff0c\u8981\u6c42\u7406\u89e3\uff0c\u8bc1\u660e\u548c\u5b9e\u73b0\u3002Quick Reply</li> </ol>"},{"location":"courses/Data_Structure%26Algorithm/outline/#recitation","title":"recitation","text":"<ul> <li> <p>\u5f52\u5e76\u6392\u5e8f\u7684\u4e24\u4e2a\u51fd\u6570</p> </li> <li> <p>\u62f7\u8d1d\u6784\u9020\u3001\u8d4b\u503c\u8fd0\u7b97\u3001\u79fb\u52a8\u6784\u9020</p> </li> <li> <p>list \u7684\u5217\u8868\u6784\u9020\uff0cfind\uff0cinsert</p> </li> <li> <p>\u8ba1\u7b97\u8868\u8fbe\u5f0f\u7684\u4e2d\u7f00\u3001\u540e\u7f00\u8868\u8fbe\uff0c\u4e92\u76f8\u8f6c\u6362\uff0c\u540e\u7f00\u8868\u8fbe\u5f0f\u8fdb\u884c\u8ba1\u7b97</p> </li> </ul> C++<pre><code>string infix2postfix(string infix){\n    std::stack&lt;char&gt; operators;\n    string postfix;\n\n    for(char c:infix){\n        if(isnum(c))postfix+=c;\n        else if (c=='(')operators.push(c);\n        else if (c==')'){\n            while(!operators.empty() &amp;&amp; operators.top()!='('){\n                prefix = operators.pop();\n            }\n            operators.pop();\n        }\n        else if(isoperator(c)){\n            while(!operators.empty() &amp;&amp; operators.top()!='(' &amp;&amp; precedence(operators.top())&gt;precedence(c))postfix+=operators.pop();// \u9ad8\u4f18\u5148\u7ea7\u7684\u6808\u5185\u5168\u90e8\u5f39\u51fa\n        }\n        operators.push(c);\n    }\n}\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/","title":"Sorting","text":"<ul> <li> <p>\u6548\u7387 | Efficiency</p> </li> <li> <p>\u539f\u5730\u6027 | In Place</p> </li> <li> <p>\u7a33\u5b9a\u6027 | Stability</p> </li> </ul> <p>A sorting algorithm is stable if elements with equal elements are left in the same order as they occur in the input.</p> <p>A stable algorithm can be suitable for Multiple targets.</p>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#insertion-sort","title":"Insertion Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#shell-sort","title":"Shell Sort","text":"<p>\\(h_t\\) = \\(\\lfloor N/2 \\rfloor\\), \\(h_k\\) = \\(\\lfloor h_{k+1}/2 \\rfloor\\).</p> <p>with Worst case \\(O(N^2)\\)</p> <p>Now we have better selection with worst case \\(O(N^{7/6})\\) with </p> \\[ h_k= \\begin{cases} 9\\cdot 4^i - 9\\cdot 2^i+1, \\quad i\\mod 2 == 0\\\\ 4^{i+1}-6 \\cdot 2^i+1, \\quad i\\mod 2 == 1 \\end{cases} \\]"},{"location":"courses/Data_Structure%26Algorithm/sorting/#heap-sort","title":"Heap Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#merge-sort","title":"Merge Sort","text":""},{"location":"courses/Data_Structure%26Algorithm/sorting/#quick-sort","title":"Quick Sort","text":"<ul> <li> <p>Picking the Pivot</p> </li> <li> <p>Partition</p> </li> </ul> Partition<pre><code>Partition(A, p, q)\n    x=A[p]\n    i=p+1\n    j=q\n    while(i!=j)\n        while(A[j]&gt;=x &amp;&amp; i&lt;j)\n            j-- // put larger element in place \n        while(A[i]&lt;=x &amp;&amp; i&lt;j)\n            i++ // put smaller element in place \n        if(i&lt;j)\n            swap(A[i], A[j])\n    swap(A[p], A[i])\n</code></pre> Another Partition<pre><code>Partition(A, p, q)\n    x=A[p]\n    i=p\n    for j=p+1 to q:\n        if(A[j]&lt;=x)\n            i=i+1\n            swap(A[i],A[j])\n    A[p]=A[i]\n    return i\n</code></pre> QuickSort<pre><code>QuickSort(A,p,q)\n    if(p&lt;q)\n        r=partition(A,p,q)\n        QuickSort(A,p,r-1)\n        QuickSort(A,r+1,q)\n</code></pre> <p>Time cost:</p> <ul> <li>Ordered sequence </li> </ul> \\[ \\begin{align*} T(n)=T(1)+T(n-1)+\\Theta(n) \\end{align*} \\] <ul> <li>Best sequence</li> </ul> \\[ T(n)=2T(n/2)+\\Theta(n) \\] <ul> <li>not Bad or Good</li> </ul> <p>Every Partition gives 1:9 sequence</p>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#randomize-pivot","title":"Randomize pivot","text":"Text Only<pre><code>Randomized_Partition(A,p,q)\n    i=random(p,q)\n    swap(A[p],A[i])\n    return Partition(A,p,q)\n</code></pre> Text Only<pre><code>randomized_quicksort(A,p,q)\n    if(p&lt;q)\n        r=randomized_Partition(A,p,q+1)\n        randomized_quicksort(A,p,r-1)\n        randomized_quicksort(A,r+1,q)\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#some-details","title":"Some Details","text":"<ul> <li>Consider repetitive elements</li> </ul> Text Only<pre><code>Hoore_Partition(A,p,q)\n    x=A[p]\n    i=p, j=q+1\n    while(1):\n        do:\n            j=j-1\n        until(A[j]&lt;=x)\n        do:\n            i=i+1\n        until(A[i]&gt;=x)\n        if(i&lt;j)\n            swap(A[i],A[j])\n        else\n            swap(A[p],A[j])\n            return j\n</code></pre>"},{"location":"courses/Data_Structure%26Algorithm/sorting/#a-general-lower-bound-for-sorting","title":"A General Lower Bound for Sorting","text":"<p>Efficiency of sorting with comparing</p> <p>The best efficiency of sorting based on comparing is \\(\\Theta(n\\log{n})\\)</p> Proof <p>left tree denotes <code>true</code>, right tree denotes <code>false</code>, every leaf node represents a result of sorting. We have \\(n!\\) nodes. And let h denote the height of the decision tree. we have </p> \\[ n!\\leq 2^h \\] <p>\"=\" holds for complete binary decision tree. So</p> \\[ h\\geq \\log{n!} \\geq \\log{(n/e)^n} =\\Omega(n\\log{n}) \\]"},{"location":"courses/Intro2Visualization/","title":"Introduction to Visualization","text":"<p>The Project has been releaced on Conan Visualization.</p>"},{"location":"courses/Intro2Visualization/#conan-visualization","title":"Conan Visualization","text":"<p>Demo has been published on Bilibili, you are free to check it.</p> <p>Our main code lies in <code>./src</code>. </p>"},{"location":"courses/Intro2Visualization/#recommended-ide-setup","title":"Recommended IDE Setup","text":"<p>VSCode + Volar (and disable Vetur).</p>"},{"location":"courses/Intro2Visualization/#customize-configuration","title":"Customize configuration","text":"<p>See Vite Configuration Reference.</p>"},{"location":"courses/Intro2Visualization/#compile-and-hot-reload-for-development","title":"Compile and Hot-Reload for Development","text":"Bash<pre><code>npm run dev\n</code></pre>"},{"location":"courses/Intro2Visualization/#type-check-compile-and-minify-for-production","title":"Type-Check, Compile and Minify for Production","text":"Bash<pre><code>npm run build\n</code></pre>"},{"location":"courses/cpp/","title":"Index","text":""},{"location":"courses/cpp/#c","title":"C++","text":"<p>Reference</p> <ul> <li>C++ Primer Plus, sixth edition, Stephen Prata. </li> </ul>"},{"location":"courses/cpp/#elementary","title":"Elementary","text":""},{"location":"courses/cpp/#class-object","title":"Class &amp; Object","text":""},{"location":"courses/cpp/#inheritance","title":"Inheritance","text":""},{"location":"courses/cpp/#dynamic-memory-allocation","title":"Dynamic Memory Allocation","text":""},{"location":"courses/cpp/#exception","title":"Exception","text":""},{"location":"courses/cpp/#streams","title":"Streams","text":""},{"location":"courses/cpp/#appendix","title":"Appendix | \u9644\u5f55","text":""},{"location":"courses/cpp/#template","title":"Template | \u5143\u4ee3\u7801","text":"<p>Reuse source code. It generates code for compiling.</p> <ul> <li>Generic programming(\u6cdb\u578b, universal type)vs \u8303\u5f62 model shape. Use type as parameters in class or function definitions.</li> </ul> <p>Function Template | \u51fd\u6570\u6a21\u677f</p> C++<pre><code>void myswap(int &amp;x, int &amp;y) // only calls when passing in \"int\"\n{\n  int temp = x;\n  x = y;\n  y = temp;\n}\n\n// T is tyep parameter class\ntemplate &lt;class T&gt;\nvoid myswap(T &amp;x, T&amp;y)\n{\n  T temp = x;\n  x = y;\n  y = temp;\n}\n\nint main()\n{\n  int a=6,a=5;\n  double c=1,d=2;\n  myswap(a.b); // right\n  myswap(a,c); // error, parameters a,c must be the same type\n}\n\n// but if we do this\ntemplate &lt;class T1,class T2&gt;\nvoid myswap(T1 &amp;x, T2&amp;y)\n{\n  T1 temp = x;\n  x = y;\n  y = temp;\n}\n</code></pre> <p>compiler need to know the type <code>T</code> if we do not explicitly give varible of type.</p> C++<pre><code>template&lt;class T&gt;\nvoid f()\n{\n  T a;\n}\n\nint main()\n{\n  f&lt;double&gt;();\n}\n</code></pre> <p>Class Template | \u7c7b\u6a21\u677f</p> <p>A class declaration.</p> <p>All the functions in the template are function template.</p> C++<pre><code>template &lt;class T&gt;\nclass vector{\n  public:\n  vector(int s)size(s){}\n  T&amp; operator[](int s);\n};\n\n// definition\ntemplate &lt;class T&gt;\nT&amp; vector&lt;T&gt;::operator[](int s){\n  return 0;\n}\n\nTemplate nest:\n\n```c++\nvector&lt; vector&lt;double*&gt; &gt;\n</code></pre> <p>Template arguments can be constant expresstions, Non-type parameters with a default argument.</p> C++<pre><code>template&lt;class T, int bounds=100&gt;\nclass FixedVector{\npublic:\n  FixedVector();\n  T &amp; operator[](int);\nprivate:\n  T elements[bounds]; \n};\n\ntemplate&lt;class T, int bounds=100&gt;\nT &amp; FixedVector&lt;T, bounds&gt;::operator[](int i){\n  return elements[i];\n}\n\nint main()\n{\n  FixedVector&lt;int, 10&gt; v1;\n  FixedVector&lt;int&gt; v2; // use default parameter 100\n\n}\n</code></pre> <p>All the content of template should be put in <code>.h</code> file for they are only declarations. Remember we also have to put inline function and static member variables in <code>.h</code> file.</p>"},{"location":"courses/cpp/#stlstandard-template-library","title":"STL(standard template library)","text":"<p>All the following identifiers in library are in <code>std</code> namespace.</p> <p>This is also called Container(lowercase).</p> <ul> <li>Sequential </li> </ul> <p>Sequential classes</p> <ul> <li>vector(variable array)</li> </ul> <p>It is easy to use index to search and save memory</p> C++<pre><code>vector&lt;typeName&gt; vt(n_ele);\nvector&lt;int&gt; x;\nx.push_back(1);\n</code></pre> <p>reload p++ </p> <p>C++<pre><code>vector&lt;int&gt;::iterator p;\nfor(p=x.begin(); p&lt;x.end(); p++)\n  cout &lt;&lt; *p &lt;&lt; \u201c \u201c;\n</code></pre> or  C++<pre><code>for(auto k: x)\ncout &lt;&lt; k &lt;&lt; \u201c \u201c;\n</code></pre></p> <p><code>auto</code> means the compiler can identify the type of the variable itself.</p> <p>for \u4ec5\u7528\u4e8e\u904d\u5386\u5168\u90e8(range-based for loops)</p> <p>\u653e\u8fdb\u5bb9\u5668\u91cc\u7684\u5185\u5bb9\u662fclone</p> <p>\u76f4\u63a5\u4f7f\u7528\u4e0b\u6807\uff0c\u4e0d\u4f7f\u7528<code>push_back</code>/<code>pop</code> \u662f\u4e0d\u4f1a\u6539\u53d8size\u7684\u3002</p> C++<pre><code>x[999] = 9; // no error but invalid\n</code></pre> <ul> <li> <p>array</p> </li> <li> <p>fixed length, use stack.</p> </li> </ul> <p>C++<pre><code>array&lt;typeName, n_ele&gt; arr;\n</code></pre> <code>n_ele</code> should be constant</p> <ul> <li>list(double-linked-list) It can insert/delete items very quickly.</li> </ul> <p>C++<pre><code>list&lt;int&gt; L;\nlist&lt;int&gt;::iterator li;\nli = L.begin();\nL.erase();\n++li; //wrong! li has been removed.\n</code></pre> right:</p> C++<pre><code>li = L.erase(li)\nli now points to nest node\n</code></pre> <ul> <li>others</li> </ul> <p>Deque(double ended queue), forward_list, maps(HashMap)</p> C++<pre><code>map&lt;string, float&gt; price;\nprice[\u201csnapple\u201d] =0.75\n</code></pre>"},{"location":"courses/cpp/#cast-operators","title":"Cast Operators","text":"<ul> <li>static_cast</li> </ul> <p>It is used in basic type/class conversion, (void *) to goal type pointer conversion, child to parent conversion. And it takes place in compiling.</p> C++<pre><code>int main(){\n  int a=10;\n  double b = static_cast&lt;double&gt;(a); // int to double\n\n  class Base {};\n  class Derived: public Base {};\n  Derived d;\n  Base* basePtr = static_cast&lt;Base*&gt;(&amp;d); // child to parent\n\n  void* voidPtr = &amp;a;\n  int* intPtr = static_cast&lt;int *&gt;(voidPtr); // void * to int *\n}\n</code></pre> <ul> <li>dynamic_cast</li> </ul> <p>usually used in down cast(see parent as child) in polymorphism.</p> C++<pre><code>class Base {\n  virtual void foo{}\n};\nclass Derived : public Base {\n};\n\nint main(){\n  Base* basePtr = new Derived();\n  Derived * derivedPtr = dynamic_cast&lt;Derived*&gt;(basePtr); // check when running program, better for locate error\n  if(derivedPtr){\n    cout &lt;&lt; \"cast succeed.\\n\";\n  }else{\n    cout&lt;&lt; \"cast failed.\\n\";\n  }\n  delete basePtr;\n}\n</code></pre> <ul> <li>const_cast</li> </ul> <p><code>volatile</code> means the variable is changable and should not be optimized by compiler.</p> <p><code>const_cast</code> remove a const variable into non-const.</p> C++<pre><code>int main(){\n  const int a=10;\n  int * b = const_cast&lt;int *&gt;(&amp;a); // remove const \n  *b = 20; \n}\n</code></pre> <ul> <li>reinterpret_cast</li> </ul> <p>used in low class conversion, which is dangerous.</p> C++<pre><code>int main(){\n  int a=65;\n  char* chPtr = reinterpret_cast&lt;char*&gt; (&amp;a);\n  cout&lt;&lt; *chPtr &lt;&lt; endl; // output 'A', whose ASCII is 65.\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/","title":"Class &amp; Object","text":""},{"location":"courses/cpp/Class_Object/#classstruct","title":"Class(Struct)","text":"<p>Intuitively, we put a function into a <code>struct</code> and it bacome <code>class</code>.(we can use functional point in C) act like a type.</p> <ul> <li>Definition We all know the principle of designing:</li> </ul> <p>separated .h &amp;.cpp are used to define one class</p> <ul> <li> <p>Header file(.h): class declaration&amp;prototype</p> </li> <li> <p>Source file(.cpp): all the bodies of functions</p> </li> </ul> <p>Hidden parameter: <code>this</code>, which is a pointer to the variable.</p> Publicprivate C++<pre><code>struct point{\n  float x;\n  float y;\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre> C++<pre><code>class point{\nprivate:\n  float x;\n  float y; //the above data is protected.\n\npublic://the followings can be accessed from outside\n  void init(int x, int y){\n    this-&gt;x=x;\n    this-&gt;y=y;\n  }\n  void print(){\n    cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt;endl;\n  }\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#object-an-instance-of-class","title":"Object | an instance of class","text":""},{"location":"courses/cpp/Class_Object/#ctor-constructor","title":"C\u2019tor (constructor)","text":"<ul> <li>Initialization List</li> </ul> <p>C++<pre><code>class A{\n  private:\n    int i;\n    int j= i;\n\n  public:\n    A():i(11){}\n}\n</code></pre> Initialization versus Assignment</p> <p>C++<pre><code>Stu::Stu(string s):name(s){} // better to use\nStu::Stu(string s){name=s;}\n</code></pre> Equivalent:</p> C++<pre><code>string place(\u201cHangzhou\u201d);\nstring place = \u201cHangzhou\u201d;\n\nint i = 6;\nint i(6); \n</code></pre> <p>A constructor function:</p> <p>C++<pre><code>point::point(int x, int y){\n  this-&gt;x=x;\n  this-&gt;y=y;\n}\n</code></pre> then use it:</p> <p>C++<pre><code>point a(1,2);\n</code></pre> And if the constructor function only takes in one parameter, like </p> <p>C++<pre><code>point::point(int dep){\n  this-&gt;x=this-&gt;y=dep;\n}\n</code></pre> then we can use it to initialize:</p> <p>C++<pre><code>point a(1);\npoint a=10;\n</code></pre> we can not initialize a point like we do in struct:</p> C++<pre><code>point a ={1,2}; // this can succeed only when the class does not have a constructor function and the parameter is public.\n</code></pre> <ul> <li>Default Constructor</li> </ul> <p>It is a function that can be called with no arguments input.</p> <p>If we don\u2019t give any constructor function, then the system can give one that does nothing.</p> <p>If we offer a constructor function that takes input of more than one parameter, we have no default constructor function.</p> <p>C++<pre><code>struct Y{\n  float y;\n  int i;\n  Y(int a);\n}\n</code></pre> then:</p> C++<pre><code>Y y1[] = {Y(1), Y(2)}; // right\nY y2[2] {Y(1)}; // wrong\nY y3[7]; //wrong\nY y4;  //wrong\n</code></pre>"},{"location":"courses/cpp/Class_Object/#dtordestructor","title":"D\u2019tor(destructor)","text":"<p>This function would be implemented before the memory is recycled.</p> <p>To design it, please add tilde <code>~</code> before the name of the class. The function have no input and output.</p> <p>It will delete local objects in a reverse manner.(caused by stack action)</p> C++<pre><code>class point{\nprivate:\n  float x;\n  float y;\n//the above data is protected.\n\npublic://can be accessed from outside\n  point(int dep); // reload\n  point(int x, int y);\n  ~point();\n  void print();\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#using-class","title":"Using Class","text":""},{"location":"courses/cpp/Class_Object/#function-overloading","title":"Function overloading","text":"<p>For function with the same name, compiler will choose from different function according to different input/labels.</p> <p>Pay attention to primitive type input</p> C++<pre><code>void f(int i){}\nvoid f(double d){}\n\nint main() {\n  f(\u2018a\u2019); //despite smaller than int, it can be transformed\n  f(2); // ambiguous\n  f(2L); // ambiguous\n  f(3.2); // ok\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#default-argument","title":"Default argument","text":"<p>we should give default argument from right to left:</p> C++<pre><code>int harpo(int n, int m, int j=5);\nint chico(int n, int m=6, int j); //illegal\n</code></pre> <p>default argument must be written in declaration, i.e. in \".h\" file. We can not write it in definition, but always in calling back.</p> C++<pre><code>void fun(int a, int b = 1, char c = 'a'); // \u58f0\u660e\u4e2d\u6307\u5b9a\u9ed8\u8ba4\u53c2\u6570\n\nvoid fun(int a, int b, char c) { // \u5b9a\u4e49\u4e2d\u4e0d\u80fd\u518d\u6307\u5b9a\u9ed8\u8ba4\u53c2\u6570\n    ...// \u51fd\u6570\u5b9e\u73b0\n}\n</code></pre>"},{"location":"courses/cpp/Class_Object/#friend","title":"Friend","text":"<p>A declaration, which cannot append.</p> <p>Two classes can be a friend relationship when they cannot be \"is-a\"(public inheritance) or \"has-a\"(embedding), like Tv and remote.</p> C++<pre><code>class Tv\n{\n  friend class Remote; // a class Remote can access all the private member functions/variables of Tv.\n}\n</code></pre> <p>In fact, a class can use member functions of another class to achieve some usage, not by accessing directly its member variables. Only some functions must access these member variables.</p> C++<pre><code>class Tv\n{\n  friend void Remote::set_chan(Tv &amp;t, int c); // here only this function can affect private member variables of Tv.\n  // compiler need to first know Remote class.\n};\n</code></pre> <p>So </p> C++<pre><code>// this is right:\nclass Tv; //forward declaration\nclass Remote{...}; // Remote::set_chan declaration\nclass Tv{...};\n\n// this is wrong\nclass Remote; // forward declaration\nclass Tv{...}; // Tv declaration in which we have set_chan which has not been declared.\nclass Remote{...};\n</code></pre> <p>But if <code>Remote</code> has inline function which calls a function of <code>Tv</code>, the above right forward declaration does not work. So the solution becomes </p> C++<pre><code>class Tv;\nclass Remote{...}; // Tv0using methods declaration without definition.\nclass Tv{...};\n</code></pre>"},{"location":"courses/cpp/Class_Object/#inline-function","title":"inline function | \u5185\u8054","text":"<p>It can check the type, which is better than Macro(\u5b8f)!</p> <p>C++<pre><code>inline double square(double x);\ninline double square(double x){ return x*x;}\n</code></pre> is only a declaration instead of a definition, so the compiler must see the body of function!</p> <ul> <li>compiler must see body so it can compile!(not just write down as a declaration)</li> </ul> <p>Body of inline function must be put in \".h\" files so it can be used in another file!</p> <ul> <li>if you write a inline function in a \".cpp\" file, you mean the function should only be used locally.</li> </ul> <p>Function that can be used <code>inline</code></p> <p>Function that can be used <code>inline</code>:</p> <ul> <li>small function</li> <li>frequently called function</li> </ul> <p>others that cannot be used <code>inline</code>:</p> <ul> <li>long function</li> <li>recursive function</li> </ul> <p>\u7c7b\u7684\u5185\u8054\u51fd\u6570\u53ef\u4ee5\u5728\u7c7b\u4f53\u5185\u5b9a\u4e49\uff0c\u6b64\u65f6\u4e0d\u9700\u8981\u663e\u5f0f\u4f7f\u7528 inline \u5173\u952e\u5b57\u3002\u5982\u679c\u9009\u62e9\u5728\u7c7b\u4f53\u5916\u5b9a\u4e49\uff0c\u5219\u9700\u8981\u4f7f\u7528 inline \u5173\u952e\u5b57\u3002</p>"},{"location":"courses/cpp/Class_Object/#const","title":"Const","text":"<p>constants are variables -    (instant \u7acb\u5373\u6570)</p> C++<pre><code>const int a = 6; \n</code></pre> <ul> <li>literal -&gt; 6</li> <li>constant -&gt; a</li> </ul> <p>Distinguish:</p> C++<pre><code>String p1(\u201cFred\u201d);\nconst string * p = &amp;p1; //(1)\nstring const * p = &amp;p1; //(2)\nstring * const p = &amp;p1; //(3)\n</code></pre> HintsAnswer <p>Const only restrict one variable.</p> <p>(1)(2) are the same: <code>(*p)</code> can not change; That is, we cannot change <code>p1</code> through <code>(*p)</code>.</p> <p>(3) means the pointer <code>p</code> itself cannot change but p1 itself can still change.</p> C++<pre><code>int i; \nconst int ci = 3;\n\nint *ip = &amp;i; \nint *ip = &amp;ci; // illegal, that is, a changeable pointer now points a non-changeable variable, which is illegal in compiler.\n\nconst int * cip = &amp;i;\nconst int * cip = &amp;ci;\n</code></pre> <ul> <li>Passing &amp; returning by const value</li> </ul> <p>We do this in case that we change some value in a function. So we let compiler check.</p> <ul> <li>Const object </li> </ul> <p>in a function, we pass a pointer instead of a copy!</p> <p>(1) public</p> <p>(2) change value by inner function</p> C++<pre><code>int get_day(void) const;\nint get_day(void) const {return day;}\n\nconst A a; // must provide a an initial value!(or constructor) because later we cannot change it! Like below:\n\nconst int i=1; // we cannot do this before C11\n\npublic:\n  A(int k): i(k){}\n</code></pre> <ul> <li>constant i cannot change during execution, but need a value in initialization.</li> </ul> <p>C++<pre><code>class A {\nprivate:\n  int i=0;\npublic:\n  void f() {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f() const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n}\n</code></pre> The above means:</p> C++<pre><code>public:\n  void f(A *this) {i=10;\n    cout &lt;&lt; \u201cA::f()\u201d&lt;&lt; end;\n  }\n  void f(const A *this) const {\n    cout &lt;&lt; \u201cA::f() const\u201d&lt;&lt; end;\n  }\n</code></pre> <p>So:</p> <p>C++<pre><code>const A a;\nA b;\na.f(); // &lt;&lt;\u201cA::f() const\u201d\nb.f(); // &lt;&lt;\u201cA::f()\u201d\n</code></pre> The above code is using overload, and there are two different f() that have been distinguished by \u201cconst\u201d.</p> <p>So a const object can only use function attached to \"const\" and cannot use function with no \"const\".</p>"},{"location":"courses/cpp/Class_Object/#static","title":"Static","text":"<p>on members which are </p> <ul> <li>Hidden</li> <li>Persistent</li> </ul> <p>static variable is actually global variable.</p> <p>static function can only access static variable!</p> <p>C++<pre><code>static int i; // can be accessed by all the objects of same class\n</code></pre> we must define the static variable (global variable) before main!</p> <p>C++<pre><code>int A::i = 0;\n</code></pre> Note: without static!</p> <p>C++<pre><code>static void sf(){\n  i++;\n}\n</code></pre> we can call static function without creating an object! Just use class!</p> C++<pre><code>int main(){\n  A::sf();\n}\n</code></pre> <p>We can eliminate global variable because we can limit it in class, which can prevent arbitrary changes!</p>"},{"location":"courses/cpp/Class_Object/#overloaded-operator","title":"Overloaded Operator","text":"<p>operators of primitive class cannot be changed. C++<pre><code>Integer x(1), y(2);\nz = x + y // x.operator+(y)\nz = x + 3 // x.operator+(Interger(3))\nz = 3 + x // not allowed\n</code></pre></p> <p>Global operator. C++<pre><code>z = 3 + 7; // pass 10 to initialize z\n</code></pre></p> <p><code>= () [] -&gt; -&gt;*</code> must be members and all other binary operators(\u53cc\u76ee) should be non-members.</p> <p>Will the operator change the operation number\uff1f</p> <ul> <li>If not, use <code>const</code>.</li> </ul> Common Operators<pre><code>// + - * / % ^ &amp; | ~\nconst T operatorX(const T&amp;I, const T&amp;L)\n\n// ! &amp;&amp; || &lt; &lt;= == &gt;= &gt;\nbool operatorX(const T&amp;I, const T&amp;L)\n\n// []\nE&amp; T::operator[](int index);\n\n// prefix ++ -- e.g. ++a\nconst Integer&amp; Integer::operator++(){\n  *this +=1;\n  return *this; // reference for old one, without copying new\n}\n\n// postfix ++ -- e.g. a++\nconst Integer&amp; Integer::operator++(int){\n  Integer old(*this);\n  ++(*this);\n  return old;\n}\n\n// Relational operators\nbool Integer::operator==(const Integer &amp; rhs) const{\n  return i == rhs.i;\n}\n\nbool Integer::operator!=(const Integer &amp; rhs) const{\n  return !(*this == rhs);\n}\n\nbool Integer::operator&lt;(const Integer &amp; rhs) const{\n  return !(i &lt; rhs.i);\n}\n\nbool Integer::operator&gt;(const Integer &amp; rhs) const{\n  return rhs &lt; *this;\n}\n\nbool Integer::operator&lt;=(const Integer &amp; rhs) const{\n  return !(rhs &lt; *this);\n}\n\nbool Integer::operator&gt;=(const Integer &amp; rhs) const{\n  return !(*this &lt; rhs);\n}\n\n// operator [] must be member function, single argument\n</code></pre> <ul> <li>Extractor &amp; Inserter</li> </ul> Extractor &amp; Inserter<pre><code>// stream extractor cin &gt;&gt;: global function\noperator&gt;&gt;(istream &amp;is, T&amp; obj){\n  // ...\n  return is; // always this\n}\n\ncin &gt;&gt; a &gt;&gt; b // ((cin &gt;&gt; a ) &gt;&gt; b)\n\n// stream inserter cout &lt;&lt;: global function\noperator&lt;&lt;(ostream&amp; os, const T&amp; obj){\n  // ...\n  return os;\n}\n</code></pre> <ul> <li>Assignment <code>=</code></li> </ul> =<pre><code>// member function\n// usually before calling \"=\" the object being assigned already has had sth. \nT &amp; T::operator=(const T &amp; rhs){\n  // check for self assignment\n  if(*this != ths) // otherwise will cause error (you will delete allocated memory and new one according to the deleted memory)\n  {\n    // ...\n  }\n  return *this;\n}\n</code></pre>"},{"location":"courses/cpp/Dynamic_Alloc/","title":"Dynamic memory allocation","text":""},{"location":"courses/cpp/Dynamic_Alloc/#new-delete","title":"<code>new</code> &amp; <code>delete</code>","text":"<p><code>new</code> (operator) has 2 steps:</p> <ul> <li>allocate space</li> <li>call the constructor function</li> </ul> C++<pre><code>int *p1 = new int // malloc(sizeof(int)); + constructor\nint *p2 = new int [10] // continuous space allocated\n</code></pre> <p><code>delete</code> + pointer</p> <p>If delete a constructed type, it will implement <code>D\u2019tor</code> function</p> <p>C++<pre><code>p2 = new student [10];\ndelete p2; // remove the first one\ndelete[] p2; // remove whole 10 objects\n</code></pre> it is safe to delete a <code>NULL</code>.</p>"},{"location":"courses/cpp/Dynamic_Alloc/#copy-constructor","title":"Copy constructor","text":"<p>has a unique signature</p> <p>C++<pre><code>T::T(const T&amp;)\n</code></pre> call by reference</p> <p>compiler (in-line) would do it automatically.</p> <p>But how?</p> <ul> <li>member-wise (versus bit-wise)</li> </ul> <p>if it has a class defined, it will iteratively call its copy function.</p> <p>It is neccesary to define a copy constructor when you have pointer member or what you don't want to be copied in your class.</p> C++<pre><code>A b(a);\nA c = a;\n\n// define function passing in value\nvoid f(A aa)\n{\n\n}\n\n// return value\nA f()\n{\n  A aa(19);\n  return aa;\n}\n</code></pre> <p>Assignment: can be done alot of time. Ctor can only be called once.</p>"},{"location":"courses/cpp/Elementary/","title":"Elementary","text":""},{"location":"courses/cpp/Elementary/#basic-ideas","title":"Basic Ideas","text":"<ul> <li>object-oriented | \u7269\u4ef6\u5bfc\u5411-\u9762\u5411\u5bf9\u8c61</li> </ul> <p>An object, or entity(either visible or invisible), is a variable in programming languages, made up of two primary components:</p> <ul> <li>Attibutes, or Data, representing the object's properties and status</li> <li>Services, or Operations, refered to as functions in programming.</li> </ul> <p>C++ Focuses on things instead of operations.</p>"},{"location":"courses/cpp/Elementary/#key-words","title":"Key words","text":"<ul> <li>interface</li> <li>communications</li> <li>protection</li> <li>the hidden implementation</li> <li>encapsulation</li> </ul>"},{"location":"courses/cpp/Elementary/#oop-characteristics","title":"OOP characteristics","text":"<ul> <li>Everything is an object.</li> <li>A program is a bunch of objects telling each other what to do/(not how to do) by sending messages</li> <li>Each object has its own memory made up of other objects.</li> <li>Every object has a type.</li> <li>All objects of a particular type can receive the same messages. (Using the method to distinguish between different types or classes)</li> </ul>"},{"location":"courses/cpp/Elementary/#header-files","title":"Header Files","text":"<p>To prevent defining repeatedly:</p> <p>x.h<pre><code># pragma once //\u53ea\u5305\u542b\u8fd9\u4e2a\u5934\u6587\u4ef6\u4e00\u6b21\n</code></pre> or the same as C language:</p> log.h<pre><code># ifndef _LOG_H\n# define _LOG_H\n//...//\n# endif\n</code></pre>"},{"location":"courses/cpp/Elementary/#declaration","title":"declaration","text":"<ul> <li>external variables</li> <li>function prototypes</li> <li>class/struct declarations</li> </ul>"},{"location":"courses/cpp/Elementary/#resolver","title":":: resolver","text":"::<pre><code>&lt;Class Name&gt;::&lt;function name&gt;//not free\n::&lt;function name&gt;\n\nvoid S::f(){\n  ::f();//would be recursive otherwise\n  ::a++;//select the global a\n  a\u2014;//select the partial a\n}\n</code></pre>"},{"location":"courses/cpp/Elementary/#string","title":"String","text":"C++<pre><code>#include&lt;string&gt;\nString age, name; \ncin &gt;&gt; age &gt;&gt; name;\ncout &lt;&lt; name; \n</code></pre> <ul> <li>A class type, not a primitive type.</li> <li>Initially <code>name</code> is all zero. No matter it is static or global.</li> <li>No <code>\\0</code> at the end of the string.</li> </ul>"},{"location":"courses/cpp/Elementary/#reference","title":"Reference","text":"<p>Reference make use of the idea of a pointer, but is used as a normal variable rather than pointer. It is widely used in passing variables into a function.</p> C++<pre><code>char c;\nchar &amp; r = c;// a reference to c;\n</code></pre> <p>Some notes on reference:</p> <ul> <li>cannot be <code>NULL</code>.</li> <li>cannot calculate.</li> <li>No reference to reference</li> </ul> C++<pre><code>int &amp;* r; // No pointer to reference \nint *&amp; r; // We have reference to pointer\n</code></pre>"},{"location":"courses/cpp/Exception/","title":"Exception","text":"<ul> <li>Exception type</li> </ul> C++<pre><code>class VectorIndexError\n{\npublic:\n  VectorIndexError(int v):m_badValue(v){}\n  VectorIndexError({}\n  void diagnostic(){\n    cout&lt;&lt;\"index\"&lt;&lt; m_badValue &lt;&lt;\"out of range!\";\n  }\nprivate:\n  int m_badValue;\n}\n</code></pre> <p>C++<pre><code>throw &lt;&lt;something&gt;&gt;\n</code></pre> <code>throw</code> raises exception.</p> <ul> <li>Try blocks can select type of exceptions</li> </ul> C++<pre><code>try {...}\n  catch {...}\n</code></pre> C++<pre><code>try{\n  func();\n} catch(VIE v){ // take a single argument\n  cout&lt;&lt; \"8\\n\";\n} catch (...){ // others\n  cout&lt;&lt; \"7\\n\";\n}\n</code></pre> <p>it can re-raise exceptions</p> <ul> <li>Hierarchy of exception types</li> </ul> C++<pre><code>class MathErr{\n  ...\n  virtual void diagnostic();\n};\n\nclass OverflowErr : public MathErr {...}\nclass UnderflowErr : public MathErr {...}\n</code></pre> <p>a catch of parent Exception Type can catch its child type.</p> C++<pre><code>try{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 7\n\ntry{\n  throw VIEE(idx);\n}catch (VIEE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIE v){\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n\ntry{\n  throw VIEE(idx);\n}catch (VIE v){\n  cout&lt;&lt; \"8\\n\";\n}catch (VIEE v){ // this expression is useless\n  cout&lt;&lt; \"7\\n\";\n}catch (...){\n  cout&lt;&lt; \"6\\n\";\n}\n// output 8\n</code></pre> <ul> <li> <p>Standard Library Exception</p> </li> <li> <p>Failure in C'tor</p> </li> </ul>"},{"location":"courses/cpp/Inheritance/","title":"Inheritance","text":"<p>Allow sharing of design for</p> <ul> <li>Member data</li> <li>Member functions</li> <li>Interfaces</li> </ul> <p>Advantages:</p> <ul> <li>extendable</li> <li>avoid code duplication</li> <li>code reuse </li> </ul> <p>B is a A:</p> <ul> <li>A: Base/super/parent class</li> <li>B: derived/sub/child class</li> </ul> employee.h<pre><code>class Employee\n{\npublic: \n  Employee(const string&amp; _name, const string &amp; _ssn): name(_name), ssn(_ssn){}\n\n  void print() const\n{\n  cout &lt;&lt; name &lt;&lt; endl;\n  cout &lt;&lt; ssn &lt;&lt; endl;\n}\n\n  void print(const string &amp; msg) const {\n  cout &lt;&lt; msg &lt;&lt; endl;\n  print(); // we rewrite the print function, then the child cannot access print from parent!\n// Name Hide!\n}\n\nprotected:\n// private: // can not access from child class\n  const string name;\n  const string ssn;\n}\n</code></pre> manager.h<pre><code>class Manager: public Employee\n{\npublic:\n  Manager(const string &amp; _name, const string &amp; _ssn, const string &amp; _title): Employee(_name, _ssn), title(_title) {}\n\n  const string &amp; getTitle() const\n{\n  return title;\n}\n\n  void print() const\n{\n  Employee::print();\n  cout &lt;&lt; title &lt;&lt; end;\n}\n\nprotected:\n  const string title;\n}\n</code></pre>"},{"location":"courses/cpp/Inheritance/#polymorphism","title":"Polymorphism","text":"<p>If a child class wants to act different with parent class in the same member function, i.e. it depends on the object that calls the function, so we have to introduce some more mechanisms. That is, </p> <ul> <li> <p>redefine function in child function.</p> </li> <li> <p>use virtual function.</p> </li> </ul> <p>Here comes an example.</p> Brass &amp; Brassplus<pre><code>class Brass\n{\nprivate:\n    std::string fullName;\n    long acctNum;\n    double balance;\n\npublic:\n    Brass(const std::string &amp; s = \"Nullbody\", long an = -1, double bal = 0.0);\n    void Deposit(double amt);\n    virtual void Withdraw(double amt);\n    double Balance() const;\n    virtual void ViewAcct() const;\n    virtual ~Brass() {}\n};\n\n// Brass Plus Account Class\nclass BrassPlus : public Brass\n{\nprivate:\n    double maxLoan;\n    double rate;\n    double owesBank;\n\npublic:\n    BrassPlus(const std::string &amp; s = \"Nullbody\", long an = -1, double bal = 0.0, double ml = 500, double r = 0.11125);\n    BrassPlus(const Brass &amp; ba, double ml = 500, double r = 0.11125);\n    virtual void ViewAcct() const;\n    virtual void Withdraw(double amt);\n    void ResetMax(double m) { maxLoan = m; }\n    void ResetRate(double r) { rate = r; }\n    void ResetOwes() { owesBank = 0; }\n};\n</code></pre> <p>Now the following executable program</p> C++<pre><code>// create an object\nBrass dom(\"DominicBanker\", 11224, 4183.45);\nBrassPlus dot(\"DorothyBanker\", 12118, 2592.00);\n\n// call member function for a real object\ndom.ViewAcct(); // use Brass::ViewAcct();\ndot.ViewAcct(); // use Brassplus::ViewAcct();\n\nBrass &amp; b1_ref = dom;\nBrass &amp; b2_ref = dot;\n</code></pre> <p>Then which function does the following call?</p> C++<pre><code>b1_ref.ViewAcct();\nb2_ref.ViewAcct();\n</code></pre> <p>Compare the difference.</p> Without <code>virtual</code>With <code>virtual</code> C++<pre><code>b1_ref.ViewAcct(); // use refernce type Brass\nb2_ref.ViewAcct(); // use refernce type Brass\n</code></pre> C++<pre><code>b1_ref.ViewAcct(); // use pbject type Brass\nb2_ref.ViewAcct(); // use object type Brassplus\n</code></pre> <p>It is good to declare member function <code>virtual</code> and destructor function <code>virtual</code> in base class.</p> <p>There are two ways of calling functions.</p>"},{"location":"courses/cpp/Inheritance/#up-casting","title":"up-casting (*&amp;=) | \u9020\u578b","text":"<p>We know cast(\u7c7b\u578b\u8f6c\u6362), like</p> C++<pre><code>int i = (int)3.62;\n</code></pre> <p>But up-casting is</p> <ul> <li> <p>is the act of converting from a derived reference or pointer to a base class reference or pointer.</p> </li> <li> <p>take an object of a derived class as an object of the base one.</p> </li> </ul> C++<pre><code>C2* pC2 = new C2();  // pC2 \u662f\u6d3e\u751f\u7c7b C2 \u7684\u6307\u9488\nC1* pC1 = pC2;       // Up-casting\uff1a\u5c06 C2* \u8f6c\u6362\u4e3a C1*\n</code></pre> C++<pre><code>C2 objC2;            // objC2 \u662f\u6d3e\u751f\u7c7b C2 \u7684\u5bf9\u8c61\nC1&amp; refC1 = objC2;   // Up-casting\uff1a\u5c06 C2 \u5bf9\u8c61\u8f6c\u6362\u4e3a C1 \u5f15\u7528\n</code></pre> <p>(\u6539\u53d8\u4e86\u773c\u5149\uff0c\u4e0d\u6539\u53d8\u5185\u5bb9)</p> <p>Other Items</p> <ul> <li>encapsulation \u5c01\u88c5\uff1b\u5305\u88c5\uff1b</li> <li>capsulation \u5c01\u88c5\uff1b[\u9ad8\u5206\u5b50] \u5305\u56ca\u5316\u4f5c\u7528\uff1b</li> <li>bonding \u90a6\u5b9a</li> </ul> C++<pre><code>void func(C1 obj);   // \u51fd\u6570\u63a5\u53d7\u57fa\u7c7b C1 \u7684\u5bf9\u8c61\n\nC2 objC2;            // objC2 \u662f\u6d3e\u751f\u7c7b C2 \u7684\u5bf9\u8c61\nfunc(objC2);         // Up-casting\uff1a\u5c06 C2 \u5bf9\u8c61\u4f5c\u4e3a C1 \u5bf9\u8c61\u4f20\u9012, and slice-off will happen.\n</code></pre>"},{"location":"courses/cpp/Inheritance/#dynamic-binding-virtual-function","title":"Dynamic Binding &amp; virtual function | \u7ed1\u5b9a\u548c\u865a\u51fd\u6570","text":"<p>Two ways of binding</p> <p>Binding: which function to be called.</p> <ul> <li> <p>Static binding: call the function as the code declared, only taking effect on  Non-virtual function. This is known at compiling, which is fast.</p> </li> <li> <p>Dynamic binding: call the function according to the actual object, only taking effect on Virtual function. This is only known at running time, which needs a virtual table to achieve this.</p> </li> </ul> <p>a class which has a virtual function: vtable -&gt; table of the address of virtual functions(8 \u4f4dfor 64)(this is formed while compiling)</p> <p>Notes on virtual function</p> <ul> <li> <p>constructor function could not be <code>virtual</code>. </p> </li> <li> <p>destructor function should be <code>virtual</code>, for derived class may have special object the need to be free and destructed.</p> </li> <li> <p>If derived class does not redefine virtual function, then the real pointer to derived class object would call function of base.</p> </li> <li> <p>redefine function parameters would cause hiding method. That is, overloaded function defined in derived class would cause virtual function of base to be hidden. See the following code as an example.</p> </li> </ul> C++<pre><code>// define class of base\nclass Dwelling {\n    public:\n        virtual void showperks(int a) const;\n    };\n// define derived class\nclass Howel : public Dwelling {\n    public:\n        virtual void showperks() const;\n    };\n\n// use function\nHovel trump;\ntrump.showperks(); // valid\ntrump.showperks(2); // invalid, cause function of base has been hidden\n</code></pre> <ul> <li>redefine return type of virtual function would cause covariance of return type, which is an exception but valid for polymorphism. See the following code as an example.</li> </ul> C++<pre><code>class Dwelling {\n  public:\n      virtual Dwelling&amp; build(int n);\n  };\n\nclass Hove1 : public Dwelling {\n    public:\n        virtual Hove1&amp; build(int n); // same function signature\n    };\n</code></pre> <ul> <li>If virtual function is overloaded in base class, then derived class must redefine all the overloaded function, otherwise would cause non-defined overloaded function to be hidden. See the following code as an example.</li> </ul> C++<pre><code>class Dwelling {\n    public:\n        // three overloaded showperks function\n        virtual void showperks(int a) const;\n        virtual void showperks(double x) const;\n        virtual void showperks() const;\n    };\n\nclass Hove1 : public Dwelling {\n    public:\n        // redefine three overloaded showperks function\n        virtual void showperks(int a) const;\n        virtual void showperks(double x) const;\n        virtual void showperks() const;\n    };\n</code></pre> <p>If you does not need to modify a virtual function, then just use it as base function, like <code>void Hovel::showperks() const {Dwelling::showperks();}</code></p> C++<pre><code>B b;\nA * p = &amp;b;\nlong long **vp (long long **)p;\n// vp is a pointer to *long long, that is, to a pointer of type long long.\n\nvoid (*pf)() = (void (*)())(**vp);\n// pf is a function pointer, which matches the definition of f()\n\npf(); // \u201cB::f()\u201d\n</code></pre> <p>Initialize: A will create A\u2019s vtable but B will than create B\u2019s vtable and change the point to the vtable.</p> <p> </p> C++<pre><code>Manager Pete(\u201cPete\u201d,\u201d4\u201d, \u201cBakery\u201d);\nEmployee* ep = &amp;pete;\nEmployee &amp; er = pete;\n</code></pre> <p>See the following code.</p> C++<pre><code>void fr(Brass &amp; rb) { rb.ViewAcct();}\n\nvoid fp(Brass * pb) { pb-&gt;ViewAcct();}\n\nvoid fv(Brass b) { b.ViewAcct();}\n\nint main() {\n    Brass b(\"Billy Bee\", 123432, 10000.0);\n    BrassPlus bp(\"Betty Beep\", 232313, 12345.0);\n\n    // dynamic binding\n    fr(b);  // uses Brass::ViewAcct()\n    fr(bp); // uses BrassPlus::ViewAcct()\n\n    // dynamic binding\n    fp(&amp;b);  // uses Brass::ViewAcct()\n    fp(&amp;bp); // uses BrassPlus::ViewAcct()\n\n    // up-casting, caused by static binding\n    fv(b);  // uses Brass::ViewAcct()\n    fv(bp); // uses Brass::ViewAcct()\n\n    return 0;\n}\n</code></pre>"},{"location":"courses/cpp/Inheritance/#override","title":"override | \u8986\u76d6","text":"C++<pre><code>class A\n{\nprotected:\n  int i;\npublic:\n  A() {i=10;}\n  virtual void f() {cout&lt;&lt; \u201cA::f()\u201d&lt;&lt; endl;}\n}\n\nclass B: public A\n{\npublic:\n  int i;\npublic: \n  B(){ i=20; cout&lt;&lt; \u201cB::i\u201d &lt;&lt; i &lt;&lt;endl;}\n  void f() {cout&lt;&lt; \u201cB::f()\u201d&lt;&lt; endl;}\n}\n\ncout &lt;&lt; sizeof(A) &lt;&lt;\u201c, \u201c &lt;&lt; sizeof(B) &lt;&lt; endl;\n// not just 4, 8 but a complex one!\n</code></pre> <ul> <li>polymorphic variable(* &amp;)</li> </ul> <p>static type  dynamic type</p>"},{"location":"courses/cpp/Inheritance/#slice-off-obj","title":"Slice off (obj=) | \u5207\u7247","text":"<ul> <li>will copy the content of child object to the parent object while ignoring the extra thins of the child.</li> </ul> <p>C++<pre><code>a = b;\np = &amp;a;\np-&gt;f(); // still execute A\u2019s func\n</code></pre> -   Never redefine an inherited non-virtual function.</p> <p>The vptr is ignored.</p> <ul> <li>Never redefine an inherited default parameter value.</li> </ul> C++<pre><code>virtual void f() =0;\n</code></pre> <ul> <li>Multiple inheritance</li> </ul>"},{"location":"courses/cpp/Inheritance/#abstract-base-class-abc","title":"Abstract Base Class | \u62bd\u8c61\u57fa\u7c7b(ABC)","text":"<p>Abstract base classes:</p> <ul> <li>has pure virtual functions</li> <li>Cannot be instantiated</li> </ul> <p>WE take Ellipse and Circle as an example. Circle is an ellipse, so we can use inheritance, but this could lead to redundency. Because circle is an ellipse if and only if the semimajor and semiminor axis of an ellipse are equal. That is, member data <code>semimajor</code> and <code>semiminor</code> is useless in circle. </p> <p>It is better to think in terms of data. What data does ellipse and circle both have? Could we create an abstract class the holds the public member data and functions needed for circle and ellipse?</p> <p>Check the following code.</p> C++<pre><code>class BaseEllipse // Abstract base class\n{\nprivate:\n    double x; // x-coordinate of the center\n    double y; // y-coordinate of the center\n\npublic:\n    // Constructor with default values for x and y coordinates\n    BaseEllipse(double x0 = 0, double y0 = 0) : x(x0), y(y0) {}\n\n    // Virtual destructor to ensure proper cleanup of derived class objects\n    virtual ~BaseEllipse() {}\n\n    // Function to move the center to new coordinates (nx, ny)\n    // same for circle and ellipse\n    void Move(int nx, int ny) { x = nx; y = ny; }\n\n    // Pure virtual function to calculate the area (must be overridden by derived classes)\n    // difference for circle and ellipse\n    virtual double Area() const = 0; // notation for pure virtual function\n};\n</code></pre> <p>Note that, when a base class has a pure virtual function, it could not be instantiated. Because we might not give a definition in base class, it depends.</p>"},{"location":"courses/cpp/Streams/","title":"Streams","text":"<p>Easy to locate in a one-demension line, which flows in a single direction.</p>"},{"location":"courses/cpp/Streams/#kinds-of-streams","title":"Kinds of streams","text":"<p>Kinds of streams</p> <p>(i) text streams</p> <ul> <li> <p>readable</p> </li> <li> <p>organized in lines</p> </li> </ul> <p>(ii) binary streams</p> C++<pre><code>cout&lt;&lt;\"Hello\\n\";\ncerr&lt;&lt;\"Byebye\\n\";\n</code></pre> <p>if we do</p> C++<pre><code>./a.out &gt; 1 // hello in 1, Byebye in terminal\n./a.out 2&gt; 1 // Byebye in 1, hello in termial\n./a.out &gt;1 2&gt; 2 // hello in 1, Byebye in 2\n</code></pre> <p><code>int get()</code>, a member function, returns the next character in the stream, EOF if no character left. <code>istream&amp; get(char&amp; ch)</code>, a manipulator.</p> C++<pre><code>// copy input to output\nint ch;\nwhile(ch=cin.get()!=EOF){\n  cout.put(ch);\n}\n</code></pre> <p>use free function instead of object</p> C++<pre><code>// recommended\nistream&amp; getline(istream&amp; is, string&amp; str, char delim='\\n') // delim is the end sign\n\n// not recommended, you need to create an array, \ncin.getline(char *, int size)\n</code></pre> <p><code>ignore(int limit=1, int delim=EOF)</code>, a member function, skip over <code>limit</code> number character or until <code>delim</code>. Like skipping until the end of line, or let wrong string to be read, such that program could continue.</p> <p><code>int gcount()</code> returns number of characters just read.</p> Text Only<pre><code>string buffer;\ngetline(cin, buffer);\ncuot&lt;&lt; \"read \"&lt;&lt; cin.gcount()&lt;&lt;\" characters\"\n</code></pre> <p><code>cin.putback(char)</code> pushes a single character into a stream</p> <p><code>char peek()</code> examines next character without reading it.</p> <p><code>flush</code> forces content in ostream to output.</p>"},{"location":"courses/cpp/Streams/#manipulators","title":"Manipulators","text":"<p>Manipulators modify the state of the stream.</p> Manipulator effect type dec, hex, oct set numberic conversion I,O endl insert new line and flush O flush flush stream O setw(int) set field width I,O setfill(char) change fill character(like <code>#</code>) I,O setbase(int) set number base(like 2) I,O ws skip whitespace I setprecision(int) set floating point precision O C++<pre><code>#include &lt;iomanip&gt;\nint n;\ncout &lt;&lt; \"enter number in hexadecimal\" &lt;&lt; flush; // only takes effect once\ncin &gt;&gt; hex &gt;&gt; n; // effexts hold\n</code></pre> C++<pre><code>#include &lt;iostream&gt;\n#include &lt;ismanip&gt;\nmain(){\n  cout &lt;&lt; setprecision(2) &lt;&lt; 1000.243 &lt;&lt; endl;\n  cout &lt;&lt; setw(10) &lt;&lt; \"OK!\";\n}\n\n// returns\n1e03\n          OK!\n</code></pre> <p>we can create our own manipulator.</p> C++<pre><code>ostream&amp; manip(ostream&amp; out){\n  ...\n  return out;\n}\n\nostream&amp; tab (ostraam&amp; out){\n  return out &lt;&lt; '\\t';\n}\n\ncout &lt;&lt; \"Hello\" &lt;&lt; tab &lt;&lt; \"world\"&lt;&lt; endl;\n</code></pre>"},{"location":"courses/cpp/Streams/#stream-flags","title":"Stream flags","text":"C++<pre><code>// set amnd reset using manipulators\nsetiosflags(flags) // set '1'\nresetiosflag(flags) // reset '0'\n\n// using `stream` member function\ncin.setf(flags)\ncout.unsetf(flags)\n</code></pre> C++<pre><code>main(){\n  // add two functions\n  cout.setf(ios::showpos | ios::scientific);\n  cout &lt;&lt; 123 &lt;&lt; \" \" &lt;&lt; 456.78 &lt;&lt;endl;\n  cout &lt;&lt; resetiosflags(ios::showpos) &lt;&lt; 123;\n  return 0;\n}\n\n// output\n+123 +4.567800e+02\n123\n</code></pre> <p><code>clear()</code> returns error stream to GOOD, which is useful for reading failure.</p> <p>checking status: <code>good()</code>, <code>eof</code>, <code>fail()</code>, <code>bad()</code> return true or false.</p> C++<pre><code>int n;\nwhile(cin.good()){// skip if eof or bad\n  cin &gt;&gt; n;\n  if(cin){ // overload operator bool() cin, cin.put\n    cin,ignore(INT_MAX, '\\n');\n    break;\n  }\n  if(cin.fail()){\n    cin.clear();\n    cin,ignore(INT_MAX, '\\n');\n    cout &lt;&lt; \"No good, try again!\" &lt;&lt; flush;\n  }\n}\n</code></pre>"},{"location":"courses/cpp/Streams/#file-streams","title":"File streams","text":"<p>In <code>&lt;fstream&gt;</code>, <code>ifstream</code>, <code>ofstream</code> connects files to streams.</p> <p>Open modes specify how to create files using flags.</p> modes purpose ios::app append ios::ate position at the end ios::binary do binary I/O ios::in open for input ios::out open for output C++<pre><code>#include &lt;iostream&gt;\n#include &lt;fstream&gt;\nint main(int argc, char *argv[]){\n  if(argc !=3){\n    cerr &lt;&lt; \"Usage: copy file1 file2\" &lt;&lt;endl;\n    exit(1);\n  }\n  ifstream in(argv[1]);\n  if(! in){\n    cerr &lt;&lt; \"Unable to open file \" &lt;&lt; argv[1];\n    exit(2);\n  }\n  ofstream out(argv[2]);\n  if(! out){\n    cerr &lt;&lt; \"Unable to open file \" &lt;&lt; argv[];\n    exit(2);\n  } \n}\n</code></pre> <p>more stream operations like <code>open(const char *, int flags, int)</code> to open a specified file, <code>close()</code> to close a stream.</p> C++<pre><code>ifstream inputS;\ninputS.open(\"somefile\", ios::in);\nif(!inputS){\n  cerr &lt;&lt; \"Unable to open somefile\";\n}\n</code></pre>"},{"location":"log/","title":"Revision Logs","text":""},{"location":"log/#2025","title":"2025","text":""},{"location":"log/#september","title":"September","text":"<p>September 15, 2025 update LA: special bilinear form</p> <p>September 14, 2025 update LA: Bilinear form</p> <p>September 13, 2025 update DG: Guass map</p> <p>September 12, 2025 update Blog: Xi'an</p>"},{"location":"log/#august","title":"August","text":"<p>August 18, 2025 update DG: update Surfaces the tangent plane</p>"},{"location":"log/#july","title":"July","text":"<p>July 31, 2025 update DG: update Surfaces change of parameters</p> <p>July 30, 2025 update DG: update Surfaces images theorems</p> <p>July 28, 2025 update DG: add Surfaces</p> <p>July 27, 2025 update DG: update Curves examples</p> <p>July 22, 2025 update BT: add details to proofs</p> <p>July 20, 2025 update DG: Curves Frenet trihedron</p> <p>July 19, 2025 update BT: Quotient space</p> <p>July 16, 2025 update DG: Curves basics</p> <p>July 15, 2025 update BT: Compactness</p> <p>July 13, 2025 update covers &amp; contents</p> <p>July 06, 2025 update SP: Martingale</p> <p>July 05, 2025 update SP: Martingale</p> <p>July 04, 2025 update SP: Martingale</p> <p>July 03, 2025 update SP: Martingale</p> <p>July 02, 2025 update SP: Martingale Radon-Nikodym Theorem</p> <p>July 01, 2025 update SP: Martingale</p>"},{"location":"log/#june","title":"June","text":"<p>June 25, 2025 update CA: Conformal Mappings SCI Boundary behavior</p> <p>June 24, 2025 update CA: Conformal Mappings Schwarz-Christoffel integral</p> <p>June 23, 2025 update CA: Conformal Mappings RMT</p> <p>June 22, 2025 update CA: Conformal Mappings</p> <p>June 09, 2025 update FA: BL Operator Hahn-Banach Theorem</p> <p>June 08, 2025 update FA: BL Operator Existence of functional</p>"},{"location":"log/#may","title":"May","text":"<p>May 30, 2025 create Linear Algebra: SVD &amp; PCA</p> <p>May 19, 2025 create Lie Algebra</p> <p>May 17, 2025 create LA: Operators on inner product space</p> <p>May 07, 2025 update SP: Brownian Motion, CA: Gamma &amp; Zeta Function, FA: Bounded Linear Operator</p>"},{"location":"log/#april","title":"April","text":"<p>April 28, 2025 update SP: Markov Chain Green function deduction</p> <p>April 24, 2025 update SP: Markov Chain inversible Markov chain proof</p> <p>April 22, 2025 update SP: Markov Chain inversible Markov chain</p> <p>April 21, 2025 update SP: Markov Chain limit distribution proof</p> <p>April 17, 2025 update SP: Markov Chain limit distribution &amp; mean recurrence time</p> <p>April 16, 2025 update SP: Markov Chain Limit Distribution</p> <p>April 14, 2025 update FA: Banach Space Examples</p> <p>April 13, 2025 update FA: Metric Space Examples &amp; Proofs</p> <p>April 12, 2025 update FA: Metric Space Examples</p> <p>April 10, 2025 update CA: Meromorphic Function</p> <p>April 09, 2025 update CA: Cauchy's integral theorem applications</p> <p>April 08, 2025 update SP: Markov Chain Limit Theorem</p> <p>April 07, 2025 update SP: Markov Chain State Space</p> <p>April 06, 2025 update FA: Banach Space</p> <p>April 05, 2025 update CA: Holomorphic function Two miracles</p> <p>April 03, 2025 update MMC: Motion Control</p> <p>April 01, 2025 update MMC: Dynamics structure</p> <p>April 01, 2025 update SP: Possion Process some proof</p>"},{"location":"log/#march","title":"March","text":"<p>March 30, 2025 update SP: Markov Process</p> <p>March 28, 2025 update MMC Dynamics: Iterative Dynamic Formulation</p> <p>March 27, 2025 update MMC Dynamics: Newton-Euler's Equation</p> <p>March 26, 2025 update MMC Dynamics: Supplementary info</p> <p>March 25, 2025 update MMC Dynamics: Supplementary info</p> <p>March 23, 2025 add MMC: Dynamics</p> <p>March 22, 2025 update SP: Possion Process Properties</p> <p>March 16, 2025 Add CA: Cauchy's Integral Formula</p> <p>March 15, 2025 Add CA: Cauchy's Theorem</p> <p>March 12, 2025 Update MMC: Static Forces</p> <p>March 11, 2025 Update SP: Possion Process &amp; MMC: Velocity Propagation</p> <p>March 09, 2025 Update FA: Compact Set</p> <p>March 08, 2025 Update MMC: Jacobians &amp; SP: Basic Process</p> <p>March 07, 2025 Update MMC: Velocities &amp; Forces</p> <p>March 05, 2025 Update About</p> <p>March 04, 2025 Update SP: Basic Processes</p> <p>March 03, 2025 Update MMC: Kinematics</p> <p>March 02, 2025 Update CA: Holomorphic function</p> <p>March 01, 2025 Update FA: Inequations</p>"},{"location":"log/#february","title":"February","text":"<p>February 28, 2025 Update SP: Convergence, MMC: Operators</p> <p>February 25, 2025 Update SP: Random Vector</p> <p>February 24, 2025 Update SP: Continuous random variable</p> <p>February 23, 2025 Update SP: Measurability</p> <p>February 22, 2025 Update SP: Mathematical Expectation</p> <p>February 21, 2025 Update FA: Basic Concepts</p> <p>February 20, 2025 Update SP: Random Variable</p> <p>February 19, 2025 Add CA: Preliminary</p> <p>February 19, 2025 Add Stochastic Processes</p> <p>February 17, 2025 Add Manipulator Modelling &amp; Control</p> <p>February 16, 2025 Update RA: Lp Space separability</p> <p>February 15, 2025 Update RA: Lp Space completeness</p> <p>February 14, 2025 Update RA: Lp Space</p> <p>February 13, 2025 Update RA: Absolute Continuous Function</p> <p>February 12, 2025 Update RA: Bounded Variation Function</p> <p>February 11, 2025 Update RA: Examples</p> <p>February 11, 2025 Update Middle School Experience</p> <p>February 09, 2025 Update RA: Lebesgue integral examples</p> <p>February 08, 2025 Update RA: Differential &amp; Integral</p> <p>February 07, 2025 Update RA: Lebesgue Integral R-Int &amp; L-Int</p> <p>February 06, 2025 Update RA: Lebesgue Integral</p> <p>February 05, 2025 Update RA: Measurable Function Extension</p>"},{"location":"log/#january","title":"January","text":"<p>January 25, 2025 Update RA: Measurable Function Egoroff Theorem</p> <p>January 23, 2025 Update RA: Measurable Function</p> <p>January 22, 2025 Update RA: Lebesgue Measure</p> <p>January 21, 2025 Update RA</p> <p>January 20, 2025 add Real Analysis</p> <p>January 20, 2025 update ODE</p> <p>January 16, 2025 update Mount_Lu Jan 16</p> <p>January 13, 2025 update Mount_Lu Jan 13</p> <p>January 13, 2025 update PDE</p> <p>January 13, 2025 update PDE &amp; schedule</p> <p>January 12, 2025 update ODE</p> <p>January 11, 2025 update Mount_Lu</p> <p>January 11, 2025 update ODE, modify the directory</p> <p>January 10, 2025 update Cpp</p> <p>January 07, 2025 update ODE</p> <p>January 06, 2025 update ODE, DSA &amp; PC</p> <p>January 05, 2025 update ODE, DSA &amp; PC</p> <p>January 05, 2025 add Travel Plan</p> <p>January 04, 2025 update PC, NA &amp; AIML</p> <p>January 04, 2025 update ODE, cpp &amp; AIML</p> <p>January 02, 2025 update NA, cpp &amp; AIML</p>"},{"location":"log/#2024","title":"2024","text":""},{"location":"log/#december","title":"December","text":"<p>December 31, 2024 update DSA &amp; PC</p> <p>December 31, 2024 update ODE, DSA &amp; AIML</p> <p>December 21, 2024 update NA</p> <p>December 20, 2024 update ODE &amp; NA</p> <p>December 19, 2024 update ODE</p> <p>December 18, 2024 update DS</p> <p>December 17, 2024 update ODE</p> <p>December 15, 2024 update ODE</p> <p>December 13, 2024 update MPM &amp; NA</p> <p>December 12, 2024 update MPM &amp; Intro2Visual</p> <p>December 11, 2024 update MPM &amp; DSA</p> <p>December 10, 2024 update MPM</p> <p>December 08, 2024 create Material Point Method</p> <p>December 07, 2024 update NA</p> <p>December 06, 2024 update NA</p> <p>December 05, 2024 update NA</p> <p>December 04, 2024 update NA Approximation</p> <p>December 03, 2024 add some changes</p>"},{"location":"log/#november","title":"November","text":"<p>November 21, 2024 update ODE &amp; NA</p> <p>November 21, 2024 update ODE</p> <p>November 17, 2024 update ODE</p> <p>November 16, 2024 update ODE</p> <p>November 15, 2024 update ODE</p> <p>November 14, 2024 update ODE</p> <p>November 13, 2024 update ODE</p> <p>November 12, 2024 update ODE &amp; cpp</p> <p>November 11, 2024 update ODE general theorem</p> <p>November 09, 2024 update ODE</p> <p>November 08, 2024 update NA</p> <p>November 07, 2024 update ODE</p> <p>November 06, 2024 update ODE</p> <p>November 05, 2024 update ODE &amp; c++</p> <p>November 04, 2024 update ODE</p> <p>November 03, 2024 update NA</p> <p>November 01, 2024 update MCT-review</p> <p>November 01, 2024 add PDF-viewer</p> <p>November 01, 2024 add ODE &amp; update NA</p>"},{"location":"log/#october","title":"October","text":"<p>October 31, 2024 modify MCT-pdf</p> <p>October 30, 2024 add MCT-pdf</p> <p>October 29, 2024 add cpp &amp; update NA</p> <p>October 28, 2024 add h_coding&amp;NA&amp;change fonts, colors</p> <p>October 27, 2024 add new color &amp; animation</p> <p>October 26, 2024 add SD</p> <p>October 25, 2024 add ZY</p> <p>October 25, 2024 update</p> <p>October 24, 2024 add h1-3</p> <p>October 24, 2024 update</p> <p>Last updated on 2025-09-15.</p>"},{"location":"tag/","title":"Tag","text":"<p>Hey!</p>"}]}